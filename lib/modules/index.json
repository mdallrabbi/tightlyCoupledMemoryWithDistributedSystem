{
  "modules": {
    "BaseHTTPServer": {
      "file": "BaseHTTPServer.py", 
      "imports": [
        "sys", 
        "time", 
        "mimetools", 
        "socket", 
        "SocketServer", 
        "warnings"
      ]
    }, 
    "Bastion": {
      "file": "Bastion.py", 
      "imports": [
        "rexec", 
        "types", 
        "warnings"
      ]
    }, 
    "CGIHTTPServer": {
      "file": "CGIHTTPServer.py", 
      "imports": [
        "BaseHTTPServer", 
        "base64", 
        "binascii", 
        "select", 
        "subprocess", 
        "sys", 
        "copy", 
        "os", 
        "SimpleHTTPServer", 
        "urllib", 
        "pwd"
      ]
    }, 
    "ConfigParser": {
      "file": "ConfigParser.py", 
      "imports": [
        "collections", 
        "re", 
        "UserDict"
      ]
    }, 
    "Cookie": {
      "file": "Cookie.py", 
      "imports": [
        "Cookie", 
        "time.gmtime", 
        "time.time", 
        "doctest", 
        "pickle", 
        "re", 
        "string", 
        "warnings", 
        "cPickle"
      ]
    }, 
    "DocXMLRPCServer": {
      "file": "DocXMLRPCServer.py", 
      "imports": [
        "sys", 
        "inspect", 
        "pydoc", 
        "re", 
        "SimpleXMLRPCServer"
      ]
    }, 
    "HTMLParser": {
      "file": "HTMLParser.py", 
      "imports": [
        "htmlentitydefs", 
        "markupbase", 
        "re"
      ]
    }, 
    "MimeWriter": {
      "file": "MimeWriter.py", 
      "imports": [
        "mimetools", 
        "test.test_MimeWriter", 
        "warnings"
      ]
    }, 
    "Queue": {
      "file": "Queue.py", 
      "imports": [
        "collections", 
        "dummy_threading", 
        "heapq", 
        "threading", 
        "time.time"
      ]
    }, 
    "SimPy": {
      "dir": "SimPy"
    }, 
    "SimPy.GUIDebug": {
      "file": "SimPy/GUIDebug.py", 
      "imports": [
        "SimPy.SimulationStep", 
        "Tkinter.*", 
        "tkinter.*", 
        "warnings"
      ]
    }, 
    "SimPy.Globals": {
      "file": "SimPy/Globals.py", 
      "imports": []
    }, 
    "SimPy.Lib": {
      "file": "SimPy/Lib.py", 
      "imports": [
        "SimPy.Globals", 
        "SimPy.Lister", 
        "SimPy.Recording", 
        "inspect", 
        "sys", 
        "types"
      ]
    }, 
    "SimPy.Lister": {
      "file": "SimPy/Lister.py", 
      "imports": []
    }, 
    "SimPy.Recording": {
      "file": "SimPy/Recording.py", 
      "imports": [
        "SimPy.Globals"
      ]
    }, 
    "SimPy.SimGUI": {
      "file": "SimPy/SimGUI.py", 
      "imports": [
        "Canvas.CanvasText", 
        "Canvas.Line", 
        "Canvas.Rectangle", 
        "SimPy", 
        "SimPy.Simulation", 
        "SimPy.tkconsole", 
        "Tkinter.*", 
        "random", 
        "sys", 
        "tkFileDialog.asksaveasfilename", 
        "tkMessageBox.*", 
        "tkinter.*", 
        "tkinter.filedialog.asksaveasfilename", 
        "tkinter.messagebox.*", 
        "warnings"
      ]
    }, 
    "SimPy.SimPlot": {
      "file": "SimPy/SimPlot.py", 
      "imports": [
        "Canvas.CanvasText", 
        "Canvas.Line", 
        "Canvas.Rectangle", 
        "SimPy.Simulation", 
        "Tkinter.*", 
        "math", 
        "math.pi", 
        "string", 
        "tkFileDialog.*", 
        "tkFileDialog.asksaveasfilename", 
        "tkMessageBox.*", 
        "tkSimpleDialog.askfloat", 
        "tkSimpleDialog.askinteger", 
        "tkSimpleDialog.askstring", 
        "tkinter.*", 
        "tkinter.filedialog.*", 
        "tkinter.filedialog.asksaveasfilename", 
        "tkinter.messagebox.*", 
        "tkinter.simpledialog.askfloat", 
        "tkinter.simpledialog.askinteger", 
        "tkinter.simpledialog.askstring", 
        "warnings"
      ]
    }, 
    "SimPy.Simulation": {
      "file": "SimPy/Simulation.py", 
      "imports": [
        "SimPy", 
        "SimPy.Globals", 
        "SimPy.Lib", 
        "SimPy.Lister", 
        "SimPy.Recording", 
        "heapq", 
        "random", 
        "sys", 
        "types"
      ]
    }, 
    "SimPy.SimulationGUIDebug": {
      "file": "SimPy/SimulationGUIDebug.py", 
      "imports": [
        "SimPy.GUIDebug", 
        "SimPy.SimulationStep", 
        "Tkinter.*", 
        "sys", 
        "tkinter.*", 
        "warnings"
      ]
    }, 
    "SimPy.SimulationRT": {
      "file": "SimPy/SimulationRT.py", 
      "imports": [
        "SimPy", 
        "SimPy.Simulation", 
        "time"
      ]
    }, 
    "SimPy.SimulationStep": {
      "file": "SimPy/SimulationStep.py", 
      "imports": [
        "SimPy.Simulation"
      ]
    }, 
    "SimPy.SimulationTrace": {
      "file": "SimPy/SimulationTrace.py", 
      "imports": [
        "SimPy.Lister", 
        "SimPy.Simulation", 
        "__future__"
      ]
    }, 
    "SimPy.__init__": {
      "file": "SimPy/__init__.py", 
      "imports": [
        "os", 
        "pytest"
      ]
    }, 
    "SimPy.stepping": {
      "file": "SimPy/stepping.py", 
      "imports": [
        "sys"
      ]
    }, 
    "SimPy.test": {
      "dir": "SimPy/test"
    }, 
    "SimPy.test.__init__": {
      "file": "SimPy/test/__init__.py", 
      "imports": []
    }, 
    "SimPy.test.support": {
      "file": "SimPy/test/support.py", 
      "imports": [
        "SimPy.Globals", 
        "SimPy.Simulation", 
        "SimPy.SimulationRT", 
        "SimPy.SimulationStep", 
        "SimPy.SimulationTrace"
      ]
    }, 
    "SimPy.test.test_monitor": {
      "file": "SimPy/test/test_monitor.py", 
      "imports": [
        "SimPy.Simulation", 
        "random"
      ]
    }, 
    "SimPy.test.test_rt_behavior": {
      "file": "SimPy/test/test_rt_behavior.py", 
      "imports": [
        "SimPy.SimulationRT"
      ]
    }, 
    "SimPy.test.test_simident": {
      "file": "SimPy/test/test_simident.py", 
      "imports": [
        "SimPy.Simulation"
      ]
    }, 
    "SimPy.test.test_simpy": {
      "file": "SimPy/test/test_simpy.py", 
      "imports": [
        "SimPy.Simulation", 
        "random", 
        "unittest"
      ]
    }, 
    "SimPy.tkconsole": {
      "file": "SimPy/tkconsole.py", 
      "imports": [
        "ExtensionClass", 
        "SimPy", 
        "Tkinter.*", 
        "__builtin__", 
        "curve", 
        "string", 
        "sys", 
        "traceback", 
        "types", 
        "warnings"
      ]
    }, 
    "SimPy.tkprogressbar": {
      "file": "SimPy/tkprogressbar.py", 
      "imports": [
        "Tkinter.*", 
        "warnings"
      ]
    }, 
    "SimpleHTTPServer": {
      "file": "SimpleHTTPServer.py", 
      "imports": [
        "BaseHTTPServer", 
        "cStringIO.StringIO", 
        "cgi", 
        "mimetypes", 
        "os", 
        "posixpath", 
        "shutil", 
        "sys", 
        "StringIO", 
        "urllib"
      ]
    }, 
    "SimpleXMLRPCServer": {
      "file": "SimpleXMLRPCServer.py", 
      "imports": [
        "BaseHTTPServer", 
        "fcntl", 
        "os", 
        "pydoc", 
        "re", 
        "sys", 
        "SocketServer", 
        "traceback", 
        "xmlrpclib"
      ]
    }, 
    "SocketServer": {
      "file": "SocketServer.py", 
      "imports": [
        "cStringIO.StringIO", 
        "dummy_threading", 
        "errno", 
        "os", 
        "select", 
        "socket", 
        "sys", 
        "threading", 
        "StringIO", 
        "traceback"
      ]
    }, 
    "StringIO": {
      "file": "StringIO.py", 
      "imports": [
        "errno.EINVAL", 
        "sys"
      ]
    }, 
    "UserDict": {
      "file": "UserDict.py", 
      "imports": [
        "_abcoll", 
        "copy"
      ]
    }, 
    "UserList": {
      "file": "UserList.py", 
      "imports": [
        "collections"
      ]
    }, 
    "UserString": {
      "file": "UserString.py", 
      "imports": [
        "collections", 
        "os", 
        "sys", 
        "test.test_support", 
        "warnings"
      ]
    }, 
    "_LWPCookieJar": {
      "file": "_LWPCookieJar.py", 
      "imports": [
        "time", 
        "cookielib", 
        "re"
      ]
    }, 
    "_MozillaCookieJar": {
      "file": "_MozillaCookieJar.py", 
      "imports": [
        "time", 
        "cookielib", 
        "re"
      ]
    }, 
    "__future__": {
      "file": "__future__.py", 
      "imports": []
    }, 
    "__init__": {
      "file": "__init__.py", 
      "imports": []
    }, 
    "__phello__.foo": {
      "file": "__phello__.foo.py", 
      "imports": []
    }, 
    "_abcoll": {
      "file": "_abcoll.py", 
      "imports": [
        "sys", 
        "abc"
      ]
    }, 
    "_codecs_cn": {
      "file": "_codecs_cn.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_hk": {
      "file": "_codecs_hk.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_iso2022": {
      "file": "_codecs_iso2022.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_jp": {
      "file": "_codecs_jp.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_kr": {
      "file": "_codecs_kr.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_tw": {
      "file": "_codecs_tw.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_collections": {
      "file": "_collections.py", 
      "imports": [
        "threading._get_ident"
      ]
    }, 
    "_csv": {
      "file": "_csv.py", 
      "imports": []
    }, 
    "_ctypes_test": {
      "file": "_ctypes_test.py", 
      "imports": [
        "_ctypes", 
        "cpyext", 
        "imp", 
        "os", 
        "_pypy_testcapi"
      ]
    }, 
    "_curses": {
      "file": "_curses.py", 
      "imports": [
        "cffi.FFI", 
        "functools", 
        "sys"
      ]
    }, 
    "_curses_panel": {
      "file": "_curses_panel.py", 
      "imports": [
        "_curses"
      ]
    }, 
    "_elementtree": {
      "file": "_elementtree.py", 
      "imports": [
        "xml.etree.ElementTree"
      ]
    }, 
    "_functools": {
      "file": "_functools.py", 
      "imports": []
    }, 
    "_marshal": {
      "file": "_marshal.py", 
      "imports": [
        "__pypy__.builtinify", 
        "sys.intern", 
        "types"
      ]
    }, 
    "_md5": {
      "file": "_md5.py", 
      "imports": [
        "copy", 
        "struct"
      ]
    }, 
    "_osx_support": {
      "file": "_osx_support.py", 
      "imports": [
        "sys", 
        "contextlib", 
        "distutils.log", 
        "os", 
        "re", 
        "tempfile"
      ]
    }, 
    "_pyio": {
      "file": "_pyio.py", 
      "imports": [
        "__future__", 
        "_io.FileIO", 
        "array", 
        "errno", 
        "errno.EINTR", 
        "thread.allocate_lock", 
        "abc", 
        "codecs", 
        "dummy_thread", 
        "io", 
        "locale", 
        "os", 
        "warnings"
      ]
    }, 
    "_pypy_interact": {
      "file": "_pypy_interact.py", 
      "imports": [
        "__main__", 
        "code", 
        "os", 
        "sys", 
        "_pypy_irc_topic", 
        "pyrepl.simple_interact"
      ]
    }, 
    "_pypy_irc_topic": {
      "file": "_pypy_irc_topic.py", 
      "imports": [
        "string", 
        "time"
      ]
    }, 
    "_pypy_testcapi": {
      "file": "_pypy_testcapi.py", 
      "imports": [
        "binascii", 
        "distutils.ccompiler", 
        "imp", 
        "os", 
        "sys", 
        "tempfile"
      ]
    }, 
    "_pypy_wait": {
      "file": "_pypy_wait.py", 
      "imports": [
        "ctypes.CDLL", 
        "ctypes.POINTER", 
        "ctypes.byref", 
        "ctypes.c_int", 
        "ctypes.util.find_library", 
        "resource"
      ]
    }, 
    "_scproxy": {
      "file": "_scproxy.py", 
      "imports": [
        "ctypes.c_char_p", 
        "ctypes.c_int", 
        "ctypes.c_int32", 
        "ctypes.c_int64", 
        "ctypes.c_void_p", 
        "ctypes.cdll", 
        "ctypes.create_string_buffer", 
        "ctypes.pointer", 
        "ctypes.util.find_library", 
        "sys"
      ]
    }, 
    "_sha": {
      "file": "_sha.py", 
      "imports": [
        "copy", 
        "struct"
      ]
    }, 
    "_sha256": {
      "file": "_sha256.py", 
      "imports": [
        "struct"
      ]
    }, 
    "_sha512": {
      "file": "_sha512.py", 
      "imports": [
        "_sha512", 
        "struct"
      ]
    }, 
    "_sqlite3": {
      "file": "_sqlite3.py", 
      "imports": [
        "__pypy__.newlist_hint", 
        "cffi.FFI", 
        "collections", 
        "functools", 
        "os", 
        "sqlite3.dump._iterdump", 
        "string", 
        "sys", 
        "threading._get_ident", 
        "weakref", 
        "datetime"
      ]
    }, 
    "_strptime": {
      "file": "_strptime.py", 
      "imports": [
        "thread.allocate_lock", 
        "time", 
        "calendar", 
        "dummy_thread", 
        "locale", 
        "re", 
        "datetime"
      ]
    }, 
    "_structseq": {
      "file": "_structseq.py", 
      "imports": []
    }, 
    "_testcapi": {
      "file": "_testcapi.py", 
      "imports": [
        "_pypy_testcapi", 
        "cpyext", 
        "imp", 
        "os"
      ]
    }, 
    "_threading_local": {
      "file": "_threading_local.py", 
      "imports": [
        "threading", 
        "threading.RLock", 
        "threading.current_thread"
      ]
    }, 
    "_weakrefset": {
      "file": "_weakrefset.py", 
      "imports": [
        "_weakref.ref"
      ]
    }, 
    "abc": {
      "file": "abc.py", 
      "imports": [
        "_weakrefset", 
        "types"
      ]
    }, 
    "aifc": {
      "file": "aifc.py", 
      "imports": [
        "__builtin__", 
        "audioop", 
        "cl", 
        "math", 
        "sys", 
        "chunk", 
        "struct"
      ]
    }, 
    "antigravity": {
      "file": "antigravity.py", 
      "imports": [
        "webbrowser"
      ]
    }, 
    "anydbm": {
      "file": "anydbm.py", 
      "imports": [
        "whichdb"
      ]
    }, 
    "argparse": {
      "file": "argparse.py", 
      "imports": [
        "sys", 
        "collections", 
        "copy", 
        "gettext", 
        "os", 
        "re", 
        "textwrap", 
        "warnings"
      ]
    }, 
    "ast": {
      "file": "ast.py", 
      "imports": [
        "_ast.*", 
        "_ast.__version__", 
        "collections", 
        "inspect"
      ]
    }, 
    "asynchat": {
      "file": "asynchat.py", 
      "imports": [
        "sys.py3kwarning", 
        "asyncore", 
        "collections", 
        "socket", 
        "warnings"
      ]
    }, 
    "asyncore": {
      "file": "asyncore.py", 
      "imports": [
        "errno.EAGAIN", 
        "errno.EALREADY", 
        "errno.EBADF", 
        "errno.ECONNABORTED", 
        "errno.ECONNRESET", 
        "errno.EINPROGRESS", 
        "errno.EINTR", 
        "errno.EINVAL", 
        "errno.EISCONN", 
        "errno.ENOTCONN", 
        "errno.EPIPE", 
        "errno.ESHUTDOWN", 
        "errno.EWOULDBLOCK", 
        "errno.errorcode", 
        "fcntl", 
        "select", 
        "sys", 
        "time", 
        "os", 
        "socket", 
        "warnings"
      ]
    }, 
    "atexit": {
      "file": "atexit.py", 
      "imports": [
        "sys", 
        "traceback"
      ]
    }, 
    "base64": {
      "file": "base64.py", 
      "imports": [
        "binascii", 
        "sys", 
        "getopt", 
        "re", 
        "struct"
      ]
    }, 
    "bdb": {
      "file": "bdb.py", 
      "imports": [
        "__main__", 
        "sys", 
        "fnmatch", 
        "linecache", 
        "os", 
        "repr", 
        "types"
      ]
    }, 
    "binhex": {
      "file": "binhex.py", 
      "imports": [
        "Carbon.File.FInfo", 
        "Carbon.File.FSSpec", 
        "MacOS.openrf", 
        "binascii", 
        "sys", 
        "os", 
        "struct"
      ]
    }, 
    "bisect": {
      "file": "bisect.py", 
      "imports": [
        "_bisect.*"
      ]
    }, 
    "cPickle": {
      "file": "cPickle.py", 
      "imports": [
        "__pypy__.builtinify", 
        "copy_reg", 
        "marshal", 
        "pickle", 
        "struct", 
        "sys", 
        "types"
      ]
    }, 
    "cProfile": {
      "file": "cProfile.py", 
      "imports": [
        "__main__", 
        "_lsprof", 
        "marshal", 
        "sys", 
        "optparse", 
        "os", 
        "pstats", 
        "types"
      ]
    }, 
    "cStringIO": {
      "file": "cStringIO.py", 
      "imports": [
        "StringIO"
      ]
    }, 
    "calendar": {
      "file": "calendar.py", 
      "imports": [
        "sys", 
        "locale", 
        "optparse", 
        "datetime"
      ]
    }, 
    "cgi": {
      "file": "cgi.py", 
      "imports": [
        "cStringIO.StringIO", 
        "operator.attrgetter", 
        "sys", 
        "mimetools", 
        "os", 
        "re", 
        "rfc822", 
        "StringIO", 
        "tempfile", 
        "traceback", 
        "urlparse", 
        "UserDict", 
        "warnings"
      ]
    }, 
    "cgitb": {
      "file": "cgitb.py", 
      "imports": [
        "sys", 
        "time", 
        "inspect", 
        "keyword", 
        "linecache", 
        "os", 
        "pydoc", 
        "tempfile", 
        "tokenize", 
        "traceback", 
        "types"
      ]
    }, 
    "chunk": {
      "file": "chunk.py", 
      "imports": [
        "struct"
      ]
    }, 
    "cmd": {
      "file": "cmd.py", 
      "imports": [
        "readline", 
        "sys", 
        "string"
      ]
    }, 
    "code": {
      "file": "code.py", 
      "imports": [
        "readline", 
        "sys", 
        "codeop", 
        "traceback"
      ]
    }, 
    "codecs": {
      "file": "codecs.py", 
      "imports": [
        "__builtin__", 
        "_codecs.*", 
        "sys", 
        "encodings"
      ]
    }, 
    "codeop": {
      "file": "codeop.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "collections": {
      "file": "collections.py", 
      "imports": [
        "__pypy__.newdict", 
        "__pypy__.reversed_dict", 
        "_abcoll", 
        "_collections.defaultdict", 
        "_collections.deque", 
        "itertools.chain", 
        "itertools.imap", 
        "itertools.repeat", 
        "itertools.starmap", 
        "operator.eq", 
        "operator.itemgetter", 
        "sys", 
        "thread.get_ident", 
        "doctest", 
        "dummy_thread", 
        "heapq", 
        "keyword", 
        "cPickle"
      ]
    }, 
    "colorsys": {
      "file": "colorsys.py", 
      "imports": []
    }, 
    "commands": {
      "file": "commands.py", 
      "imports": [
        "os", 
        "warnings"
      ]
    }, 
    "compileall": {
      "file": "compileall.py", 
      "imports": [
        "imp", 
        "sys", 
        "getopt", 
        "os", 
        "py_compile", 
        "re", 
        "struct"
      ]
    }, 
    "compiler": {
      "dir": "compiler"
    }, 
    "compiler.__init__": {
      "file": "compiler/__init__.py", 
      "imports": [
        "compiler.pycodegen", 
        "compiler.transformer", 
        "compiler.visitor", 
        "warnings"
      ]
    }, 
    "compiler.ast": {
      "file": "compiler/ast.py", 
      "imports": [
        "compiler.consts"
      ]
    }, 
    "compiler.consts": {
      "file": "compiler/consts.py", 
      "imports": []
    }, 
    "compiler.future": {
      "file": "compiler/future.py", 
      "imports": [
        "compiler", 
        "compiler.ast", 
        "sys"
      ]
    }, 
    "compiler.misc": {
      "file": "compiler/misc.py", 
      "imports": []
    }, 
    "compiler.pyassem": {
      "file": "compiler/pyassem.py", 
      "imports": [
        "compiler.consts", 
        "compiler.misc", 
        "sys", 
        "dis", 
        "types"
      ]
    }, 
    "compiler.pycodegen": {
      "file": "compiler/pycodegen.py", 
      "imports": [
        "cStringIO.StringIO", 
        "compiler", 
        "compiler.ast", 
        "compiler.consts", 
        "compiler.future", 
        "compiler.misc", 
        "compiler.pyassem", 
        "compiler.symbols", 
        "compiler.syntax", 
        "imp", 
        "marshal", 
        "sys", 
        "os", 
        "pprint", 
        "struct"
      ]
    }, 
    "compiler.symbols": {
      "file": "compiler/symbols.py", 
      "imports": [
        "compiler", 
        "compiler.ast", 
        "compiler.consts", 
        "compiler.misc", 
        "symtable", 
        "sys", 
        "types"
      ]
    }, 
    "compiler.syntax": {
      "file": "compiler/syntax.py", 
      "imports": [
        "compiler", 
        "compiler.ast"
      ]
    }, 
    "compiler.transformer": {
      "file": "compiler/transformer.py", 
      "imports": [
        "compiler.ast", 
        "compiler.consts", 
        "parser", 
        "symbol", 
        "token"
      ]
    }, 
    "compiler.visitor": {
      "file": "compiler/visitor.py", 
      "imports": [
        "compiler.ast"
      ]
    }, 
    "contextlib": {
      "file": "contextlib.py", 
      "imports": [
        "sys", 
        "functools", 
        "warnings"
      ]
    }, 
    "cookielib": {
      "file": "cookielib.py", 
      "imports": [
        "_LWPCookieJar", 
        "_MozillaCookieJar", 
        "calendar", 
        "threading", 
        "time", 
        "copy", 
        "dummy_threading", 
        "httplib", 
        "logging", 
        "re", 
        "StringIO", 
        "traceback", 
        "urllib", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "copy": {
      "file": "copy.py", 
      "imports": [
        "org.python.core.PyStringMap", 
        "sys", 
        "copy_reg", 
        "repr", 
        "types", 
        "weakref"
      ]
    }, 
    "copy_reg": {
      "file": "copy_reg.py", 
      "imports": [
        "types"
      ]
    }, 
    "csv": {
      "file": "csv.py", 
      "imports": [
        "_csv.Dialect", 
        "_csv.Error", 
        "_csv.QUOTE_ALL", 
        "_csv.QUOTE_MINIMAL", 
        "_csv.QUOTE_NONE", 
        "_csv.QUOTE_NONNUMERIC", 
        "_csv.__doc__", 
        "_csv.__version__", 
        "_csv.field_size_limit", 
        "_csv.get_dialect", 
        "_csv.list_dialects", 
        "_csv.reader", 
        "_csv.register_dialect", 
        "_csv.unregister_dialect", 
        "_csv.writer", 
        "cStringIO.StringIO", 
        "functools", 
        "re", 
        "StringIO"
      ]
    }, 
    "ctypes_config_cache": {
      "dir": "ctypes_config_cache"
    }, 
    "ctypes_config_cache.__init__": {
      "file": "ctypes_config_cache/__init__.py", 
      "imports": []
    }, 
    "ctypes_config_cache._locale_32_": {
      "file": "ctypes_config_cache/_locale_32_.py", 
      "imports": [
        "ctypes"
      ]
    }, 
    "ctypes_config_cache._locale_64_": {
      "file": "ctypes_config_cache/_locale_64_.py", 
      "imports": [
        "ctypes"
      ]
    }, 
    "ctypes_config_cache._locale_cache": {
      "file": "ctypes_config_cache/_locale_cache.py", 
      "imports": [
        "sys"
      ]
    }, 
    "ctypes_config_cache._resource_32_": {
      "file": "ctypes_config_cache/_resource_32_.py", 
      "imports": [
        "ctypes"
      ]
    }, 
    "ctypes_config_cache._resource_64_": {
      "file": "ctypes_config_cache/_resource_64_.py", 
      "imports": [
        "ctypes"
      ]
    }, 
    "ctypes_config_cache._resource_cache": {
      "file": "ctypes_config_cache/_resource_cache.py", 
      "imports": [
        "sys"
      ]
    }, 
    "ctypes_config_cache.dumpcache": {
      "file": "ctypes_config_cache/dumpcache.py", 
      "imports": [
        "ctypes_configure.dumpcache", 
        "os", 
        "sys"
      ]
    }, 
    "ctypes_config_cache.locale.ctc": {
      "file": "ctypes_config_cache/locale.ctc.py", 
      "imports": [
        "ctypes_config_cache.dumpcache", 
        "ctypes_configure.configure.ConstantInteger", 
        "ctypes_configure.configure.DefinedConstantInteger", 
        "ctypes_configure.configure.ExternalCompilationInfo", 
        "ctypes_configure.configure.SimpleType", 
        "ctypes_configure.configure.check_eci", 
        "ctypes_configure.configure.configure"
      ]
    }, 
    "ctypes_config_cache.rebuild": {
      "file": "ctypes_config_cache/rebuild.py", 
      "imports": [
        "os", 
        "py", 
        "rpython.tool.ansi_print.ansi_log", 
        "sys"
      ]
    }, 
    "ctypes_config_cache.resource.ctc": {
      "file": "ctypes_config_cache/resource.ctc.py", 
      "imports": [
        "ctypes.sizeof", 
        "ctypes_config_cache.dumpcache", 
        "ctypes_configure.configure.ConstantInteger", 
        "ctypes_configure.configure.DefinedConstantInteger", 
        "ctypes_configure.configure.ExternalCompilationInfo", 
        "ctypes_configure.configure.SimpleType", 
        "ctypes_configure.configure.configure"
      ]
    }, 
    "datetime": {
      "file": "datetime.py", 
      "imports": [
        "__future__", 
        "_strptime", 
        "math", 
        "struct", 
        "time"
      ]
    }, 
    "dbhash": {
      "file": "dbhash.py", 
      "imports": [
        "bsddb", 
        "sys", 
        "warnings"
      ]
    }, 
    "dbm": {
      "file": "dbm.py", 
      "imports": [
        "ctypes.CDLL", 
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.c_char", 
        "ctypes.c_char_p", 
        "ctypes.c_int", 
        "ctypes.c_void_p", 
        "ctypes.util", 
        "os", 
        "sys"
      ]
    }, 
    "decimal": {
      "file": "decimal.py", 
      "imports": [
        "collections", 
        "copy", 
        "itertools.chain", 
        "itertools.repeat", 
        "math", 
        "sys", 
        "threading", 
        "doctest", 
        "locale", 
        "numbers", 
        "re"
      ]
    }, 
    "difflib": {
      "file": "difflib.py", 
      "imports": [
        "collections", 
        "difflib", 
        "doctest", 
        "functools", 
        "heapq", 
        "re"
      ]
    }, 
    "dircache": {
      "file": "dircache.py", 
      "imports": [
        "os", 
        "warnings"
      ]
    }, 
    "dis": {
      "file": "dis.py", 
      "imports": [
        "sys", 
        "opcode", 
        "types"
      ]
    }, 
    "distutils": {
      "dir": "distutils"
    }, 
    "distutils.__init__": {
      "file": "distutils/__init__.py", 
      "imports": []
    }, 
    "distutils.archive_util": {
      "file": "distutils/archive_util.py", 
      "imports": [
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.spawn", 
        "sys", 
        "os", 
        "tarfile", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "distutils.bcppcompiler": {
      "file": "distutils/bcppcompiler.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "os"
      ]
    }, 
    "distutils.ccompiler": {
      "file": "distutils/ccompiler.py", 
      "imports": [
        "distutils.debug", 
        "distutils.dep_util", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.util", 
        "sys", 
        "os", 
        "re", 
        "tempfile"
      ]
    }, 
    "distutils.cmd": {
      "file": "distutils/cmd.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.debug", 
        "distutils.dep_util", 
        "distutils.dir_util", 
        "distutils.dist", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.util", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "distutils.command": {
      "dir": "distutils/command"
    }, 
    "distutils.command.__init__": {
      "file": "distutils/command/__init__.py", 
      "imports": []
    }, 
    "distutils.command.bdist": {
      "file": "distutils/command/bdist.py", 
      "imports": [
        "distutils.core", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.util", 
        "os"
      ]
    }, 
    "distutils.command.bdist_dumb": {
      "file": "distutils/command/bdist_dumb.py", 
      "imports": [
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.util", 
        "os"
      ]
    }, 
    "distutils.command.bdist_msi": {
      "file": "distutils/command/bdist_msi.py", 
      "imports": [
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.util", 
        "distutils.version", 
        "msilib", 
        "msilib.Dialog", 
        "msilib.Directory", 
        "msilib.Feature", 
        "msilib.add_data", 
        "msilib.schema", 
        "msilib.sequence", 
        "msilib.text", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.command.bdist_rpm": {
      "file": "distutils/command/bdist_rpm.py", 
      "imports": [
        "distutils.core", 
        "distutils.debug", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "string"
      ]
    }, 
    "distutils.command.bdist_wininst": {
      "file": "distutils/command/bdist_wininst.py", 
      "imports": [
        "distutils", 
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.msvccompiler", 
        "distutils.sysconfig", 
        "distutils.util", 
        "sys", 
        "time", 
        "os", 
        "string", 
        "struct", 
        "tempfile"
      ]
    }, 
    "distutils.command.build": {
      "file": "distutils/command/build.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.util", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.command.build_clib": {
      "file": "distutils/command/build_clib.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "os"
      ]
    }, 
    "distutils.command.build_ext": {
      "file": "distutils/command/build_ext.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.log", 
        "distutils.msvccompiler", 
        "distutils.sysconfig", 
        "distutils.util", 
        "imp", 
        "sys", 
        "os", 
        "re", 
        "site", 
        "string", 
        "types"
      ]
    }, 
    "distutils.command.build_py": {
      "file": "distutils/command/build_py.py", 
      "imports": [
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.util", 
        "sys", 
        "glob", 
        "os"
      ]
    }, 
    "distutils.command.build_scripts": {
      "file": "distutils/command/build_scripts.py", 
      "imports": [
        "distutils.core", 
        "distutils.dep_util", 
        "distutils.log", 
        "distutils.util", 
        "os", 
        "re", 
        "stat"
      ]
    }, 
    "distutils.command.check": {
      "file": "distutils/command/check.py", 
      "imports": [
        "distutils.core", 
        "distutils.dist", 
        "distutils.errors", 
        "docutils.frontend", 
        "docutils.nodes", 
        "docutils.parsers.rst.Parser", 
        "docutils.utils.Reporter", 
        "StringIO"
      ]
    }, 
    "distutils.command.clean": {
      "file": "distutils/command/clean.py", 
      "imports": [
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.log", 
        "os"
      ]
    }, 
    "distutils.command.config": {
      "file": "distutils/command/config.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "os", 
        "re"
      ]
    }, 
    "distutils.command.install": {
      "file": "distutils/command/install.py", 
      "imports": [
        "distutils.core", 
        "distutils.debug", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.util", 
        "sys", 
        "os", 
        "pprint", 
        "site", 
        "string", 
        "types"
      ]
    }, 
    "distutils.command.install_data": {
      "file": "distutils/command/install_data.py", 
      "imports": [
        "distutils.core", 
        "distutils.util", 
        "os"
      ]
    }, 
    "distutils.command.install_egg_info": {
      "file": "distutils/command/install_egg_info.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.dir_util", 
        "distutils.log", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "distutils.command.install_headers": {
      "file": "distutils/command/install_headers.py", 
      "imports": [
        "distutils.core"
      ]
    }, 
    "distutils.command.install_lib": {
      "file": "distutils/command/install_lib.py", 
      "imports": [
        "distutils.core", 
        "distutils.errors", 
        "distutils.util", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.command.install_scripts": {
      "file": "distutils/command/install_scripts.py", 
      "imports": [
        "distutils.core", 
        "distutils.log", 
        "os", 
        "stat"
      ]
    }, 
    "distutils.command.register": {
      "file": "distutils/command/register.py", 
      "imports": [
        "distutils.core", 
        "distutils.log", 
        "getpass", 
        "urllib2", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "distutils.command.sdist": {
      "file": "distutils/command/sdist.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.core", 
        "distutils.dep_util", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.filelist", 
        "distutils.log", 
        "distutils.text_file", 
        "distutils.util", 
        "sys", 
        "glob", 
        "os", 
        "string", 
        "warnings"
      ]
    }, 
    "distutils.command.upload": {
      "file": "distutils/command/upload.py", 
      "imports": [
        "base64", 
        "cStringIO", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.spawn", 
        "hashlib", 
        "os", 
        "platform", 
        "socket", 
        "urllib2", 
        "urlparse"
      ]
    }, 
    "distutils.config": {
      "file": "distutils/config.py", 
      "imports": [
        "ConfigParser", 
        "distutils.cmd", 
        "os"
      ]
    }, 
    "distutils.core": {
      "file": "distutils/core.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.config", 
        "distutils.debug", 
        "distutils.dist", 
        "distutils.errors", 
        "distutils.extension", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.cygwinccompiler": {
      "file": "distutils/cygwinccompiler.py", 
      "imports": [
        "copy", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.unixccompiler", 
        "distutils.version", 
        "sys", 
        "os", 
        "re", 
        "string"
      ]
    }, 
    "distutils.debug": {
      "file": "distutils/debug.py", 
      "imports": [
        "os"
      ]
    }, 
    "distutils.dep_util": {
      "file": "distutils/dep_util.py", 
      "imports": [
        "distutils.errors", 
        "os", 
        "stat"
      ]
    }, 
    "distutils.dir_util": {
      "file": "distutils/dir_util.py", 
      "imports": [
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "errno", 
        "os"
      ]
    }, 
    "distutils.dist": {
      "file": "distutils/dist.py", 
      "imports": [
        "ConfigParser", 
        "distutils.cmd", 
        "distutils.command", 
        "distutils.core", 
        "distutils.debug", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.log", 
        "distutils.util", 
        "distutils.versionpredicate", 
        "sys", 
        "email", 
        "os", 
        "pprint", 
        "re", 
        "warnings"
      ]
    }, 
    "distutils.emxccompiler": {
      "file": "distutils/emxccompiler.py", 
      "imports": [
        "copy", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.unixccompiler", 
        "distutils.version", 
        "sys", 
        "os", 
        "re", 
        "string"
      ]
    }, 
    "distutils.errors": {
      "file": "distutils/errors.py", 
      "imports": []
    }, 
    "distutils.extension": {
      "file": "distutils/extension.py", 
      "imports": [
        "distutils.sysconfig", 
        "distutils.text_file", 
        "distutils.util", 
        "sys", 
        "os", 
        "string", 
        "types", 
        "warnings"
      ]
    }, 
    "distutils.fancy_getopt": {
      "file": "distutils/fancy_getopt.py", 
      "imports": [
        "distutils.errors", 
        "sys", 
        "getopt", 
        "re", 
        "string"
      ]
    }, 
    "distutils.file_util": {
      "file": "distutils/file_util.py", 
      "imports": [
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.log", 
        "errno", 
        "os", 
        "stat"
      ]
    }, 
    "distutils.filelist": {
      "file": "distutils/filelist.py", 
      "imports": [
        "distutils.debug", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.util", 
        "fnmatch", 
        "os", 
        "re", 
        "stat"
      ]
    }, 
    "distutils.log": {
      "file": "distutils/log.py", 
      "imports": [
        "sys"
      ]
    }, 
    "distutils.msvc9compiler": {
      "file": "distutils/msvc9compiler.py", 
      "imports": [
        "_winreg", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.util", 
        "subprocess", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "distutils.msvccompiler": {
      "file": "distutils/msvccompiler.py", 
      "imports": [
        "_winreg", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.msvc9compiler", 
        "sys", 
        "win32api", 
        "win32con", 
        "os", 
        "string"
      ]
    }, 
    "distutils.spawn": {
      "file": "distutils/spawn.py", 
      "imports": [
        "distutils.debug", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "errno", 
        "subprocess", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.sysconfig": {
      "file": "distutils/sysconfig.py", 
      "imports": [
        "distutils.sysconfig_cpython", 
        "distutils.sysconfig_pypy", 
        "sys"
      ]
    }, 
    "distutils.sysconfig_cpython": {
      "file": "distutils/sysconfig_cpython.py", 
      "imports": [
        "_osx_support", 
        "_sysconfigdata.build_time_vars", 
        "distutils.errors", 
        "distutils.text_file", 
        "sys", 
        "os", 
        "re", 
        "string"
      ]
    }, 
    "distutils.sysconfig_pypy": {
      "file": "distutils/sysconfig_pypy.py", 
      "imports": [
        "distutils.errors", 
        "distutils.sysconfig_cpython", 
        "sys", 
        "os", 
        "shlex"
      ]
    }, 
    "distutils.tests": {
      "dir": "distutils/tests"
    }, 
    "distutils.tests.__init__": {
      "file": "distutils/tests/__init__.py", 
      "imports": [
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.setuptools_build_ext": {
      "file": "distutils/tests/setuptools_build_ext.py", 
      "imports": [
        "Pyrex.Distutils.build_ext.build_ext", 
        "distutils.ccompiler", 
        "distutils.command.build_ext", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.tests.setuptools_extension", 
        "distutils.util", 
        "dl.RTLD_NOW", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.tests.setuptools_extension": {
      "file": "distutils/tests/setuptools_extension.py", 
      "imports": [
        "Pyrex.Distutils.build_ext.build_ext", 
        "distutils.core", 
        "distutils.extension", 
        "sys"
      ]
    }, 
    "distutils.tests.support": {
      "file": "distutils/tests/support.py", 
      "imports": [
        "copy", 
        "distutils.core", 
        "distutils.log", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "shutil", 
        "tempfile", 
        "unittest", 
        "warnings"
      ]
    }, 
    "distutils.tests.test_archive_util": {
      "file": "distutils/tests/test_archive_util.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.spawn", 
        "distutils.tests.support", 
        "sys", 
        "zlib", 
        "os", 
        "tarfile", 
        "test.test_support", 
        "unittest", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "distutils.tests.test_bdist": {
      "file": "distutils/tests/test_bdist.py", 
      "imports": [
        "distutils.command.bdist", 
        "distutils.tests.support", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_bdist_dumb": {
      "file": "distutils/tests/test_bdist_dumb.py", 
      "imports": [
        "distutils.command.bdist_dumb", 
        "distutils.core", 
        "distutils.tests.support", 
        "sys", 
        "zlib", 
        "os", 
        "test.test_support", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "distutils.tests.test_bdist_msi": {
      "file": "distutils/tests/test_bdist_msi.py", 
      "imports": [
        "distutils.command.bdist_msi", 
        "distutils.tests.support", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_bdist_rpm": {
      "file": "distutils/tests/test_bdist_rpm.py", 
      "imports": [
        "distutils.command.bdist_rpm", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.spawn", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_bdist_wininst": {
      "file": "distutils/tests/test_bdist_wininst.py", 
      "imports": [
        "distutils.command.bdist_wininst", 
        "distutils.tests.support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build": {
      "file": "distutils/tests/test_build.py", 
      "imports": [
        "distutils.command.build", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_clib": {
      "file": "distutils/tests/test_build_clib.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.command.build_clib", 
        "distutils.errors", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_ext": {
      "file": "distutils/tests/test_build_ext.py", 
      "imports": [
        "distutils.command.build_ext", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.sysconfig", 
        "distutils.tests.setuptools_build_ext", 
        "distutils.tests.setuptools_extension", 
        "distutils.tests.support", 
        "sys", 
        "xx", 
        "os", 
        "site", 
        "StringIO", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_py": {
      "file": "distutils/tests/test_build_py.py", 
      "imports": [
        "distutils.command.build_py", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_scripts": {
      "file": "distutils/tests/test_build_scripts.py", 
      "imports": [
        "distutils.command.build_scripts", 
        "distutils.core", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_ccompiler": {
      "file": "distutils/tests/test_ccompiler.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.debug", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_check": {
      "file": "distutils/tests/test_check.py", 
      "imports": [
        "distutils.command.check", 
        "distutils.errors", 
        "distutils.tests.support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_clean": {
      "file": "distutils/tests/test_clean.py", 
      "imports": [
        "distutils.command.clean", 
        "distutils.tests.support", 
        "sys", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_cmd": {
      "file": "distutils/tests/test_cmd.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.debug", 
        "distutils.dist", 
        "distutils.errors", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_config": {
      "file": "distutils/tests/test_config.py", 
      "imports": [
        "distutils.core", 
        "distutils.log", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_config_cmd": {
      "file": "distutils/tests/test_config_cmd.py", 
      "imports": [
        "distutils.command.config", 
        "distutils.log", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_core": {
      "file": "distutils/tests/test_core.py", 
      "imports": [
        "distutils.core", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_dep_util": {
      "file": "distutils/tests/test_dep_util.py", 
      "imports": [
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.tests.support", 
        "time", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_dir_util": {
      "file": "distutils/tests/test_dir_util.py", 
      "imports": [
        "distutils.dir_util", 
        "distutils.log", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "stat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_dist": {
      "file": "distutils/tests/test_dist.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.dist", 
        "distutils.tests.support", 
        "distutils.tests.test_dist", 
        "sys", 
        "os", 
        "StringIO", 
        "test.test_support", 
        "textwrap", 
        "unittest", 
        "warnings"
      ]
    }, 
    "distutils.tests.test_file_util": {
      "file": "distutils/tests/test_file_util.py", 
      "imports": [
        "distutils.file_util", 
        "distutils.log", 
        "distutils.tests.support", 
        "os", 
        "shutil", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_filelist": {
      "file": "distutils/tests/test_filelist.py", 
      "imports": [
        "distutils.debug", 
        "distutils.errors", 
        "distutils.filelist", 
        "distutils.log", 
        "distutils.tests.support", 
        "os", 
        "re", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install": {
      "file": "distutils/tests/test_install.py", 
      "imports": [
        "distutils.command.build_ext", 
        "distutils.command.install", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "site", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_data": {
      "file": "distutils/tests/test_install_data.py", 
      "imports": [
        "distutils.command.install_data", 
        "distutils.tests.support", 
        "sys", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_headers": {
      "file": "distutils/tests/test_install_headers.py", 
      "imports": [
        "distutils.command.install_headers", 
        "distutils.tests.support", 
        "sys", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_lib": {
      "file": "distutils/tests/test_install_lib.py", 
      "imports": [
        "distutils.command.install_lib", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_scripts": {
      "file": "distutils/tests/test_install_scripts.py", 
      "imports": [
        "distutils.command.install_scripts", 
        "distutils.core", 
        "distutils.tests.support", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_msvc9compiler": {
      "file": "distutils/tests/test_msvc9compiler.py", 
      "imports": [
        "_winreg", 
        "distutils.errors", 
        "distutils.msvc9compiler", 
        "distutils.msvccompiler", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_register": {
      "file": "distutils/tests/test_register.py", 
      "imports": [
        "distutils.command.register", 
        "distutils.errors", 
        "distutils.tests.test_config", 
        "docutils", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest", 
        "urllib2", 
        "warnings"
      ]
    }, 
    "distutils.tests.test_sdist": {
      "file": "distutils/tests/test_sdist.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.command.sdist", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.filelist", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.tests.test_config", 
        "zlib", 
        "os", 
        "tarfile", 
        "test.test_support", 
        "textwrap", 
        "unittest", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "distutils.tests.test_spawn": {
      "file": "distutils/tests/test_spawn.py", 
      "imports": [
        "distutils.errors", 
        "distutils.spawn", 
        "distutils.tests.support", 
        "time", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_sysconfig": {
      "file": "distutils/tests/test_sysconfig.py", 
      "imports": [
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "os", 
        "shutil", 
        "test.test_support", 
        "test", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_text_file": {
      "file": "distutils/tests/test_text_file.py", 
      "imports": [
        "distutils.tests.support", 
        "distutils.text_file", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_unixccompiler": {
      "file": "distutils/tests/test_unixccompiler.py", 
      "imports": [
        "distutils.sysconfig", 
        "distutils.unixccompiler", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_upload": {
      "file": "distutils/tests/test_upload.py", 
      "imports": [
        "distutils.command.upload", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.tests.test_config", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_util": {
      "file": "distutils/tests/test_util.py", 
      "imports": [
        "distutils.errors", 
        "distutils.util", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_version": {
      "file": "distutils/tests/test_version.py", 
      "imports": [
        "distutils.version", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_versionpredicate": {
      "file": "distutils/tests/test_versionpredicate.py", 
      "imports": [
        "distutils.versionpredicate", 
        "doctest", 
        "test.test_support"
      ]
    }, 
    "distutils.text_file": {
      "file": "distutils/text_file.py", 
      "imports": [
        "sys"
      ]
    }, 
    "distutils.unixccompiler": {
      "file": "distutils/unixccompiler.py", 
      "imports": [
        "_osx_support", 
        "distutils.ccompiler", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "platform", 
        "re", 
        "types"
      ]
    }, 
    "distutils.util": {
      "file": "distutils/util.py", 
      "imports": [
        "_osx_support", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "py_compile", 
        "re", 
        "string", 
        "tempfile", 
        "pwd"
      ]
    }, 
    "distutils.version": {
      "file": "distutils/version.py", 
      "imports": [
        "re", 
        "string", 
        "types"
      ]
    }, 
    "distutils.versionpredicate": {
      "file": "distutils/versionpredicate.py", 
      "imports": [
        "distutils.version", 
        "operator", 
        "re"
      ]
    }, 
    "doctest": {
      "file": "doctest.py", 
      "imports": [
        "__future__", 
        "collections", 
        "difflib", 
        "sys", 
        "inspect", 
        "linecache", 
        "os", 
        "pdb", 
        "re", 
        "StringIO", 
        "tempfile", 
        "traceback", 
        "types", 
        "unittest", 
        "warnings"
      ]
    }, 
    "dumbdbm": {
      "file": "dumbdbm.py", 
      "imports": [
        "__builtin__", 
        "os", 
        "UserDict"
      ]
    }, 
    "dummy_thread": {
      "file": "dummy_thread.py", 
      "imports": [
        "traceback"
      ]
    }, 
    "dummy_threading": {
      "file": "dummy_threading.py", 
      "imports": [
        "_dummy_threading.*", 
        "_dummy_threading.__all__", 
        "dummy_thread", 
        "sys.modules", 
        "threading"
      ]
    }, 
    "email": {
      "dir": "email"
    }, 
    "email.__init__": {
      "file": "email/__init__.py", 
      "imports": [
        "email.mime", 
        "email.parser", 
        "sys"
      ]
    }, 
    "email._parseaddr": {
      "file": "email/_parseaddr.py", 
      "imports": [
        "calendar", 
        "time"
      ]
    }, 
    "email.base64mime": {
      "file": "email/base64mime.py", 
      "imports": [
        "binascii.a2b_base64", 
        "binascii.b2a_base64", 
        "email.utils"
      ]
    }, 
    "email.charset": {
      "file": "email/charset.py", 
      "imports": [
        "codecs", 
        "email.base64mime", 
        "email.encoders", 
        "email.errors", 
        "email.quoprimime"
      ]
    }, 
    "email.encoders": {
      "file": "email/encoders.py", 
      "imports": [
        "base64", 
        "quopri"
      ]
    }, 
    "email.errors": {
      "file": "email/errors.py", 
      "imports": []
    }, 
    "email.feedparser": {
      "file": "email/feedparser.py", 
      "imports": [
        "email.errors", 
        "email.message", 
        "re"
      ]
    }, 
    "email.generator": {
      "file": "email/generator.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email.header", 
        "sys", 
        "time", 
        "random", 
        "re", 
        "warnings"
      ]
    }, 
    "email.header": {
      "file": "email/header.py", 
      "imports": [
        "binascii", 
        "email.base64mime", 
        "email.charset", 
        "email.errors", 
        "email.quoprimime", 
        "re"
      ]
    }, 
    "email.iterators": {
      "file": "email/iterators.py", 
      "imports": [
        "cStringIO.StringIO", 
        "sys"
      ]
    }, 
    "email.message": {
      "file": "email/message.py", 
      "imports": [
        "binascii", 
        "cStringIO.StringIO", 
        "email.charset", 
        "email.errors", 
        "email.generator", 
        "email.iterators", 
        "email.utils", 
        "re", 
        "uu", 
        "warnings"
      ]
    }, 
    "email.mime": {
      "dir": "email/mime"
    }, 
    "email.mime.__init__": {
      "file": "email/mime/__init__.py", 
      "imports": []
    }, 
    "email.mime.application": {
      "file": "email/mime/application.py", 
      "imports": [
        "email.encoders", 
        "email.mime.nonmultipart"
      ]
    }, 
    "email.mime.audio": {
      "file": "email/mime/audio.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email.encoders", 
        "email.mime.nonmultipart", 
        "sndhdr"
      ]
    }, 
    "email.mime.base": {
      "file": "email/mime/base.py", 
      "imports": [
        "email.message"
      ]
    }, 
    "email.mime.image": {
      "file": "email/mime/image.py", 
      "imports": [
        "email.encoders", 
        "email.mime.nonmultipart", 
        "imghdr"
      ]
    }, 
    "email.mime.message": {
      "file": "email/mime/message.py", 
      "imports": [
        "email.message", 
        "email.mime.nonmultipart"
      ]
    }, 
    "email.mime.multipart": {
      "file": "email/mime/multipart.py", 
      "imports": [
        "email.mime.base"
      ]
    }, 
    "email.mime.nonmultipart": {
      "file": "email/mime/nonmultipart.py", 
      "imports": [
        "email.errors", 
        "email.mime.base"
      ]
    }, 
    "email.mime.text": {
      "file": "email/mime/text.py", 
      "imports": [
        "email.encoders", 
        "email.mime.nonmultipart"
      ]
    }, 
    "email.parser": {
      "file": "email/parser.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email.feedparser", 
        "email.message", 
        "warnings"
      ]
    }, 
    "email.quoprimime": {
      "file": "email/quoprimime.py", 
      "imports": [
        "email.utils", 
        "re", 
        "string"
      ]
    }, 
    "email.test": {
      "dir": "email/test"
    }, 
    "email.test.__init__": {
      "file": "email/test/__init__.py", 
      "imports": []
    }, 
    "email.test.test_email": {
      "file": "email/test/test_email.py", 
      "imports": [
        "base64", 
        "cStringIO.StringIO", 
        "difflib", 
        "email", 
        "email.feedparser", 
        "email.test", 
        "sys", 
        "time", 
        "os", 
        "re", 
        "test.test_support", 
        "textwrap", 
        "unittest", 
        "warnings"
      ]
    }, 
    "email.test.test_email_codecs": {
      "file": "email/test/test_email_codecs.py", 
      "imports": [
        "email.charset", 
        "email.header", 
        "email.message", 
        "email.test.test_email", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "email.test.test_email_codecs_renamed": {
      "file": "email/test/test_email_codecs_renamed.py", 
      "imports": [
        "email.charset", 
        "email.header", 
        "email.message", 
        "email.test.test_email", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "email.test.test_email_renamed": {
      "file": "email/test/test_email_renamed.py", 
      "imports": [
        "base64", 
        "cStringIO.StringIO", 
        "difflib", 
        "email", 
        "email.base64mime", 
        "email.charset", 
        "email.encoders", 
        "email.errors", 
        "email.generator", 
        "email.header", 
        "email.iterators", 
        "email.message", 
        "email.mime.application", 
        "email.mime.audio", 
        "email.mime.base", 
        "email.mime.image", 
        "email.mime.message", 
        "email.mime.multipart", 
        "email.mime.text", 
        "email.parser", 
        "email.quoprimime", 
        "email.test", 
        "email.utils", 
        "sys", 
        "time", 
        "os", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "email.test.test_email_torture": {
      "file": "email/test/test_email_torture.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email", 
        "email.iterators", 
        "email.test.test_email", 
        "sys", 
        "os", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "email.utils": {
      "file": "email/utils.py", 
      "imports": [
        "base64", 
        "email._parseaddr", 
        "email.encoders", 
        "time", 
        "os", 
        "quopri", 
        "random", 
        "re", 
        "socket", 
        "urllib", 
        "warnings"
      ]
    }, 
    "encodings": {
      "dir": "encodings"
    }, 
    "encodings.__init__": {
      "file": "encodings/__init__.py", 
      "imports": [
        "__builtin__", 
        "codecs", 
        "encodings.aliases"
      ]
    }, 
    "encodings.aliases": {
      "file": "encodings/aliases.py", 
      "imports": []
    }, 
    "encodings.ascii": {
      "file": "encodings/ascii.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.base64_codec": {
      "file": "encodings/base64_codec.py", 
      "imports": [
        "base64", 
        "codecs"
      ]
    }, 
    "encodings.big5": {
      "file": "encodings/big5.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_tw"
      ]
    }, 
    "encodings.big5hkscs": {
      "file": "encodings/big5hkscs.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_hk"
      ]
    }, 
    "encodings.bz2_codec": {
      "file": "encodings/bz2_codec.py", 
      "imports": [
        "bz2", 
        "codecs"
      ]
    }, 
    "encodings.charmap": {
      "file": "encodings/charmap.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp037": {
      "file": "encodings/cp037.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1006": {
      "file": "encodings/cp1006.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1026": {
      "file": "encodings/cp1026.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1140": {
      "file": "encodings/cp1140.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1250": {
      "file": "encodings/cp1250.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1251": {
      "file": "encodings/cp1251.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1252": {
      "file": "encodings/cp1252.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1253": {
      "file": "encodings/cp1253.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1254": {
      "file": "encodings/cp1254.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1255": {
      "file": "encodings/cp1255.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1256": {
      "file": "encodings/cp1256.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1257": {
      "file": "encodings/cp1257.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1258": {
      "file": "encodings/cp1258.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp424": {
      "file": "encodings/cp424.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp437": {
      "file": "encodings/cp437.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp500": {
      "file": "encodings/cp500.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp720": {
      "file": "encodings/cp720.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp737": {
      "file": "encodings/cp737.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp775": {
      "file": "encodings/cp775.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp850": {
      "file": "encodings/cp850.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp852": {
      "file": "encodings/cp852.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp855": {
      "file": "encodings/cp855.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp856": {
      "file": "encodings/cp856.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp857": {
      "file": "encodings/cp857.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp858": {
      "file": "encodings/cp858.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp860": {
      "file": "encodings/cp860.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp861": {
      "file": "encodings/cp861.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp862": {
      "file": "encodings/cp862.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp863": {
      "file": "encodings/cp863.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp864": {
      "file": "encodings/cp864.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp865": {
      "file": "encodings/cp865.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp866": {
      "file": "encodings/cp866.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp869": {
      "file": "encodings/cp869.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp874": {
      "file": "encodings/cp874.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp875": {
      "file": "encodings/cp875.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp932": {
      "file": "encodings/cp932.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.cp949": {
      "file": "encodings/cp949.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_kr"
      ]
    }, 
    "encodings.cp950": {
      "file": "encodings/cp950.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_tw"
      ]
    }, 
    "encodings.euc_jis_2004": {
      "file": "encodings/euc_jis_2004.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.euc_jisx0213": {
      "file": "encodings/euc_jisx0213.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.euc_jp": {
      "file": "encodings/euc_jp.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.euc_kr": {
      "file": "encodings/euc_kr.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_kr"
      ]
    }, 
    "encodings.gb18030": {
      "file": "encodings/gb18030.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.gb2312": {
      "file": "encodings/gb2312.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.gbk": {
      "file": "encodings/gbk.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.hex_codec": {
      "file": "encodings/hex_codec.py", 
      "imports": [
        "binascii", 
        "codecs"
      ]
    }, 
    "encodings.hp_roman8": {
      "file": "encodings/hp_roman8.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.hz": {
      "file": "encodings/hz.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.idna": {
      "file": "encodings/idna.py", 
      "imports": [
        "codecs", 
        "unicodedata.ucd_3_2_0", 
        "re", 
        "stringprep"
      ]
    }, 
    "encodings.iso2022_jp": {
      "file": "encodings/iso2022_jp.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_1": {
      "file": "encodings/iso2022_jp_1.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_2": {
      "file": "encodings/iso2022_jp_2.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_2004": {
      "file": "encodings/iso2022_jp_2004.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_3": {
      "file": "encodings/iso2022_jp_3.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_ext": {
      "file": "encodings/iso2022_jp_ext.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_kr": {
      "file": "encodings/iso2022_kr.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso8859_1": {
      "file": "encodings/iso8859_1.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_10": {
      "file": "encodings/iso8859_10.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_11": {
      "file": "encodings/iso8859_11.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_13": {
      "file": "encodings/iso8859_13.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_14": {
      "file": "encodings/iso8859_14.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_15": {
      "file": "encodings/iso8859_15.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_16": {
      "file": "encodings/iso8859_16.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_2": {
      "file": "encodings/iso8859_2.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_3": {
      "file": "encodings/iso8859_3.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_4": {
      "file": "encodings/iso8859_4.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_5": {
      "file": "encodings/iso8859_5.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_6": {
      "file": "encodings/iso8859_6.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_7": {
      "file": "encodings/iso8859_7.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_8": {
      "file": "encodings/iso8859_8.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_9": {
      "file": "encodings/iso8859_9.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.johab": {
      "file": "encodings/johab.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_kr"
      ]
    }, 
    "encodings.koi8_r": {
      "file": "encodings/koi8_r.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.koi8_u": {
      "file": "encodings/koi8_u.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.latin_1": {
      "file": "encodings/latin_1.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_arabic": {
      "file": "encodings/mac_arabic.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_centeuro": {
      "file": "encodings/mac_centeuro.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_croatian": {
      "file": "encodings/mac_croatian.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_cyrillic": {
      "file": "encodings/mac_cyrillic.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_farsi": {
      "file": "encodings/mac_farsi.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_greek": {
      "file": "encodings/mac_greek.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_iceland": {
      "file": "encodings/mac_iceland.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_latin2": {
      "file": "encodings/mac_latin2.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_roman": {
      "file": "encodings/mac_roman.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_romanian": {
      "file": "encodings/mac_romanian.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_turkish": {
      "file": "encodings/mac_turkish.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mbcs": {
      "file": "encodings/mbcs.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.palmos": {
      "file": "encodings/palmos.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.ptcp154": {
      "file": "encodings/ptcp154.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.punycode": {
      "file": "encodings/punycode.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.quopri_codec": {
      "file": "encodings/quopri_codec.py", 
      "imports": [
        "cStringIO.StringIO", 
        "codecs", 
        "quopri", 
        "StringIO"
      ]
    }, 
    "encodings.raw_unicode_escape": {
      "file": "encodings/raw_unicode_escape.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.rot_13": {
      "file": "encodings/rot_13.py", 
      "imports": [
        "codecs", 
        "sys"
      ]
    }, 
    "encodings.shift_jis": {
      "file": "encodings/shift_jis.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.shift_jis_2004": {
      "file": "encodings/shift_jis_2004.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.shift_jisx0213": {
      "file": "encodings/shift_jisx0213.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.string_escape": {
      "file": "encodings/string_escape.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.tis_620": {
      "file": "encodings/tis_620.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.undefined": {
      "file": "encodings/undefined.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.unicode_escape": {
      "file": "encodings/unicode_escape.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.unicode_internal": {
      "file": "encodings/unicode_internal.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_16": {
      "file": "encodings/utf_16.py", 
      "imports": [
        "codecs", 
        "sys"
      ]
    }, 
    "encodings.utf_16_be": {
      "file": "encodings/utf_16_be.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_16_le": {
      "file": "encodings/utf_16_le.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_32": {
      "file": "encodings/utf_32.py", 
      "imports": [
        "codecs", 
        "sys"
      ]
    }, 
    "encodings.utf_32_be": {
      "file": "encodings/utf_32_be.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_32_le": {
      "file": "encodings/utf_32_le.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_7": {
      "file": "encodings/utf_7.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_8": {
      "file": "encodings/utf_8.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_8_sig": {
      "file": "encodings/utf_8_sig.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.uu_codec": {
      "file": "encodings/uu_codec.py", 
      "imports": [
        "binascii", 
        "binascii.a2b_uu", 
        "binascii.b2a_uu", 
        "cStringIO.StringIO", 
        "codecs"
      ]
    }, 
    "encodings.zlib_codec": {
      "file": "encodings/zlib_codec.py", 
      "imports": [
        "codecs", 
        "zlib"
      ]
    }, 
    "filecmp": {
      "file": "filecmp.py", 
      "imports": [
        "itertools.ifilter", 
        "itertools.ifilterfalse", 
        "itertools.imap", 
        "itertools.izip", 
        "sys", 
        "getopt", 
        "os", 
        "stat"
      ]
    }, 
    "fileinput": {
      "file": "fileinput.py", 
      "imports": [
        "bz2", 
        "sys", 
        "getopt", 
        "gzip", 
        "io", 
        "os"
      ]
    }, 
    "fnmatch": {
      "file": "fnmatch.py", 
      "imports": [
        "os", 
        "posixpath", 
        "re"
      ]
    }, 
    "formatter": {
      "file": "formatter.py", 
      "imports": [
        "sys"
      ]
    }, 
    "fpformat": {
      "file": "fpformat.py", 
      "imports": [
        "re", 
        "warnings"
      ]
    }, 
    "fractions": {
      "file": "fractions.py", 
      "imports": [
        "__future__", 
        "decimal", 
        "math", 
        "operator", 
        "numbers", 
        "re"
      ]
    }, 
    "ftplib": {
      "file": "ftplib.py", 
      "imports": [
        "SOCKS", 
        "ssl", 
        "sys", 
        "os", 
        "re", 
        "socket"
      ]
    }, 
    "functools": {
      "file": "functools.py", 
      "imports": [
        "_functools"
      ]
    }, 
    "future_builtins": {
      "file": "future_builtins.py", 
      "imports": [
        "itertools.ifilter", 
        "itertools.imap", 
        "itertools.izip"
      ]
    }, 
    "gdbm": {
      "file": "gdbm.py", 
      "imports": [
        "cffi", 
        "os"
      ]
    }, 
    "genericpath": {
      "file": "genericpath.py", 
      "imports": [
        "os", 
        "stat"
      ]
    }, 
    "getopt": {
      "file": "getopt.py", 
      "imports": [
        "sys", 
        "os"
      ]
    }, 
    "getpass": {
      "file": "getpass.py", 
      "imports": [
        "EasyDialogs.AskPassword", 
        "msvcrt", 
        "sys", 
        "termios", 
        "os", 
        "warnings", 
        "pwd"
      ]
    }, 
    "gettext": {
      "file": "gettext.py", 
      "imports": [
        "__builtin__", 
        "cStringIO.StringIO", 
        "copy", 
        "errno.ENOENT", 
        "sys", 
        "token", 
        "locale", 
        "os", 
        "re", 
        "StringIO", 
        "struct", 
        "tokenize"
      ]
    }, 
    "glob": {
      "file": "glob.py", 
      "imports": [
        "fnmatch", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "greenlet": {
      "file": "greenlet.py", 
      "imports": [
        "_continuation", 
        "sys", 
        "threading.local"
      ]
    }, 
    "grp": {
      "file": "grp.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_structseq", 
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.c_char_p", 
        "ctypes.c_int", 
        "ctypes_support.standard_c_lib", 
        "sys"
      ]
    }, 
    "gzip": {
      "file": "gzip.py", 
      "imports": [
        "__builtin__", 
        "errno", 
        "sys", 
        "time", 
        "zlib", 
        "io", 
        "os", 
        "struct", 
        "warnings"
      ]
    }, 
    "hashlib": {
      "file": "hashlib.py", 
      "imports": [
        "_hashlib", 
        "_hashlib.pbkdf2_hmac", 
        "_md5", 
        "_sha", 
        "binascii", 
        "logging", 
        "struct", 
        "_sha256", 
        "_sha512"
      ]
    }, 
    "heapq": {
      "file": "heapq.py", 
      "imports": [
        "_heapq.*", 
        "doctest", 
        "itertools.chain", 
        "itertools.count", 
        "itertools.imap", 
        "itertools.islice", 
        "itertools.izip", 
        "itertools.tee", 
        "operator.itemgetter"
      ]
    }, 
    "hmac": {
      "file": "hmac.py", 
      "imports": [
        "hashlib", 
        "operator._compare_digest", 
        "warnings"
      ]
    }, 
    "htmlentitydefs": {
      "file": "htmlentitydefs.py", 
      "imports": []
    }, 
    "htmllib": {
      "file": "htmllib.py", 
      "imports": [
        "formatter", 
        "htmlentitydefs", 
        "sys", 
        "sgmllib", 
        "warnings"
      ]
    }, 
    "httplib": {
      "file": "httplib.py", 
      "imports": [
        "array.array", 
        "cStringIO.StringIO", 
        "ssl", 
        "sys.py3kwarning", 
        "mimetools", 
        "os", 
        "socket", 
        "StringIO", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "identity_dict": {
      "file": "identity_dict.py", 
      "imports": [
        "UserDict", 
        "__pypy__.identity_dict"
      ]
    }, 
    "ihooks": {
      "file": "ihooks.py", 
      "imports": [
        "__builtin__", 
        "imp", 
        "imp.C_BUILTIN", 
        "imp.C_EXTENSION", 
        "imp.PKG_DIRECTORY", 
        "imp.PY_COMPILED", 
        "imp.PY_FROZEN", 
        "imp.PY_SOURCE", 
        "marshal", 
        "sys", 
        "os", 
        "warnings"
      ]
    }, 
    "imaplib": {
      "file": "imaplib.py", 
      "imports": [
        "binascii", 
        "errno", 
        "getopt", 
        "getpass", 
        "hmac", 
        "ssl", 
        "subprocess", 
        "sys", 
        "time", 
        "random", 
        "re", 
        "socket"
      ]
    }, 
    "imghdr": {
      "file": "imghdr.py", 
      "imports": [
        "glob", 
        "sys", 
        "os"
      ]
    }, 
    "importlib": {
      "dir": "importlib"
    }, 
    "importlib.__init__": {
      "file": "importlib/__init__.py", 
      "imports": [
        "sys"
      ]
    }, 
    "imputil": {
      "file": "imputil.py", 
      "imports": [
        "__builtin__", 
        "dos.stat", 
        "imp", 
        "marshal", 
        "nt.stat", 
        "os2.stat", 
        "posix.stat", 
        "sys", 
        "struct", 
        "warnings"
      ]
    }, 
    "inspect": {
      "file": "inspect.py", 
      "imports": [
        "collections", 
        "dis", 
        "imp", 
        "operator.attrgetter", 
        "sys", 
        "linecache", 
        "os", 
        "re", 
        "string", 
        "tokenize", 
        "types"
      ]
    }, 
    "io": {
      "file": "io.py", 
      "imports": [
        "_io", 
        "_io.BlockingIOError", 
        "_io.BufferedRWPair", 
        "_io.BufferedRandom", 
        "_io.BufferedReader", 
        "_io.BufferedWriter", 
        "_io.BytesIO", 
        "_io.DEFAULT_BUFFER_SIZE", 
        "_io.FileIO", 
        "_io.IncrementalNewlineDecoder", 
        "_io.StringIO", 
        "_io.TextIOWrapper", 
        "_io.UnsupportedOperation", 
        "_io.open", 
        "abc"
      ]
    }, 
    "json": {
      "dir": "json"
    }, 
    "json.__init__": {
      "file": "json/__init__.py", 
      "imports": [
        "_pypyjson", 
        "json.decoder", 
        "json.encoder"
      ]
    }, 
    "json.decoder": {
      "file": "json/decoder.py", 
      "imports": [
        "_json.scanstring", 
        "json.scanner", 
        "sys", 
        "re", 
        "struct"
      ]
    }, 
    "json.encoder": {
      "file": "json/encoder.py", 
      "imports": [
        "__pypy__.builders.StringBuilder", 
        "__pypy__.builders.UnicodeBuilder", 
        "_pypyjson.raw_encode_basestring_ascii", 
        "re"
      ]
    }, 
    "json.scanner": {
      "file": "json/scanner.py", 
      "imports": [
        "_json.make_scanner", 
        "re"
      ]
    }, 
    "json.tests": {
      "dir": "json/tests"
    }, 
    "json.tests.__init__": {
      "file": "json/tests/__init__.py", 
      "imports": [
        "doctest", 
        "json", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "json.tests.test_check_circular": {
      "file": "json/tests/test_check_circular.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_decode": {
      "file": "json/tests/test_decode.py", 
      "imports": [
        "collections", 
        "decimal", 
        "json.tests", 
        "StringIO"
      ]
    }, 
    "json.tests.test_default": {
      "file": "json/tests/test_default.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_dump": {
      "file": "json/tests/test_dump.py", 
      "imports": [
        "cStringIO.StringIO", 
        "json.tests"
      ]
    }, 
    "json.tests.test_encode_basestring_ascii": {
      "file": "json/tests/test_encode_basestring_ascii.py", 
      "imports": [
        "collections", 
        "json.tests"
      ]
    }, 
    "json.tests.test_fail": {
      "file": "json/tests/test_fail.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_float": {
      "file": "json/tests/test_float.py", 
      "imports": [
        "json.tests", 
        "math"
      ]
    }, 
    "json.tests.test_indent": {
      "file": "json/tests/test_indent.py", 
      "imports": [
        "json.tests", 
        "StringIO", 
        "textwrap"
      ]
    }, 
    "json.tests.test_pass1": {
      "file": "json/tests/test_pass1.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_pass2": {
      "file": "json/tests/test_pass2.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_pass3": {
      "file": "json/tests/test_pass3.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_recursion": {
      "file": "json/tests/test_recursion.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_scanstring": {
      "file": "json/tests/test_scanstring.py", 
      "imports": [
        "json.tests", 
        "sys"
      ]
    }, 
    "json.tests.test_separators": {
      "file": "json/tests/test_separators.py", 
      "imports": [
        "json.tests", 
        "textwrap"
      ]
    }, 
    "json.tests.test_speedups": {
      "file": "json/tests/test_speedups.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_tool": {
      "file": "json/tests/test_tool.py", 
      "imports": [
        "subprocess", 
        "sys", 
        "os", 
        "test.test_support", 
        "test.script_helper", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "json.tests.test_unicode": {
      "file": "json/tests/test_unicode.py", 
      "imports": [
        "collections", 
        "json.tests"
      ]
    }, 
    "json.tool": {
      "file": "json/tool.py", 
      "imports": [
        "json", 
        "sys"
      ]
    }, 
    "keyword": {
      "file": "keyword.py", 
      "imports": [
        "sys", 
        "re"
      ]
    }, 
    "lib2to3": {
      "dir": "lib2to3"
    }, 
    "lib2to3.__init__": {
      "file": "lib2to3/__init__.py", 
      "imports": []
    }, 
    "lib2to3.__main__": {
      "file": "lib2to3/__main__.py", 
      "imports": [
        "lib2to3.main", 
        "sys"
      ]
    }, 
    "lib2to3.btm_matcher": {
      "file": "lib2to3/btm_matcher.py", 
      "imports": [
        "collections", 
        "itertools", 
        "lib2to3.btm_utils", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "logging"
      ]
    }, 
    "lib2to3.btm_utils": {
      "file": "lib2to3/btm_utils.py", 
      "imports": [
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixer_base": {
      "file": "lib2to3/fixer_base.py", 
      "imports": [
        "itertools", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp", 
        "lib2to3.pygram", 
        "logging"
      ]
    }, 
    "lib2to3.fixer_util": {
      "file": "lib2to3/fixer_util.py", 
      "imports": [
        "itertools.islice", 
        "lib2to3.patcomp", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes": {
      "dir": "lib2to3/fixes"
    }, 
    "lib2to3.fixes.__init__": {
      "file": "lib2to3/fixes/__init__.py", 
      "imports": []
    }, 
    "lib2to3.fixes.fix_apply": {
      "file": "lib2to3/fixes/fix_apply.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_basestring": {
      "file": "lib2to3/fixes/fix_basestring.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_buffer": {
      "file": "lib2to3/fixes/fix_buffer.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_callable": {
      "file": "lib2to3/fixes/fix_callable.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_dict": {
      "file": "lib2to3/fixes/fix_dict.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_except": {
      "file": "lib2to3/fixes/fix_except.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_exec": {
      "file": "lib2to3/fixes/fix_exec.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_execfile": {
      "file": "lib2to3/fixes/fix_execfile.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_exitfunc": {
      "file": "lib2to3/fixes/fix_exitfunc.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_filter": {
      "file": "lib2to3/fixes/fix_filter.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_funcattrs": {
      "file": "lib2to3/fixes/fix_funcattrs.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_future": {
      "file": "lib2to3/fixes/fix_future.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_getcwdu": {
      "file": "lib2to3/fixes/fix_getcwdu.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_has_key": {
      "file": "lib2to3/fixes/fix_has_key.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_idioms": {
      "file": "lib2to3/fixes/fix_idioms.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_import": {
      "file": "lib2to3/fixes/fix_import.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "os"
      ]
    }, 
    "lib2to3.fixes.fix_imports": {
      "file": "lib2to3/fixes/fix_imports.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_imports2": {
      "file": "lib2to3/fixes/fix_imports2.py", 
      "imports": [
        "lib2to3.fixes.fix_imports"
      ]
    }, 
    "lib2to3.fixes.fix_input": {
      "file": "lib2to3/fixes/fix_input.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp"
      ]
    }, 
    "lib2to3.fixes.fix_intern": {
      "file": "lib2to3/fixes/fix_intern.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_isinstance": {
      "file": "lib2to3/fixes/fix_isinstance.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_itertools": {
      "file": "lib2to3/fixes/fix_itertools.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_itertools_imports": {
      "file": "lib2to3/fixes/fix_itertools_imports.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_long": {
      "file": "lib2to3/fixes/fix_long.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_map": {
      "file": "lib2to3/fixes/fix_map.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram"
      ]
    }, 
    "lib2to3.fixes.fix_metaclass": {
      "file": "lib2to3/fixes/fix_metaclass.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pygram"
      ]
    }, 
    "lib2to3.fixes.fix_methodattrs": {
      "file": "lib2to3/fixes/fix_methodattrs.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_ne": {
      "file": "lib2to3/fixes/fix_ne.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_next": {
      "file": "lib2to3/fixes/fix_next.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram"
      ]
    }, 
    "lib2to3.fixes.fix_nonzero": {
      "file": "lib2to3/fixes/fix_nonzero.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_numliterals": {
      "file": "lib2to3/fixes/fix_numliterals.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_operator": {
      "file": "lib2to3/fixes/fix_operator.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_paren": {
      "file": "lib2to3/fixes/fix_paren.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_print": {
      "file": "lib2to3/fixes/fix_print.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_raise": {
      "file": "lib2to3/fixes/fix_raise.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_raw_input": {
      "file": "lib2to3/fixes/fix_raw_input.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_reduce": {
      "file": "lib2to3/fixes/fix_reduce.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_renames": {
      "file": "lib2to3/fixes/fix_renames.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_repr": {
      "file": "lib2to3/fixes/fix_repr.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_set_literal": {
      "file": "lib2to3/fixes/fix_set_literal.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_standarderror": {
      "file": "lib2to3/fixes/fix_standarderror.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_sys_exc": {
      "file": "lib2to3/fixes/fix_sys_exc.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_throw": {
      "file": "lib2to3/fixes/fix_throw.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_tuple_params": {
      "file": "lib2to3/fixes/fix_tuple_params.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_types": {
      "file": "lib2to3/fixes/fix_types.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_unicode": {
      "file": "lib2to3/fixes/fix_unicode.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_urllib": {
      "file": "lib2to3/fixes/fix_urllib.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.fixes.fix_imports"
      ]
    }, 
    "lib2to3.fixes.fix_ws_comma": {
      "file": "lib2to3/fixes/fix_ws_comma.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_xrange": {
      "file": "lib2to3/fixes/fix_xrange.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp"
      ]
    }, 
    "lib2to3.fixes.fix_xreadlines": {
      "file": "lib2to3/fixes/fix_xreadlines.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_zip": {
      "file": "lib2to3/fixes/fix_zip.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.main": {
      "file": "lib2to3/main.py", 
      "imports": [
        "__future__", 
        "difflib", 
        "lib2to3.refactor", 
        "sys", 
        "logging", 
        "optparse", 
        "os", 
        "shutil"
      ]
    }, 
    "lib2to3.patcomp": {
      "file": "lib2to3/patcomp.py", 
      "imports": [
        "lib2to3.pgen2.driver", 
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.literals", 
        "lib2to3.pgen2.parse", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "os", 
        "StringIO"
      ]
    }, 
    "lib2to3.pgen2": {
      "dir": "lib2to3/pgen2"
    }, 
    "lib2to3.pgen2.__init__": {
      "file": "lib2to3/pgen2/__init__.py", 
      "imports": []
    }, 
    "lib2to3.pgen2.conv": {
      "file": "lib2to3/pgen2/conv.py", 
      "imports": [
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.token", 
        "re"
      ]
    }, 
    "lib2to3.pgen2.driver": {
      "file": "lib2to3/pgen2/driver.py", 
      "imports": [
        "codecs", 
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.parse", 
        "lib2to3.pgen2.pgen", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "sys", 
        "logging", 
        "os", 
        "StringIO"
      ]
    }, 
    "lib2to3.pgen2.grammar": {
      "file": "lib2to3/pgen2/grammar.py", 
      "imports": [
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "pickle", 
        "pprint"
      ]
    }, 
    "lib2to3.pgen2.literals": {
      "file": "lib2to3/pgen2/literals.py", 
      "imports": [
        "re"
      ]
    }, 
    "lib2to3.pgen2.parse": {
      "file": "lib2to3/pgen2/parse.py", 
      "imports": [
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.pgen2.pgen": {
      "file": "lib2to3/pgen2/pgen.py", 
      "imports": [
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize"
      ]
    }, 
    "lib2to3.pgen2.token": {
      "file": "lib2to3/pgen2/token.py", 
      "imports": []
    }, 
    "lib2to3.pgen2.tokenize": {
      "file": "lib2to3/pgen2/tokenize.py", 
      "imports": [
        "codecs", 
        "lib2to3.pgen2.token", 
        "sys", 
        "re", 
        "string"
      ]
    }, 
    "lib2to3.pygram": {
      "file": "lib2to3/pygram.py", 
      "imports": [
        "lib2to3.pgen2.driver", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree", 
        "os"
      ]
    }, 
    "lib2to3.pytree": {
      "file": "lib2to3/pytree.py", 
      "imports": [
        "lib2to3.pygram", 
        "sys", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "lib2to3.refactor": {
      "file": "lib2to3/refactor.py", 
      "imports": [
        "__future__", 
        "codecs", 
        "collections", 
        "itertools.chain", 
        "lib2to3.btm_matcher", 
        "lib2to3.btm_utils", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.driver", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "multiprocessing", 
        "operator", 
        "sys", 
        "logging", 
        "os", 
        "StringIO"
      ]
    }, 
    "lib2to3.tests": {
      "dir": "lib2to3/tests"
    }, 
    "lib2to3.tests.__init__": {
      "file": "lib2to3/tests/__init__.py", 
      "imports": [
        "lib2to3.tests.support", 
        "os", 
        "types", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.pytree_idempotency": {
      "file": "lib2to3/tests/pytree_idempotency.py", 
      "imports": [
        "lib2to3.pgen2", 
        "lib2to3.pgen2.driver", 
        "lib2to3.pytree", 
        "lib2to3.tests.support", 
        "sys", 
        "logging", 
        "os"
      ]
    }, 
    "lib2to3.tests.support": {
      "file": "lib2to3/tests/support.py", 
      "imports": [
        "lib2to3.pgen2.driver", 
        "lib2to3.pytree", 
        "lib2to3.refactor", 
        "sys", 
        "os", 
        "re", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_all_fixers": {
      "file": "lib2to3/tests/test_all_fixers.py", 
      "imports": [
        "lib2to3.refactor", 
        "lib2to3.tests.support", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_fixers": {
      "file": "lib2to3/tests/test_fixers.py", 
      "imports": [
        "itertools.chain", 
        "lib2to3.fixer_util", 
        "lib2to3.fixes.fix_import", 
        "lib2to3.fixes.fix_imports", 
        "lib2to3.fixes.fix_imports2", 
        "lib2to3.fixes.fix_urllib", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "lib2to3.refactor", 
        "lib2to3.tests.support", 
        "operator.itemgetter", 
        "os", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_main": {
      "file": "lib2to3/tests/test_main.py", 
      "imports": [
        "codecs", 
        "lib2to3.main", 
        "sys", 
        "logging", 
        "os", 
        "re", 
        "shutil", 
        "StringIO", 
        "tempfile", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_parser": {
      "file": "lib2to3/tests/test_parser.py", 
      "imports": [
        "__future__", 
        "lib2to3.pgen2.parse", 
        "lib2to3.pgen2.tokenize", 
        "lib2to3.pygram", 
        "lib2to3.tests.support", 
        "sys", 
        "os"
      ]
    }, 
    "lib2to3.tests.test_pytree": {
      "file": "lib2to3/tests/test_pytree.py", 
      "imports": [
        "__future__", 
        "lib2to3.pytree", 
        "lib2to3.tests.support", 
        "sys", 
        "warnings"
      ]
    }, 
    "lib2to3.tests.test_refactor": {
      "file": "lib2to3/tests/test_refactor.py", 
      "imports": [
        "__future__", 
        "codecs", 
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram", 
        "lib2to3.refactor", 
        "lib2to3.tests.support", 
        "myfixes.fix_explicit.FixExplicit", 
        "myfixes.fix_first.FixFirst", 
        "myfixes.fix_last.FixLast", 
        "myfixes.fix_parrot.FixParrot", 
        "myfixes.fix_preorder.FixPreorder", 
        "operator", 
        "sys", 
        "os", 
        "shutil", 
        "StringIO", 
        "tempfile", 
        "unittest", 
        "warnings"
      ]
    }, 
    "lib2to3.tests.test_util": {
      "file": "lib2to3/tests/test_util.py", 
      "imports": [
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree", 
        "lib2to3.tests.support", 
        "os"
      ]
    }, 
    "linecache": {
      "file": "linecache.py", 
      "imports": [
        "sys", 
        "os"
      ]
    }, 
    "locale": {
      "file": "locale.py", 
      "imports": [
        "_locale", 
        "_locale.*", 
        "encodings", 
        "encodings.aliases", 
        "functools", 
        "operator", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "logging": {
      "dir": "logging"
    }, 
    "logging.__init__": {
      "file": "logging/__init__.py", 
      "imports": [
        "atexit", 
        "cStringIO", 
        "codecs", 
        "collections", 
        "sys", 
        "thread", 
        "threading", 
        "time", 
        "os", 
        "traceback", 
        "warnings", 
        "weakref"
      ]
    }, 
    "logging.config": {
      "file": "logging/config.py", 
      "imports": [
        "ConfigParser", 
        "cStringIO", 
        "errno", 
        "io", 
        "json", 
        "logging", 
        "logging.handlers", 
        "select", 
        "sys", 
        "thread", 
        "threading", 
        "os", 
        "re", 
        "socket", 
        "SocketServer", 
        "struct", 
        "tempfile", 
        "traceback", 
        "types"
      ]
    }, 
    "logging.handlers": {
      "file": "logging/handlers.py", 
      "imports": [
        "codecs", 
        "email.utils", 
        "errno", 
        "httplib", 
        "logging", 
        "time", 
        "win32evtlog", 
        "win32evtlogutil", 
        "os", 
        "re", 
        "smtplib", 
        "socket", 
        "stat", 
        "struct", 
        "urllib", 
        "cPickle"
      ]
    }, 
    "macurl2path": {
      "file": "macurl2path.py", 
      "imports": [
        "os", 
        "urllib"
      ]
    }, 
    "mailbox": {
      "file": "mailbox.py", 
      "imports": [
        "calendar", 
        "copy", 
        "email", 
        "email.generator", 
        "email.message", 
        "errno", 
        "fcntl", 
        "sys", 
        "time", 
        "os", 
        "re", 
        "rfc822", 
        "socket", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "mailcap": {
      "file": "mailcap.py", 
      "imports": [
        "sys", 
        "os"
      ]
    }, 
    "markupbase": {
      "file": "markupbase.py", 
      "imports": [
        "re"
      ]
    }, 
    "marshal": {
      "file": "marshal.py", 
      "imports": [
        "_marshal"
      ]
    }, 
    "md5": {
      "file": "md5.py", 
      "imports": [
        "hashlib", 
        "warnings"
      ]
    }, 
    "mhlib": {
      "file": "mhlib.py", 
      "imports": [
        "bisect", 
        "cStringIO.StringIO", 
        "sys", 
        "mimetools", 
        "multifile", 
        "os", 
        "re", 
        "shutil", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "mimetools": {
      "file": "mimetools.py", 
      "imports": [
        "base64", 
        "dummy_thread", 
        "sys", 
        "thread", 
        "time", 
        "os", 
        "quopri", 
        "rfc822", 
        "socket", 
        "tempfile", 
        "uu", 
        "warnings"
      ]
    }, 
    "mimetypes": {
      "file": "mimetypes.py", 
      "imports": [
        "_winreg", 
        "getopt", 
        "sys", 
        "os", 
        "posixpath", 
        "urllib"
      ]
    }, 
    "mimify": {
      "file": "mimify.py", 
      "imports": [
        "base64", 
        "getopt", 
        "sys", 
        "os", 
        "re", 
        "warnings"
      ]
    }, 
    "modulefinder": {
      "file": "modulefinder.py", 
      "imports": [
        "__future__", 
        "dis", 
        "getopt", 
        "imp", 
        "marshal", 
        "sys", 
        "os", 
        "struct", 
        "types"
      ]
    }, 
    "multifile": {
      "file": "multifile.py", 
      "imports": [
        "warnings"
      ]
    }, 
    "mutex": {
      "file": "mutex.py", 
      "imports": [
        "collections", 
        "warnings"
      ]
    }, 
    "netrc": {
      "file": "netrc.py", 
      "imports": [
        "os", 
        "shlex", 
        "stat", 
        "pwd"
      ]
    }, 
    "new": {
      "file": "new.py", 
      "imports": [
        "types", 
        "warnings"
      ]
    }, 
    "nntplib": {
      "file": "nntplib.py", 
      "imports": [
        "netrc", 
        "os", 
        "re", 
        "socket"
      ]
    }, 
    "nturl2path": {
      "file": "nturl2path.py", 
      "imports": [
        "string", 
        "urllib"
      ]
    }, 
    "numbers": {
      "file": "numbers.py", 
      "imports": [
        "__future__", 
        "abc"
      ]
    }, 
    "opcode": {
      "file": "opcode.py", 
      "imports": []
    }, 
    "optparse": {
      "file": "optparse.py", 
      "imports": [
        "__builtin__", 
        "gettext", 
        "sys", 
        "os", 
        "textwrap", 
        "types"
      ]
    }, 
    "os": {
      "file": "os.py", 
      "imports": [
        "_emx_link.link", 
        "ce", 
        "ce.*", 
        "ce._exit", 
        "copy_reg", 
        "errno", 
        "nt", 
        "nt.*", 
        "nt._exit", 
        "ntpath", 
        "os", 
        "os2", 
        "os2.*", 
        "os2._exit", 
        "os2emxpath", 
        "posix", 
        "posix.*", 
        "posix._exit", 
        "riscos", 
        "riscos.*", 
        "riscos._exit", 
        "riscosenviron._Environ", 
        "riscospath", 
        "subprocess", 
        "sys", 
        "posixpath", 
        "UserDict", 
        "warnings"
      ]
    }, 
    "pdb": {
      "file": "pdb.py", 
      "imports": [
        "__main__", 
        "bdb", 
        "cmd", 
        "linecache", 
        "os", 
        "pdb", 
        "readline", 
        "sys", 
        "pprint", 
        "re", 
        "repr", 
        "shlex", 
        "traceback"
      ]
    }, 
    "pickle": {
      "file": "pickle.py", 
      "imports": [
        "binascii", 
        "cStringIO.StringIO", 
        "copy_reg", 
        "doctest", 
        "marshal", 
        "org.python.core.PyStringMap", 
        "sys", 
        "re", 
        "StringIO", 
        "struct", 
        "types"
      ]
    }, 
    "pickletools": {
      "file": "pickletools.py", 
      "imports": [
        "cStringIO", 
        "doctest", 
        "pickle", 
        "re", 
        "struct"
      ]
    }, 
    "pipes": {
      "file": "pipes.py", 
      "imports": [
        "os", 
        "re", 
        "string", 
        "tempfile"
      ]
    }, 
    "pkgutil": {
      "file": "pkgutil.py", 
      "imports": [
        "imp", 
        "inspect", 
        "marshal", 
        "os", 
        "sys", 
        "zipimport", 
        "zipimport.zipimporter", 
        "types"
      ]
    }, 
    "platform": {
      "file": "platform.py", 
      "imports": [
        "MacOS", 
        "_winreg", 
        "gestalt", 
        "gestalt.gestalt", 
        "java.lang", 
        "java.lang.System", 
        "os", 
        "subprocess", 
        "sys", 
        "vms_lib", 
        "win32api", 
        "win32api.GetVersionEx", 
        "win32api.RegCloseKey", 
        "win32api.RegOpenKeyEx", 
        "win32api.RegQueryValueEx", 
        "win32con.HKEY_LOCAL_MACHINE", 
        "win32con.VER_NT_WORKSTATION", 
        "win32con.VER_PLATFORM_WIN32_NT", 
        "win32con.VER_PLATFORM_WIN32_WINDOWS", 
        "win32pipe", 
        "plistlib", 
        "re", 
        "socket", 
        "string", 
        "struct", 
        "tempfile"
      ]
    }, 
    "plistlib": {
      "file": "plistlib.py", 
      "imports": [
        "Carbon.File.FSGetResourceForkName", 
        "Carbon.File.FSRef", 
        "Carbon.Files.fsRdPerm", 
        "Carbon.Files.fsRdWrPerm", 
        "Carbon.Res", 
        "binascii", 
        "cStringIO.StringIO", 
        "re", 
        "warnings", 
        "xml.parsers.expat", 
        "datetime"
      ]
    }, 
    "popen2": {
      "file": "popen2.py", 
      "imports": [
        "os", 
        "sys", 
        "warnings"
      ]
    }, 
    "poplib": {
      "file": "poplib.py", 
      "imports": [
        "hashlib", 
        "ssl", 
        "sys", 
        "re", 
        "socket"
      ]
    }, 
    "posixfile": {
      "file": "posixfile.py", 
      "imports": [
        "__builtin__", 
        "fcntl", 
        "os", 
        "posix", 
        "sys", 
        "struct", 
        "types", 
        "warnings"
      ]
    }, 
    "posixpath": {
      "file": "posixpath.py", 
      "imports": [
        "genericpath", 
        "os", 
        "sys", 
        "re", 
        "stat", 
        "warnings", 
        "pwd"
      ]
    }, 
    "pprint": {
      "file": "pprint.py", 
      "imports": [
        "cStringIO.StringIO", 
        "sys", 
        "time", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "profile": {
      "file": "profile.py", 
      "imports": [
        "__main__", 
        "marshal", 
        "optparse", 
        "os", 
        "sys", 
        "time", 
        "pstats", 
        "resource"
      ]
    }, 
    "pstats": {
      "file": "pstats.py", 
      "imports": [
        "cmd", 
        "functools", 
        "marshal", 
        "os", 
        "readline", 
        "sys", 
        "time", 
        "re"
      ]
    }, 
    "pty": {
      "file": "pty.py", 
      "imports": [
        "fcntl.I_PUSH", 
        "fcntl.ioctl", 
        "os", 
        "select.select", 
        "sgi", 
        "tty"
      ]
    }, 
    "pwd": {
      "file": "pwd.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_structseq", 
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.c_char_p", 
        "ctypes.c_int", 
        "ctypes.c_long", 
        "ctypes_support.standard_c_lib", 
        "os", 
        "sys"
      ]
    }, 
    "py_compile": {
      "file": "py_compile.py", 
      "imports": [
        "__builtin__", 
        "imp", 
        "marshal", 
        "os", 
        "sys", 
        "traceback"
      ]
    }, 
    "pyclbr": {
      "file": "pyclbr.py", 
      "imports": [
        "imp", 
        "operator.itemgetter", 
        "os", 
        "sys", 
        "token.DEDENT", 
        "token.NAME", 
        "token.OP", 
        "tokenize"
      ]
    }, 
    "pydoc": {
      "file": "pydoc.py", 
      "imports": [
        "BaseHTTPServer", 
        "Tkinter", 
        "__builtin__", 
        "collections", 
        "formatter", 
        "getopt", 
        "imp", 
        "inspect", 
        "locale", 
        "mimetools", 
        "nturl2path", 
        "os", 
        "pkgutil", 
        "select", 
        "sys", 
        "threading", 
        "pydoc_data.topics", 
        "re", 
        "repr", 
        "string", 
        "StringIO", 
        "tempfile", 
        "traceback", 
        "tty", 
        "types", 
        "warnings", 
        "webbrowser"
      ]
    }, 
    "pydoc_data": {
      "dir": "pydoc_data"
    }, 
    "pydoc_data.__init__": {
      "file": "pydoc_data/__init__.py", 
      "imports": []
    }, 
    "pydoc_data.topics": {
      "file": "pydoc_data/topics.py", 
      "imports": []
    }, 
    "pyrepl": {
      "dir": "pyrepl"
    }, 
    "pyrepl.__init__": {
      "file": "pyrepl/__init__.py", 
      "imports": []
    }, 
    "pyrepl.cmdrepl": {
      "file": "pyrepl/cmdrepl.py", 
      "imports": [
        "__future__", 
        "cmd", 
        "pyrepl.completer", 
        "pyrepl.completing_reader", 
        "pyrepl.reader"
      ]
    }, 
    "pyrepl.commands": {
      "file": "pyrepl/commands.py", 
      "imports": [
        "os", 
        "pyrepl.input", 
        "signal", 
        "sys"
      ]
    }, 
    "pyrepl.completer": {
      "file": "pyrepl/completer.py", 
      "imports": [
        "__builtin__", 
        "keyword", 
        "re"
      ]
    }, 
    "pyrepl.completing_reader": {
      "file": "pyrepl/completing_reader.py", 
      "imports": [
        "pyrepl.commands", 
        "pyrepl.reader", 
        "re"
      ]
    }, 
    "pyrepl.console": {
      "file": "pyrepl/console.py", 
      "imports": []
    }, 
    "pyrepl.copy_code": {
      "file": "pyrepl/copy_code.py", 
      "imports": [
        "new"
      ]
    }, 
    "pyrepl.curses": {
      "file": "pyrepl/curses.py", 
      "imports": [
        "_curses", 
        "_minimal_curses", 
        "pyrepl.curses", 
        "sys"
      ]
    }, 
    "pyrepl.fancy_termios": {
      "file": "pyrepl/fancy_termios.py", 
      "imports": [
        "termios"
      ]
    }, 
    "pyrepl.historical_reader": {
      "file": "pyrepl/historical_reader.py", 
      "imports": [
        "pyrepl.commands", 
        "pyrepl.input", 
        "pyrepl.reader", 
        "pyrepl.unix_console"
      ]
    }, 
    "pyrepl.input": {
      "file": "pyrepl/input.py", 
      "imports": [
        "pyrepl.keymap", 
        "pyrepl.unicodedata_"
      ]
    }, 
    "pyrepl.keymap": {
      "file": "pyrepl/keymap.py", 
      "imports": []
    }, 
    "pyrepl.keymaps": {
      "file": "pyrepl/keymaps.py", 
      "imports": []
    }, 
    "pyrepl.module_lister": {
      "file": "pyrepl/module_lister.py", 
      "imports": [
        "imp", 
        "os", 
        "sys"
      ]
    }, 
    "pyrepl.pygame_console": {
      "file": "pyrepl/pygame_console.py", 
      "imports": [
        "pygame", 
        "pygame.locals.*", 
        "pyrepl.console", 
        "pyrepl.pygame_keymap", 
        "pyrepl.reader", 
        "types"
      ]
    }, 
    "pyrepl.pygame_keymap": {
      "file": "pyrepl/pygame_keymap.py", 
      "imports": [
        "pygame.locals.*"
      ]
    }, 
    "pyrepl.python_reader": {
      "file": "pyrepl/python_reader.py", 
      "imports": [
        "_tkinter", 
        "atexit", 
        "cPickle", 
        "cocoasupport.CocoaInteracter", 
        "code", 
        "imp", 
        "locale", 
        "new", 
        "os", 
        "pickle", 
        "pyrepl.commands", 
        "pyrepl.completer", 
        "pyrepl.completing_reader", 
        "pyrepl.copy_code", 
        "pyrepl.historical_reader", 
        "pyrepl.module_lister", 
        "pyrepl.pygame_console", 
        "pyrepl.reader", 
        "pyrepl.unix_console", 
        "re", 
        "signal", 
        "sys", 
        "traceback", 
        "twisted.internet.abstract.FileDescriptor", 
        "twisted.internet.reactor", 
        "warnings"
      ]
    }, 
    "pyrepl.reader": {
      "file": "pyrepl/reader.py", 
      "imports": [
        "_pyrepl_utils.disp_str", 
        "_pyrepl_utils.init_unctrl_map", 
        "pyrepl.commands", 
        "pyrepl.input", 
        "pyrepl.unicodedata_", 
        "pyrepl.unix_console", 
        "re", 
        "types"
      ]
    }, 
    "pyrepl.readline": {
      "file": "pyrepl/readline.py", 
      "imports": [
        "__builtin__", 
        "os", 
        "pyrepl.commands", 
        "pyrepl.completing_reader", 
        "pyrepl.historical_reader", 
        "pyrepl.unix_console", 
        "sys", 
        "warnings"
      ]
    }, 
    "pyrepl.simple_interact": {
      "file": "pyrepl/simple_interact.py", 
      "imports": [
        "__main__", 
        "code", 
        "pyrepl.readline", 
        "sys"
      ]
    }, 
    "pyrepl.unicodedata_": {
      "file": "pyrepl/unicodedata_.py", 
      "imports": [
        "unicodedata.*"
      ]
    }, 
    "pyrepl.unix_console": {
      "file": "pyrepl/unix_console.py", 
      "imports": [
        "errno", 
        "fcntl.ioctl", 
        "os", 
        "pyrepl.console", 
        "pyrepl.curses", 
        "pyrepl.fancy_termios", 
        "pyrepl.unix_eventqueue", 
        "re", 
        "select", 
        "signal", 
        "struct", 
        "sys", 
        "termios", 
        "time"
      ]
    }, 
    "pyrepl.unix_eventqueue": {
      "file": "pyrepl/unix_eventqueue.py", 
      "imports": [
        "os", 
        "pyrepl.console", 
        "pyrepl.curses", 
        "pyrepl.keymap", 
        "termios.VERASE", 
        "termios.tcgetattr"
      ]
    }, 
    "quopri": {
      "file": "quopri.py", 
      "imports": [
        "binascii.a2b_qp", 
        "binascii.b2a_qp", 
        "cStringIO.StringIO", 
        "getopt", 
        "sys"
      ]
    }, 
    "random": {
      "file": "random.py", 
      "imports": [
        "__future__", 
        "_random", 
        "binascii.hexlify", 
        "hashlib", 
        "math.acos", 
        "math.ceil", 
        "math.cos", 
        "math.e", 
        "math.exp", 
        "math.log", 
        "math.pi", 
        "math.sin", 
        "math.sqrt", 
        "os", 
        "time", 
        "warnings"
      ]
    }, 
    "re": {
      "file": "re.py", 
      "imports": [
        "copy_reg", 
        "sys", 
        "sre_compile", 
        "sre_constants", 
        "sre_parse"
      ]
    }, 
    "repr": {
      "file": "repr.py", 
      "imports": [
        "__builtin__", 
        "itertools.islice"
      ]
    }, 
    "resource": {
      "file": "resource.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_structseq", 
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.byref", 
        "ctypes.c_int", 
        "ctypes.c_long", 
        "ctypes_config_cache._resource_cache", 
        "ctypes_support.get_errno", 
        "ctypes_support.standard_c_lib", 
        "errno.EINVAL", 
        "errno.EPERM", 
        "os", 
        "sys"
      ]
    }, 
    "rexec": {
      "file": "rexec.py", 
      "imports": [
        "__builtin__", 
        "code", 
        "getopt", 
        "ihooks", 
        "imp", 
        "os", 
        "readline", 
        "sys", 
        "traceback", 
        "warnings"
      ]
    }, 
    "rfc822": {
      "file": "rfc822.py", 
      "imports": [
        "os", 
        "sys", 
        "time", 
        "warnings"
      ]
    }, 
    "rlcompleter": {
      "file": "rlcompleter.py", 
      "imports": [
        "__builtin__", 
        "__main__", 
        "keyword", 
        "re", 
        "readline"
      ]
    }, 
    "robotparser": {
      "file": "robotparser.py", 
      "imports": [
        "time", 
        "urllib", 
        "urlparse"
      ]
    }, 
    "runpy": {
      "file": "runpy.py", 
      "imports": [
        "imp", 
        "imp.get_loader", 
        "pkgutil", 
        "sys"
      ]
    }, 
    "sched": {
      "file": "sched.py", 
      "imports": [
        "collections", 
        "heapq"
      ]
    }, 
    "sets": {
      "file": "sets.py", 
      "imports": [
        "copy", 
        "itertools.ifilter", 
        "itertools.ifilterfalse", 
        "warnings"
      ]
    }, 
    "sgmllib": {
      "file": "sgmllib.py", 
      "imports": [
        "markupbase", 
        "re", 
        "sys", 
        "warnings"
      ]
    }, 
    "sha": {
      "file": "sha.py", 
      "imports": [
        "hashlib", 
        "warnings"
      ]
    }, 
    "shelve": {
      "file": "shelve.py", 
      "imports": [
        "anydbm", 
        "cStringIO.StringIO", 
        "pickle", 
        "StringIO", 
        "UserDict", 
        "cPickle"
      ]
    }, 
    "shlex": {
      "file": "shlex.py", 
      "imports": [
        "cStringIO.StringIO", 
        "collections", 
        "os", 
        "sys", 
        "StringIO"
      ]
    }, 
    "shutil": {
      "file": "shutil.py", 
      "imports": [
        "collections", 
        "distutils.errors", 
        "distutils.spawn", 
        "errno", 
        "fnmatch", 
        "os", 
        "sys", 
        "stat", 
        "tarfile", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "simso": {
      "dir": "simso"
    }, 
    "simso.__init__": {
      "file": "simso/__init__.py", 
      "imports": []
    }, 
    "simso.configuration": {
      "dir": "simso/configuration"
    }, 
    "simso.configuration.Configuration": {
      "file": "simso/configuration/Configuration.py", 
      "imports": [
        "fractions", 
        "os", 
        "re", 
        "simso.configuration.GenerateConfiguration", 
        "simso.configuration.parser", 
        "simso.core.Processor", 
        "simso.core.Scheduler", 
        "simso.core.Task", 
        "xml.dom.minidom"
      ]
    }, 
    "simso.configuration.GenerateConfiguration": {
      "file": "simso/configuration/GenerateConfiguration.py", 
      "imports": [
        "os", 
        "xml.dom.minidom", 
        "xml.etree.ElementTree"
      ]
    }, 
    "simso.configuration.__init__": {
      "file": "simso/configuration/__init__.py", 
      "imports": [
        "simso.configuration.Configuration"
      ]
    }, 
    "simso.configuration.parser": {
      "file": "simso/configuration/parser.py", 
      "imports": [
        "os", 
        "simso.core.Caches", 
        "simso.core.Processor", 
        "simso.core.Scheduler", 
        "simso.core.Task", 
        "xml.dom.minidom"
      ]
    }, 
    "simso.core": {
      "dir": "simso/core"
    }, 
    "simso.core.CSDP": {
      "file": "simso/core/CSDP.py", 
      "imports": []
    }, 
    "simso.core.Caches": {
      "file": "simso/core/Caches.py", 
      "imports": []
    }, 
    "simso.core.Job": {
      "file": "simso/core/Job.py", 
      "imports": [
        "math.ceil", 
        "simso.core.JobEvent", 
        "SimPy.Simulation"
      ]
    }, 
    "simso.core.JobEvent": {
      "file": "simso/core/JobEvent.py", 
      "imports": []
    }, 
    "simso.core.Logger": {
      "file": "simso/core/Logger.py", 
      "imports": [
        "SimPy.Simulation"
      ]
    }, 
    "simso.core.Model": {
      "file": "simso/core/Model.py", 
      "imports": [
        "simso.core.Logger", 
        "simso.core.Processor", 
        "simso.core.Task", 
        "simso.core.Timer", 
        "simso.core.etm", 
        "simso.core.results", 
        "SimPy.Simulation"
      ]
    }, 
    "simso.core.ProcEvent": {
      "file": "simso/core/ProcEvent.py", 
      "imports": []
    }, 
    "simso.core.Processor": {
      "file": "simso/core/Processor.py", 
      "imports": [
        "collections", 
        "simso.core.ProcEvent", 
        "SimPy.Simulation"
      ]
    }, 
    "simso.core.Scheduler": {
      "file": "simso/core/Scheduler.py", 
      "imports": [
        "__future__", 
        "imp", 
        "importlib", 
        "inspect", 
        "os", 
        "pkgutil", 
        "pyi_importers", 
        "simso.core.SchedulerEvent", 
        "simso.schedulers",
        "sys", 
        "SimPy.Simulation"
      ]
    }, 
    "simso.core.SchedulerEvent": {
      "file": "simso/core/SchedulerEvent.py", 
      "imports": []
    }, 
    "simso.core.Task": {
      "file": "simso/core/Task.py", 
      "imports": [
        "collections", 
        "os", 
        "simso.core.CSDP", 
        "simso.core.Job", 
        "simso.core.Timer", 
        "SimPy.Simulation"
      ]
    }, 
    "simso.core.Timer": {
      "file": "simso/core/Timer.py", 
      "imports": [
        "SimPy.Simulation"
      ]
    }, 
    "simso.core.__init__": {
      "file": "simso/core/__init__.py", 
      "imports": [
        "simso.core.JobEvent", 
        "simso.core.Model", 
        "simso.core.ProcEvent", 
        "simso.core.Processor", 
        "simso.core.Scheduler", 
        "simso.core.Timer", 
        "simso.core.results"
      ]
    }, 
    "simso.core.etm": {
      "dir": "simso/core/etm"
    }, 
    "simso.core.etm.ACET": {
      "file": "simso/core/etm/ACET.py", 
      "imports": [
        "random", 
        "simso.core.etm.AbstractExecutionTimeModel"
      ]
    }, 
    "simso.core.etm.AbstractExecutionTimeModel": {
      "file": "simso/core/etm/AbstractExecutionTimeModel.py", 
      "imports": [
        "abc"
      ]
    }, 
    "simso.core.etm.CacheModel": {
      "file": "simso/core/etm/CacheModel.py", 
      "imports": [
        "simso.core.etm.AbstractExecutionTimeModel"
      ]
    }, 
    "simso.core.etm.FixedPenalty": {
      "file": "simso/core/etm/FixedPenalty.py", 
      "imports": [
        "simso.core.etm.AbstractExecutionTimeModel"
      ]
    }, 
    "simso.core.etm.WCET": {
      "file": "simso/core/etm/WCET.py", 
      "imports": [
        "simso.core.etm.AbstractExecutionTimeModel"
      ]
    }, 
    "simso.core.etm.__init__": {
      "file": "simso/core/etm/__init__.py", 
      "imports": [
        "simso.core.etm.ACET", 
        "simso.core.etm.CacheModel", 
        "simso.core.etm.FixedPenalty", 
        "simso.core.etm.WCET"
      ]
    }, 
    "simso.core.results": {
      "file": "simso/core/results.py", 
      "imports": [
        "simso.core.JobEvent", 
        "simso.core.ProcEvent", 
        "simso.core.SchedulerEvent"
      ]
    }, 
    "simso.generator": {
      "dir": "simso/generator"
    }, 
    "simso.generator.__init__": {
      "file": "simso/generator/__init__.py", 
      "imports": []
    }, 
    "simso.generator.task_generator": {
      "file": "simso/generator/task_generator.py", 
      "imports": [
        "math", 
        "numpy", 
        "random"
      ]
    }, 
    "simso.schedulers": {
      "dir": "simso/schedulers"
    }, 
    "simso.schedulers.BF": {
      "file": "simso/schedulers/BF.py", 
      "imports": [
        "fractions", 
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.CC_EDF": {
      "file": "simso/schedulers/CC_EDF.py", 
      "imports": [
        "simso.core.Scheduler"
      ]
    }, 
    "simso.schedulers.DP_WRAP": {
      "file": "simso/schedulers/DP_WRAP.py", 
      "imports": [
        "math.ceil", 
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.EDCL": {
      "file": "simso/schedulers/EDCL.py", 
      "imports": [
        "simso.core.Scheduler"
      ]
    }, 
    "simso.schedulers.EDF": {
      "file": "simso/schedulers/EDF.py", 
      "imports": [
        "simso.core.Scheduler"
      ]
    }, 
    "simso.schedulers.EDF2": {
      "file": "simso/schedulers/EDF2.py", 
      "imports": [
        "simso.core.Scheduler"
      ]
    }, 
    "simso.schedulers.EDF_US": {
      "file": "simso/schedulers/EDF_US.py", 
      "imports": [
        "simso.core.Scheduler"
      ]
    }, 
    "simso.schedulers.EDF_mono": {
      "file": "simso/schedulers/EDF_mono.py", 
      "imports": [
        "simso.core.Scheduler"
      ]
    }, 
    "simso.schedulers.EDHS": {
      "file": "simso/schedulers/EDHS.py", 
      "imports": [
        "fractions", 
        "math.ceil", 
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.EDZL": {
      "file": "simso/schedulers/EDZL.py", 
      "imports": [
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.EKG": {
      "file": "simso/schedulers/EKG.py", 
      "imports": [
        "fractions", 
        "math.ceil", 
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.EPDF": {
      "file": "simso/schedulers/EPDF.py", 
      "imports": [
        "math.ceil", 
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.ER_PD2": {
      "file": "simso/schedulers/ER_PD2.py", 
      "imports": [
        "math.ceil", 
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.FP": {
      "file": "simso/schedulers/FP.py", 
      "imports": [
        "simso.core.Scheduler"
      ]
    }, 
    "simso.schedulers.Fixed_PEDF": {
      "file": "simso/schedulers/Fixed_PEDF.py", 
      "imports": [
        "simso.core.Scheduler", 
        "simso.schedulers.EDF_mono", 
        "simso.utils.PartitionedScheduler"
      ]
    }, 
    "simso.schedulers.G_FL": {
      "file": "simso/schedulers/G_FL.py", 
      "imports": [
        "simso.core.Scheduler"
      ]
    }, 
    "simso.schedulers.G_FL_ZL": {
      "file": "simso/schedulers/G_FL_ZL.py", 
      "imports": [
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.LB_P_EDF": {
      "file": "simso/schedulers/LB_P_EDF.py", 
      "imports": [
        "simso.core.Scheduler", 
        "simso.schedulers.EDF_mono", 
        "simso.utils.PartitionedScheduler"
      ]
    }, 
    "simso.schedulers.LLF": {
      "file": "simso/schedulers/LLF.py", 
      "imports": [
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.LLREF": {
      "file": "simso/schedulers/LLREF.py", 
      "imports": [
        "math.ceil", 
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.LLREF2": {
      "file": "simso/schedulers/LLREF2.py", 
      "imports": [
        "math.ceil", 
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.LRE_TL": {
      "file": "simso/schedulers/LRE_TL.py", 
      "imports": [
        "heapq", 
        "math.ceil", 
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.MLLF": {
      "file": "simso/schedulers/MLLF.py", 
      "imports": [
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.NVNLF": {
      "file": "simso/schedulers/NVNLF.py", 
      "imports": [
        "math.ceil", 
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.PD2": {
      "file": "simso/schedulers/PD2.py", 
      "imports": [
        "math.ceil", 
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.P_EDF": {
      "file": "simso/schedulers/P_EDF.py", 
      "imports": [
        "simso.core.Scheduler", 
        "simso.schedulers.EDF_mono", 
        "simso.utils.PartitionedScheduler"
      ]
    }, 
    "simso.schedulers.P_EDF2": {
      "file": "simso/schedulers/P_EDF2.py", 
      "imports": [
        "simso.core.Scheduler", 
        "simso.schedulers.EDF_mono"
      ]
    }, 
    "simso.schedulers.P_EDF_WF": {
      "file": "simso/schedulers/P_EDF_WF.py", 
      "imports": [
        "simso.core.Scheduler", 
        "simso.schedulers.EDF_mono", 
        "simso.utils.PartitionedScheduler"
      ]
    }, 
    "simso.schedulers.P_RM": {
      "file": "simso/schedulers/P_RM.py", 
      "imports": [
        "simso.core.Scheduler", 
        "simso.schedulers.RM_mono", 
        "simso.utils.PartitionedScheduler"
      ]
    }, 
    "simso.schedulers.PriD": {
      "file": "simso/schedulers/PriD.py", 
      "imports": [
        "math.ceil", 
        "simso.core.Scheduler"
      ]
    }, 
    "simso.schedulers.RM": {
      "file": "simso/schedulers/RM.py", 
      "imports": [
        "simso.core.Scheduler"
      ]
    }, 
    "simso.schedulers.RM_mono": {
      "file": "simso/schedulers/RM_mono.py", 
      "imports": [
        "simso.core.Scheduler"
      ]
    }, 
    "simso.schedulers.RUN": {
      "file": "simso/schedulers/RUN.py", 
      "imports": [
        "collections", 
        "simso.core.Scheduler", 
        "simso.core.Timer", 
        "simso.schedulers.RUNServer"
      ]
    }, 
    "simso.schedulers.RUNServer": {
      "file": "simso/schedulers/RUNServer.py", 
      "imports": [
        "fractions"
      ]
    }, 
    "simso.schedulers.Static_EDF": {
      "file": "simso/schedulers/Static_EDF.py", 
      "imports": [
        "simso.core.Scheduler"
      ]
    }, 
    "simso.schedulers.U_EDF": {
      "file": "simso/schedulers/U_EDF.py", 
      "imports": [
        "math.ceil", 
        "simso.core.Scheduler", 
        "simso.core.Timer"
      ]
    }, 
    "simso.schedulers.WC_RUN": {
      "file": "simso/schedulers/WC_RUN.py", 
      "imports": [
        "simso.schedulers.RUN"
      ]
    }, 
    "simso.schedulers.WC_U_EDF": {
      "file": "simso/schedulers/WC_U_EDF.py", 
      "imports": [
        "simso.schedulers.U_EDF"
      ]
    }, 
    "simso.schedulers.__init__": {
      "file": "simso/schedulers/__init__.py", 
      "imports": []
    }, 
    "simso.utils": {
      "dir": "simso/utils"
    }, 
    "simso.utils.PartitionedScheduler": {
      "file": "simso/utils/PartitionedScheduler.py", 
      "imports": [
        "simso.core.Scheduler"
      ]
    }, 
    "simso.utils.SchedulingTests": {
      "file": "simso/utils/SchedulingTests.py", 
      "imports": []
    }, 
    "simso.utils.__init__": {
      "file": "simso/utils/__init__.py", 
      "imports": [
        "simso.utils.PartitionedScheduler"
      ]
    }, 
    "site": {
      "file": "site.py", 
      "imports": [
        "__builtin__", 
        "codecs", 
        "distutils.sysconfig", 
        "encodings", 
        "exceptions", 
        "locale", 
        "os", 
        "pydoc", 
        "sitecustomize", 
        "sys", 
        "usercustomize", 
        "zipimport", 
        "sysconfig", 
        "textwrap", 
        "traceback"
      ]
    }, 
    "smtpd": {
      "file": "smtpd.py", 
      "imports": [
        "Mailman.MailList", 
        "Mailman.Message", 
        "Mailman.Utils", 
        "__main__", 
        "asynchat", 
        "asyncore", 
        "cStringIO.StringIO", 
        "errno", 
        "getopt", 
        "os", 
        "sys", 
        "time", 
        "smtplib", 
        "socket", 
        "pwd"
      ]
    }, 
    "smtplib": {
      "file": "smtplib.py", 
      "imports": [
        "base64", 
        "email.base64mime", 
        "email.utils", 
        "hmac", 
        "re", 
        "ssl", 
        "sys", 
        "sys.stderr", 
        "socket"
      ]
    }, 
    "sndhdr": {
      "file": "sndhdr.py", 
      "imports": [
        "aifc", 
        "glob", 
        "os", 
        "sys"
      ]
    }, 
    "socket": {
      "file": "socket.py", 
      "imports": [
        "_socket", 
        "_socket.*", 
        "_ssl", 
        "_ssl.RAND_add", 
        "_ssl.RAND_egd", 
        "_ssl.RAND_status", 
        "_ssl.SSLError", 
        "_ssl.SSL_ERROR_EOF", 
        "_ssl.SSL_ERROR_INVALID_ERROR_CODE", 
        "_ssl.SSL_ERROR_SSL", 
        "_ssl.SSL_ERROR_SYSCALL", 
        "_ssl.SSL_ERROR_WANT_CONNECT", 
        "_ssl.SSL_ERROR_WANT_READ", 
        "_ssl.SSL_ERROR_WANT_WRITE", 
        "_ssl.SSL_ERROR_WANT_X509_LOOKUP", 
        "_ssl.SSL_ERROR_ZERO_RETURN", 
        "cStringIO.StringIO", 
        "errno", 
        "os", 
        "ssl", 
        "sys", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "sre": {
      "file": "sre.py", 
      "imports": [
        "re", 
        "warnings"
      ]
    }, 
    "sre_compile": {
      "file": "sre_compile.py", 
      "imports": [
        "_sre", 
        "array", 
        "sys", 
        "sre_constants", 
        "sre_parse"
      ]
    }, 
    "sre_constants": {
      "file": "sre_constants.py", 
      "imports": [
        "_sre", 
        "_sre.MAXREPEAT"
      ]
    }, 
    "sre_parse": {
      "file": "sre_parse.py", 
      "imports": [
        "__pypy__.newdict", 
        "sre_constants", 
        "sys"
      ]
    }, 
    "stackless": {
      "file": "stackless.py", 
      "imports": [
        "_continuation", 
        "collections", 
        "operator", 
        "threading.local"
      ]
    }, 
    "stat": {
      "file": "stat.py", 
      "imports": []
    }, 
    "statvfs": {
      "file": "statvfs.py", 
      "imports": [
        "warnings"
      ]
    }, 
    "string": {
      "file": "string.py", 
      "imports": [
        "re", 
        "strop.lowercase", 
        "strop.maketrans", 
        "strop.uppercase", 
        "strop.whitespace"
      ]
    }, 
    "stringold": {
      "file": "stringold.py", 
      "imports": [
        "stringold", 
        "strop.lowercase", 
        "strop.maketrans", 
        "strop.uppercase", 
        "strop.whitespace", 
        "warnings"
      ]
    }, 
    "stringprep": {
      "file": "stringprep.py", 
      "imports": [
        "unicodedata.ucd_3_2_0"
      ]
    }, 
    "struct": {
      "file": "struct.py", 
      "imports": [
        "_struct.*", 
        "_struct.__doc__", 
        "_struct._clearcache"
      ]
    }, 
    "symbol": {
      "file": "symbol.py", 
      "imports": [
        "sys", 
        "token"
      ]
    }, 
    "sysconfig": {
      "file": "sysconfig.py", 
      "imports": [
        "_osx_support", 
        "imp", 
        "os", 
        "pprint", 
        "re", 
        "sys"
      ]
    }, 
    "syslog": {
      "file": "syslog.py", 
      "imports": [
        "__pypy__.builtinify", 
        "cffi.FFI", 
        "sys"
      ]
    }, 
    "tabnanny": {
      "file": "tabnanny.py", 
      "imports": [
        "getopt", 
        "os", 
        "sys", 
        "tokenize"
      ]
    }, 
    "tarfile": {
      "file": "tarfile.py", 
      "imports": [
        "StringIO", 
        "bz2", 
        "cStringIO.StringIO", 
        "calendar", 
        "copy", 
        "errno", 
        "gzip", 
        "operator", 
        "os", 
        "re", 
        "shutil", 
        "stat", 
        "struct", 
        "sys", 
        "time", 
        "zlib", 
        "warnings", 
        "grp", 
        "pwd"
      ]
    }, 
    "telnetlib": {
      "file": "telnetlib.py", 
      "imports": [
        "errno", 
        "re", 
        "select", 
        "socket", 
        "sys", 
        "thread", 
        "time.time"
      ]
    }, 
    "tempfile": {
      "file": "tempfile.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "dummy_thread", 
        "errno", 
        "fcntl", 
        "io", 
        "os", 
        "random", 
        "thread"
      ]
    }, 
    "test": {
      "dir": "test"
    }, 
    "test.__init__": {
      "file": "test/__init__.py", 
      "imports": []
    }, 
    "test.audiotests": {
      "file": "test/audiotests.py", 
      "imports": [
        "array", 
        "base64", 
        "io", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.autotest": {
      "file": "test/autotest.py", 
      "imports": [
        "test.regrtest"
      ]
    }, 
    "test.bad_coding": {
      "file": "test/bad_coding.py", 
      "imports": []
    }, 
    "test.bad_coding2": {
      "file": "test/bad_coding2.py", 
      "imports": []
    }, 
    "test.bad_coding3": {
      "file": "test/bad_coding3.py", 
      "imports": []
    }, 
    "test.badsyntax_future3": {
      "file": "test/badsyntax_future3.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future4": {
      "file": "test/badsyntax_future4.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future5": {
      "file": "test/badsyntax_future5.py", 
      "imports": [
        "__future__", 
        "foo"
      ]
    }, 
    "test.badsyntax_future6": {
      "file": "test/badsyntax_future6.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future7": {
      "file": "test/badsyntax_future7.py", 
      "imports": [
        "__future__", 
        "string"
      ]
    }, 
    "test.badsyntax_future8": {
      "file": "test/badsyntax_future8.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future9": {
      "file": "test/badsyntax_future9.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_nocaret": {
      "file": "test/badsyntax_nocaret.py", 
      "imports": []
    }, 
    "test.buffer_tests": {
      "file": "test/buffer_tests.py", 
      "imports": [
        "struct", 
        "sys"
      ]
    }, 
    "test.curses_tests": {
      "file": "test/curses_tests.py", 
      "imports": [
        "curses", 
        "curses.textpad"
      ]
    }, 
    "test.doctest_aliases": {
      "file": "test/doctest_aliases.py", 
      "imports": []
    }, 
    "test.double_const": {
      "file": "test/double_const.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.fork_wait": {
      "file": "test/fork_wait.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.gdb_sample": {
      "file": "test/gdb_sample.py", 
      "imports": []
    }, 
    "test.infinite_reload": {
      "file": "test/infinite_reload.py", 
      "imports": [
        "imp", 
        "test.infinite_reload"
      ]
    }, 
    "test.inspect_fodder": {
      "file": "test/inspect_fodder.py", 
      "imports": [
        "inspect", 
        "sys"
      ]
    }, 
    "test.inspect_fodder2": {
      "file": "test/inspect_fodder2.py", 
      "imports": []
    }, 
    "test.leakers": {
      "dir": "test/leakers"
    }, 
    "test.leakers.__init__": {
      "file": "test/leakers/__init__.py", 
      "imports": []
    }, 
    "test.leakers.test_ctypes": {
      "file": "test/leakers/test_ctypes.py", 
      "imports": [
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.c_int", 
        "gc"
      ]
    }, 
    "test.leakers.test_dictself": {
      "file": "test/leakers/test_dictself.py", 
      "imports": [
        "gc"
      ]
    }, 
    "test.leakers.test_gestalt": {
      "file": "test/leakers/test_gestalt.py", 
      "imports": [
        "MacOS", 
        "gestalt.gestalt", 
        "sys"
      ]
    }, 
    "test.leakers.test_selftype": {
      "file": "test/leakers/test_selftype.py", 
      "imports": [
        "gc"
      ]
    }, 
    "test.list_tests": {
      "file": "test/list_tests.py", 
      "imports": [
        "os", 
        "sys", 
        "test.seq_tests", 
        "test.test_support"
      ]
    }, 
    "test.lock_tests": {
      "file": "test/lock_tests.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "thread.get_ident", 
        "thread.start_new_thread", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.mapping_tests": {
      "file": "test/mapping_tests.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "UserDict"
      ]
    }, 
    "test.mp_fork_bomb": {
      "file": "test/mp_fork_bomb.py", 
      "imports": [
        "multiprocessing"
      ]
    }, 
    "test.outstanding_bugs": {
      "file": "test/outstanding_bugs.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.pickletester": {
      "file": "test/pickletester.py", 
      "imports": [
        "StringIO", 
        "__main__", 
        "cStringIO", 
        "copy_reg", 
        "locale", 
        "os", 
        "pickle", 
        "pickletools", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.profilee": {
      "file": "test/profilee.py", 
      "imports": [
        "sys"
      ]
    }, 
    "test.pyclbr_input": {
      "file": "test/pyclbr_input.py", 
      "imports": []
    }, 
    "test.pydoc_mod": {
      "file": "test/pydoc_mod.py", 
      "imports": []
    }, 
    "test.pydocfodder": {
      "file": "test/pydocfodder.py", 
      "imports": [
        "types"
      ]
    }, 
    "test.pystone": {
      "file": "test/pystone.py", 
      "imports": [
        "sys", 
        "time.time"
      ]
    }, 
    "test.re_tests": {
      "file": "test/re_tests.py", 
      "imports": []
    }, 
    "test.regrtest": {
      "file": "test/regrtest.py", 
      "imports": [
        "Queue", 
        "StringIO", 
        "_abcoll", 
        "_pyio", 
        "_strptime", 
        "copy_reg", 
        "ctypes", 
        "distutils.dir_util", 
        "doctest", 
        "filecmp", 
        "gc", 
        "getopt", 
        "imp", 
        "json", 
        "linecache", 
        "mimetypes", 
        "os", 
        "platform", 
        "random", 
        "re", 
        "shutil", 
        "stat", 
        "struct", 
        "subprocess.PIPE", 
        "subprocess.Popen", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.test_support", 
        "test.test_timeout", 
        "threading.Thread", 
        "time", 
        "zipimport", 
        "textwrap", 
        "trace", 
        "traceback", 
        "unittest", 
        "urllib", 
        "urllib2", 
        "urlparse", 
        "warnings", 
        "resource"
      ]
    }, 
    "test.relimport": {
      "file": "test/relimport.py", 
      "imports": [
        "test.test_import"
      ]
    }, 
    "test.reperf": {
      "file": "test/reperf.py", 
      "imports": [
        "re", 
        "time"
      ]
    }, 
    "test.sample_doctest": {
      "file": "test/sample_doctest.py", 
      "imports": [
        "doctest"
      ]
    }, 
    "test.sample_doctest_no_docstrings": {
      "file": "test/sample_doctest_no_docstrings.py", 
      "imports": []
    }, 
    "test.sample_doctest_no_doctests": {
      "file": "test/sample_doctest_no_doctests.py", 
      "imports": []
    }, 
    "test.script_helper": {
      "file": "test/script_helper.py", 
      "imports": [
        "contextlib", 
        "os", 
        "py_compile", 
        "re", 
        "shutil", 
        "subprocess", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "zipfile"
      ]
    }, 
    "test.seq_tests": {
      "file": "test/seq_tests.py", 
      "imports": [
        "itertools.chain", 
        "itertools.imap", 
        "sys", 
        "unittest"
      ]
    }, 
    "test.sortperf": {
      "file": "test/sortperf.py", 
      "imports": [
        "marshal", 
        "os", 
        "random", 
        "sys", 
        "tempfile", 
        "time"
      ]
    }, 
    "test.string_tests": {
      "file": "test/string_tests.py", 
      "imports": [
        "string", 
        "struct", 
        "sys", 
        "test.test_support", 
        "zlib", 
        "unittest", 
        "UserList", 
        "_testcapi"
      ]
    }, 
    "test.symlink_support": {
      "file": "test/symlink_support.py", 
      "imports": [
        "ctypes.wintypes", 
        "os", 
        "platform", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_MimeWriter": {
      "file": "test/test_MimeWriter.py", 
      "imports": [
        "MimeWriter", 
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_SimpleHTTPServer": {
      "file": "test/test_SimpleHTTPServer.py", 
      "imports": [
        "SimpleHTTPServer", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_StringIO": {
      "file": "test/test_StringIO.py", 
      "imports": [
        "StringIO", 
        "array", 
        "cStringIO", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test___all__": {
      "file": "test/test___all__.py", 
      "imports": [
        "__future__", 
        "_socket", 
        "locale", 
        "os", 
        "rlcompleter", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test___future__": {
      "file": "test/test___future__.py", 
      "imports": [
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test__locale": {
      "file": "test/test__locale.py", 
      "imports": [
        "_locale.Error", 
        "_locale.LC_NUMERIC", 
        "_locale.RADIXCHAR", 
        "_locale.THOUSEP", 
        "_locale.localeconv", 
        "_locale.nl_langinfo", 
        "_locale.setlocale", 
        "platform", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test__osx_support": {
      "file": "test/test__osx_support.py", 
      "imports": [
        "_osx_support", 
        "os", 
        "platform", 
        "shutil", 
        "stat", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_abc": {
      "file": "test/test_abc.py", 
      "imports": [
        "abc", 
        "inspect", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_abstract_numbers": {
      "file": "test/test_abstract_numbers.py", 
      "imports": [
        "math", 
        "numbers", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_aepack": {
      "file": "test/test_aepack.py", 
      "imports": [
        "Carbon.File", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_aifc": {
      "file": "test/test_aifc.py", 
      "imports": [
        "aifc", 
        "io", 
        "os", 
        "struct", 
        "sys", 
        "test.audiotests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_al": {
      "file": "test/test_al.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.test_anydbm": {
      "file": "test/test_anydbm.py", 
      "imports": [
        "glob", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_applesingle": {
      "file": "test/test_applesingle.py", 
      "imports": [
        "applesingle", 
        "os", 
        "struct", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_argparse": {
      "file": "test/test_argparse.py", 
      "imports": [
        "StringIO", 
        "argparse", 
        "codecs", 
        "gc", 
        "inspect", 
        "os", 
        "shutil", 
        "stat", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_array": {
      "file": "test/test_array.py", 
      "imports": [
        "array", 
        "cStringIO", 
        "copy", 
        "gc", 
        "sys", 
        "sys.maxsize", 
        "test.test_support", 
        "unittest", 
        "warnings", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_ascii_formatd": {
      "file": "test/test_ascii_formatd.py", 
      "imports": [
        "ctypes.byref", 
        "ctypes.c_double", 
        "ctypes.create_string_buffer", 
        "ctypes.pythonapi", 
        "ctypes.sizeof", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ast": {
      "file": "test/test_ast.py", 
      "imports": [
        "ast", 
        "itertools", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_asynchat": {
      "file": "test/test_asynchat.py", 
      "imports": [
        "asynchat", 
        "asyncore", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_asyncore": {
      "file": "test/test_asyncore.py", 
      "imports": [
        "StringIO", 
        "asyncore", 
        "errno", 
        "os", 
        "select", 
        "socket", 
        "struct", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_atexit": {
      "file": "test/test_atexit.py", 
      "imports": [
        "StringIO", 
        "atexit", 
        "imp.reload", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_audioop": {
      "file": "test/test_audioop.py", 
      "imports": [
        "audioop", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_augassign": {
      "file": "test/test_augassign.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_base64": {
      "file": "test/test_base64.py", 
      "imports": [
        "base64", 
        "cStringIO.StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bastion": {
      "file": "test/test_bastion.py", 
      "imports": []
    }, 
    "test.test_bigaddrspace": {
      "file": "test/test_bigaddrspace.py", 
      "imports": [
        "operator", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bigmem": {
      "file": "test/test_bigmem.py", 
      "imports": [
        "operator", 
        "string", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_binascii": {
      "file": "test/test_binascii.py", 
      "imports": [
        "array", 
        "binascii", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_binhex": {
      "file": "test/test_binhex.py", 
      "imports": [
        "binhex", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_binop": {
      "file": "test/test_binop.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bisect": {
      "file": "test/test_bisect.py", 
      "imports": [
        "bisect", 
        "gc", 
        "random", 
        "sys", 
        "test.test_bisect", 
        "test.test_support", 
        "unittest", 
        "UserList"
      ]
    }, 
    "test.test_bool": {
      "file": "test/test_bool.py", 
      "imports": [
        "marshal", 
        "operator", 
        "os", 
        "pickle", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_bsddb": {
      "file": "test/test_bsddb.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bsddb185": {
      "file": "test/test_bsddb185.py", 
      "imports": [
        "anydbm", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "whichdb"
      ]
    }, 
    "test.test_bsddb3": {
      "file": "test/test_bsddb3.py", 
      "imports": [
        "bsddb.db", 
        "bsddb.test.test_all", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_buffer": {
      "file": "test/test_buffer.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bufio": {
      "file": "test/test_bufio.py", 
      "imports": [
        "_pyio", 
        "io", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_builtin": {
      "file": "test/test_builtin.py", 
      "imports": [
        "cStringIO", 
        "gc", 
        "marshal", 
        "math.sqrt", 
        "operator.neg", 
        "os", 
        "platform", 
        "random", 
        "string", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest", 
        "UserDict", 
        "UserList", 
        "warnings"
      ]
    }, 
    "test.test_bytes": {
      "file": "test/test_bytes.py", 
      "imports": [
        "copy", 
        "functools", 
        "os", 
        "pickle", 
        "re", 
        "sys", 
        "tempfile", 
        "test.buffer_tests", 
        "test.string_tests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bz2": {
      "file": "test/test_bz2.py", 
      "imports": [
        "bz2.BZ2Compressor", 
        "bz2.BZ2Decompressor", 
        "bz2.BZ2File", 
        "cStringIO.StringIO", 
        "os", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_calendar": {
      "file": "test/test_calendar.py", 
      "imports": [
        "calendar", 
        "locale", 
        "test.test_support", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_call": {
      "file": "test/test_call.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_capi": {
      "file": "test/test_capi.py", 
      "imports": [
        "__future__", 
        "random", 
        "sys", 
        "test.test_support", 
        "thread", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_cd": {
      "file": "test/test_cd.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.test_cfgparser": {
      "file": "test/test_cfgparser.py", 
      "imports": [
        "ConfigParser", 
        "StringIO", 
        "os", 
        "pickle", 
        "test.test_support", 
        "unittest", 
        "UserDict"
      ]
    }, 
    "test.test_cgi": {
      "file": "test/test_cgi.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "cgi", 
        "collections", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_charmapcodec": {
      "file": "test/test_charmapcodec.py", 
      "imports": [
        "codecs", 
        "test.test_support", 
        "test.testcodec", 
        "unittest"
      ]
    }, 
    "test.test_cl": {
      "file": "test/test_cl.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.test_class": {
      "file": "test/test_class.py", 
      "imports": [
        "gc", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_cmath": {
      "file": "test/test_cmath.py", 
      "imports": [
        "cmath", 
        "cmath.phase", 
        "cmath.pi", 
        "cmath.polar", 
        "cmath.rect", 
        "math", 
        "test.test_math", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cmd": {
      "file": "test/test_cmd.py", 
      "imports": [
        "StringIO", 
        "cmd", 
        "re", 
        "sys", 
        "test.test_cmd", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cmd_line": {
      "file": "test/test_cmd_line.py", 
      "imports": [
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cmd_line_script": {
      "file": "test/test_cmd_line_script.py", 
      "imports": [
        "os", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_code": {
      "file": "test/test_code.py", 
      "imports": [
        "test.test_code", 
        "test.test_support", 
        "unittest", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_codeccallbacks": {
      "file": "test/test_codeccallbacks.py", 
      "imports": [
        "codecs", 
        "htmlentitydefs", 
        "sys", 
        "test.test_support", 
        "unicodedata", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_cn": {
      "file": "test/test_codecencodings_cn.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_hk": {
      "file": "test/test_codecencodings_hk.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_iso2022": {
      "file": "test/test_codecencodings_iso2022.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_jp": {
      "file": "test/test_codecencodings_jp.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_kr": {
      "file": "test/test_codecencodings_kr.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_tw": {
      "file": "test/test_codecencodings_tw.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_cn": {
      "file": "test/test_codecmaps_cn.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_hk": {
      "file": "test/test_codecmaps_hk.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_jp": {
      "file": "test/test_codecmaps_jp.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_kr": {
      "file": "test/test_codecmaps_kr.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_tw": {
      "file": "test/test_codecmaps_tw.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecs": {
      "file": "test/test_codecs.py", 
      "imports": [
        "StringIO", 
        "array", 
        "bz2", 
        "codecs", 
        "encodings.cp1140", 
        "encodings.idna", 
        "locale", 
        "sys", 
        "test.test_support", 
        "zlib", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_codeop": {
      "file": "test/test_codeop.py", 
      "imports": [
        "cStringIO", 
        "codeop", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_coding": {
      "file": "test/test_coding.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_coercion": {
      "file": "test/test_coercion.py", 
      "imports": [
        "copy", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_collections": {
      "file": "test/test_collections.py", 
      "imports": [
        "collections", 
        "copy", 
        "doctest", 
        "inspect", 
        "keyword", 
        "operator", 
        "pickle", 
        "random", 
        "re", 
        "sets", 
        "string", 
        "sys", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_colorsys": {
      "file": "test/test_colorsys.py", 
      "imports": [
        "colorsys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_commands": {
      "file": "test/test_commands.py", 
      "imports": [
        "os", 
        "re", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_compare": {
      "file": "test/test_compare.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_compile": {
      "file": "test/test_compile.py", 
      "imports": [
        "__builtin__", 
        "__mangled_mod", 
        "__package__.module", 
        "_ast", 
        "math", 
        "sys", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_compileall": {
      "file": "test/test_compileall.py", 
      "imports": [
        "compileall", 
        "imp", 
        "os", 
        "py_compile", 
        "shutil", 
        "struct", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_compiler": {
      "file": "test/test_compiler.py", 
      "imports": [
        "StringIO", 
        "compiler.ast", 
        "math.*", 
        "os", 
        "random", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_complex": {
      "file": "test/test_complex.py", 
      "imports": [
        "math.atan2", 
        "math.copysign", 
        "math.isnan", 
        "random", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_complex_args": {
      "file": "test/test_complex_args.py", 
      "imports": [
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_contains": {
      "file": "test/test_contains.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_contextlib": {
      "file": "test/test_contextlib.py", 
      "imports": [
        "contextlib", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_cookie": {
      "file": "test/test_cookie.py", 
      "imports": [
        "Cookie", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cookielib": {
      "file": "test/test_cookielib.py", 
      "imports": [
        "StringIO", 
        "cookielib", 
        "mimetools", 
        "os", 
        "re", 
        "test.test_support", 
        "time", 
        "traceback", 
        "unittest", 
        "urllib2"
      ]
    }, 
    "test.test_copy": {
      "file": "test/test_copy.py", 
      "imports": [
        "copy", 
        "copy_reg", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_copy_reg": {
      "file": "test/test_copy_reg.py", 
      "imports": [
        "copy", 
        "copy_reg", 
        "test.pickletester", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cpickle": {
      "file": "test/test_cpickle.py", 
      "imports": [
        "cStringIO", 
        "io", 
        "test.pickletester", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_cprofile": {
      "file": "test/test_cprofile.py", 
      "imports": [
        "_lsprof", 
        "cProfile", 
        "sys", 
        "test.test_profile", 
        "test.test_support"
      ]
    }, 
    "test.test_crypt": {
      "file": "test/test_crypt.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_csv": {
      "file": "test/test_csv.py", 
      "imports": [
        "StringIO", 
        "array", 
        "csv", 
        "gc", 
        "io", 
        "itertools", 
        "os", 
        "string", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ctypes": {
      "file": "test/test_ctypes.py", 
      "imports": [
        "ctypes.test", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_curses": {
      "file": "test/test_curses.py", 
      "imports": [
        "curses.ascii", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_datetime": {
      "file": "test/test_datetime.py", 
      "imports": [
        "__future__", 
        "_strptime", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "cPickle", 
        "datetime"
      ]
    }, 
    "test.test_dbm": {
      "file": "test/test_dbm.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_decimal": {
      "file": "test/test_decimal.py", 
      "imports": [
        "copy", 
        "decimal", 
        "locale", 
        "math", 
        "numbers", 
        "operator", 
        "optparse", 
        "os", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_decorators": {
      "file": "test/test_decorators.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_defaultdict": {
      "file": "test/test_defaultdict.py", 
      "imports": [
        "collections", 
        "copy", 
        "os", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_deque": {
      "file": "test/test_deque.py", 
      "imports": [
        "collections", 
        "copy", 
        "gc", 
        "random", 
        "struct", 
        "sys", 
        "test.seq_tests", 
        "test.test_deque", 
        "test.test_support", 
        "unittest", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_descr": {
      "file": "test/test_descr.py", 
      "imports": [
        "__builtin__", 
        "abc", 
        "binascii", 
        "cStringIO", 
        "copy", 
        "gc", 
        "operator", 
        "pickle", 
        "popen2", 
        "sys", 
        "test.test_support", 
        "xxsubtype", 
        "types", 
        "unittest", 
        "weakref", 
        "_testcapi", 
        "cPickle"
      ]
    }, 
    "test.test_descrtut": {
      "file": "test/test_descrtut.py", 
      "imports": [
        "pprint", 
        "test.test_descrtut", 
        "test.test_support"
      ]
    }, 
    "test.test_dict": {
      "file": "test/test_dict.py", 
      "imports": [
        "gc", 
        "random", 
        "string", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest", 
        "UserDict", 
        "weakref"
      ]
    }, 
    "test.test_dictcomps": {
      "file": "test/test_dictcomps.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_dictviews": {
      "file": "test/test_dictviews.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_difflib": {
      "file": "test/test_difflib.py", 
      "imports": [
        "difflib", 
        "doctest", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_dircache": {
      "file": "test/test_dircache.py", 
      "imports": [
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_dis": {
      "file": "test/test_dis.py", 
      "imports": [
        "StringIO", 
        "difflib", 
        "dis", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_distutils": {
      "file": "test/test_distutils.py", 
      "imports": [
        "distutils.tests", 
        "test.test_support"
      ]
    }, 
    "test.test_dl": {
      "file": "test/test_dl.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_doctest": {
      "file": "test/test_doctest.py", 
      "imports": [
        "doctest", 
        "sys", 
        "test.test_doctest", 
        "test.test_support"
      ]
    }, 
    "test.test_doctest2": {
      "file": "test/test_doctest2.py", 
      "imports": [
        "doctest", 
        "sys", 
        "test.test_doctest2", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_docxmlrpc": {
      "file": "test/test_docxmlrpc.py", 
      "imports": [
        "DocXMLRPCServer", 
        "httplib", 
        "socket", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_dumbdbm": {
      "file": "test/test_dumbdbm.py", 
      "imports": [
        "dumbdbm", 
        "os", 
        "random", 
        "stat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_dummy_thread": {
      "file": "test/test_dummy_thread.py", 
      "imports": [
        "Queue", 
        "dummy_thread", 
        "random", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_dummy_threading": {
      "file": "test/test_dummy_threading.py", 
      "imports": [
        "dummy_threading", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_email": {
      "file": "test/test_email.py", 
      "imports": [
        "email.test.test_email", 
        "email.test.test_email_renamed", 
        "test.test_support"
      ]
    }, 
    "test.test_email_codecs": {
      "file": "test/test_email_codecs.py", 
      "imports": [
        "email.test.test_email_codecs", 
        "email.test.test_email_codecs_renamed", 
        "test.test_support"
      ]
    }, 
    "test.test_email_renamed": {
      "file": "test/test_email_renamed.py", 
      "imports": [
        "email.test.test_email_renamed", 
        "test.test_support"
      ]
    }, 
    "test.test_enumerate": {
      "file": "test/test_enumerate.py", 
      "imports": [
        "sys", 
        "test.test_iterlen", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_eof": {
      "file": "test/test_eof.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_epoll": {
      "file": "test/test_epoll.py", 
      "imports": [
        "errno", 
        "select", 
        "socket", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_errno": {
      "file": "test/test_errno.py", 
      "imports": [
        "errno", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_exception_variations": {
      "file": "test/test_exception_variations.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_exceptions": {
      "file": "test/test_exceptions.py", 
      "imports": [
        "exceptions", 
        "imp.reload", 
        "os", 
        "pickle", 
        "sys", 
        "test.test_pep352", 
        "test.test_support", 
        "unittest", 
        "_testcapi", 
        "cPickle"
      ]
    }, 
    "test.test_extcall": {
      "file": "test/test_extcall.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fcntl": {
      "file": "test/test_fcntl.py", 
      "imports": [
        "os", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_file": {
      "file": "test/test_file.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "array.array", 
        "io", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "UserList", 
        "weakref"
      ]
    }, 
    "test.test_file2k": {
      "file": "test/test_file2k.py", 
      "imports": [
        "array.array", 
        "itertools", 
        "os", 
        "select", 
        "signal", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "UserList", 
        "weakref"
      ]
    }, 
    "test.test_file_eintr": {
      "file": "test/test_file_eintr.py", 
      "imports": [
        "_io.FileIO", 
        "os", 
        "select", 
        "signal", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_filecmp": {
      "file": "test/test_filecmp.py", 
      "imports": [
        "filecmp", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fileinput": {
      "file": "test/test_fileinput.py", 
      "imports": [
        "StringIO", 
        "fileinput", 
        "re", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fileio": {
      "file": "test/test_fileio.py", 
      "imports": [
        "__future__", 
        "_io.FileIO", 
        "array.array", 
        "errno", 
        "functools", 
        "msvcrt", 
        "os", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "unittest", 
        "UserList", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_float": {
      "file": "test/test_float.py", 
      "imports": [
        "fractions", 
        "locale", 
        "math", 
        "math.copysign", 
        "math.isinf", 
        "math.isnan", 
        "math.ldexp", 
        "operator", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fnmatch": {
      "file": "test/test_fnmatch.py", 
      "imports": [
        "fnmatch", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fork1": {
      "file": "test/test_fork1.py", 
      "imports": [
        "imp", 
        "os", 
        "signal", 
        "sys", 
        "test.fork_wait", 
        "test.test_support", 
        "time"
      ]
    }, 
    "test.test_format": {
      "file": "test/test_format.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_fpformat": {
      "file": "test/test_fpformat.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fractions": {
      "file": "test/test_fractions.py", 
      "imports": [
        "copy", 
        "decimal", 
        "fractions", 
        "math", 
        "numbers", 
        "operator", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_frozen": {
      "file": "test/test_frozen.py", 
      "imports": [
        "__hello__", 
        "__phello__", 
        "__phello__.foo", 
        "__phello__.spam", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ftplib": {
      "file": "test/test_ftplib.py", 
      "imports": [
        "StringIO", 
        "asynchat", 
        "asyncore", 
        "errno", 
        "ftplib", 
        "os", 
        "socket", 
        "ssl", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_funcattrs": {
      "file": "test/test_funcattrs.py", 
      "imports": [
        "test.test_support", 
        "types", 
        "unittest", 
        "UserDict"
      ]
    }, 
    "test.test_functools": {
      "file": "test/test_functools.py", 
      "imports": [
        "functools", 
        "gc", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_future": {
      "file": "test/test_future.py", 
      "imports": [
        "re", 
        "test.badsyntax_future3", 
        "test.badsyntax_future4", 
        "test.badsyntax_future5", 
        "test.badsyntax_future6", 
        "test.badsyntax_future7", 
        "test.badsyntax_future8", 
        "test.badsyntax_future9", 
        "test.test_future1", 
        "test.test_future2", 
        "test.test_future3", 
        "test.test_future5", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future1": {
      "file": "test/test_future1.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.test_future2": {
      "file": "test/test_future2.py", 
      "imports": [
        "__future__", 
        "string"
      ]
    }, 
    "test.test_future3": {
      "file": "test/test_future3.py", 
      "imports": [
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future4": {
      "file": "test/test_future4.py", 
      "imports": [
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future5": {
      "file": "test/test_future5.py", 
      "imports": [
        "__future__", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future_builtins": {
      "file": "test/test_future_builtins.py", 
      "imports": [
        "itertools.ifilter", 
        "itertools.imap", 
        "itertools.izip", 
        "test.test_support", 
        "unittest", 
        "future_builtins"
      ]
    }, 
    "test.test_gc": {
      "file": "test/test_gc.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_gdb": {
      "file": "test/test_gdb.py", 
      "imports": [
        "os", 
        "re", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_gdbm": {
      "file": "test/test_gdbm.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_generators": {
      "file": "test/test_generators.py", 
      "imports": [
        "test.test_generators", 
        "test.test_support"
      ]
    }, 
    "test.test_genericpath": {
      "file": "test/test_genericpath.py", 
      "imports": [
        "genericpath", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_genexps": {
      "file": "test/test_genexps.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.test_genexps", 
        "test.test_support"
      ]
    }, 
    "test.test_getargs": {
      "file": "test/test_getargs.py", 
      "imports": [
        "marshal", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_getargs2": {
      "file": "test/test_getargs2.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "warnings", 
        "_testcapi"
      ]
    }, 
    "test.test_getopt": {
      "file": "test/test_getopt.py", 
      "imports": [
        "getopt", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_gettext": {
      "file": "test/test_gettext.py", 
      "imports": [
        "__builtin__", 
        "base64", 
        "gettext", 
        "os", 
        "shutil", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_gl": {
      "file": "test/test_gl.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_glob": {
      "file": "test/test_glob.py", 
      "imports": [
        "glob", 
        "os", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_global": {
      "file": "test/test_global.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_grammar": {
      "file": "test/test_grammar.py", 
      "imports": [
        "StringIO", 
        "sys", 
        "sys.*", 
        "sys.argv", 
        "sys.maxint", 
        "sys.path", 
        "test.test_support", 
        "time", 
        "time.time", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_grp": {
      "file": "test/test_grp.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_gzip": {
      "file": "test/test_gzip.py", 
      "imports": [
        "io", 
        "os", 
        "struct", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_hash": {
      "file": "test/test_hash.py", 
      "imports": [
        "collections", 
        "os", 
        "struct", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_hashlib": {
      "file": "test/test_hashlib.py", 
      "imports": [
        "_md5", 
        "array", 
        "binascii.unhexlify", 
        "hashlib", 
        "itertools", 
        "string", 
        "sys", 
        "test.test_support", 
        "threading", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_heapq": {
      "file": "test/test_heapq.py", 
      "imports": [
        "gc", 
        "itertools.chain", 
        "itertools.imap", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_hmac": {
      "file": "test/test_hmac.py", 
      "imports": [
        "hashlib", 
        "hmac", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_hotshot": {
      "file": "test/test_hotshot.py", 
      "imports": [
        "_hotshot", 
        "gc", 
        "hotshot.log.ENTER", 
        "hotshot.log.EXIT", 
        "hotshot.log.LINE", 
        "hotshot.stats", 
        "os", 
        "pprint", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_htmllib": {
      "file": "test/test_htmllib.py", 
      "imports": [
        "formatter", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_htmlparser": {
      "file": "test/test_htmlparser.py", 
      "imports": [
        "HTMLParser", 
        "pprint", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_httplib": {
      "file": "test/test_httplib.py", 
      "imports": [
        "StringIO", 
        "array", 
        "errno", 
        "httplib", 
        "socket", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_httpservers": {
      "file": "test/test_httpservers.py", 
      "imports": [
        "BaseHTTPServer", 
        "CGIHTTPServer", 
        "SimpleHTTPServer", 
        "StringIO", 
        "base64", 
        "httplib", 
        "os", 
        "re", 
        "shutil", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "urllib"
      ]
    }, 
    "test.test_idle": {
      "file": "test/test_idle.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_imageop": {
      "file": "test/test_imageop.py", 
      "imports": [
        "imgfile", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "uu"
      ]
    }, 
    "test.test_imaplib": {
      "file": "test/test_imaplib.py", 
      "imports": [
        "SocketServer", 
        "contextlib", 
        "imaplib", 
        "os", 
        "ssl", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_imgfile": {
      "file": "test/test_imgfile.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "uu"
      ]
    }, 
    "test.test_imghdr": {
      "file": "test/test_imghdr.py", 
      "imports": [
        "imghdr", 
        "io", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_imp": {
      "file": "test/test_imp.py", 
      "imports": [
        "imp", 
        "marshal", 
        "os", 
        "test.test_support", 
        "thread", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_import": {
      "file": "test/test_import.py", 
      "imports": [
        "RAnDoM", 
        "errno", 
        "imp", 
        "marshal", 
        "os", 
        "py_compile", 
        "random", 
        "shutil", 
        "socket", 
        "stat", 
        "struct", 
        "sys", 
        "test", 
        "test.double_const", 
        "test.infinite_reload", 
        "test.relimport", 
        "test.script_helper", 
        "test.symlink_support", 
        "test.test_import", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_importhooks": {
      "file": "test/test_importhooks.py", 
      "imports": [
        "hooktestmodule", 
        "hooktestpackage", 
        "hooktestpackage.futrel", 
        "hooktestpackage.newabs", 
        "hooktestpackage.newrel", 
        "hooktestpackage.oldabs", 
        "hooktestpackage.sub", 
        "hooktestpackage.sub.subber", 
        "hooktestpackage.sub.subber.subest", 
        "imp", 
        "os", 
        "reloadmodule", 
        "sub", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_importlib": {
      "file": "test/test_importlib.py", 
      "imports": [
        "contextlib", 
        "imp", 
        "importlib", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_index": {
      "file": "test/test_index.py", 
      "imports": [
        "operator", 
        "sys.maxint", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_inspect": {
      "file": "test/test_inspect.py", 
      "imports": [
        "__builtin__", 
        "abc", 
        "inspect", 
        "linecache", 
        "re", 
        "sys", 
        "test.inspect_fodder", 
        "test.inspect_fodder2", 
        "test.test_support", 
        "unicodedata", 
        "types", 
        "unittest", 
        "UserDict", 
        "UserList"
      ]
    }, 
    "test.test_int": {
      "file": "test/test_int.py", 
      "imports": [
        "math", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_int_literal": {
      "file": "test/test_int_literal.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_io": {
      "file": "test/test_io.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "abc", 
        "array", 
        "codecs", 
        "collections", 
        "contextlib", 
        "errno", 
        "fcntl", 
        "io", 
        "itertools.count", 
        "itertools.cycle", 
        "os", 
        "random", 
        "signal", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "UserList", 
        "warnings", 
        "weakref"
      ]
    }, 
    "test.test_ioctl": {
      "file": "test/test_ioctl.py", 
      "imports": [
        "array", 
        "os", 
        "pty", 
        "struct", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_isinstance": {
      "file": "test/test_isinstance.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_iter": {
      "file": "test/test_iter.py", 
      "imports": [
        "operator.add", 
        "operator.countOf", 
        "operator.indexOf", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_iterlen": {
      "file": "test/test_iterlen.py", 
      "imports": [
        "__builtin__.len", 
        "collections", 
        "itertools.repeat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_itertools": {
      "file": "test/test_itertools.py", 
      "imports": [
        "copy", 
        "decimal", 
        "fractions", 
        "functools", 
        "gc", 
        "itertools.*", 
        "operator", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_iterlen", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_json": {
      "file": "test/test_json.py", 
      "imports": [
        "json.tests", 
        "test.test_support"
      ]
    }, 
    "test.test_kqueue": {
      "file": "test/test_kqueue.py", 
      "imports": [
        "errno", 
        "select", 
        "socket", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_largefile": {
      "file": "test/test_largefile.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "io", 
        "os", 
        "signal", 
        "stat", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_lib2to3": {
      "file": "test/test_lib2to3.py", 
      "imports": [
        "lib2to3.tests.test_fixers", 
        "lib2to3.tests.test_main", 
        "lib2to3.tests.test_parser", 
        "lib2to3.tests.test_pytree", 
        "lib2to3.tests.test_refactor", 
        "lib2to3.tests.test_util", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_linecache": {
      "file": "test/test_linecache.py", 
      "imports": [
        "linecache", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_linuxaudiodev": {
      "file": "test/test_linuxaudiodev.py", 
      "imports": [
        "audioop", 
        "errno", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_list": {
      "file": "test/test_list.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.list_tests", 
        "test.test_support"
      ]
    }, 
    "test.test_locale": {
      "file": "test/test_locale.py", 
      "imports": [
        "codecs", 
        "locale", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_logging": {
      "file": "test/test_logging.py", 
      "imports": [
        "SocketServer", 
        "cStringIO", 
        "codecs", 
        "gc", 
        "json", 
        "logging", 
        "logging.config", 
        "logging.handlers", 
        "os", 
        "random", 
        "re", 
        "select", 
        "socket", 
        "struct", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "time", 
        "textwrap", 
        "unittest", 
        "warnings", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_long": {
      "file": "test/test_long.py", 
      "imports": [
        "math", 
        "random", 
        "sys", 
        "test.test_int", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_long_future": {
      "file": "test/test_long_future.py", 
      "imports": [
        "__future__", 
        "math", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_longexp": {
      "file": "test/test_longexp.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macos": {
      "file": "test/test_macos.py", 
      "imports": [
        "os", 
        "subprocess", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macostools": {
      "file": "test/test_macostools.py", 
      "imports": [
        "Carbon.File", 
        "macostools", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macpath": {
      "file": "test/test_macpath.py", 
      "imports": [
        "macpath", 
        "test.test_genericpath", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macurl2path": {
      "file": "test/test_macurl2path.py", 
      "imports": [
        "macurl2path", 
        "unittest"
      ]
    }, 
    "test.test_mailbox": {
      "file": "test/test_mailbox.py", 
      "imports": [
        "StringIO", 
        "email", 
        "email.message", 
        "email.parser", 
        "fcntl", 
        "glob", 
        "mailbox", 
        "os", 
        "re", 
        "shutil", 
        "socket", 
        "stat", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_marshal": {
      "file": "test/test_marshal.py", 
      "imports": [
        "marshal", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_math": {
      "file": "test/test_math.py", 
      "imports": [
        "doctest", 
        "math", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "sys.float_info", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_md5": {
      "file": "test/test_md5.py", 
      "imports": [
        "md5", 
        "string", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_memoryio": {
      "file": "test/test_memoryio.py", 
      "imports": [
        "__future__", 
        "__main__", 
        "_pyio", 
        "array", 
        "io", 
        "pickle", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_memoryview": {
      "file": "test/test_memoryview.py", 
      "imports": [
        "array", 
        "gc", 
        "io", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_mhlib": {
      "file": "test/test_mhlib.py", 
      "imports": [
        "StringIO", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_mimetools": {
      "file": "test/test_mimetools.py", 
      "imports": [
        "StringIO", 
        "string", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_mimetypes": {
      "file": "test/test_mimetypes.py", 
      "imports": [
        "StringIO", 
        "_winreg", 
        "mimetypes", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_minidom": {
      "file": "test/test_minidom.py", 
      "imports": [
        "StringIO", 
        "pickle", 
        "test.test_support", 
        "unittest", 
        "xml.parsers.expat", 
        "xml.dom.pulldom", 
        "xml.dom.minidom", 
        "xml.dom"
      ]
    }, 
    "test.test_mmap": {
      "file": "test/test_mmap.py", 
      "imports": [
        "itertools", 
        "os", 
        "re", 
        "socket", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_module": {
      "file": "test/test_module.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_modulefinder": {
      "file": "test/test_modulefinder.py", 
      "imports": [
        "__future__", 
        "distutils.dir_util", 
        "modulefinder", 
        "os", 
        "sets", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_msilib": {
      "file": "test/test_msilib.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_multibytecodec": {
      "file": "test/test_multibytecodec.py", 
      "imports": [
        "StringIO", 
        "_multibytecodec", 
        "codecs", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_multibytecodec_support": {
      "file": "test/test_multibytecodec_support.py", 
      "imports": [
        "StringIO", 
        "codecs", 
        "htmlentitydefs", 
        "httplib", 
        "os", 
        "re", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_multifile": {
      "file": "test/test_multifile.py", 
      "imports": [
        "cStringIO", 
        "test.test_support"
      ]
    }, 
    "test.test_multiprocessing": {
      "file": "test/test_multiprocessing.py", 
      "imports": [
        "Queue", 
        "StringIO", 
        "array", 
        "ctypes.Structure", 
        "ctypes.c_double", 
        "ctypes.c_int", 
        "errno", 
        "gc", 
        "json", 
        "logging", 
        "msvcrt", 
        "multiprocessing.connection", 
        "multiprocessing.dummy", 
        "multiprocessing.forking", 
        "multiprocessing.heap", 
        "multiprocessing.managers", 
        "multiprocessing.managers.BaseManager", 
        "multiprocessing.managers.BaseProxy", 
        "multiprocessing.managers.RemoteError", 
        "multiprocessing.pool", 
        "multiprocessing.pool.MaybeEncodingError", 
        "multiprocessing.reduction", 
        "multiprocessing.sharedctypes.Value", 
        "multiprocessing.sharedctypes.copy", 
        "multiprocessing.util", 
        "os", 
        "random", 
        "signal", 
        "socket", 
        "subprocess", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_mutants": {
      "file": "test/test_mutants.py", 
      "imports": [
        "os", 
        "random", 
        "test.test_support"
      ]
    }, 
    "test.test_mutex": {
      "file": "test/test_mutex.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_netrc": {
      "file": "test/test_netrc.py", 
      "imports": [
        "netrc", 
        "os", 
        "sys", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_new": {
      "file": "test/test_new.py", 
      "imports": [
        "Spam", 
        "__builtin__", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_nis": {
      "file": "test/test_nis.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_nntplib": {
      "file": "test/test_nntplib.py", 
      "imports": [
        "nntplib", 
        "socket", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_normalization": {
      "file": "test/test_normalization.py", 
      "imports": [
        "httplib", 
        "os", 
        "sys", 
        "test.test_support", 
        "unicodedata.normalize", 
        "unicodedata.unidata_version", 
        "unittest"
      ]
    }, 
    "test.test_ntpath": {
      "file": "test/test_ntpath.py", 
      "imports": [
        "nt", 
        "ntpath", 
        "os", 
        "sys", 
        "test.test_genericpath", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_old_mailbox": {
      "file": "test/test_old_mailbox.py", 
      "imports": [
        "email.parser", 
        "mailbox", 
        "os", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_opcodes": {
      "file": "test/test_opcodes.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_openpty": {
      "file": "test/test_openpty.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_operator": {
      "file": "test/test_operator.py", 
      "imports": [
        "gc", 
        "operator", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_optparse": {
      "file": "test/test_optparse.py", 
      "imports": [
        "StringIO", 
        "copy", 
        "optparse", 
        "os", 
        "re", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_os": {
      "file": "test/test_os.py", 
      "imports": [
        "ctypes", 
        "ctypes.wintypes", 
        "errno", 
        "mmap", 
        "msvcrt", 
        "os", 
        "signal", 
        "stat", 
        "subprocess", 
        "sys", 
        "test.mapping_tests", 
        "test.script_helper", 
        "test.test_support", 
        "time", 
        "unittest", 
        "uuid", 
        "warnings", 
        "resource"
      ]
    }, 
    "test.test_ossaudiodev": {
      "file": "test/test_ossaudiodev.py", 
      "imports": [
        "audioop", 
        "errno", 
        "ossaudiodev.AFMT_S16_NE", 
        "sunau", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_parser": {
      "file": "test/test_parser.py", 
      "imports": [
        "parser", 
        "struct", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pdb": {
      "file": "test/test_pdb.py", 
      "imports": [
        "imp", 
        "os", 
        "subprocess", 
        "sys", 
        "test.test_doctest", 
        "test.test_pdb", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_peepholer": {
      "file": "test/test_peepholer.py", 
      "imports": [
        "cStringIO.StringIO", 
        "dis", 
        "gc", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pep247": {
      "file": "test/test_pep247.py", 
      "imports": [
        "hmac", 
        "md5", 
        "sha", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_pep263": {
      "file": "test/test_pep263.py", 
      "imports": [
        "test.bad_coding3", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pep277": {
      "file": "test/test_pep277.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unicodedata.normalize", 
        "unittest"
      ]
    }, 
    "test.test_pep292": {
      "file": "test/test_pep292.py", 
      "imports": [
        "string", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pep352": {
      "file": "test/test_pep352.py", 
      "imports": [
        "__builtin__", 
        "exceptions", 
        "os", 
        "platform", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_pickle": {
      "file": "test/test_pickle.py", 
      "imports": [
        "cStringIO.StringIO", 
        "pickle", 
        "test.pickletester", 
        "test.test_support"
      ]
    }, 
    "test.test_pickletools": {
      "file": "test/test_pickletools.py", 
      "imports": [
        "pickle", 
        "pickletools", 
        "test.pickletester", 
        "test.test_support"
      ]
    }, 
    "test.test_pipes": {
      "file": "test/test_pipes.py", 
      "imports": [
        "os", 
        "pipes", 
        "string", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pkg": {
      "file": "test/test_pkg.py", 
      "imports": [
        "os", 
        "sys", 
        "t1", 
        "t2.sub", 
        "t2.sub.subsub", 
        "t2.sub.subsub.spam", 
        "t3.sub.subsub", 
        "t5", 
        "t6", 
        "t7", 
        "t7.sub", 
        "t7.sub.subsub", 
        "t7.sub.subsub.spam", 
        "t8", 
        "tempfile", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_pkgimport": {
      "file": "test/test_pkgimport.py", 
      "imports": [
        "os", 
        "random", 
        "string", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pkgutil": {
      "file": "test/test_pkgutil.py", 
      "imports": [
        "foo", 
        "imp", 
        "os", 
        "pkgutil", 
        "shutil", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "zipimport", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_platform": {
      "file": "test/test_platform.py", 
      "imports": [
        "gestalt", 
        "os", 
        "platform", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_plistlib": {
      "file": "test/test_plistlib.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "os", 
        "plistlib", 
        "test.test_support", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_poll": {
      "file": "test/test_poll.py", 
      "imports": [
        "os", 
        "random", 
        "select", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_popen": {
      "file": "test/test_popen.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_popen2": {
      "file": "test/test_popen2.py", 
      "imports": [
        "os", 
        "popen2", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_poplib": {
      "file": "test/test_poplib.py", 
      "imports": [
        "asynchat", 
        "asyncore", 
        "errno", 
        "os", 
        "poplib", 
        "socket", 
        "ssl", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_posix": {
      "file": "test/test_posix.py", 
      "imports": [
        "errno", 
        "os", 
        "platform", 
        "shutil", 
        "stat", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest", 
        "warnings", 
        "pwd"
      ]
    }, 
    "test.test_posixpath": {
      "file": "test/test_posixpath.py", 
      "imports": [
        "os", 
        "posixpath", 
        "test.test_genericpath", 
        "test.test_support", 
        "unittest", 
        "pwd"
      ]
    }, 
    "test.test_pow": {
      "file": "test/test_pow.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pprint": {
      "file": "test/test_pprint.py", 
      "imports": [
        "pprint", 
        "test.test_set", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_print": {
      "file": "test/test_print.py", 
      "imports": [
        "StringIO", 
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_profile": {
      "file": "test/test_profile.py", 
      "imports": [
        "StringIO", 
        "profile", 
        "pstats", 
        "sys", 
        "test.profilee", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_property": {
      "file": "test/test_property.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pstats": {
      "file": "test/test_pstats.py", 
      "imports": [
        "pstats", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pty": {
      "file": "test/test_pty.py", 
      "imports": [
        "errno", 
        "os", 
        "pty", 
        "select", 
        "signal", 
        "socket", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pwd": {
      "file": "test/test_pwd.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_py3kwarn": {
      "file": "test/test_py3kwarn.py", 
      "imports": [
        "operator.add", 
        "operator.isCallable", 
        "operator.sequenceIncludes", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "UserString", 
        "warnings"
      ]
    }, 
    "test.test_py_compile": {
      "file": "test/test_py_compile.py", 
      "imports": [
        "imp", 
        "os", 
        "py_compile", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pyclbr": {
      "file": "test/test_pyclbr.py", 
      "imports": [
        "commands", 
        "pyclbr", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_pydoc": {
      "file": "test/test_pydoc.py", 
      "imports": [
        "__builtin__", 
        "collections", 
        "contextlib", 
        "difflib", 
        "inspect", 
        "keyword", 
        "nturl2path", 
        "os", 
        "pkgutil", 
        "pydoc", 
        "re", 
        "sys", 
        "test.pydoc_mod", 
        "test.pydocfodder", 
        "test.script_helper", 
        "test.test_support", 
        "types", 
        "unittest", 
        "xml.etree"
      ]
    }, 
    "test.test_pyexpat": {
      "file": "test/test_pyexpat.py", 
      "imports": [
        "StringIO", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "xml.parsers.expat"
      ]
    }, 
    "test.test_queue": {
      "file": "test/test_queue.py", 
      "imports": [
        "Queue", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_quopri": {
      "file": "test/test_quopri.py", 
      "imports": [
        "cStringIO", 
        "quopri", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_random": {
      "file": "test/test_random.py", 
      "imports": [
        "functools", 
        "math.exp", 
        "math.fsum", 
        "math.ldexp", 
        "math.log", 
        "math.pi", 
        "math.sin", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_re": {
      "file": "test/test_re.py", 
      "imports": [
        "_sre", 
        "_sre.MAXREPEAT", 
        "array", 
        "pickle", 
        "re", 
        "sre", 
        "sre_constants", 
        "string", 
        "sys", 
        "test.re_tests", 
        "test.test_support", 
        "traceback", 
        "unittest", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_readline": {
      "file": "test/test_readline.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_repr": {
      "file": "test/test_repr.py", 
      "imports": [
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.bar", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.baz", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.foo", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.qux", 
        "array.array", 
        "collections", 
        "os", 
        "repr", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_resource": {
      "file": "test/test_resource.py", 
      "imports": [
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_rfc822": {
      "file": "test/test_rfc822.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_richcmp": {
      "file": "test/test_richcmp.py", 
      "imports": [
        "operator", 
        "random", 
        "test.test_support", 
        "unittest", 
        "UserList"
      ]
    }, 
    "test.test_rlcompleter": {
      "file": "test/test_rlcompleter.py", 
      "imports": [
        "__builtin__", 
        "rlcompleter", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_robotparser": {
      "file": "test/test_robotparser.py", 
      "imports": [
        "StringIO", 
        "robotparser", 
        "test.test_support", 
        "unittest", 
        "urllib2"
      ]
    }, 
    "test.test_runpy": {
      "file": "test/test_runpy.py", 
      "imports": [
        "os", 
        "re", 
        "runpy", 
        "sys", 
        "tempfile", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sax": {
      "file": "test/test_sax.py", 
      "imports": [
        "cStringIO.StringIO", 
        "io", 
        "os", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "xml.sax.xmlreader", 
        "xml.sax.saxutils", 
        "xml.sax.handler", 
        "xml.sax.expatreader", 
        "xml.sax"
      ]
    }, 
    "test.test_scope": {
      "file": "test/test_scope.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_scriptpackages": {
      "file": "test/test_scriptpackages.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_select": {
      "file": "test/test_select.py", 
      "imports": [
        "os", 
        "select", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_set": {
      "file": "test/test_set.py", 
      "imports": [
        "collections", 
        "copy", 
        "gc", 
        "itertools.chain", 
        "itertools.imap", 
        "operator", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_setcomps": {
      "file": "test/test_setcomps.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.test_setcomps", 
        "test.test_support"
      ]
    }, 
    "test.test_sets": {
      "file": "test/test_sets.py", 
      "imports": [
        "copy", 
        "doctest", 
        "operator", 
        "pickle", 
        "random", 
        "sets", 
        "test.test_sets", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sgmllib": {
      "file": "test/test_sgmllib.py", 
      "imports": [
        "pprint", 
        "re", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sha": {
      "file": "test/test_sha.py", 
      "imports": [
        "sha", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_shelve": {
      "file": "test/test_shelve.py", 
      "imports": [
        "glob", 
        "os", 
        "shelve", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_shlex": {
      "file": "test/test_shlex.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "shlex", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_shutil": {
      "file": "test/test_shutil.py", 
      "imports": [
        "distutils.spawn", 
        "errno", 
        "os", 
        "shutil", 
        "stat", 
        "sys", 
        "tarfile", 
        "tempfile", 
        "test.test_support", 
        "zlib", 
        "unittest", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "test.test_signal": {
      "file": "test/test_signal.py", 
      "imports": [
        "contextlib", 
        "errno", 
        "fcntl", 
        "gc", 
        "os", 
        "pickle", 
        "select", 
        "signal", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "time", 
        "traceback", 
        "unittest"
      ]
    }, 
    "test.test_site": {
      "file": "test/test_site.py", 
      "imports": [
        "__builtin__", 
        "copy", 
        "encodings", 
        "locale", 
        "os", 
        "re", 
        "site", 
        "sitecustomize", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_slice": {
      "file": "test/test_slice.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_smtplib": {
      "file": "test/test_smtplib.py", 
      "imports": [
        "StringIO", 
        "asyncore", 
        "email.utils", 
        "select", 
        "smtpd", 
        "smtplib", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_smtpnet": {
      "file": "test/test_smtpnet.py", 
      "imports": [
        "smtplib", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_socket": {
      "file": "test/test_socket.py", 
      "imports": [
        "Queue", 
        "array", 
        "contextlib", 
        "errno", 
        "math", 
        "os", 
        "select", 
        "signal", 
        "socket", 
        "sys", 
        "test.test_support", 
        "thread", 
        "threading", 
        "time", 
        "traceback", 
        "unittest", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_socketserver": {
      "file": "test/test_socketserver.py", 
      "imports": [
        "SocketServer", 
        "contextlib", 
        "errno", 
        "imp", 
        "os", 
        "select", 
        "signal", 
        "socket", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_softspace": {
      "file": "test/test_softspace.py", 
      "imports": [
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sort": {
      "file": "test/test_sort.py", 
      "imports": [
        "gc", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_spwd": {
      "file": "test/test_spwd.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sqlite": {
      "file": "test/test_sqlite.py", 
      "imports": [
        "sqlite3.test.dbapi", 
        "sqlite3.test.dump", 
        "sqlite3.test.factory", 
        "sqlite3.test.hooks", 
        "sqlite3.test.py25tests", 
        "sqlite3.test.regression", 
        "sqlite3.test.transactions", 
        "sqlite3.test.types", 
        "sqlite3.test.userfunctions", 
        "test.test_support"
      ]
    }, 
    "test.test_ssl": {
      "file": "test/test_ssl.py", 
      "imports": [
        "BaseHTTPServer", 
        "SimpleHTTPServer", 
        "_ssl", 
        "asyncore", 
        "errno", 
        "functools", 
        "gc", 
        "os", 
        "platform", 
        "pprint", 
        "select", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "traceback", 
        "unittest", 
        "urllib", 
        "urlparse", 
        "weakref"
      ]
    }, 
    "test.test_startfile": {
      "file": "test/test_startfile.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "time.sleep", 
        "unittest"
      ]
    }, 
    "test.test_stat": {
      "file": "test/test_stat.py", 
      "imports": [
        "os", 
        "stat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_str": {
      "file": "test/test_str.py", 
      "imports": [
        "struct", 
        "sys", 
        "test.string_tests", 
        "test.test_support", 
        "unittest", 
        "_testcapi", 
        "datetime"
      ]
    }, 
    "test.test_strftime": {
      "file": "test/test_strftime.py", 
      "imports": [
        "calendar", 
        "java", 
        "locale", 
        "re", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_string": {
      "file": "test/test_string.py", 
      "imports": [
        "string", 
        "test.string_tests", 
        "test.test_support", 
        "unittest", 
        "UserList"
      ]
    }, 
    "test.test_stringprep": {
      "file": "test/test_stringprep.py", 
      "imports": [
        "stringprep", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_strop": {
      "file": "test/test_strop.py", 
      "imports": [
        "strop", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_strptime": {
      "file": "test/test_strptime.py", 
      "imports": [
        "_strptime", 
        "locale", 
        "re", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_strtod": {
      "file": "test/test_strtod.py", 
      "imports": [
        "random", 
        "re", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_struct": {
      "file": "test/test_struct.py", 
      "imports": [
        "array", 
        "binascii", 
        "inspect", 
        "math", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_structmembers": {
      "file": "test/test_structmembers.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_structseq": {
      "file": "test/test_structseq.py", 
      "imports": [
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_subprocess": {
      "file": "test/test_subprocess.py", 
      "imports": [
        "errno", 
        "os", 
        "re", 
        "signal", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "resource"
      ]
    }, 
    "test.test_sunau": {
      "file": "test/test_sunau.py", 
      "imports": [
        "sunau", 
        "sys", 
        "test.audiotests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sunaudiodev": {
      "file": "test/test_sunaudiodev.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sundry": {
      "file": "test/test_sundry.py", 
      "imports": [
        "CGIHTTPServer", 
        "audiodev", 
        "bdb", 
        "cgitb", 
        "code", 
        "compileall", 
        "distutils.bcppcompiler", 
        "distutils.ccompiler", 
        "distutils.command.bdist", 
        "distutils.command.bdist_dumb", 
        "distutils.command.bdist_msi", 
        "distutils.command.bdist_rpm", 
        "distutils.command.bdist_wininst", 
        "distutils.command.build", 
        "distutils.command.build_clib", 
        "distutils.command.build_ext", 
        "distutils.command.clean", 
        "distutils.command.config", 
        "distutils.command.install_data", 
        "distutils.command.install_egg_info", 
        "distutils.command.install_headers", 
        "distutils.command.install_lib", 
        "distutils.command.register", 
        "distutils.command.sdist", 
        "distutils.command.upload", 
        "distutils.cygwinccompiler", 
        "distutils.emxccompiler", 
        "distutils.filelist", 
        "distutils.msvccompiler", 
        "distutils.text_file", 
        "distutils.unixccompiler", 
        "encodings", 
        "formatter", 
        "getpass", 
        "htmlentitydefs", 
        "ihooks", 
        "imputil", 
        "keyword", 
        "linecache", 
        "mailcap", 
        "mimify", 
        "nntplib", 
        "nturl2path", 
        "opcode", 
        "os2emxpath", 
        "pdb", 
        "posixfile", 
        "pstats", 
        "py_compile", 
        "rexec", 
        "sched", 
        "sndhdr", 
        "statvfs", 
        "stringold", 
        "sunau", 
        "sunaudio", 
        "symbol", 
        "sys", 
        "tabnanny", 
        "test.test_support", 
        "token", 
        "timeit", 
        "toaiff", 
        "tty", 
        "unittest", 
        "webbrowser", 
        "xml"
      ]
    }, 
    "test.test_support": {
      "file": "test/test_support.py", 
      "imports": [
        "StringIO", 
        "Tkinter.Tk", 
        "contextlib", 
        "ctypes", 
        "ctypes.Structure", 
        "ctypes.c_int", 
        "ctypes.cdll", 
        "ctypes.pointer", 
        "ctypes.util.find_library", 
        "ctypes.wintypes", 
        "doctest", 
        "errno", 
        "functools", 
        "gc", 
        "importlib", 
        "locale", 
        "os", 
        "pdb", 
        "platform", 
        "re", 
        "shutil", 
        "socket", 
        "struct", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test", 
        "thread", 
        "time", 
        "traceback", 
        "unittest", 
        "urllib2", 
        "urlparse", 
        "UserDict", 
        "warnings", 
        "_testcapi"
      ]
    }, 
    "test.test_symtable": {
      "file": "test/test_symtable.py", 
      "imports": [
        "symtable", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_syntax": {
      "file": "test/test_syntax.py", 
      "imports": [
        "re", 
        "test.test_support", 
        "test.test_syntax", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_sys": {
      "file": "test/test_sys.py", 
      "imports": [
        "__builtin__", 
        "_ast", 
        "cStringIO", 
        "codecs", 
        "encodings.iso8859_3", 
        "imp", 
        "inspect", 
        "operator", 
        "os", 
        "re", 
        "struct", 
        "subprocess", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "thread", 
        "threading", 
        "traceback", 
        "types", 
        "unittest", 
        "weakref", 
        "_testcapi", 
        "datetime"
      ]
    }, 
    "test.test_sys_setprofile": {
      "file": "test/test_sys_setprofile.py", 
      "imports": [
        "gc", 
        "pprint", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sys_settrace": {
      "file": "test/test_sys_settrace.py", 
      "imports": [
        "difflib", 
        "gc", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sysconfig": {
      "file": "test/test_sysconfig.py", 
      "imports": [
        "_osx_support", 
        "copy", 
        "os", 
        "shutil", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_tarfile": {
      "file": "test/test_tarfile.py", 
      "imports": [
        "StringIO", 
        "bz2", 
        "errno", 
        "gzip", 
        "hashlib", 
        "os", 
        "shutil", 
        "sys", 
        "tarfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_tcl": {
      "file": "test/test_tcl.py", 
      "imports": [
        "Tkinter.Tcl", 
        "_tkinter.TclError", 
        "os", 
        "subprocess.PIPE", 
        "subprocess.Popen", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_telnetlib": {
      "file": "test/test_telnetlib.py", 
      "imports": [
        "Queue", 
        "socket", 
        "telnetlib", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_tempfile": {
      "file": "test/test_tempfile.py", 
      "imports": [
        "contextlib", 
        "errno", 
        "io", 
        "os", 
        "re", 
        "shutil", 
        "signal", 
        "stat", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_textwrap": {
      "file": "test/test_textwrap.py", 
      "imports": [
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_thread": {
      "file": "test/test_thread.py", 
      "imports": [
        "os", 
        "random", 
        "sys", 
        "test.lock_tests", 
        "test.test_support", 
        "time", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_threaded_import": {
      "file": "test/test_threaded_import.py", 
      "imports": [
        "imp", 
        "random", 
        "sys", 
        "test.test_support", 
        "test.threaded_import_hangers", 
        "unittest"
      ]
    }, 
    "test.test_threadedtempfile": {
      "file": "test/test_threadedtempfile.py", 
      "imports": [
        "StringIO", 
        "tempfile", 
        "test.test_support", 
        "traceback", 
        "unittest"
      ]
    }, 
    "test.test_threading": {
      "file": "test/test_threading.py", 
      "imports": [
        "ctypes", 
        "os", 
        "random", 
        "re", 
        "subprocess", 
        "sys", 
        "test.lock_tests", 
        "test.script_helper", 
        "test.test_support", 
        "time", 
        "unittest", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_threading_local": {
      "file": "test/test_threading_local.py", 
      "imports": [
        "_threading_local", 
        "doctest", 
        "gc", 
        "test.test_support", 
        "thread._local", 
        "time", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_threadsignals": {
      "file": "test/test_threadsignals.py", 
      "imports": [
        "os", 
        "signal", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_time": {
      "file": "test/test_time.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_timeout": {
      "file": "test/test_timeout.py", 
      "imports": [
        "socket", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_tk": {
      "file": "test/test_tk.py", 
      "imports": [
        "os", 
        "runtktests", 
        "test.test_support"
      ]
    }, 
    "test.test_tokenize": {
      "file": "test/test_tokenize.py", 
      "imports": [
        "StringIO", 
        "os", 
        "test.test_support", 
        "test.test_tokenize", 
        "tokenize", 
        "unittest"
      ]
    }, 
    "test.test_tools": {
      "file": "test/test_tools.py", 
      "imports": [
        "os", 
        "shutil", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.script_helper", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_trace": {
      "file": "test/test_trace.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "test.tracedmodules.testmod", 
        "trace", 
        "unittest"
      ]
    }, 
    "test.test_traceback": {
      "file": "test/test_traceback.py", 
      "imports": [
        "StringIO", 
        "imp.reload", 
        "os", 
        "sys", 
        "tempfile", 
        "test.badsyntax_nocaret", 
        "test.test_support", 
        "test_bug737473", 
        "time", 
        "traceback", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_transformer": {
      "file": "test/test_transformer.py", 
      "imports": [
        "compiler", 
        "compiler.ast", 
        "compiler.transformer", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ttk_guionly": {
      "file": "test/test_ttk_guionly.py", 
      "imports": [
        "_tkinter.TclError", 
        "os", 
        "runtktests", 
        "test.test_support", 
        "test_ttk.support.get_tk_root", 
        "ttk", 
        "unittest"
      ]
    }, 
    "test.test_ttk_textonly": {
      "file": "test/test_ttk_textonly.py", 
      "imports": [
        "os", 
        "runtktests", 
        "test.test_support"
      ]
    }, 
    "test.test_tuple": {
      "file": "test/test_tuple.py", 
      "imports": [
        "gc", 
        "test.seq_tests", 
        "test.test_support"
      ]
    }, 
    "test.test_typechecks": {
      "file": "test/test_typechecks.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_types": {
      "file": "test/test_types.py", 
      "imports": [
        "array", 
        "locale", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ucn": {
      "file": "test/test_ucn.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unicodedata", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_unary": {
      "file": "test/test_unary.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_undocumented_details": {
      "file": "test/test_undocumented_details.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_unicode": {
      "file": "test/test_unicode.py", 
      "imports": [
        "codecs", 
        "imp", 
        "struct", 
        "sys", 
        "test.string_tests", 
        "test.test_support", 
        "unittest", 
        "_testcapi", 
        "datetime"
      ]
    }, 
    "test.test_unicode_file": {
      "file": "test/test_unicode_file.py", 
      "imports": [
        "glob", 
        "os", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "time", 
        "unicodedata", 
        "unittest"
      ]
    }, 
    "test.test_unicodedata": {
      "file": "test/test_unicodedata.py", 
      "imports": [
        "hashlib", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unicodedata", 
        "unittest"
      ]
    }, 
    "test.test_unittest": {
      "file": "test/test_unittest.py", 
      "imports": [
        "test.test_support", 
        "unittest.test"
      ]
    }, 
    "test.test_univnewlines": {
      "file": "test/test_univnewlines.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "io", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_univnewlines2k": {
      "file": "test/test_univnewlines2k.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_unpack": {
      "file": "test/test_unpack.py", 
      "imports": [
        "test.test_support", 
        "test.test_unpack"
      ]
    }, 
    "test.test_urllib": {
      "file": "test/test_urllib.py", 
      "imports": [
        "StringIO", 
        "base64", 
        "httplib", 
        "mimetools", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "urllib", 
        "warnings"
      ]
    }, 
    "test.test_urllib2": {
      "file": "test/test_urllib2.py", 
      "imports": [
        "StringIO", 
        "base64", 
        "cookielib", 
        "copy", 
        "ftplib", 
        "httplib", 
        "mimetools", 
        "os", 
        "rfc822", 
        "socket", 
        "string", 
        "test.test_cookielib", 
        "test.test_support", 
        "test.test_urllib2", 
        "unittest", 
        "urllib", 
        "urllib2"
      ]
    }, 
    "test.test_urllib2_localnet": {
      "file": "test/test_urllib2_localnet.py", 
      "imports": [
        "BaseHTTPServer", 
        "hashlib", 
        "test.test_support", 
        "unittest", 
        "urllib2", 
        "urlparse"
      ]
    }, 
    "test.test_urllib2net": {
      "file": "test/test_urllib2net.py", 
      "imports": [
        "httplib", 
        "logging", 
        "os", 
        "socket", 
        "sys", 
        "test.test_support", 
        "test.test_urllib2", 
        "time", 
        "unittest", 
        "urllib2"
      ]
    }, 
    "test.test_urllibnet": {
      "file": "test/test_urllibnet.py", 
      "imports": [
        "os", 
        "socket", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "urllib"
      ]
    }, 
    "test.test_urlparse": {
      "file": "test/test_urlparse.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "urlparse"
      ]
    }, 
    "test.test_userdict": {
      "file": "test/test_userdict.py", 
      "imports": [
        "test.mapping_tests", 
        "test.test_support", 
        "UserDict"
      ]
    }, 
    "test.test_userlist": {
      "file": "test/test_userlist.py", 
      "imports": [
        "test.list_tests", 
        "test.test_support", 
        "UserList"
      ]
    }, 
    "test.test_userstring": {
      "file": "test/test_userstring.py", 
      "imports": [
        "string", 
        "test.string_tests", 
        "test.test_support", 
        "UserString", 
        "warnings"
      ]
    }, 
    "test.test_uu": {
      "file": "test/test_uu.py", 
      "imports": [
        "cStringIO", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "uu"
      ]
    }, 
    "test.test_uuid": {
      "file": "test/test_uuid.py", 
      "imports": [
        "io", 
        "os", 
        "test.test_support", 
        "unittest", 
        "uuid"
      ]
    }, 
    "test.test_wait3": {
      "file": "test/test_wait3.py", 
      "imports": [
        "os", 
        "test.fork_wait", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_wait4": {
      "file": "test/test_wait4.py", 
      "imports": [
        "os", 
        "sys", 
        "test.fork_wait", 
        "test.test_support", 
        "time"
      ]
    }, 
    "test.test_warnings": {
      "file": "test/test_warnings.py", 
      "imports": [
        "StringIO", 
        "contextlib", 
        "linecache", 
        "os", 
        "subprocess", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "test.warning_tests", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_wave": {
      "file": "test/test_wave.py", 
      "imports": [
        "sys", 
        "test.audiotests", 
        "test.test_support", 
        "wave", 
        "unittest"
      ]
    }, 
    "test.test_weakref": {
      "file": "test/test_weakref.py", 
      "imports": [
        "contextlib", 
        "copy", 
        "gc", 
        "operator", 
        "sys", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest", 
        "UserList", 
        "weakref"
      ]
    }, 
    "test.test_weakset": {
      "file": "test/test_weakset.py", 
      "imports": [
        "collections", 
        "contextlib", 
        "copy", 
        "gc", 
        "operator", 
        "os", 
        "random", 
        "string", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "UserString", 
        "warnings", 
        "weakref"
      ]
    }, 
    "test.test_whichdb": {
      "file": "test/test_whichdb.py", 
      "imports": [
        "glob", 
        "os", 
        "test.test_support", 
        "unittest", 
        "whichdb"
      ]
    }, 
    "test.test_winreg": {
      "file": "test/test_winreg.py", 
      "imports": [
        "_winreg.*", 
        "errno", 
        "os", 
        "platform", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_winsound": {
      "file": "test/test_winsound.py", 
      "imports": [
        "_winreg", 
        "os", 
        "subprocess", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_with": {
      "file": "test/test_with.py", 
      "imports": [
        "collections", 
        "contextlib", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_wsgiref": {
      "file": "test/test_wsgiref.py", 
      "imports": [
        "SocketServer", 
        "StringIO", 
        "__future__", 
        "os", 
        "re", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "wsgiref.validate", 
        "wsgiref.util", 
        "wsgiref.simple_server", 
        "wsgiref.headers", 
        "wsgiref.handlers"
      ]
    }, 
    "test.test_xdrlib": {
      "file": "test/test_xdrlib.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "xdrlib"
      ]
    }, 
    "test.test_xml_etree": {
      "file": "test/test_xml_etree.py", 
      "imports": [
        "StringIO", 
        "cgi", 
        "sys", 
        "test.test_support", 
        "test.test_xml_etree", 
        "xml.etree.ElementTree", 
        "xml.etree.ElementPath"
      ]
    }, 
    "test.test_xml_etree_c": {
      "file": "test/test_xml_etree_c.py", 
      "imports": [
        "test.test_support", 
        "test.test_xml_etree", 
        "test.test_xml_etree_c", 
        "unittest"
      ]
    }, 
    "test.test_xmllib": {
      "file": "test/test_xmllib.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_xmlrpc": {
      "file": "test/test_xmlrpc.py", 
      "imports": [
        "SimpleXMLRPCServer", 
        "StringIO", 
        "base64", 
        "gzip", 
        "httplib", 
        "mimetools", 
        "os", 
        "re", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "xmlrpclib", 
        "datetime"
      ]
    }, 
    "test.test_xpickle": {
      "file": "test/test_xpickle.py", 
      "imports": [
        "os", 
        "pickle", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_xrange": {
      "file": "test/test_xrange.py", 
      "imports": [
        "itertools", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_zipfile": {
      "file": "test/test_zipfile.py", 
      "imports": [
        "StringIO", 
        "email", 
        "io", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "zlib", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_zipfile64": {
      "file": "test/test_zipfile64.py", 
      "imports": [
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "zlib", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_zipimport": {
      "file": "test/test_zipimport.py", 
      "imports": [
        "StringIO", 
        "doctest", 
        "imp", 
        "inspect", 
        "linecache", 
        "marshal", 
        "os", 
        "struct", 
        "sys", 
        "test.test_importhooks", 
        "test.test_support", 
        "time", 
        "zipimport", 
        "zlib", 
        "traceback", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_zipimport_support": {
      "file": "test/test_zipimport_support.py", 
      "imports": [
        "doctest", 
        "inspect", 
        "linecache", 
        "os", 
        "pdb", 
        "sys", 
        "test.sample_doctest", 
        "test.sample_doctest_no_docstrings", 
        "test.sample_doctest_no_doctests", 
        "test.script_helper", 
        "test.test_doctest", 
        "test.test_importhooks", 
        "test.test_support", 
        "test_zipped_doctest", 
        "zip_pkg", 
        "zipimport", 
        "textwrap", 
        "warnings", 
        "zipfile"
      ]
    }, 
    "test.test_zlib": {
      "file": "test/test_zlib.py", 
      "imports": [
        "binascii", 
        "mmap", 
        "os", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.testall": {
      "file": "test/testall.py", 
      "imports": [
        "sys", 
        "test.regrtest", 
        "warnings"
      ]
    }, 
    "test.testcodec": {
      "file": "test/testcodec.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "test.tf_inherit_check": {
      "file": "test/tf_inherit_check.py", 
      "imports": [
        "os", 
        "sys"
      ]
    }, 
    "test.threaded_import_hangers": {
      "file": "test/threaded_import_hangers.py", 
      "imports": [
        "os", 
        "tempfile", 
        "threading"
      ]
    }, 
    "test.time_hashlib": {
      "file": "test/time_hashlib.py", 
      "imports": [
        "_hashlib", 
        "hashlib", 
        "sys", 
        "time"
      ]
    }, 
    "test.tracedmodules": {
      "dir": "test/tracedmodules"
    }, 
    "test.tracedmodules.__init__": {
      "file": "test/tracedmodules/__init__.py", 
      "imports": []
    }, 
    "test.tracedmodules.testmod": {
      "file": "test/tracedmodules/testmod.py", 
      "imports": []
    }, 
    "test.warning_tests": {
      "file": "test/warning_tests.py", 
      "imports": [
        "warnings"
      ]
    }, 
    "test.win_console_handler": {
      "file": "test/win_console_handler.py", 
      "imports": [
        "ctypes", 
        "ctypes.WINFUNCTYPE", 
        "ctypes.wintypes", 
        "mmap", 
        "signal", 
        "sys"
      ]
    }, 
    "test.xmltests": {
      "file": "test/xmltests.py", 
      "imports": [
        "sys", 
        "test.test_support"
      ]
    }, 
    "textwrap": {
      "file": "textwrap.py", 
      "imports": [
        "re", 
        "string"
      ]
    }, 
    "this": {
      "file": "this.py", 
      "imports": []
    }, 
    "timeit": {
      "file": "timeit.py", 
      "imports": [
        "gc", 
        "getopt", 
        "linecache", 
        "os", 
        "sys", 
        "time", 
        "traceback"
      ]
    }, 
    "toaiff": {
      "file": "toaiff.py", 
      "imports": [
        "os", 
        "pipes", 
        "sndhdr", 
        "tempfile", 
        "warnings"
      ]
    }, 
    "token": {
      "file": "token.py", 
      "imports": [
        "re", 
        "sys"
      ]
    }, 
    "tokenize": {
      "file": "tokenize.py", 
      "imports": [
        "itertools.chain", 
        "re", 
        "string", 
        "sys", 
        "token"
      ]
    }, 
    "tputil": {
      "file": "tputil.py", 
      "imports": [
        "__pypy__.tproxy", 
        "types"
      ]
    }, 
    "trace": {
      "file": "trace.py", 
      "imports": [
        "__main__", 
        "dis", 
        "gc", 
        "getopt", 
        "inspect", 
        "linecache", 
        "os", 
        "pickle", 
        "re", 
        "sys", 
        "threading", 
        "time", 
        "token", 
        "tokenize", 
        "cPickle"
      ]
    }, 
    "traceback": {
      "file": "traceback.py", 
      "imports": [
        "linecache", 
        "sys", 
        "types"
      ]
    }, 
    "tty": {
      "file": "tty.py", 
      "imports": [
        "termios.*"
      ]
    }, 
    "types": {
      "file": "types.py", 
      "imports": [
        "sys"
      ]
    }, 
    "unittest": {
      "dir": "unittest"
    }, 
    "unittest.__init__": {
      "file": "unittest/__init__.py", 
      "imports": [
        "unittest.case", 
        "unittest.loader", 
        "unittest.main", 
        "unittest.result", 
        "unittest.runner", 
        "unittest.signals", 
        "unittest.suite"
      ]
    }, 
    "unittest.__main__": {
      "file": "unittest/__main__.py", 
      "imports": [
        "sys", 
        "unittest.main"
      ]
    }, 
    "unittest.case": {
      "file": "unittest/case.py", 
      "imports": [
        "collections", 
        "difflib", 
        "functools", 
        "pprint", 
        "re", 
        "sys", 
        "types", 
        "unittest.result", 
        "unittest.util", 
        "warnings"
      ]
    }, 
    "unittest.loader": {
      "file": "unittest/loader.py", 
      "imports": [
        "fnmatch", 
        "functools", 
        "os", 
        "re", 
        "sys", 
        "traceback", 
        "types", 
        "unittest.case", 
        "unittest.suite"
      ]
    }, 
    "unittest.main": {
      "file": "unittest/main.py", 
      "imports": [
        "getopt", 
        "optparse", 
        "os", 
        "sys", 
        "types", 
        "unittest.loader", 
        "unittest.runner", 
        "unittest.signals"
      ]
    }, 
    "unittest.result": {
      "file": "unittest/result.py", 
      "imports": [
        "StringIO", 
        "functools", 
        "os", 
        "sys", 
        "traceback", 
        "unittest.util"
      ]
    }, 
    "unittest.runner": {
      "file": "unittest/runner.py", 
      "imports": [
        "sys", 
        "time", 
        "unittest.result", 
        "unittest.signals"
      ]
    }, 
    "unittest.signals": {
      "file": "unittest/signals.py", 
      "imports": [
        "functools", 
        "signal", 
        "weakref"
      ]
    }, 
    "unittest.suite": {
      "file": "unittest/suite.py", 
      "imports": [
        "sys", 
        "unittest.case", 
        "unittest.util"
      ]
    }, 
    "unittest.test": {
      "dir": "unittest/test"
    }, 
    "unittest.test.__init__": {
      "file": "unittest/test/__init__.py", 
      "imports": [
        "os", 
        "sys", 
        "unittest"
      ]
    }, 
    "unittest.test.dummy": {
      "file": "unittest/test/dummy.py", 
      "imports": []
    }, 
    "unittest.test.support": {
      "file": "unittest/test/support.py", 
      "imports": [
        "unittest"
      ]
    }, 
    "unittest.test.test_assertions": {
      "file": "unittest/test/test_assertions.py", 
      "imports": [
        "unittest", 
        "datetime"
      ]
    }, 
    "unittest.test.test_break": {
      "file": "unittest/test/test_break.py", 
      "imports": [
        "cStringIO.StringIO", 
        "gc", 
        "os", 
        "signal", 
        "sys", 
        "unittest", 
        "weakref"
      ]
    }, 
    "unittest.test.test_case": {
      "file": "unittest/test/test_case.py", 
      "imports": [
        "copy", 
        "difflib", 
        "pickle", 
        "pprint", 
        "re", 
        "sys", 
        "unittest", 
        "unittest.test", 
        "unittest.test.support"
      ]
    }, 
    "unittest.test.test_discovery": {
      "file": "unittest/test/test_discovery.py", 
      "imports": [
        "os", 
        "re", 
        "sys", 
        "unittest"
      ]
    }, 
    "unittest.test.test_functiontestcase": {
      "file": "unittest/test/test_functiontestcase.py", 
      "imports": [
        "unittest", 
        "unittest.test.support"
      ]
    }, 
    "unittest.test.test_loader": {
      "file": "unittest/test/test_loader.py", 
      "imports": [
        "sys", 
        "types", 
        "unittest"
      ]
    }, 
    "unittest.test.test_program": {
      "file": "unittest/test/test_program.py", 
      "imports": [
        "cStringIO.StringIO", 
        "os", 
        "sys", 
        "unittest"
      ]
    }, 
    "unittest.test.test_result": {
      "file": "unittest/test/test_result.py", 
      "imports": [
        "StringIO", 
        "sys", 
        "textwrap", 
        "traceback", 
        "unittest", 
        "unittest.test"
      ]
    }, 
    "unittest.test.test_runner": {
      "file": "unittest/test/test_runner.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "pickle", 
        "unittest", 
        "unittest.test.support"
      ]
    }, 
    "unittest.test.test_setups": {
      "file": "unittest/test/test_setups.py", 
      "imports": [
        "cStringIO.StringIO", 
        "sys", 
        "unittest"
      ]
    }, 
    "unittest.test.test_skipping": {
      "file": "unittest/test/test_skipping.py", 
      "imports": [
        "unittest", 
        "unittest.test.support"
      ]
    }, 
    "unittest.test.test_suite": {
      "file": "unittest/test/test_suite.py", 
      "imports": [
        "sys", 
        "unittest", 
        "unittest.test.support"
      ]
    }, 
    "unittest.util": {
      "file": "unittest/util.py", 
      "imports": [
        "collections"
      ]
    }, 
    "urllib": {
      "file": "urllib.py", 
      "imports": [
        "StringIO", 
        "_winreg", 
        "base64", 
        "cStringIO.StringIO", 
        "email.utils", 
        "fnmatch", 
        "ftplib", 
        "getpass", 
        "httplib", 
        "mimetools", 
        "mimetypes", 
        "nturl2path", 
        "os", 
        "re", 
        "rourl2path.pathname2url", 
        "rourl2path.url2pathname", 
        "socket", 
        "ssl", 
        "string", 
        "sys", 
        "tempfile", 
        "time", 
        "urlparse", 
        "warnings", 
        "_scproxy"
      ]
    }, 
    "urllib2": {
      "file": "urllib2.py", 
      "imports": [
        "StringIO", 
        "base64", 
        "bisect", 
        "cStringIO.StringIO", 
        "cookielib", 
        "email.utils", 
        "ftplib", 
        "hashlib", 
        "httplib", 
        "mimetools", 
        "mimetypes", 
        "os", 
        "posixpath", 
        "random", 
        "re", 
        "socket", 
        "sys", 
        "time", 
        "types", 
        "urllib", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "urlparse": {
      "file": "urlparse.py", 
      "imports": [
        "collections", 
        "re"
      ]
    }, 
    "user": {
      "file": "user.py", 
      "imports": [
        "os", 
        "warnings"
      ]
    }, 
    "uu": {
      "file": "uu.py", 
      "imports": [
        "binascii", 
        "optparse", 
        "os", 
        "sys"
      ]
    }, 
    "uuid": {
      "file": "uuid.py", 
      "imports": [
        "ctypes", 
        "ctypes.util", 
        "hashlib", 
        "netbios", 
        "os", 
        "random", 
        "re", 
        "socket", 
        "struct", 
        "sys", 
        "time", 
        "win32wnet"
      ]
    }, 
    "warnings": {
      "file": "warnings.py", 
      "imports": [
        "_warnings.default_action", 
        "_warnings.filters", 
        "_warnings.once_registry", 
        "_warnings.warn", 
        "_warnings.warn_explicit", 
        "linecache", 
        "re", 
        "sys", 
        "types"
      ]
    }, 
    "weakref": {
      "file": "weakref.py", 
      "imports": [
        "UserDict", 
        "_weakref.CallableProxyType", 
        "_weakref.ProxyType", 
        "_weakref.ReferenceType", 
        "_weakref.getweakrefcount", 
        "_weakref.getweakrefs", 
        "_weakref.proxy", 
        "_weakref.ref", 
        "_weakrefset", 
        "copy", 
        "exceptions.ReferenceError"
      ]
    }, 
    "webbrowser": {
      "file": "webbrowser.py", 
      "imports": [
        "copy", 
        "getopt", 
        "glob", 
        "os", 
        "shlex", 
        "socket", 
        "stat", 
        "subprocess", 
        "sys", 
        "tempfile", 
        "time", 
        "pwd"
      ]
    }, 
    "whichdb": {
      "file": "whichdb.py", 
      "imports": [
        "os", 
        "struct", 
        "sys", 
        "dbm"
      ]
    }, 
    "wsgiref": {
      "dir": "wsgiref"
    }, 
    "wsgiref.__init__": {
      "file": "wsgiref/__init__.py", 
      "imports": []
    }, 
    "wsgiref.handlers": {
      "file": "wsgiref/handlers.py", 
      "imports": [
        "os", 
        "sys", 
        "time", 
        "traceback", 
        "types", 
        "wsgiref.headers", 
        "wsgiref.util"
      ]
    }, 
    "wsgiref.headers": {
      "file": "wsgiref/headers.py", 
      "imports": [
        "re", 
        "types"
      ]
    }, 
    "wsgiref.simple_server": {
      "file": "wsgiref/simple_server.py", 
      "imports": [
        "BaseHTTPServer", 
        "StringIO", 
        "sys", 
        "urllib", 
        "webbrowser", 
        "wsgiref.handlers"
      ]
    }, 
    "wsgiref.util": {
      "file": "wsgiref/util.py", 
      "imports": [
        "StringIO", 
        "posixpath", 
        "urllib"
      ]
    }, 
    "wsgiref.validate": {
      "file": "wsgiref/validate.py", 
      "imports": [
        "re", 
        "sys", 
        "types", 
        "warnings"
      ]
    }, 
    "xdrlib": {
      "file": "xdrlib.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "struct"
      ]
    }, 
    "xml": {
      "dir": "xml"
    }, 
    "xml.__init__": {
      "file": "xml/__init__.py", 
      "imports": [
        "_xmlplus", 
        "sys"
      ]
    }, 
    "xml.dom": {
      "dir": "xml/dom"
    }, 
    "xml.dom.NodeFilter": {
      "file": "xml/dom/NodeFilter.py", 
      "imports": []
    }, 
    "xml.dom.__init__": {
      "file": "xml/dom/__init__.py", 
      "imports": [
        "xml.dom.domreg"
      ]
    }, 
    "xml.dom.domreg": {
      "file": "xml/dom/domreg.py", 
      "imports": [
        "os", 
        "xml.dom.minicompat"
      ]
    }, 
    "xml.dom.expatbuilder": {
      "file": "xml/dom/expatbuilder.py", 
      "imports": [
        "xml.dom", 
        "xml.dom.NodeFilter", 
        "xml.dom.minicompat", 
        "xml.dom.minidom", 
        "xml.dom.xmlbuilder", 
        "xml.parsers.expat"
      ]
    }, 
    "xml.dom.minicompat": {
      "file": "xml/dom/minicompat.py", 
      "imports": [
        "xml.dom"
      ]
    }, 
    "xml.dom.minidom": {
      "file": "xml/dom/minidom.py", 
      "imports": [
        "StringIO", 
        "codecs", 
        "xml.dom", 
        "xml.dom.domreg", 
        "xml.dom.expatbuilder", 
        "xml.dom.minicompat", 
        "xml.dom.pulldom", 
        "xml.dom.xmlbuilder"
      ]
    }, 
    "xml.dom.pulldom": {
      "file": "xml/dom/pulldom.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "types", 
        "xml.dom", 
        "xml.dom.minidom", 
        "xml.sax", 
        "xml.sax.handler"
      ]
    }, 
    "xml.dom.xmlbuilder": {
      "file": "xml/dom/xmlbuilder.py", 
      "imports": [
        "copy", 
        "posixpath", 
        "urllib2", 
        "urlparse", 
        "xml.dom", 
        "xml.dom.NodeFilter", 
        "xml.dom.expatbuilder"
      ]
    }, 
    "xml.etree": {
      "dir": "xml/etree"
    }, 
    "xml.etree.ElementInclude": {
      "file": "xml/etree/ElementInclude.py", 
      "imports": [
        "copy", 
        "xml.etree.ElementTree"
      ]
    }, 
    "xml.etree.ElementPath": {
      "file": "xml/etree/ElementPath.py", 
      "imports": [
        "re"
      ]
    }, 
    "xml.etree.ElementTree": {
      "file": "xml/etree/ElementTree.py", 
      "imports": [
        "ElementC14N._serialize_c14n", 
        "pyexpat", 
        "re", 
        "sys", 
        "warnings", 
        "xml.etree.ElementPath", 
        "xml.parsers.expat"
      ]
    }, 
    "xml.etree.__init__": {
      "file": "xml/etree/__init__.py", 
      "imports": []
    }, 
    "xml.etree.cElementTree": {
      "file": "xml/etree/cElementTree.py", 
      "imports": [
        "_elementtree"
      ]
    }, 
    "xml.parsers": {
      "dir": "xml/parsers"
    }, 
    "xml.parsers.__init__": {
      "file": "xml/parsers/__init__.py", 
      "imports": []
    }, 
    "xml.parsers.expat": {
      "file": "xml/parsers/expat.py", 
      "imports": [
        "pyexpat.*"
      ]
    }, 
    "xml.sax": {
      "dir": "xml/sax"
    }, 
    "xml.sax.__init__": {
      "file": "xml/sax/__init__.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "org.python.core.imp", 
        "os", 
        "sys", 
        "xml.sax._exceptions", 
        "xml.sax.expatreader", 
        "xml.sax.handler", 
        "xml.sax.xmlreader"
      ]
    }, 
    "xml.sax._exceptions": {
      "file": "xml/sax/_exceptions.py", 
      "imports": [
        "java.lang.Exception", 
        "sys"
      ]
    }, 
    "xml.sax.expatreader": {
      "file": "xml/sax/expatreader.py", 
      "imports": [
        "_weakref", 
        "sys", 
        "weakref", 
        "xml.parsers.expat", 
        "xml.sax._exceptions", 
        "xml.sax.handler", 
        "xml.sax.saxutils", 
        "xml.sax.xmlreader"
      ]
    }, 
    "xml.sax.handler": {
      "file": "xml/sax/handler.py", 
      "imports": []
    }, 
    "xml.sax.saxutils": {
      "file": "xml/sax/saxutils.py", 
      "imports": [
        "io", 
        "os", 
        "sys", 
        "types", 
        "urllib", 
        "urlparse", 
        "xml.sax.handler", 
        "xml.sax.xmlreader"
      ]
    }, 
    "xml.sax.xmlreader": {
      "file": "xml/sax/xmlreader.py", 
      "imports": [
        "xml.sax._exceptions", 
        "xml.sax.handler", 
        "xml.sax.saxutils"
      ]
    }, 
    "xmllib": {
      "file": "xmllib.py", 
      "imports": [
        "getopt", 
        "re", 
        "string", 
        "sys", 
        "time.time", 
        "warnings"
      ]
    }, 
    "xmlrpclib": {
      "file": "xmlrpclib.py", 
      "imports": [
        "StringIO", 
        "_xmlrpclib", 
        "base64", 
        "cStringIO", 
        "errno", 
        "gzip", 
        "httplib", 
        "operator", 
        "re", 
        "socket", 
        "string", 
        "sys.modules", 
        "time", 
        "types", 
        "urllib", 
        "xml.parsers.expat", 
        "xmllib", 
        "datetime"
      ]
    }, 
    "zipfile": {
      "file": "zipfile.py", 
      "imports": [
        "binascii", 
        "cStringIO", 
        "io", 
        "os", 
        "py_compile", 
        "re", 
        "shutil", 
        "stat", 
        "string", 
        "struct", 
        "sys", 
        "textwrap", 
        "time", 
        "warnings", 
        "zlib"
      ]
    }
  }, 
  "preload": {
    "SimPy.Globals": "# coding=utf-8\n\"\"\"\nThis file provides a global Simulation object and the global simulation methods\nused by SimPy up to version 1.9.1.\n\n\"\"\"\nglobal sim\nsim = None\n\ndef initialize():\n    sim.initialize()\n\ndef now():\n    return sim.now()\n\ndef stopSimulation():\n    \"\"\"Application function to stop simulation run\"\"\"\n    sim.stopSimulation()\n\ndef allEventNotices():\n    \"\"\"Returns string with eventlist as;\n            t1: processname, processname2\n            t2: processname4, processname5, . . .\n                . . .  .\n        \"\"\"\n    return sim.allEventNotices()\n\ndef allEventTimes():\n    \"\"\"Returns list of all times for which events are scheduled.\n    \"\"\"\n    return sim.allEventTimes()\n\ndef startCollection(when = 0.0, monitors = None, tallies = None):\n    \"\"\"Starts data collection of all designated Monitor and Tally objects\n    (default = all) at time 'when'.\n    \"\"\"\n    sim.startCollection( when = when, monitors = monitors, tallies = tallies)\n\ndef _startWUStepping():\n    \"\"\"Application function to start stepping through simulation for waituntil\n    construct.\"\"\"\n    sim._startWUStepping()\n\ndef _stopWUStepping():\n    \"\"\"Application function to stop stepping through simulation.\"\"\"\n    sim._stopWUStepping()\n\ndef activate(obj, process, at = 'undefined', delay = 'undefined',\n              prior = False):\n    \"\"\"Application function to activate passive process.\"\"\"\n    sim.activate(obj, process, at = at, delay = delay, prior = prior)\n\ndef reactivate(obj, at = 'undefined', delay = 'undefined', prior = False):\n    \"\"\"Application function to reactivate a process which is active,\n    suspended or passive.\"\"\"\n    sim.reactivate(obj, at = at, delay = delay, prior = prior)\n\ndef simulate(until = 0):\n    return sim.simulate(until = until)\n", 
    "SimPy.Lib": "# coding=utf-8\n\"\"\"\nThis file contains Simerror, FatalSimerror, Process, SimEvent, the resources\nResource, Level and Storage as well as their dependencies Buffer, Queue, FIFO\nand PriorityQ.\n\n\"\"\"\nimport inspect\nimport sys\nimport types\n\nfrom SimPy.Lister import Lister\nfrom SimPy.Recording import Monitor, Tally\n\n# Required for backward compatiblity\nimport SimPy.Globals as Globals\n\n\nclass Simerror(Exception):\n    \"\"\" SimPy error which terminates \"simulate\" with an error message\"\"\"\n    def __init__(self, value):\n        self.value = value\n\n    def __str__(self):\n        return repr(self.value)\n\nclass FatalSimerror(Simerror):\n    \"\"\" SimPy error which terminates script execution with an exception\"\"\"\n    def __init__(self, value):\n        Simerror.__init__(self, value)\n        self.value = value\n\nclass Process(Lister):\n    \"\"\"Superclass of classes which may use generator functions\"\"\"\n    def __init__(self, name = 'a_process', sim = None):\n        if sim is None: sim = Globals.sim # Use global simulation object if sim is None\n        self.sim = sim\n        #the reference to this Process instances single process (==generator)\n        self._nextpoint = None\n        if isinstance(name, str) or (sys.version_info.major == 2 and\n                isinstance(name, basestring)):\n            self.name = name\n        else:\n            raise FatalSimerror(\"Process name parameter '%s' is not a string\"%name)\n        self._nextTime = None #next activation time\n        self._remainService = 0\n        self._preempted = 0\n        self._priority={}\n        self._getpriority={}\n        self._putpriority={}\n        self._terminated = False\n        self._inInterrupt = False\n        self.eventsFired = [] #which events process waited / queued for occurred\n        if hasattr(sim, 'trace'):\n            self._doTracing = True\n        else:\n            self._doTracing = False\n\n    def active(self):\n        return self._nextTime != None and not self._inInterrupt\n\n    def passive(self):\n        return self._nextTime is None and not self._terminated\n\n    def terminated(self):\n        return self._terminated\n\n    def interrupted(self):\n        return self._inInterrupt and not self._terminated\n\n    def queuing(self, resource):\n        return self in resource.waitQ\n\n    def cancel(self, victim):\n        \"\"\"Application function to cancel all event notices for this Process\n        instance;(should be all event notices for the _generator_).\"\"\"\n        self.sim._unpost(whom = victim)\n\n    def start(self, pem = None, at = 'undefined', delay = 'undefined', prior = False):\n        \"\"\"Activates PEM of this Process.\n        p.start(p.pemname([args])[,{at = t | delay = period}][, prior = False]) or\n        p.start([p.ACTIONS()][,{at = t | delay = period}][, prior = False]) (ACTIONS\n                parameter optional)\n        \"\"\"\n        if pem is None:\n            try:\n                pem = self.ACTIONS()\n            except AttributeError:\n                raise FatalSimerror\\\n                       ('no generator function to activate')\n        else:\n            pass\n        if not (type(pem) == types.GeneratorType):\n            raise FatalSimerror('activating function which'+\n                           ' is not a generator (contains no \\'yield\\')')\n        if not self._terminated and not self._nextTime:\n            #store generator reference in object; needed for reactivation\n            self._nextpoint = pem\n            if at == 'undefined':\n                at = self.sim._t\n            if delay == 'undefined':\n                zeit = max(self.sim._t, at)\n            else:\n                zeit = max(self.sim._t, self.sim._t + delay)\n            if self._doTracing:\n                self.sim.trace.recordActivate(who = self, when = zeit,\n                                               prior = prior)\n            self.sim._post(what = self, at = zeit, prior = prior)\n\n    def _hold(self, a):\n        if len(a[0]) == 3: ## yield hold,self,delay\n            delay = a[0][2]\n            if delay < 0:\n                raise FatalSimerror('hold: delay time negative: %s, in %s' % (\n                                     delay, str(a[0][1])))\n        else:              ## yield hold,self\n            delay = 0\n        who = a[1]\n        self.interruptLeft = delay\n        self._inInterrupt = False\n        self.interruptCause = None\n        self.sim._post(what = who, at = self.sim._t + delay)\n\n    def _passivate(self, a):\n        a[0][1]._nextTime = None\n\n    def interrupt(self, victim):\n        \"\"\"Application function to interrupt active processes\"\"\"\n        # can't interrupt terminated / passive / interrupted process\n        if victim.active():\n            if self._doTracing:\n                save = self.sim.trace._comment\n                self.sim.trace._comment = None\n            victim.interruptCause = self  # self causes interrupt\n            left = victim._nextTime - self.sim._t\n            victim.interruptLeft = left   # time left in current 'hold'\n            victim._inInterrupt = True\n            self.sim.reactivate(victim)\n            if self._doTracing:\n                self.sim.trace._comment = save\n                self.sim.trace.recordInterrupt(self, victim)\n            return left\n        else: #victim not active -- can't interrupt\n            return None\n\n    def interruptReset(self):\n        \"\"\"\n        Application function for an interrupt victim to get out of\n        'interrupted' state.\n        \"\"\"\n        self._inInterrupt = False\n\n    def acquired(self, res):\n        \"\"\"Multi - functional test for reneging for 'request' and 'get':\n        (1)If res of type Resource:\n            Tests whether resource res was acquired when proces reactivated.\n            If yes, the parallel wakeup process is killed.\n            If not, process is removed from res.waitQ (reneging).\n        (2)If res of type Store:\n            Tests whether item(s) gotten from Store res.\n            If yes, the parallel wakeup process is killed.\n            If no, process is removed from res.getQ\n        (3)If res of type Level:\n            Tests whether units gotten from Level res.\n            If yes, the parallel wakeup process is killed.\n            If no, process is removed from res.getQ.\n        \"\"\"\n        if isinstance(res, Resource):\n            test = self in res.activeQ\n            if test:\n                self.cancel(self._holder)\n            else:\n                res.waitQ.remove(self)\n                if res.monitored:\n                    res.waitMon.observe(len(res.waitQ),t = self.sim.now())\n            return test\n        elif isinstance(res, Store):\n            test = len(self.got)\n            if test:\n                self.cancel(self._holder)\n            else:\n                res.getQ.remove(self)\n                if res.monitored:\n                    res.getQMon.observe(len(res.getQ),t = self.sim.now())\n            return test\n        elif isinstance(res, Level):\n            test = not (self.got is None)\n            if test:\n                self.cancel(self._holder)\n            else:\n                res.getQ.remove(self)\n                if res.monitored:\n                    res.getQMon.observe(len(res.getQ),t = self.sim.now())\n            return test\n\n    def stored(self, buffer):\n        \"\"\"Test for reneging for 'yield put . . .' compound statement (Level and\n        Store. Returns True if not reneged.\n        If self not in buffer.putQ, kill wakeup process, else take self out of\n        buffer.putQ (reneged)\"\"\"\n        test = self in buffer.putQ\n        if test:    #reneged\n            buffer.putQ.remove(self)\n            if buffer.monitored:\n                buffer.putQMon.observe(len(buffer.putQ),t = self.sim.now())\n        else:\n            self.cancel(self._holder)\n        return not test\n\n\nclass SimEvent(Lister):\n    \"\"\"Supports one - shot signalling between processes. All processes waiting for an event to occur\n    get activated when its occurrence is signalled. From the processes queuing for an event, only\n    the first gets activated.\n    \"\"\"\n    def __init__(self, name = 'a_SimEvent', sim = None):\n        if sim is None: sim = Globals.sim # Use global simulation if sim is None\n        self.sim = sim\n        self.name = name\n        self.waits = []\n        self.queues = []\n        self.occurred = False\n        self.signalparam = None\n        if hasattr(sim, 'trace'):\n            self._doTracing = True\n        else:\n            self._doTracing = False\n\n    def signal(self, param = None):\n        \"\"\"Produces a signal to self;\n        Fires this event (makes it occur).\n        Reactivates ALL processes waiting for this event. (Cleanup waits lists\n        of other events if wait was for an event - group (OR).)\n        Reactivates the first process for which event(s) it is queuing for\n        have fired. (Cleanup queues of other events if wait was for an event - group (OR).)\n        \"\"\"\n        self.signalparam = param\n        if self._doTracing:\n            self.sim.trace.recordSignal(self)\n        if not self.waits and not self.queues:\n            self.occurred = True\n        else:\n            #reactivate all waiting processes\n            for p in self.waits:\n                p[0].eventsFired.append(self)\n                self.sim.reactivate(p[0], prior = True)\n                #delete waits entries for this process in other events\n                for ev in p[1]:\n                    if ev != self:\n                        if ev.occurred:\n                            p[0].eventsFired.append(ev)\n                        for iev in ev.waits:\n                            if iev[0] == p[0]:\n                                ev.waits.remove(iev)\n                                break\n            self.waits = []\n            if self.queues:\n                proc = self.queues.pop(0)[0]\n                proc.eventsFired.append(self)\n                self.sim.reactivate(proc)\n\n    def _wait(self, par):\n        \"\"\"Consumes a signal if it has occurred, otherwise process 'proc'\n        waits for this event.\n        \"\"\"\n        proc = par[0][1] #the process issuing the yield waitevent command\n        # test that process and SimEvent belong to same Simulation instance\n        if __debug__:\n            if not (proc.sim == self.sim):\n                raise FatalSimerror(\"waitevent: Process %s, SimEvent %s not in \"\n                        \"same Simulation instance\" % (proc.name,self.name))\n        proc.eventsFired = []\n        if not self.occurred:\n            self.waits.append([proc, [self]])\n            proc._nextTime = None #passivate calling process\n        else:\n            proc.eventsFired.append(self)\n            self.occurred = False\n            self.sim._post(proc, at = self.sim._t, prior = 1)\n\n    def _waitOR(self, par):\n        \"\"\"Handles waiting for an OR of events in a tuple / list.\n        \"\"\"\n        proc = par[0][1]\n        evlist = par[0][2]\n        proc.eventsFired = []\n        anyoccur = False\n        for ev in evlist:\n            # test that process and SimEvent belong to same Simulation instance\n            if __debug__:\n                if not (proc.sim == ev.sim):\n                    raise FatalSimerror(\n                                        \"waitevent: Process %s, SimEvent %s not in \"\\\n                                        \"same Simulation instance\"%(proc.name,ev.name))\n            if ev.occurred:\n                anyoccur = True\n                proc.eventsFired.append(ev)\n                ev.occurred = False\n        if anyoccur: #at least one event has fired; continue process\n            self.sim._post(proc, at = self.sim._t, prior = 1)\n\n        else: #no event in list has fired, enter process in all 'waits' lists\n            proc.eventsFired = []\n            proc._nextTime = None #passivate calling process\n            for ev in evlist:\n                ev.waits.append([proc, evlist])\n\n    def _queue(self, par):\n        \"\"\"Consumes a signal if it has occurred, otherwise process 'proc'\n        queues for this event.\n        \"\"\"\n        proc = par[0][1] #the process issuing the yield queueevent command\n        proc.eventsFired = []\n        # test that process and SimEvent belong to same Simulation instance\n        if __debug__:\n            if not (proc.sim == self.sim):\n                raise FatalSimerror(\"queueevent: Process %s, SimEvent %s not in \"\n                        \"same Simulation instance\"%(proc.name,self.name))\n        if not self.occurred:\n            self.queues.append([proc, [self]])\n            proc._nextTime = None #passivate calling process\n        else:\n            proc.eventsFired.append(self)\n            self.occurred = False\n            self.sim._post(proc, at = self.sim._t, prior = 1)\n\n    def _queueOR(self, par):\n        \"\"\"Handles queueing for an OR of events in a tuple / list.\n        \"\"\"\n        proc = par[0][1]\n        evlist = par[0][2]\n        proc.eventsFired = []\n        anyoccur = False\n        for ev in evlist:\n        # test that process and SimEvent belong to same Simulation instance\n            if __debug__:\n                if not (proc.sim == ev.sim):\n                    raise FatalSimerror(\"yield queueevent: Process %s, SimEvent %s not in \"\n                            \"same Simulation instance\"%(proc.name,ev.name))\n            if ev.occurred:\n                anyoccur = True\n                proc.eventsFired.append(ev)\n                ev.occurred = False\n        if anyoccur: #at least one event has fired; continue process\n            self.sim._post(proc, at = self.sim._t, prior = 1)\n\n        else: #no event in list has fired, enter process in all 'waits' lists\n            proc.eventsFired = []\n            proc._nextTime = None #passivate calling process\n            for ev in evlist:\n                ev.queues.append([proc, evlist])\n\n\nclass Queue(list):\n    def __init__(self, res, moni):\n        if not moni is None: #moni == []:\n            self.monit = True # True if a type of Monitor / Tally attached\n        else:\n            self.monit = False\n        self.moni = moni # The Monitor / Tally\n        self.resource = res # the resource / buffer this queue belongs to\n\n    def enter(self, obj):\n        pass\n\n    def leave(self):\n        pass\n\n    def takeout(self, obj):\n        self.remove(obj)\n        if self.monit:\n            self.moni.observe(len(self), t = self.moni.sim.now())\n\nclass FIFO(Queue):\n    def __init__(self, res, moni):\n        Queue.__init__(self, res, moni)\n\n    def enter(self, obj):\n        self.append(obj)\n        if self.monit:\n            self.moni.observe(len(self),t = self.moni.sim.now())\n\n    def enterGet(self, obj):\n        self.enter(obj)\n\n    def enterPut(self, obj):\n        self.enter(obj)\n\n    def leave(self):\n        a = self.pop(0)\n        if self.monit:\n            self.moni.observe(len(self),t = self.moni.sim.now())\n        return a\n\nclass PriorityQ(FIFO):\n    \"\"\"Queue is always ordered according to priority.\n    Higher value of priority attribute == higher priority.\n    \"\"\"\n    def __init__(self, res, moni):\n        FIFO.__init__(self, res, moni)\n\n    def enter(self, obj):\n        \"\"\"Handles request queue for Resource\"\"\"\n        if len(self):\n            ix = self.resource\n            if self[-1]._priority[ix] >= obj._priority[ix]:\n                self.append(obj)\n            else:\n                z = 0\n                while self[z]._priority[ix] >= obj._priority[ix]:\n                    z += 1\n                self.insert(z, obj)\n        else:\n            self.append(obj)\n        if self.monit:\n            self.moni.observe(len(self),t = self.moni.sim.now())\n\n    def enterGet(self, obj):\n        \"\"\"Handles getQ in Buffer\"\"\"\n        if len(self):\n            ix = self.resource\n            if self[-1]._getpriority[ix] >= obj._getpriority[ix]:\n                self.append(obj)\n            else:\n                z = 0\n                while self[z]._getpriority[ix] >= obj._getpriority[ix]:\n                    z += 1\n                self.insert(z, obj)\n        else:\n            self.append(obj)\n        if self.monit:\n            self.moni.observe(len(self),t = self.moni.sim.now())\n\n    def enterPut(self, obj):\n        \"\"\"Handles putQ in Buffer\"\"\"\n        if len(self):\n            ix = self.resource\n            if self[-1]._putpriority[ix] >= obj._putpriority[ix]:\n                self.append(obj)\n            else:\n                z = 0\n                while self[z]._putpriority[ix] >= obj._putpriority[ix]:\n                    z += 1\n                self.insert(z, obj)\n        else:\n            self.append(obj)\n        if self.monit:\n            self.moni.observe(len(self),t = self.moni.sim.now())\n\nclass Resource(Lister):\n    \"\"\"Models shared, limited capacity resources with queuing;\n    FIFO is default queuing discipline.\n    \"\"\"\n\n    def __init__(self, capacity = 1, name = 'a_resource', unitName = 'units',\n                 qType = FIFO, preemptable = 0, monitored = False,\n                 monitorType = Monitor,sim = None):\n        \"\"\"\n        monitorType={Monitor(default) | Tally}\n        \"\"\"\n        if capacity < 0:\n            raise ValueError('capacity should be >= 0, but is: %s' % capacity)\n\n        if sim is None: sim = Globals.sim # Use global simulation if sim is Non\n        self.sim = sim\n        self.name = name          # resource name\n        self.capacity = capacity  # resource units in this resource\n        self.unitName = unitName  # type name of resource units\n        self.n = capacity         # uncommitted resource units\n        self.monitored = monitored\n\n        if self.monitored:           # Monitor waitQ, activeQ\n            self.actMon = monitorType(name = 'Active Queue Monitor %s'%self.name,\n                                 ylab = 'nr in queue', tlab = 'time',\n                                 sim = self.sim)\n            monact = self.actMon\n            self.waitMon = monitorType(name = 'Wait Queue Monitor %s'%self.name,\n                                 ylab = 'nr in queue', tlab = 'time',\n                                 sim = self.sim)\n            monwait = self.waitMon\n        else:\n            monwait = None\n            monact = None\n        self.waitQ = qType(self, monwait)\n        self.preemptable = preemptable\n        self.activeQ = qType(self, monact)\n        self.priority_default = 0\n        # Initialize monitors\n        if self.monitored:\n            monact.observe(t = self.sim.now(), y = len(self.activeQ))\n            monwait.observe(t = self.sim.now(), y = len(self.waitQ))\n\n    def _request(self, arg):\n        \"\"\"Process request event for this resource\"\"\"\n        obj = arg[1]\n        # test that process and Resource belong to same Simulation instance\n        if __debug__:\n            if not (obj.sim == self.sim):\n                raise FatalSimerror(\"yield request: Process %s, Resource %s not in \"\\\n                        \"same Simulation instance\"%(obj.name,self.name))\n        if len(arg[0]) == 4:        # yield request, self, resource, priority\n            obj._priority[self] = arg[0][3]\n        else:                       # yield request, self, resource\n            obj._priority[self] = self.priority_default\n        if self.preemptable and self.n == 0: # No free resource\n            # test for preemption condition\n            preempt = obj._priority[self] > self.activeQ[-1]._priority[self]\n            # If yes:\n            if preempt:\n                z = self.activeQ[-1]\n                # Keep track of preempt level\n                z._preempted += 1\n                # suspend lowest priority process being served\n                # record remaining service time at first preempt only\n                if z._preempted == 1:\n                    z._remainService = z._nextTime - self.sim._t\n                    # cancel only at first preempt\n                    Process(sim=self.sim).cancel(z)\n                # remove from activeQ\n                self.activeQ.remove(z)\n                # put into front of waitQ\n                self.waitQ.insert(0, z)\n                # if self is monitored, update waitQ monitor\n                if self.monitored:\n                    self.waitMon.observe(len(self.waitQ), self.sim.now())\n                # passivate re - queued process\n                z._nextTime = None\n                # assign resource unit to preemptor\n                self.activeQ.enter(obj)\n                # post event notice for preempting process\n                self.sim._post(obj, at = self.sim._t, prior = 1)\n            else:\n                self.waitQ.enter(obj)\n                # passivate queuing process\n                obj._nextTime = None\n        else: # treat non - preemption case\n            if self.n == 0:\n                self.waitQ.enter(obj)\n                # passivate queuing process\n                obj._nextTime = None\n            else:\n                self.n -= 1\n                self.activeQ.enter(obj)\n                self.sim._post(obj, at = self.sim._t, prior = 1)\n\n    def _release(self, arg):\n        \"\"\"Process release request for this resource\"\"\"\n        actor = arg[1]\n        self.n += 1\n        self.activeQ.remove(arg[1])\n        if self.monitored:\n            self.actMon.observe(len(self.activeQ),t = self.sim.now())\n        #reactivate first waiting requestor if any; assign Resource to it\n        if self.waitQ:\n            obj = self.waitQ.leave()\n            self.n -= 1             #assign 1 resource unit to object\n            self.activeQ.enter(obj)\n            # if resource preemptable:\n            if self.preemptable:\n                # if object had been preempted:\n                if obj._preempted:\n                    # keep track of preempt level\n                    obj._preempted -= 1\n                    # reactivate object delay = remaining service time\n                    # but only, if all other preempts are over\n                    if obj._preempted == 0:\n                        self.sim.reactivate(obj, delay = obj._remainService,\n                                            prior = 1)\n                # else reactivate right away\n                else:\n                    self.sim.reactivate(obj, delay = 0, prior = 1)\n            # else:\n            else:\n                self.sim.reactivate(obj, delay = 0, prior = 1)\n        self.sim._post(arg[1], at = self.sim._t, prior = 1)\n\nclass Buffer(Lister):\n    \"\"\"Abstract class for buffers\n    Blocks a process when a put would cause buffer overflow or a get would cause\n    buffer underflow.\n    Default queuing discipline for blocked processes is FIFO.\"\"\"\n\n    priorityDefault = 0\n    def __init__(self, name = None, capacity = 'unbounded', unitName = 'units',\n                putQType = FIFO, getQType = FIFO,\n                monitored = False, monitorType = Monitor, initialBuffered = None,\n                sim = None):\n        if sim is None: sim = Globals.sim # Use global simulation if sim is None\n        self.sim = sim\n        if capacity == 'unbounded':\n            capacity = sys.maxsize\n        elif capacity < 0:\n            raise ValueError('capacity should be >= 0, but is: %s' % capacity)\n        self.capacity = capacity\n        self.name = name\n        self.putQType = putQType\n        self.getQType = getQType\n        self.monitored = monitored\n        self.initialBuffered = initialBuffered\n        self.unitName = unitName\n        if self.monitored:\n            ## monitor for Producer processes' queue\n            self.putQMon = monitorType(name = 'Producer Queue Monitor %s'%self.name,\n                                    ylab = 'nr in queue', tlab = 'time',\n                                    sim=self.sim)\n            ## monitor for Consumer processes' queue\n            self.getQMon = monitorType(name = 'Consumer Queue Monitor %s'%self.name,\n                                    ylab = 'nr in queue', tlab = 'time',\n                                    sim=self.sim)\n            ## monitor for nr items in buffer\n            self.bufferMon = monitorType(name = 'Buffer Monitor %s'%self.name,\n                                    ylab = 'nr in buffer', tlab = 'time',\n                                    sim=self.sim)\n        else:\n            self.putQMon = None\n            self.getQMon = None\n            self.bufferMon = None\n        self.putQ = self.putQType(res = self, moni = self.putQMon)\n        self.getQ = self.getQType(res = self, moni = self.getQMon)\n        if self.monitored:\n            self.putQMon.observe(y = len(self.putQ),t = self.sim.now())\n            self.getQMon.observe(y = len(self.getQ),t = self.sim.now())\n        self._putpriority={}\n        self._getpriority={}\n\n        def _put(self):\n            pass\n        def _get(self):\n            pass\n\nclass Level(Buffer):\n    \"\"\"Models buffers for processes putting / getting un - distinguishable items.\n    \"\"\"\n    def getamount(self):\n        return self.nrBuffered\n\n    def gettheBuffer(self):\n        return self.nrBuffered\n\n    theBuffer = property(gettheBuffer)\n\n    def __init__(self,**pars):\n        Buffer.__init__(self,**pars)\n        if self.name is None:\n            self.name = 'a_level'   ## default name\n\n        if (type(self.capacity) != type(1.0) and\\\n                type(self.capacity) != type(1)) or\\\n                self.capacity < 0:\n                raise FatalSimerror('Level: capacity parameter not a positive number: %s'\\\n                    %self.initialBuffered)\n\n        if type(self.initialBuffered) == type(1.0) or\\\n                type(self.initialBuffered) == type(1):\n            if self.initialBuffered > self.capacity:\n                raise FatalSimerror('initialBuffered exceeds capacity')\n            if self.initialBuffered >= 0:\n                self.nrBuffered = self.initialBuffered ## nr items initially in buffer\n                                        ## buffer is just a counter (int type)\n            else:\n                raise FatalSimerror('initialBuffered param of Level negative: %s'\\\n                %self.initialBuffered)\n        elif self.initialBuffered is None:\n            self.initialBuffered = 0\n            self.nrBuffered = 0\n        else:\n            raise FatalSimerror('Level: wrong type of initialBuffered (parameter=%s)'\\\n                %self.initialBuffered)\n        if self.monitored:\n            self.bufferMon.observe(y = self.amount, t = self.sim.now())\n    amount = property(getamount)\n\n    def _put(self, arg):\n        \"\"\"Handles put requests for Level instances\"\"\"\n        obj = arg[1]\n        whichSim=self.sim\n        # test that process and Level belong to same Simulation instance\n        if __debug__:\n            if not (obj.sim == self.sim):\n                raise FatalSimerror(\n                        \"put: Process %s, Level %s not in \"\\\n                        \"same Simulation instance\"%(obj.name,self.name))\n        if len(arg[0]) == 5:        # yield put, self, buff, whattoput, priority\n            obj._putpriority[self] = arg[0][4]\n            whatToPut = arg[0][3]\n        elif len(arg[0]) == 4:      # yield get, self, buff, whattoput\n            obj._putpriority[self] = Buffer.priorityDefault #default\n            whatToPut = arg[0][3]\n        else:                       # yield get, self, buff\n            obj._putpriority[self] = Buffer.priorityDefault #default\n            whatToPut = 1\n        if type(whatToPut) != type(1) and type(whatToPut) != type(1.0):\n            raise FatalSimerror('Level: put parameter not a number')\n        if not whatToPut >= 0.0:\n            raise FatalSimerror('Level: put parameter not positive number')\n        whatToPutNr = whatToPut\n        if whatToPutNr + self.amount > self.capacity:\n            obj._nextTime = None      #passivate put requestor\n            obj._whatToPut = whatToPutNr\n            self.putQ.enterPut(obj)    #and queue, with size of put\n        else:\n            self.nrBuffered += whatToPutNr\n            if self.monitored:\n                self.bufferMon.observe(y = self.amount, t = self.sim.now())\n            # service any getters waiting\n            # service in queue - order; do not serve second in queue before first\n            # has been served\n            while len(self.getQ) and self.amount > 0:\n                proc = self.getQ[0]\n                if proc._nrToGet <= self.amount:\n                    proc.got = proc._nrToGet\n                    self.nrBuffered -= proc.got\n                    if self.monitored:\n                        self.bufferMon.observe(y = self.amount, t = self.sim.now())\n                    self.getQ.takeout(proc) # get requestor's record out of queue\n                    whichSim._post(proc, at = whichSim._t) # continue a blocked get requestor\n                else:\n                    break\n            whichSim._post(obj, at = whichSim._t, prior = 1) # continue the put requestor\n\n    def _get(self, arg):\n        \"\"\"Handles get requests for Level instances\"\"\"\n        obj = arg[1]\n        # test that process and Store belong to same Simulation instance\n        if __debug__:\n            if not (obj.sim == self.sim):\n                raise FatalSimerror(\n                        \"get: Process %s, Level %s not in \"\\\n                        \"same Simulation instance\"%(obj.name,self.name))\n        obj.got = None\n        if len(arg[0]) == 5:        # yield get, self, buff, whattoget, priority\n            obj._getpriority[self] = arg[0][4]\n            nrToGet = arg[0][3]\n        elif len(arg[0]) == 4:      # yield get, self, buff, whattoget\n            obj._getpriority[self] = Buffer.priorityDefault #default\n            nrToGet = arg[0][3]\n        else:                       # yield get, self, buff\n            obj._getpriority[self] = Buffer.priorityDefault\n            nrToGet = 1\n        if type(nrToGet) != type(1.0) and type(nrToGet) != type(1):\n            raise FatalSimerror('Level: get parameter not a number: %s'%nrToGet)\n        if nrToGet < 0:\n            raise FatalSimerror('Level: get parameter not positive number: %s'%nrToGet)\n        if self.amount < nrToGet:\n            obj._nrToGet = nrToGet\n            self.getQ.enterGet(obj)\n            # passivate queuing process\n            obj._nextTime = None\n        else:\n            obj.got = nrToGet\n            self.nrBuffered -= nrToGet\n            if self.monitored:\n                self.bufferMon.observe(y = self.amount, t = self.sim.now())\n            self.sim._post(obj, at = self.sim._t, prior = 1)\n            # reactivate any put requestors for which space is now available\n            # service in queue - order; do not serve second in queue before first\n            # has been served\n            while len(self.putQ): #test for queued producers\n                proc = self.putQ[0]\n                if proc._whatToPut + self.amount <= self.capacity:\n                    self.nrBuffered += proc._whatToPut\n                    if self.monitored:\n                        self.bufferMon.observe(y = self.amount, t = self.sim.now())\n                    self.putQ.takeout(proc)#requestor's record out of queue\n                    self.sim._post(proc, at = self.sim._t) # continue a blocked put requestor\n                else:\n                    break\n\nclass Store(Buffer):\n    \"\"\"Models buffers for processes coupled by putting / getting distinguishable\n    items.\n    Blocks a process when a put would cause buffer overflow or a get would cause\n    buffer underflow.\n    Default queuing discipline for blocked processes is priority FIFO.\n    \"\"\"\n    def getnrBuffered(self):\n        return len(self.theBuffer)\n    nrBuffered = property(getnrBuffered)\n\n    def getbuffered(self):\n        return self.theBuffer\n    buffered = property(getbuffered)\n\n    def __init__(self,**pars):\n        Buffer.__init__(self,**pars)\n        self.theBuffer = []\n        if self.name is None:\n            self.name = 'a_store' ## default name\n        if type(self.capacity) != type(1) or self.capacity <= 0:\n            raise FatalSimerror\\\n                ('Store: capacity parameter not a positive integer: %s'\\\n                    %self.capacity)\n        if type(self.initialBuffered) == type([]):\n            if len(self.initialBuffered) > self.capacity:\n                raise FatalSimerror\\\n                   ('Store: number initialBuffered exceeds capacity')\n            else:\n                ## buffer receives list of objects\n                self.theBuffer[:] = self.initialBuffered\n        elif self.initialBuffered is None:\n            self.theBuffer = []\n        else:\n            raise FatalSimerror\\\n                ('Store: initialBuffered not a list')\n        if self.monitored:\n            self.bufferMon.observe(y = self.nrBuffered, t = self.sim.now())\n        self._sort = None\n\n\n\n    def addSort(self, sortFunc):\n        \"\"\"Adds buffer sorting to this instance of Store. It maintains\n        theBuffer sorted by the sortAttr attribute of the objects in the\n        buffer.\n        The user - provided 'sortFunc' must look like this:\n\n        def mySort(self, par):\n            tmplist = [(x.sortAttr, x) for x in par]\n            tmplist.sort()\n            return [x for (key, x) in tmplist]\n\n        \"\"\"\n\n        self._sort = sortFunc.__get__(self, self.__class__)\n        self.theBuffer = self._sort(self.theBuffer)\n\n    def _put(self, arg):\n        \"\"\"Handles put requests for Store instances\"\"\"\n        obj = arg[1]\n        # test that process and Store belong to same Simulation instance\n        if __debug__:\n            if not (obj.sim == self.sim):\n                raise FatalSimerror(\n                                \"put: Process %s, Store %s not in \"\\\n                                \"same Simulation instance\"%(obj.name,self.name))\n        whichSim=self.sim\n        if len(arg[0]) == 5:        # yield put, self, buff, whattoput, priority\n            obj._putpriority[self] = arg[0][4]\n            whatToPut = arg[0][3]\n        elif len(arg[0]) == 4:      # yield put, self, buff, whattoput\n            obj._putpriority[self] = Buffer.priorityDefault #default\n            whatToPut = arg[0][3]\n        else:                       # error, whattoput missing\n            raise FatalSimerror('Item to put missing in yield put stmt')\n        if type(whatToPut) != type([]):\n            raise FatalSimerror('put parameter is not a list')\n        whatToPutNr = len(whatToPut)\n        if whatToPutNr + self.nrBuffered > self.capacity:\n            obj._nextTime = None      #passivate put requestor\n            obj._whatToPut = whatToPut\n            self.putQ.enterPut(obj) #and queue, with items to put\n        else:\n            self.theBuffer.extend(whatToPut)\n            if not(self._sort is None):\n                self.theBuffer = self._sort(self.theBuffer)\n            if self.monitored:\n                self.bufferMon.observe(y = self.nrBuffered, t = whichSim.now())\n\n            # service any waiting getters\n            # service in queue order: do not serve second in queue before first\n            # has been served\n            #\n            # [jkoomen@xeroxlabs.com / 2011-08-16]\n            # Documentation says that\n            # \"yield get requests with a numerical parameter are honored in priority/FIFO order\"\n            # but\n            # \"yield get requests with a filter function parameter are not necessarily honored in priority/FIFO order, but rather according to the filter function.\"\n#            while self.nrBuffered > 0 and len(self.getQ):\n            idx = 0\n            while self.nrBuffered > 0 and idx < len(self.getQ):\n                proc = self.getQ[idx]\n                if inspect.isfunction(proc._nrToGet):\n                    movCand = proc._nrToGet(self.theBuffer) #predicate parameter\n                    if movCand:\n                        proc.got = movCand[:]\n                        for i in movCand:\n                            self.theBuffer.remove(i)\n                        self.getQ.takeout(proc)\n                        if self.monitored:\n                            self.bufferMon.observe(\n                                    y = self.nrBuffered, t = whichSim._t)\n                        whichSim._post(what = proc, at = whichSim._t) # continue a blocked get requestor\n                    else:\n#                        break\n                        idx += 1\n                else: #numerical parameter\n                    if proc._nrToGet <= self.nrBuffered:\n                        nrToGet = proc._nrToGet\n                        proc.got = []\n                        proc.got[:] = self.theBuffer[0:nrToGet]\n                        self.theBuffer[:] = self.theBuffer[nrToGet:]\n                        if self.monitored:\n                            self.bufferMon.observe(\n                                       y = self.nrBuffered, t = whichSim._t)\n                        # take this get requestor's record out of queue:\n                        self.getQ.takeout(proc)\n                        whichSim._post(what = proc, at = whichSim._t) # continue a blocked get requestor\n                    else:\n                        break\n\n            whichSim._post(what = obj, at = whichSim._t, prior = 1) # continue the put requestor\n\n    def _get(self, arg):\n        \"\"\"Handles get requests\"\"\"\n        filtfunc = None\n        obj = arg[1]\n        # test that process and Store belong to same Simulation instance\n        if __debug__:\n            if not (obj.sim == self.sim):\n                raise FatalSimerror(\n                                \"get: Process %s, Store %s not in \"\\\n                                \"same Simulation instance\"%(obj.name,self.name))\n        whichSim=obj.sim\n        obj.got = []                  # the list of items retrieved by 'get'\n        if len(arg[0]) == 5:        # yield get, self, buff, whattoget, priority\n            obj._getpriority[self] = arg[0][4]\n            if inspect.isfunction(arg[0][3]):\n                filtfunc = arg[0][3]\n            else:\n                nrToGet = arg[0][3]\n        elif len(arg[0]) == 4:      # yield get, self, buff, whattoget\n            obj._getpriority[self] = Buffer.priorityDefault #default\n            if inspect.isfunction(arg[0][3]):\n                filtfunc = arg[0][3]\n            else:\n                nrToGet = arg[0][3]\n        else:                       # yield get, self, buff\n            obj._getpriority[self] = Buffer.priorityDefault\n            nrToGet = 1\n        if not filtfunc: #number specifies nr items to get\n            if nrToGet < 0:\n                raise FatalSimerror\\\n                    ('Store: get parameter not positive number: %s'%nrToGet)\n            if self.nrBuffered < nrToGet:\n                obj._nrToGet = nrToGet\n                self.getQ.enterGet(obj)\n                # passivate / block queuing 'get' process\n                obj._nextTime = None\n            else:\n                for i in range(nrToGet):\n                    obj.got.append(self.theBuffer.pop(0)) # move items from\n                                                # buffer to requesting process\n                if self.monitored:\n                    self.bufferMon.observe(y = self.nrBuffered, t = whichSim.now())\n                whichSim._post(obj, at = whichSim._t, prior = 1)\n                # reactivate any put requestors for which space is now available\n                # serve in queue order: do not serve second in queue before first\n                # has been served\n                while len(self.putQ):\n                    proc = self.putQ[0]\n                    if len(proc._whatToPut) + self.nrBuffered <= self.capacity:\n                        for i in proc._whatToPut:\n                            self.theBuffer.append(i) #move items to buffer\n                        if not(self._sort is None):\n                            self.theBuffer = self._sort(self.theBuffer)\n                        if self.monitored:\n                            self.bufferMon.observe(\n                                        y = self.nrBuffered, t = whichSim.now())\n                        self.putQ.takeout(proc) # dequeue requestor's record\n                        whichSim._post(proc, at = whichSim._t) # continue a blocked put requestor\n                    else:\n                        break\n        else: # items to get determined by filtfunc\n            movCand = filtfunc(self.theBuffer)\n            if movCand: # get succeded\n                whichSim._post(obj, at = whichSim._t, prior = 1)\n                obj.got = movCand[:]\n                for item in movCand:\n                    self.theBuffer.remove(item)\n                if self.monitored:\n                    self.bufferMon.observe(y = self.nrBuffered, t = whichSim.now())\n                # reactivate any put requestors for which space is now available\n                # serve in queue order: do not serve second in queue before first\n                # has been served\n                while len(self.putQ):\n                    proc = self.putQ[0]\n                    if len(proc._whatToPut) + self.nrBuffered <= self.capacity:\n                        for i in proc._whatToPut:\n                            self.theBuffer.append(i) #move items to buffer\n                        if not(self._sort is None):\n                            self.theBuffer = self._sort(self.theBuffer)\n                        if self.monitored:\n                            self.bufferMon.observe(\n                                        y = self.nrBuffered, t = whichSim.now())\n                        self.putQ.takeout(proc) # dequeue requestor's record\n                        whichSim._post(proc, at = whichSim._t) # continue a blocked put requestor\n                    else:\n                        break\n            else: # get did not succeed, block\n                obj._nrToGet = filtfunc\n                self.getQ.enterGet(obj)\n                # passivate / block queuing 'get' process\n                obj._nextTime = None\n", 
    "SimPy.Lister": "# coding=utf-8\n\"\"\"\nPretty-printer for SimPy class objects\n\n\"\"\"\nclass Lister(object):\n\n    indent = 0\n\n    def __str__(self):\n        Lister.indent += 1\n        if Lister.indent > 3:\n            # In case of recursion, avoid infinite loop\n            result = ' ... '\n        else:\n            result = '< Instance of %s, id %s:\\n%s%s>' % (\n                self.__class__.__name__,\n                id(self),\n                self.attrnames(),\n                '\\t' * (Lister.indent - 1),\n            )\n        Lister.indent -= 1\n        return result\n\n    def attrnames(self):\n        result = ''\n        for attr in self.__dict__:\n            # Ignore built-in and private attributes\n            if not (attr[:2] == '__' or attr[0] == '_'):\n                result += '\\t' * Lister.indent + '.%s=%s\\n' % (attr,\n                        self.__dict__[attr])\n        return result\n\n    def __repr__(self):\n        return self.__str__()\n", 
    "SimPy.Recording": "# coding=utf-8\n\"\"\"\nThis file contains the classes for recording simulation results, Histogram,\nMonitor and Tally.\n\n\"\"\"\n# Required for backward compatibility\nimport SimPy.Globals as Globals\n\n\nclass Histogram(list):\n    \"\"\" A histogram gathering and sampling class\"\"\"\n\n    def __init__(self, name = '', low = 0.0, high = 100.0, nbins = 10):\n        list.__init__(self)\n        self.name  = name\n        self.low   = float(low)\n        self.high  = float(high)\n        self.nbins = nbins\n        self.binsize = (self.high - self.low) / nbins\n        self._nrObs = 0\n        self._sum = 0\n        self[:] = [[low + (i - 1) * self.binsize, 0] for i in range(self.nbins + 2)]\n\n    def addIn(self, y):\n        \"\"\" add a value into the correct bin\"\"\"\n        self._nrObs += 1\n        self._sum += y\n        b = int((y - self.low + self.binsize) / self.binsize)\n        if b < 0: b = 0\n        if b > self.nbins + 1: b = self.nbins + 1\n        assert 0 <= b <=self.nbins + 1, 'Histogram.addIn: b out of range: %s'%b\n        self[b][1] += 1\n\n    def __str__(self):\n        histo = self\n        ylab = 'value'\n        nrObs = self._nrObs\n        width = len(str(nrObs))\n        res = []\n        res.append(' < Histogram %s:'%self.name)\n        res.append('\\nNumber of observations: %s'%nrObs)\n        if nrObs:\n            su = self._sum\n            cum = histo[0][1]\n            fmt = '%s'\n            line = '\\n%s <= %s < %s: %s (cum: %s/%s%s)'\\\n                 %(fmt, '%s', fmt, '%s', '%s', '%5.1f', '%s')\n            line1 = '\\n%s%s < %s: %s (cum: %s/%s%s)'\\\n                 %('%s', '%s', fmt, '%s', '%s', '%5.1f', '%s')\n            l1width = len(('%s <= '%fmt)%histo[1][0])\n            res.append(line1\\\n                       %(' ' * l1width, ylab, histo[1][0], str(histo[0][1]).rjust(width),\\\n                         str(cum).rjust(width),(float(cum) / nrObs) * 100, '%')\n                      )\n            for i in range(1, len(histo) - 1):\n                cum += histo[i][1]\n                res.append(line\\\n                       %(histo[i][0], ylab, histo[i + 1][0], str(histo[i][1]).rjust(width),\\\n                         str(cum).rjust(width),(float(cum) / nrObs) * 100, '%')\n                          )\n            cum += histo[-1][1]\n            linen = '\\n%s <= %s %s : %s (cum: %s/%s%s)'\\\n                  %(fmt, '%s', '%s', '%s', '%s', '%5.1f', '%s')\n            lnwidth = len(('<%s'%fmt)%histo[1][0])\n            res.append(linen\\\n                       %(histo[-1][0], ylab, ' ' * lnwidth, str(histo[-1][1]).rjust(width),\\\n                       str(cum).rjust(width),(float(cum) / nrObs) * 100, '%')\n                       )\n        res.append('\\n > ')\n        return ' '.join(res)\n\n\nclass Monitor(list):\n    \"\"\" Monitored variables\n\n    A Class for monitored variables, that is, variables that allow one\n    to gather simple statistics.  A Monitor is a subclass of list and\n    list operations can be performed on it. An object is established\n    using m = Monitor(name = '..'). It can be given a\n    unique name for use in debugging and in tracing and ylab and tlab\n    strings for labelling graphs.\n    \"\"\"\n    def __init__(self, name = 'a_Monitor', ylab = 'y', tlab = 't', sim = None):\n        list.__init__(self)\n        if not sim: sim = Globals.sim # Use global simulation if sim is None\n        self.sim = sim\n        self.startTime = 0.0\n        self.name = name\n        self.ylab = ylab\n        self.tlab = tlab\n        self.sim.allMonitors.append(self)\n\n    def setHistogram(self, name = '', low = 0.0, high = 100.0, nbins = 10):\n        \"\"\"Sets histogram parameters.\n        Must be called before call to getHistogram\"\"\"\n        if name == '':\n            histname = self.name\n        else:\n            histname = name\n        self.histo = Histogram(name = histname, low = low, high = high, nbins = nbins)\n\n    def observe(self, y,t = None):\n        \"\"\"record y and t\"\"\"\n        if t is  None: t = self.sim.now()\n        self.append([t, y])\n\n    def tally(self, y):\n        \"\"\" deprecated: tally for backward compatibility\"\"\"\n        self.observe(y, 0)\n\n    def accum(self, y,t = None):\n        \"\"\" deprecated:  accum for backward compatibility\"\"\"\n        self.observe(y, t)\n\n    def reset(self, t = None):\n        \"\"\"reset the sums and counts for the monitored variable \"\"\"\n        self[:] = []\n        if t is None: t = self.sim.now()\n        self.startTime = t\n\n    def tseries(self):\n        \"\"\" the series of measured times\"\"\"\n        return list(zip(*self))[0]\n\n    def yseries(self):\n        \"\"\" the series of measured values\"\"\"\n        return list(zip(*self))[1]\n\n    def count(self):\n        \"\"\" deprecated: the number of observations made \"\"\"\n        return self.__len__()\n\n    def total(self):\n        \"\"\" the sum of the y\"\"\"\n        if self.__len__() == 0:  return 0\n        else:\n            sum = 0.0\n            for i in range(self.__len__()):\n                sum += self[i][1]\n            return sum # replace by sum() later\n\n    def mean(self):\n        \"\"\" the simple average of the monitored variable\"\"\"\n        try:\n            return 1.0 * self.total() / self.__len__()\n        except ZeroDivisionError:\n            raise ZeroDivisionError('SimPy: No observations for mean')\n\n    def var(self):\n        \"\"\" the sample variance of the monitored variable \"\"\"\n        n = len(self)\n        tot = self.total()\n        ssq = 0.0\n        for i in range(self.__len__()):\n            ssq += self[i][1] ** 2 # replace by sum() eventually\n        try:\n            return (ssq - float(tot * tot) / n) / n\n        except:\n            raise ZeroDivisionError(\n                    'SimPy: No observations for sample variance')\n\n    def timeAverage(self, t = None):\n        \"\"\"\n        The time-weighted average of the monitored variable.\n\n        If t is used it is assumed to be the current time,\n        otherwise t =  self.sim.now()\n\n        \"\"\"\n        N = self.__len__()\n        if N  == 0:\n            return None\n\n        if t is None: t = self.sim.now()\n        sum = 0.0\n        tlast = self[0][0]\n        ylast = self[0][1]\n        for i in range(N):\n            ti, yi = self[i]\n            sum += ylast * (ti - tlast)\n            tlast = ti\n            ylast = yi\n        sum += ylast * (t - tlast)\n        T = t - self[0][0]\n        if T == 0:\n             return None\n        return sum / float(T)\n\n    def timeVariance(self, t = None):\n        \"\"\" the time - weighted Variance of the monitored variable.\n\n            If t is used it is assumed to be the current time,\n            otherwise t =  self.sim.now()\n        \"\"\"\n        N = self.__len__()\n        if N  == 0:\n            return None\n        if t is None: t = self.sim.now()\n        sm = 0.0\n        ssq = 0.0\n        tlast = self[0][0]\n        # print 'DEBUG: 1 twVar ', t, tlast\n        ylast = self[0][1]\n        for i in range(N):\n            ti, yi = self[i]\n            sm  += ylast * (ti - tlast)\n            ssq += ylast * ylast * (ti - tlast)\n            tlast = ti\n            ylast = yi\n        sm  += ylast * (t - tlast)\n        ssq += ylast * ylast * (t - tlast)\n        T = t - self[0][0]\n        if T == 0:\n             return None\n        mn = sm / float(T)\n        return ssq / float(T) - mn * mn\n\n\n    def histogram(self, low = 0.0, high = 100.0, nbins = 10):\n        \"\"\" A histogram of the monitored y data values.\n        \"\"\"\n        h = Histogram(name = self.name, low = low, high = high, nbins = nbins)\n        ys = self.yseries()\n        for y in ys: h.addIn(y)\n        return h\n\n    def getHistogram(self):\n        \"\"\"Returns a histogram based on the parameters provided in\n        preceding call to setHistogram.\n        \"\"\"\n        ys = self.yseries()\n        h = self.histo\n        for y in ys: h.addIn(y)\n        return h\n\n    def printHistogram(self, fmt = '%s'):\n        \"\"\"Returns formatted frequency distribution table string from Monitor.\n        Precondition: setHistogram must have been called.\n        fmt == format of bin range values\n        \"\"\"\n        try:\n            histo = self.getHistogram()\n        except:\n            raise FatalSimerror('histogramTable: call setHistogram first'\\\n                                ' for Monitor %s'%self.name)\n        ylab = self.ylab\n        nrObs = self.count()\n        width = len(str(nrObs))\n        res = []\n        res.append('\\nHistogram for %s:'%histo.name)\n        res.append('\\nNumber of observations: %s'%nrObs)\n        su = sum(self.yseries())\n        cum = histo[0][1]\n        line = '\\n%s <= %s < %s: %s (cum: %s/%s%s)'\\\n             %(fmt, '%s', fmt, '%s', '%s', '%5.1f', '%s')\n        line1 = '\\n%s%s < %s: %s (cum: %s/%s%s)'\\\n             %('%s', '%s', fmt, '%s', '%s', '%5.1f', '%s')\n        l1width = len(('%s <= '%fmt)%histo[1][0])\n        res.append(line1\\\n                   %(' ' * l1width, ylab, histo[1][0], str(histo[0][1]).rjust(width),\\\n                     str(cum).rjust(width),(float(cum) / nrObs) * 100, '%')\n                  )\n        for i in range(1, len(histo) - 1):\n            cum += histo[i][1]\n            res.append(line\\\n                   %(histo[i][0], ylab, histo[i + 1][0], str(histo[i][1]).rjust(width),\\\n                     str(cum).rjust(width),(float(cum) / nrObs) * 100, '%')\n                      )\n        cum += histo[-1][1]\n        linen = '\\n%s <= %s %s : %s (cum: %s/%s%s)'\\\n              %(fmt, '%s', '%s', '%s', '%s', '%5.1f', '%s')\n        lnwidth = len(('<%s'%fmt)%histo[1][0])\n        res.append(linen\\\n                   %(histo[-1][0], ylab, ' ' * lnwidth, str(histo[-1][1]).rjust(width),\\\n                   str(cum).rjust(width),(float(cum) / nrObs) * 100, '%')\n                   )\n        return ' '.join(res)\n\nclass Tally:\n    def __init__(self, name = 'a_Tally', ylab = 'y', tlab = 't', sim = None):\n        if not sim: sim = Globals.sim # use global simulation if sim is None\n        self.sim = sim\n        self.name = name\n        self.ylab = ylab\n        self.tlab = tlab\n        self.reset()\n        self.startTime = 0.0\n        self.histo = None\n        self.sum = 0.0\n        self._sum_of_squares = 0\n        self._integral = 0.0    # time - weighted sum\n        self._integral2 = 0.0   # time - weighted sum of squares\n        self.sim.allTallies.append(self)\n\n    def setHistogram(self, name = '', low = 0.0, high = 100.0, nbins = 10):\n        \"\"\"Sets histogram parameters.\n        Must be called to prior to observations initiate data collection\n        for histogram.\n        \"\"\"\n        if name == '':\n            hname = self.name\n        else:\n            hname = name\n        self.histo = Histogram(name = hname, low = low, high = high, nbins = nbins)\n\n    def observe(self, y, t = None):\n        if t is None:\n            t = self.sim.now()\n        self._integral += (t - self._last_timestamp) * self._last_observation\n        yy =  self._last_observation * self._last_observation\n        self._integral2 += (t - self._last_timestamp) * yy\n        self._last_timestamp = t\n        self._last_observation = y\n        self._total += y\n        self._count += 1\n        self._sum += y\n        self._sum_of_squares += y * y\n        if self.histo:\n            self.histo.addIn(y)\n\n    def reset(self, t = None):\n        if t is None:\n            t = self.sim.now()\n        self.startTime = t\n        self._last_timestamp = t\n        self._last_observation = 0.0\n        self._count = 0\n        self._total = 0.0\n        self._integral = 0.0\n        self._integral2 = 0.0\n        self._sum = 0.0\n        self._sum_of_squares = 0.0\n\n    def count(self):\n        return self._count\n\n    def total(self):\n        return self._total\n\n    def mean(self):\n        return 1.0 * self._total / self._count\n\n    def timeAverage(self, t = None):\n        if t is None:\n            t = self.sim.now()\n        integ = self._integral + (t - self._last_timestamp) * self._last_observation\n        if (t > self.startTime):\n            return 1.0 * integ / (t - self.startTime)\n        else:\n            return None\n\n    def var(self):\n        return 1.0 * (self._sum_of_squares - (1.0 * (self._sum * self._sum)\\\n               / self._count)) / (self._count)\n\n    def timeVariance(self, t = None):\n        \"\"\" the time - weighted Variance of the Tallied variable.\n\n            If t is used it is assumed to be the current time,\n            otherwise t =  self.sim.now()\n        \"\"\"\n        if t is None:\n            t = self.sim.now()\n        twAve = self.timeAverage(t)\n        #print 'Tally timeVariance DEBUG: twave:', twAve\n        last =  self._last_observation\n        twinteg2 = self._integral2 + (t - self._last_timestamp) * last * last\n        #print 'Tally timeVariance DEBUG:tinteg2:', twinteg2\n        if (t > self.startTime):\n            return 1.0 * twinteg2 / (t - self.startTime) - twAve * twAve\n        else:\n            return None\n\n\n\n    def __len__(self):\n        return self._count\n\n    def __eq__(self, l):\n        return len(l) == self._count\n\n    def getHistogram(self):\n        return self.histo\n\n    def printHistogram(self, fmt = '%s'):\n        \"\"\"Returns formatted frequency distribution table string from Tally.\n        Precondition: setHistogram must have been called.\n        fmt == format of bin range values\n        \"\"\"\n        try:\n            histo = self.getHistogram()\n        except:\n            raise FatalSimerror('histogramTable: call setHistogram first'\\\n                                ' for Tally %s'%self.name)\n        ylab = self.ylab\n        nrObs = self.count()\n        width = len(str(nrObs))\n        res = []\n        res.append('\\nHistogram for %s:'%histo.name)\n        res.append('\\nNumber of observations: %s'%nrObs)\n        su = self.total()\n        cum = histo[0][1]\n        line = '\\n%s <= %s < %s: %s (cum: %s/%s%s)'\\\n             %(fmt, '%s', fmt, '%s', '%s', '%5.1f', '%s')\n        line1 = '\\n%s%s < %s: %s (cum: %s/%s%s)'\\\n             %('%s', '%s', fmt, '%s', '%s', '%5.1f', '%s')\n        l1width = len(('%s <= '%fmt)%histo[1][0])\n        res.append(line1\\\n                   %(' ' * l1width, ylab, histo[1][0], str(histo[0][1]).rjust(width),\\\n                     str(cum).rjust(width),(float(cum) / nrObs) * 100, '%')\n                  )\n        for i in range(1, len(histo) - 1):\n            cum += histo[i][1]\n            res.append(line\\\n                   %(histo[i][0], ylab, histo[i + 1][0], str(histo[i][1]).rjust(width),\\\n                     str(cum).rjust(width),(float(cum) / nrObs) * 100, '%')\n                      )\n        cum += histo[-1][1]\n        linen = '\\n%s <= %s %s : %s (cum: %s/%s%s)'\\\n              %(fmt, '%s', '%s', '%s', '%s', '%5.1f', '%s')\n        lnwidth = len(('<%s'%fmt)%histo[1][0])\n        res.append(linen\\\n                   %(histo[-1][0], ylab, ' ' * lnwidth, str(histo[-1][1]).rjust(width),\\\n                   str(cum).rjust(width),(float(cum) / nrObs) * 100, '%')\n                   )\n        return ' '.join(res)\n", 
    "SimPy.Simulation": "# coding=utf-8\n\"\"\"\nSimulation implements SimPy Processes, Resources, Buffers, and the backbone simulation\nscheduling by coroutine calls. Provides data collection through classes\nMonitor and Tally.\nBased on generators\n\n\"\"\"\nimport random\nimport sys\nimport types\nfrom heapq import heappush, heappop\n\nfrom SimPy.Lister import Lister\nfrom SimPy.Recording import Monitor, Tally\nfrom SimPy.Lib import Process, SimEvent, PriorityQ, Resource, Level, \\\n                      Store, Simerror, FatalSimerror, FIFO\n\n# Required for backward compatibility\nimport SimPy\nimport SimPy.Globals as Globals\nfrom SimPy.Globals import initialize, simulate, now, stopSimulation, \\\n        allEventNotices, allEventTimes, startCollection,\\\n        _startWUStepping, _stopWUStepping, activate, reactivate\n\n\n# yield keywords\nhold = 1\npassivate = 2\nrequest = 3\nrelease = 4\nwaitevent = 5\nqueueevent = 6\nwaituntil = 7\nget = 8\nput = 9\n\n\nclass Infinity(object):\n    def __cmp__(self, other):\n        return 1\n\ninfinity = Infinity()\n\ndef holdfunc(a):\n    a[0][1]._hold(a)\n\ndef requestfunc(a):\n    \"\"\"Handles 'yield request, self, res' and\n    'yield (request, self, res),(<code>,self, par)'.\n    <code > can be 'hold' or 'waitevent'.\n    \"\"\"\n    if type(a[0][0]) == tuple:\n        ## Compound yield request statement\n        ## first tuple in ((request, self, res),(xx, self, yy))\n        b = a[0][0]\n        ## b[2] == res (the resource requested)\n        ##process the first part of the compound yield statement\n        ##a[1] is the Process instance\n        b[2]._request(arg = (b, a[1]))\n        ##deal with add - on condition to command\n        ##Trigger processes for reneging\n        class _Holder(Process):\n            \"\"\"Provides timeout process\"\"\"\n            def __init__(self,name,sim=None):\n                Process.__init__(self,name=name,sim=sim)\n            def trigger(self, delay):\n                yield hold, self, delay\n                if not proc in b[2].activeQ:\n                    proc.sim.reactivate(proc)\n\n        class _EventWait(Process):\n            \"\"\"Provides event waiting process\"\"\"\n            def __init__(self,name,sim=None):\n                Process.__init__(self,name=name,sim=sim)\n            def trigger(self, event):\n                yield waitevent, self, event\n                if not proc in b[2].activeQ:\n                    proc.eventsFired = self.eventsFired\n                    proc.sim.reactivate(proc)\n\n        #activate it\n        proc = a[0][0][1] # the process to be woken up\n        actCode = a[0][1][0]\n        if actCode == hold:\n            proc._holder = _Holder(name = 'RENEGE - hold for %s'%proc.name,\n                                   sim=proc.sim)\n            ##                                          the timeout delay\n            proc.sim.activate(proc._holder, proc._holder.trigger(a[0][1][2]))\n        elif actCode == waituntil:\n            raise FatalSimerror('Illegal code for reneging: waituntil')\n        elif actCode == waitevent:\n            proc._holder = _EventWait(name = 'RENEGE - waitevent for %s'\\\n                                      %proc.name,sim=proc.sim)\n            ##                                          the event\n            proc.sim.activate(proc._holder, proc._holder.trigger(a[0][1][2]))\n        elif actCode == queueevent:\n            raise FatalSimerror('Illegal code for reneging: queueevent')\n        else:\n            raise FatalSimerror('Illegal code for reneging %s'%actCode)\n    else:\n        ## Simple yield request command\n        a[0][2]._request(a)\n\ndef releasefunc(a):\n    a[0][2]._release(a)\n\ndef passivatefunc(a):\n    a[0][1]._passivate(a)\n\ndef waitevfunc(a):\n    #if waiting for one event only (not a tuple or list)\n    evtpar = a[0][2]\n    if isinstance(evtpar, SimEvent):\n        a[0][2]._wait(a)\n    # else, if waiting for an OR of events (list / tuple):\n    else: #it should be a list / tuple of events\n        # call _waitOR for first event\n        evtpar[0]._waitOR(a)\n\ndef queueevfunc(a):\n    #if queueing for one event only (not a tuple or list)\n    evtpar = a[0][2]\n    if isinstance(evtpar, SimEvent):\n        a[0][2]._queue(a)\n    #else, if queueing for an OR of events (list / tuple):\n    else: #it should be a list / tuple of events\n        # call _queueOR for first event\n        evtpar[0]._queueOR(a)\n\ndef waituntilfunc(par):\n    par[0][1].sim._waitUntilFunc(par[0][1], par[0][2])\n\ndef getfunc(a):\n    \"\"\"Handles 'yield get, self, buffer, what, priority' and\n    'yield (get, self, buffer, what, priority),(<code>,self, par)'.\n    <code > can be 'hold' or 'waitevent'.\n    \"\"\"\n    if type(a[0][0]) == tuple:\n        ## Compound yield request statement\n        ## first tuple in ((request, self, res),(xx, self, yy))\n        b = a[0][0]\n        ## b[2] == res (the resource requested)\n        ##process the first part of the compound yield statement\n        ##a[1] is the Process instance\n        b[2]._get(arg = (b, a[1]))\n        ##deal with add - on condition to command\n        ##Trigger processes for reneging\n        class _Holder(Process):\n            \"\"\"Provides timeout process\"\"\"\n            def __init__(self,**par):\n                Process.__init__(self,**par)\n            def trigger(self, delay):\n                yield hold, self, delay\n                #if not proc in b[2].activeQ:\n                if proc in b[2].getQ:\n                    a[1].sim.reactivate(proc)\n\n        class _EventWait(Process):\n            \"\"\"Provides event waiting process\"\"\"\n            def __init__(self,**par):\n                Process.__init__(self,**par)\n            def trigger(self, event):\n                yield waitevent, self, event\n                if proc in b[2].getQ:\n                    a[1].eventsFired = self.eventsFired\n                    a[1].sim.reactivate(proc)\n\n        #activate it\n        proc = a[0][0][1] # the process to be woken up\n        actCode = a[0][1][0]\n        if actCode == hold:\n            proc._holder = _Holder(name='RENEGE - hold for %s'%proc.name,\n                                   sim=proc.sim)\n            ##                                          the timeout delay\n            a[1].sim.activate(proc._holder, proc._holder.trigger(a[0][1][2]))\n        elif actCode == waituntil:\n            raise FatalSimerror('waituntil: Illegal code for reneging: waituntil')\n        elif actCode == waitevent:\n            proc._holder = _EventWait(name=\"RENEGE - waitevent for%s\"\\\n                                      %proc.name,sim=proc.sim)\n            ##                                          the event\n            a[1].sim.activate(proc._holder, proc._holder.trigger(a[0][1][2]))\n        elif actCode == queueevent:\n            raise FatalSimerror('Illegal code for reneging: queueevent')\n        else:\n            raise FatalSimerror('Illegal code for reneging %s'%actCode)\n    else:\n        ## Simple yield request command\n        a[0][2]._get(a)\n\n\ndef putfunc(a):\n    \"\"\"Handles 'yield put' (simple and compound hold / waitevent)\n    \"\"\"\n    if type(a[0][0]) == tuple:\n        ## Compound yield request statement\n        ## first tuple in ((request, self, res),(xx, self, yy))\n        b = a[0][0]\n        ## b[2] == res (the resource requested)\n        ##process the first part of the compound yield statement\n        ##a[1] is the Process instance\n        b[2]._put(arg = (b, a[1]))\n        ##deal with add - on condition to command\n        ##Trigger processes for reneging\n        class _Holder(Process):\n            \"\"\"Provides timeout process\"\"\"\n            def __init__(self,**par):\n                Process.__init__(self,**par)\n            def trigger(self, delay):\n                yield hold, self, delay\n                #if not proc in b[2].activeQ:\n                if proc in b[2].putQ:\n                    a[1].sim.reactivate(proc)\n\n        class _EventWait(Process):\n            \"\"\"Provides event waiting process\"\"\"\n            def __init__(self,**par):\n                Process.__init__(self,**par)\n            def trigger(self, event):\n                yield waitevent, self, event\n                if proc in b[2].putQ:\n                    a[1].eventsFired = self.eventsFired\n                    a[1].sim.reactivate(proc)\n\n        #activate it\n        proc = a[0][0][1] # the process to be woken up\n        actCode = a[0][1][0]\n        if actCode == hold:\n            proc._holder = _Holder(name='RENEGE - hold for %s'%proc.name,\n                                   sim=proc.sim)\n            ##                                          the timeout delay\n            a[1].sim.activate(proc._holder, proc._holder.trigger(a[0][1][2]))\n        elif actCode == waituntil:\n            raise FatalSimerror('Illegal code for reneging: waituntil')\n        elif actCode == waitevent:\n            proc._holder = _EventWait(name='RENEGE - waitevent for %s'\\\n                                      %proc.name,sim=proc.sim)\n            ##                                          the event\n            a[1].sim.activate(proc._holder, proc._holder.trigger(a[0][1][2]))\n        elif actCode == queueevent:\n            raise FatalSimerror('Illegal code for reneging: queueevent')\n        else:\n            raise FatalSimerror('Illegal code for reneging %s'%actCode)\n    else:\n        ## Simple yield request command\n        a[0][2]._put(a)\n\n\nclass Simulation(object):\n    _dispatch = {\n            hold: holdfunc, request: requestfunc, release: releasefunc,\n            passivate: passivatefunc, waitevent: waitevfunc,\n            queueevent: queueevfunc, waituntil: waituntilfunc, get: getfunc,\n            put: putfunc,\n    }\n    _commandcodes = list(_dispatch.keys())\n    _commandwords = {\n            hold: 'hold', request: 'request', release: 'release',\n            passivate: 'passivate', waitevent: 'waitevent',\n            queueevent: 'queueevent', waituntil: 'waituntil', get: 'get',\n            put: 'put'\n    }\n\n    def __init__(self):\n        self.initialize()\n\n    def initialize(self):\n        self._t = 0\n        self.next_time = 0\n\n        # Eventqueue stuff.\n        self._timestamps = []\n        self._sortpr = 0\n\n        self._start = False\n        self._stop = False\n        self.condQ = []\n        self.allMonitors = []\n        self.allTallies = []\n\n    def now(self):\n        return self._t\n\n    def stopSimulation(self):\n        \"\"\"Application function to stop simulation run\"\"\"\n        self._stop = True\n\n    def _post(self, what, at, prior = False):\n        \"\"\"Post an event notice for process what for time at\"\"\"\n        # event notices are Process instances\n        if at < self._t:\n            raise FatalSimerror('Attempt to schedule event in the past')\n        what._nextTime = at\n        self._sortpr -= 1\n        if prior:\n            # before all other event notices at this time\n            # heappush with highest priority value so far (negative of\n            # monotonely decreasing number)\n            # store event notice in process instance\n            what._rec = [at, self._sortpr, what, False]\n            # make event list refer to it\n            heappush(self._timestamps, what._rec)\n        else:\n            # heappush with lowest priority\n            # store event notice in process instance\n            what._rec = [at,-self._sortpr, what, False]\n            # make event list refer to it\n            heappush(self._timestamps, what._rec)\n\n    def _unpost(self, whom):\n        \"\"\"\n        Mark event notice for whom as cancelled if whom is a suspended process\n        \"\"\"\n        if whom._nextTime is not None:  # check if whom was actually active\n            whom._rec[3] = True ## Mark as cancelled\n            whom._nextTime = None\n\n    def allEventNotices(self):\n        \"\"\"Returns string with eventlist as;\n                t1: processname, processname2\n                t2: processname4, processname5, . . .\n                . . .  .\n        \"\"\"\n        ret = ''\n        tempList = []\n        tempList[:] = self._timestamps\n        tempList.sort()\n        # return only event notices which are not cancelled\n        tempList = [[x[0],x[2].name] for x in tempList if not x[3]]\n        tprev = -1\n        for t in tempList:\n            # if new time, new line\n            if t[0] == tprev:\n                # continue line\n                ret += ', %s'%t[1]\n            else:\n                # new time\n                if tprev == -1:\n                    ret = '%s: %s' % (t[0],t[1])\n                else:\n                    ret += '\\n%s: %s' % (t[0],t[1])\n                tprev = t[0]\n        return ret + '\\n'\n\n    def allEventTimes(self):\n        \"\"\"Returns list of all times for which events are scheduled.\n        \"\"\"\n        r = []\n        r[:] = self._timestamps\n        r.sort()\n        # return only event times of not cancelled event notices\n        r1 = [x[0] for x in r if not x[3]]\n        tprev = -1\n        ret = []\n        for t in r1:\n            if t == tprev:\n                #skip time, already in list\n                pass\n            else:\n                ret.append(t)\n                tprev = t\n        return ret\n\n    def activate(self, obj, process, at = 'undefined', delay = 'undefined',\n                 prior = False):\n        \"\"\"Application function to activate passive process.\"\"\"\n        if __debug__:\n            if not (obj.sim == self):\n                raise FatalSimerror('activate: Process %s not in activating '\n                        'Simulation instance' % obj.name)\n            if not (type(process) == types.GeneratorType):\n                raise FatalSimerror('Activating function which'+\n                    ' is not a generator (contains no \\'yield\\')')\n        if not obj._terminated and not obj._nextTime:\n            #store generator reference in object; needed for reactivation\n            obj._nextpoint = process\n            if at == 'undefined':\n                at = self._t\n            if delay == 'undefined':\n                zeit = max(self._t, at)\n            else:\n                zeit = max(self._t, self._t + delay)\n            self._post(obj, at = zeit, prior = prior)\n\n    def reactivate(self, obj, at = 'undefined', delay = 'undefined',\n                   prior = False):\n        \"\"\"Application function to reactivate a process which is active,\n        suspended or passive.\"\"\"\n        # Object may be active, suspended or passive\n        if not obj._terminated:\n            a = Process('SimPysystem',sim=self)\n            a.cancel(obj)\n            # object now passive\n            if at == 'undefined':\n                at = self._t\n            if delay == 'undefined':\n                zeit = max(self._t, at)\n            else:\n                zeit = max(self._t, self._t + delay)\n            self._post(obj, at = zeit, prior = prior)\n\n    def startCollection(self, when = 0.0, monitors = None, tallies = None):\n        \"\"\"Starts data collection of all designated Monitor and Tally objects\n        (default = all) at time 'when'.\n        \"\"\"\n        class Starter(Process):\n            def collect(self, monitors, tallies):\n                for m in monitors:\n                    m.reset()\n                for t in tallies:\n                    t.reset()\n                yield hold, self\n        if monitors is None:\n            monitors = self.allMonitors\n        if tallies is None:\n            tallies = self.allTallies\n        if when == 0.0:\n            for m in monitors:\n                try:\n                    ylast = m[-1][1]\n                    empty = False\n                except IndexError:\n                    empty = True\n                m.reset()\n                if not empty:\n                    m.observe(t = now(), y = ylast)\n            for t in tallies:\n                t.reset()\n        else:\n            s = Starter(sim = self)\n            self.activate(s, s.collect(monitors = monitors, tallies = tallies),\\\n                      at = when, prior = True)\n\n\n    def _waitUntilFunc(self, proc, cond):\n        \"\"\"\n        Puts a process 'proc' waiting for a condition into a waiting queue.\n        'cond' is a predicate function which returns True if the condition is\n        satisfied.\n        \"\"\"\n        if not cond():\n            self.condQ.append(proc)\n            proc.cond = cond\n            # passivate calling process\n            proc._nextTime = None\n        else:\n            #schedule continuation of calling process\n            self._post(proc, at = self._t, prior = 1)\n\n    def _terminate(self, process):\n        \"\"\"Marks a process as terminated.\"\"\"\n        process._nextpoint = None\n        process._terminated = True\n        process._nextTime = None\n\n    def has_events(self):\n        \"\"\"\n        Checks if there are events which can be processed. Returns ``True`` if\n        there are events and the simulation has not been stopped.\n        \"\"\"\n        return not self._stop and self._timestamps\n\n    def peek(self):\n        \"\"\"\n        Returns the time of the next event or infinity, if no\n        more events are scheduled.\n        \"\"\"\n        if not self._timestamps:\n            return infinity\n        else:\n            return self._timestamps[0][0]\n\n    def step(self):\n        \"\"\"\n        Executes the next uncancelled event in the eventqueue.\n        \"\"\"\n\n        # Fetch next process and advance its process execution method.\n        noActiveNotice = True\n        # Get an uncancelled event\n        while noActiveNotice:\n            if self._timestamps:\n                _tnotice, p, proc, cancelled = heappop(self._timestamps)\n                noActiveNotice = cancelled\n            else:\n                return None\n\n        # Advance simulation time.\n        proc._rec = None\n        self._t = _tnotice\n\n        # Execute the event. This will advance the process execution method.\n        try:\n            resultTuple = next(proc._nextpoint)\n\n            # Process the command function which has been yielded by the\n            # process.\n            if type(resultTuple[0]) == tuple:\n                # allowing for reneges, e.g.:\n                # >>> yield (request, self, res),(waituntil, self, cond)\n                command = resultTuple[0][0]\n            else:\n                command = resultTuple[0]\n            if __debug__:\n                if not command in self._commandcodes:\n                    raise FatalSimerror('Illegal command: yield %s'%command)\n            self._dispatch[command]((resultTuple, proc))\n        except StopIteration:\n            # Process execution method has terminated.\n            self._terminate(proc)\n\n        # Test the conditions for all waiting processes if there are any at\n        # all. Where condition are satisfied, reactivate that process\n        # immediately and remove it from queue.\n        # Always test the wait conditions. They might be triggered by on a\n        # terminating process execution method (e.g. the above next() call\n        # raises the StopIteration exception)\n        if self.condQ:\n            i = 0\n            while i < len(self.condQ):\n                proc = self.condQ[i]\n                if proc.cond():\n                    self.condQ.pop(i)\n                    self.reactivate(proc)\n                else:\n                    i += 1\n\n        # Return time of the next scheduled event.\n        #return self._timestamps[0][0] if self._timestamps else None\n        if self._timestamps:\n            return self._timestamps[0][0]\n        else:\n            return None\n\n    def simulate(self, until=0):\n        \"\"\"\n        Start the simulation and run its loop until the timeout ``until`` is\n        reached, stopSimulation is called, or no more events are scheduled.\n        \"\"\"\n        try:\n            if not self._timestamps:\n                return 'SimPy: No activities scheduled'\n\n            # Some speedups. Storing these values in local variables prevents\n            # the self-lookup. Note that this can't be done for _stop because\n            # this variable will get overwritten, bools are immutable.\n            step = self.step\n            timestamps = self._timestamps\n            while not self._stop and timestamps and timestamps[0][0] <= until:\n                step()\n\n            if not self._stop and timestamps:\n                # Timestamps left, simulation not stopped\n                self._t = until\n                return 'SimPy: Normal exit at time %s' % self._t\n            elif not timestamps:\n                # No more timestamps\n                return 'SimPy: No more events at time %s' % self._t\n            else:\n                # Stopped by call of stopSimulation\n                return 'SimPy: Run stopped at time %s' % self._t\n        # Delete the excepts?\n        except FatalSimerror as error:\n            raise FatalSimerror('SimPy: ' + error.value)\n        except Simerror as error:\n            return 'SimPy: ' + error.value\n        finally:\n            self._stop = True\n\n# For backward compatibility\nGlobals.sim = Simulation()\n\npeek = Globals.sim.peek\n\nstep = Globals.sim.step\n\nallMonitors = Globals.sim.allMonitors\n\nallTallies = Globals.sim.allTallies\n# End backward compatibility\n", 
    "SimPy.__init__": "# coding=utf-8\n\"\"\"\nSimPy, a process - based simulation package in Python\n\nContains the following modules:\nLib - module with all base classes of SimPy\nGlobals - module providing a global Simulation object\nSimulation - module implementing processes and resources\nMonitor - dummy module for backward compatibility\nSimulationTrace - module implementing event tracing\nSimulationRT - module for simulation speed control\nSimulationStep - module for stepping through simulation event by event\nSimPlot - Tk - based plotting module\nSimGui - Tk - based SimPy GUI module\nLister - module for prettyprinting class instances\nLib - module containing SimPy entity classes (Process etc.)\nRecording - module containing SimPy classes for recording results (Monitor,\n        Tally)\nGlobals - module providing global Simulation object and the global\n        simulation methods\nstepping - a simple interactive debugger\n\n\"\"\"\n__version__ = '2.3.1'\n\n\ndef test():\n    import os.path\n    try:\n        import pytest\n    except ImportError:\n        print('You need pytest and mock to run the tests. '\n              'Try \"pip install pytest mock\".')\n    else:\n        pytest.main([os.path.dirname(__file__)])\n", 
    "StringIO": "r\"\"\"File-like objects that read from or write to a string buffer.\n\nThis implements (nearly) all stdio methods.\n\nf = StringIO()      # ready for writing\nf = StringIO(buf)   # ready for reading\nf.close()           # explicitly release resources held\nflag = f.isatty()   # always false\npos = f.tell()      # get current position\nf.seek(pos)         # set current position\nf.seek(pos, mode)   # mode 0: absolute; 1: relative; 2: relative to EOF\nbuf = f.read()      # read until EOF\nbuf = f.read(n)     # read up to n bytes\nbuf = f.readline()  # read until end of line ('\\n') or EOF\nlist = f.readlines()# list of f.readline() results until EOF\nf.truncate([size])  # truncate file at to at most size (default: current pos)\nf.write(buf)        # write at current position\nf.writelines(list)  # for line in list: f.write(line)\nf.getvalue()        # return whole file's contents as a string\n\nNotes:\n- Using a real file is often faster (but less convenient).\n- There's also a much faster implementation in C, called cStringIO, but\n  it's not subclassable.\n- fileno() is left unimplemented so that code which uses it triggers\n  an exception early.\n- Seeking far beyond EOF and then writing will insert real null\n  bytes that occupy space in the buffer.\n- There's a simple test set (see end of this file).\n\"\"\"\ntry:\n    from errno import EINVAL\nexcept ImportError:\n    EINVAL = 22\n\n__all__ = [\"StringIO\"]\n\ndef _complain_ifclosed(closed):\n    if closed:\n        raise ValueError, \"I/O operation on closed file\"\n\nclass StringIO:\n    \"\"\"class StringIO([buffer])\n\n    When a StringIO object is created, it can be initialized to an existing\n    string by passing the string to the constructor. If no string is given,\n    the StringIO will start empty.\n\n    The StringIO object can accept either Unicode or 8-bit strings, but\n    mixing the two may take some care. If both are used, 8-bit strings that\n    cannot be interpreted as 7-bit ASCII (that use the 8th bit) will cause\n    a UnicodeError to be raised when getvalue() is called.\n    \"\"\"\n    def __init__(self, buf = ''):\n        # Force self.buf to be a string or unicode\n        if not isinstance(buf, basestring):\n            buf = str(buf)\n        self.buf = buf\n        self.len = len(buf)\n        self.buflist = []\n        self.pos = 0\n        self.closed = False\n        self.softspace = 0\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        \"\"\"A file object is its own iterator, for example iter(f) returns f\n        (unless f is closed). When a file is used as an iterator, typically\n        in a for loop (for example, for line in f: print line), the next()\n        method is called repeatedly. This method returns the next input line,\n        or raises StopIteration when EOF is hit.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        r = self.readline()\n        if not r:\n            raise StopIteration\n        return r\n\n    def close(self):\n        \"\"\"Free the memory buffer.\n        \"\"\"\n        if not self.closed:\n            self.closed = True\n            del self.buf, self.pos\n\n    def isatty(self):\n        \"\"\"Returns False because StringIO objects are not connected to a\n        tty-like device.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        return False\n\n    def seek(self, pos, mode = 0):\n        \"\"\"Set the file's current position.\n\n        The mode argument is optional and defaults to 0 (absolute file\n        positioning); other values are 1 (seek relative to the current\n        position) and 2 (seek relative to the file's end).\n\n        There is no return value.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        if mode == 1:\n            pos += self.pos\n        elif mode == 2:\n            pos += self.len\n        self.pos = max(0, pos)\n\n    def tell(self):\n        \"\"\"Return the file's current position.\"\"\"\n        _complain_ifclosed(self.closed)\n        return self.pos\n\n    def read(self, n = -1):\n        \"\"\"Read at most size bytes from the file\n        (less if the read hits EOF before obtaining size bytes).\n\n        If the size argument is negative or omitted, read all data until EOF\n        is reached. The bytes are returned as a string object. An empty\n        string is returned when EOF is encountered immediately.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        if n is None or n < 0:\n            newpos = self.len\n        else:\n            newpos = min(self.pos+n, self.len)\n        r = self.buf[self.pos:newpos]\n        self.pos = newpos\n        return r\n\n    def readline(self, length=None):\n        r\"\"\"Read one entire line from the file.\n\n        A trailing newline character is kept in the string (but may be absent\n        when a file ends with an incomplete line). If the size argument is\n        present and non-negative, it is a maximum byte count (including the\n        trailing newline) and an incomplete line may be returned.\n\n        An empty string is returned only when EOF is encountered immediately.\n\n        Note: Unlike stdio's fgets(), the returned string contains null\n        characters ('\\0') if they occurred in the input.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        i = self.buf.find('\\n', self.pos)\n        if i < 0:\n            newpos = self.len\n        else:\n            newpos = i+1\n        if length is not None and length >= 0:\n            if self.pos + length < newpos:\n                newpos = self.pos + length\n        r = self.buf[self.pos:newpos]\n        self.pos = newpos\n        return r\n\n    def readlines(self, sizehint = 0):\n        \"\"\"Read until EOF using readline() and return a list containing the\n        lines thus read.\n\n        If the optional sizehint argument is present, instead of reading up\n        to EOF, whole lines totalling approximately sizehint bytes (or more\n        to accommodate a final whole line).\n        \"\"\"\n        total = 0\n        lines = []\n        line = self.readline()\n        while line:\n            lines.append(line)\n            total += len(line)\n            if 0 < sizehint <= total:\n                break\n            line = self.readline()\n        return lines\n\n    def truncate(self, size=None):\n        \"\"\"Truncate the file's size.\n\n        If the optional size argument is present, the file is truncated to\n        (at most) that size. The size defaults to the current position.\n        The current file position is not changed unless the position\n        is beyond the new file size.\n\n        If the specified size exceeds the file's current size, the\n        file remains unchanged.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if size is None:\n            size = self.pos\n        elif size < 0:\n            raise IOError(EINVAL, \"Negative size not allowed\")\n        elif size < self.pos:\n            self.pos = size\n        self.buf = self.getvalue()[:size]\n        self.len = size\n\n    def write(self, s):\n        \"\"\"Write a string to the file.\n\n        There is no return value.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if not s: return\n        # Force s to be a string or unicode\n        if not isinstance(s, basestring):\n            s = str(s)\n        spos = self.pos\n        slen = self.len\n        if spos == slen:\n            self.buflist.append(s)\n            self.len = self.pos = spos + len(s)\n            return\n        if spos > slen:\n            self.buflist.append('\\0'*(spos - slen))\n            slen = spos\n        newpos = spos + len(s)\n        if spos < slen:\n            if self.buflist:\n                self.buf += ''.join(self.buflist)\n            self.buflist = [self.buf[:spos], s, self.buf[newpos:]]\n            self.buf = ''\n            if newpos > slen:\n                slen = newpos\n        else:\n            self.buflist.append(s)\n            slen = newpos\n        self.len = slen\n        self.pos = newpos\n\n    def writelines(self, iterable):\n        \"\"\"Write a sequence of strings to the file. The sequence can be any\n        iterable object producing strings, typically a list of strings. There\n        is no return value.\n\n        (The name is intended to match readlines(); writelines() does not add\n        line separators.)\n        \"\"\"\n        write = self.write\n        for line in iterable:\n            write(line)\n\n    def flush(self):\n        \"\"\"Flush the internal buffer\n        \"\"\"\n        _complain_ifclosed(self.closed)\n\n    def getvalue(self):\n        \"\"\"\n        Retrieve the entire contents of the \"file\" at any time before\n        the StringIO object's close() method is called.\n\n        The StringIO object can accept either Unicode or 8-bit strings,\n        but mixing the two may take some care. If both are used, 8-bit\n        strings that cannot be interpreted as 7-bit ASCII (that use the\n        8th bit) will cause a UnicodeError to be raised when getvalue()\n        is called.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        return self.buf\n\n\n# A little test suite\n\ndef test():\n    import sys\n    if sys.argv[1:]:\n        file = sys.argv[1]\n    else:\n        file = '/etc/passwd'\n    lines = open(file, 'r').readlines()\n    text = open(file, 'r').read()\n    f = StringIO()\n    for line in lines[:-2]:\n        f.write(line)\n    f.writelines(lines[-2:])\n    if f.getvalue() != text:\n        raise RuntimeError, 'write failed'\n    length = f.tell()\n    print 'File length =', length\n    f.seek(len(lines[0]))\n    f.write(lines[1])\n    f.seek(0)\n    print 'First line =', repr(f.readline())\n    print 'Position =', f.tell()\n    line = f.readline()\n    print 'Second line =', repr(line)\n    f.seek(-len(line), 1)\n    line2 = f.read(len(line))\n    if line != line2:\n        raise RuntimeError, 'bad result after seek back'\n    f.seek(len(line2), 1)\n    list = f.readlines()\n    line = list[-1]\n    f.seek(f.tell() - len(line))\n    line2 = f.read()\n    if line != line2:\n        raise RuntimeError, 'bad result after seek back from EOF'\n    print 'Read', len(list), 'more lines'\n    print 'File length =', f.tell()\n    if f.tell() != length:\n        raise RuntimeError, 'bad length'\n    f.truncate(length/2)\n    f.seek(0, 2)\n    print 'Truncated length =', f.tell()\n    if f.tell() != length/2:\n        raise RuntimeError, 'truncate did not adjust length'\n    f.close()\n\nif __name__ == '__main__':\n    test()\n", 
    "UserDict": "\"\"\"A more or less complete user-defined wrapper around dictionary objects.\"\"\"\n\nclass UserDict:\n    def __init__(self, dict=None, **kwargs):\n        self.data = {}\n        if dict is not None:\n            self.update(dict)\n        if len(kwargs):\n            self.update(kwargs)\n    def __repr__(self): return repr(self.data)\n    def __cmp__(self, dict):\n        if isinstance(dict, UserDict):\n            return cmp(self.data, dict.data)\n        else:\n            return cmp(self.data, dict)\n    __hash__ = None # Avoid Py3k warning\n    def __len__(self): return len(self.data)\n    def __getitem__(self, key):\n        if key in self.data:\n            return self.data[key]\n        if hasattr(self.__class__, \"__missing__\"):\n            return self.__class__.__missing__(self, key)\n        raise KeyError(key)\n    def __setitem__(self, key, item): self.data[key] = item\n    def __delitem__(self, key): del self.data[key]\n    def clear(self): self.data.clear()\n    def copy(self):\n        if self.__class__ is UserDict:\n            return UserDict(self.data.copy())\n        import copy\n        data = self.data\n        try:\n            self.data = {}\n            c = copy.copy(self)\n        finally:\n            self.data = data\n        c.update(self)\n        return c\n    def keys(self): return self.data.keys()\n    def items(self): return self.data.items()\n    def iteritems(self): return self.data.iteritems()\n    def iterkeys(self): return self.data.iterkeys()\n    def itervalues(self): return self.data.itervalues()\n    def values(self): return self.data.values()\n    def has_key(self, key): return key in self.data\n    def update(self, dict=None, **kwargs):\n        if dict is None:\n            pass\n        elif isinstance(dict, UserDict):\n            self.data.update(dict.data)\n        elif isinstance(dict, type({})) or not hasattr(dict, 'items'):\n            self.data.update(dict)\n        else:\n            for k, v in dict.items():\n                self[k] = v\n        if len(kwargs):\n            self.data.update(kwargs)\n    def get(self, key, failobj=None):\n        if key not in self:\n            return failobj\n        return self[key]\n    def setdefault(self, key, failobj=None):\n        if key not in self:\n            self[key] = failobj\n        return self[key]\n    def pop(self, key, *args):\n        return self.data.pop(key, *args)\n    def popitem(self):\n        return self.data.popitem()\n    def __contains__(self, key):\n        return key in self.data\n    @classmethod\n    def fromkeys(cls, iterable, value=None):\n        d = cls()\n        for key in iterable:\n            d[key] = value\n        return d\n\nclass IterableUserDict(UserDict):\n    def __iter__(self):\n        return iter(self.data)\n\ntry:\n    import _abcoll\nexcept ImportError:\n    pass    # e.g. no '_weakref' module on this pypy\nelse:\n    _abcoll.MutableMapping.register(IterableUserDict)\n\n\nclass DictMixin:\n    # Mixin defining all dictionary methods for classes that already have\n    # a minimum dictionary interface including getitem, setitem, delitem,\n    # and keys. Without knowledge of the subclass constructor, the mixin\n    # does not define __init__() or copy().  In addition to the four base\n    # methods, progressively more efficiency comes with defining\n    # __contains__(), __iter__(), and iteritems().\n\n    # second level definitions support higher levels\n    def __iter__(self):\n        for k in self.keys():\n            yield k\n    def has_key(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        return True\n    def __contains__(self, key):\n        return self.has_key(key)\n\n    # third level takes advantage of second level definitions\n    def iteritems(self):\n        for k in self:\n            yield (k, self[k])\n    def iterkeys(self):\n        return self.__iter__()\n\n    # fourth level uses definitions from lower levels\n    def itervalues(self):\n        for _, v in self.iteritems():\n            yield v\n    def values(self):\n        return [v for _, v in self.iteritems()]\n    def items(self):\n        return list(self.iteritems())\n    def clear(self):\n        for key in self.keys():\n            del self[key]\n    def setdefault(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n    def pop(self, key, *args):\n        if len(args) > 1:\n            raise TypeError, \"pop expected at most 2 arguments, got \"\\\n                              + repr(1 + len(args))\n        try:\n            value = self[key]\n        except KeyError:\n            if args:\n                return args[0]\n            raise\n        del self[key]\n        return value\n    def popitem(self):\n        try:\n            k, v = self.iteritems().next()\n        except StopIteration:\n            raise KeyError, 'container is empty'\n        del self[k]\n        return (k, v)\n    def update(self, other=None, **kwargs):\n        # Make progressively weaker assumptions about \"other\"\n        if other is None:\n            pass\n        elif hasattr(other, 'iteritems'):  # iteritems saves memory and lookups\n            for k, v in other.iteritems():\n                self[k] = v\n        elif hasattr(other, 'keys'):\n            for k in other.keys():\n                self[k] = other[k]\n        else:\n            for k, v in other:\n                self[k] = v\n        if kwargs:\n            self.update(kwargs)\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default\n    def __repr__(self):\n        return repr(dict(self.iteritems()))\n    def __cmp__(self, other):\n        if other is None:\n            return 1\n        if isinstance(other, DictMixin):\n            other = dict(other.iteritems())\n        return cmp(dict(self.iteritems()), other)\n    def __len__(self):\n        return len(self.keys())\n", 
    "_LWPCookieJar": "\"\"\"Load / save to libwww-perl (LWP) format files.\n\nActually, the format is slightly extended from that used by LWP's\n(libwww-perl's) HTTP::Cookies, to avoid losing some RFC 2965 information\nnot recorded by LWP.\n\nIt uses the version string \"2.0\", though really there isn't an LWP Cookies\n2.0 format.  This indicates that there is extra information in here\n(domain_dot and # port_spec) while still being compatible with\nlibwww-perl, I hope.\n\n\"\"\"\n\nimport time, re\nfrom cookielib import (_warn_unhandled_exception, FileCookieJar, LoadError,\n                       Cookie, MISSING_FILENAME_TEXT,\n                       join_header_words, split_header_words,\n                       iso2time, time2isoz)\n\ndef lwp_cookie_str(cookie):\n    \"\"\"Return string representation of Cookie in an the LWP cookie file format.\n\n    Actually, the format is extended a bit -- see module docstring.\n\n    \"\"\"\n    h = [(cookie.name, cookie.value),\n         (\"path\", cookie.path),\n         (\"domain\", cookie.domain)]\n    if cookie.port is not None: h.append((\"port\", cookie.port))\n    if cookie.path_specified: h.append((\"path_spec\", None))\n    if cookie.port_specified: h.append((\"port_spec\", None))\n    if cookie.domain_initial_dot: h.append((\"domain_dot\", None))\n    if cookie.secure: h.append((\"secure\", None))\n    if cookie.expires: h.append((\"expires\",\n                               time2isoz(float(cookie.expires))))\n    if cookie.discard: h.append((\"discard\", None))\n    if cookie.comment: h.append((\"comment\", cookie.comment))\n    if cookie.comment_url: h.append((\"commenturl\", cookie.comment_url))\n\n    keys = cookie._rest.keys()\n    keys.sort()\n    for k in keys:\n        h.append((k, str(cookie._rest[k])))\n\n    h.append((\"version\", str(cookie.version)))\n\n    return join_header_words([h])\n\nclass LWPCookieJar(FileCookieJar):\n    \"\"\"\n    The LWPCookieJar saves a sequence of \"Set-Cookie3\" lines.\n    \"Set-Cookie3\" is the format used by the libwww-perl libary, not known\n    to be compatible with any browser, but which is easy to read and\n    doesn't lose information about RFC 2965 cookies.\n\n    Additional methods\n\n    as_lwp_str(ignore_discard=True, ignore_expired=True)\n\n    \"\"\"\n\n    def as_lwp_str(self, ignore_discard=True, ignore_expires=True):\n        \"\"\"Return cookies as a string of \"\\\\n\"-separated \"Set-Cookie3\" headers.\n\n        ignore_discard and ignore_expires: see docstring for FileCookieJar.save\n\n        \"\"\"\n        now = time.time()\n        r = []\n        for cookie in self:\n            if not ignore_discard and cookie.discard:\n                continue\n            if not ignore_expires and cookie.is_expired(now):\n                continue\n            r.append(\"Set-Cookie3: %s\" % lwp_cookie_str(cookie))\n        return \"\\n\".join(r+[\"\"])\n\n    def save(self, filename=None, ignore_discard=False, ignore_expires=False):\n        if filename is None:\n            if self.filename is not None: filename = self.filename\n            else: raise ValueError(MISSING_FILENAME_TEXT)\n\n        f = open(filename, \"w\")\n        try:\n            # There really isn't an LWP Cookies 2.0 format, but this indicates\n            # that there is extra information in here (domain_dot and\n            # port_spec) while still being compatible with libwww-perl, I hope.\n            f.write(\"#LWP-Cookies-2.0\\n\")\n            f.write(self.as_lwp_str(ignore_discard, ignore_expires))\n        finally:\n            f.close()\n\n    def _really_load(self, f, filename, ignore_discard, ignore_expires):\n        magic = f.readline()\n        if not re.search(self.magic_re, magic):\n            msg = (\"%r does not look like a Set-Cookie3 (LWP) format \"\n                   \"file\" % filename)\n            raise LoadError(msg)\n\n        now = time.time()\n\n        header = \"Set-Cookie3:\"\n        boolean_attrs = (\"port_spec\", \"path_spec\", \"domain_dot\",\n                         \"secure\", \"discard\")\n        value_attrs = (\"version\",\n                       \"port\", \"path\", \"domain\",\n                       \"expires\",\n                       \"comment\", \"commenturl\")\n\n        try:\n            while 1:\n                line = f.readline()\n                if line == \"\": break\n                if not line.startswith(header):\n                    continue\n                line = line[len(header):].strip()\n\n                for data in split_header_words([line]):\n                    name, value = data[0]\n                    standard = {}\n                    rest = {}\n                    for k in boolean_attrs:\n                        standard[k] = False\n                    for k, v in data[1:]:\n                        if k is not None:\n                            lc = k.lower()\n                        else:\n                            lc = None\n                        # don't lose case distinction for unknown fields\n                        if (lc in value_attrs) or (lc in boolean_attrs):\n                            k = lc\n                        if k in boolean_attrs:\n                            if v is None: v = True\n                            standard[k] = v\n                        elif k in value_attrs:\n                            standard[k] = v\n                        else:\n                            rest[k] = v\n\n                    h = standard.get\n                    expires = h(\"expires\")\n                    discard = h(\"discard\")\n                    if expires is not None:\n                        expires = iso2time(expires)\n                    if expires is None:\n                        discard = True\n                    domain = h(\"domain\")\n                    domain_specified = domain.startswith(\".\")\n                    c = Cookie(h(\"version\"), name, value,\n                               h(\"port\"), h(\"port_spec\"),\n                               domain, domain_specified, h(\"domain_dot\"),\n                               h(\"path\"), h(\"path_spec\"),\n                               h(\"secure\"),\n                               expires,\n                               discard,\n                               h(\"comment\"),\n                               h(\"commenturl\"),\n                               rest)\n                    if not ignore_discard and c.discard:\n                        continue\n                    if not ignore_expires and c.is_expired(now):\n                        continue\n                    self.set_cookie(c)\n\n        except IOError:\n            raise\n        except Exception:\n            _warn_unhandled_exception()\n            raise LoadError(\"invalid Set-Cookie3 format file %r: %r\" %\n                            (filename, line))\n", 
    "_MozillaCookieJar": "\"\"\"Mozilla / Netscape cookie loading / saving.\"\"\"\n\nimport re, time\n\nfrom cookielib import (_warn_unhandled_exception, FileCookieJar, LoadError,\n                       Cookie, MISSING_FILENAME_TEXT)\n\nclass MozillaCookieJar(FileCookieJar):\n    \"\"\"\n\n    WARNING: you may want to backup your browser's cookies file if you use\n    this class to save cookies.  I *think* it works, but there have been\n    bugs in the past!\n\n    This class differs from CookieJar only in the format it uses to save and\n    load cookies to and from a file.  This class uses the Mozilla/Netscape\n    `cookies.txt' format.  lynx uses this file format, too.\n\n    Don't expect cookies saved while the browser is running to be noticed by\n    the browser (in fact, Mozilla on unix will overwrite your saved cookies if\n    you change them on disk while it's running; on Windows, you probably can't\n    save at all while the browser is running).\n\n    Note that the Mozilla/Netscape format will downgrade RFC2965 cookies to\n    Netscape cookies on saving.\n\n    In particular, the cookie version and port number information is lost,\n    together with information about whether or not Path, Port and Discard were\n    specified by the Set-Cookie2 (or Set-Cookie) header, and whether or not the\n    domain as set in the HTTP header started with a dot (yes, I'm aware some\n    domains in Netscape files start with a dot and some don't -- trust me, you\n    really don't want to know any more about this).\n\n    Note that though Mozilla and Netscape use the same format, they use\n    slightly different headers.  The class saves cookies using the Netscape\n    header by default (Mozilla can cope with that).\n\n    \"\"\"\n    magic_re = \"#( Netscape)? HTTP Cookie File\"\n    header = \"\"\"\\\n# Netscape HTTP Cookie File\n# http://curl.haxx.se/rfc/cookie_spec.html\n# This is a generated file!  Do not edit.\n\n\"\"\"\n\n    def _really_load(self, f, filename, ignore_discard, ignore_expires):\n        now = time.time()\n\n        magic = f.readline()\n        if not re.search(self.magic_re, magic):\n            f.close()\n            raise LoadError(\n                \"%r does not look like a Netscape format cookies file\" %\n                filename)\n\n        try:\n            while 1:\n                line = f.readline()\n                if line == \"\": break\n\n                # last field may be absent, so keep any trailing tab\n                if line.endswith(\"\\n\"): line = line[:-1]\n\n                # skip comments and blank lines XXX what is $ for?\n                if (line.strip().startswith((\"#\", \"$\")) or\n                    line.strip() == \"\"):\n                    continue\n\n                domain, domain_specified, path, secure, expires, name, value = \\\n                        line.split(\"\\t\")\n                secure = (secure == \"TRUE\")\n                domain_specified = (domain_specified == \"TRUE\")\n                if name == \"\":\n                    # cookies.txt regards 'Set-Cookie: foo' as a cookie\n                    # with no name, whereas cookielib regards it as a\n                    # cookie with no value.\n                    name = value\n                    value = None\n\n                initial_dot = domain.startswith(\".\")\n                assert domain_specified == initial_dot\n\n                discard = False\n                if expires == \"\":\n                    expires = None\n                    discard = True\n\n                # assume path_specified is false\n                c = Cookie(0, name, value,\n                           None, False,\n                           domain, domain_specified, initial_dot,\n                           path, False,\n                           secure,\n                           expires,\n                           discard,\n                           None,\n                           None,\n                           {})\n                if not ignore_discard and c.discard:\n                    continue\n                if not ignore_expires and c.is_expired(now):\n                    continue\n                self.set_cookie(c)\n\n        except IOError:\n            raise\n        except Exception:\n            _warn_unhandled_exception()\n            raise LoadError(\"invalid Netscape format cookies file %r: %r\" %\n                            (filename, line))\n\n    def save(self, filename=None, ignore_discard=False, ignore_expires=False):\n        if filename is None:\n            if self.filename is not None: filename = self.filename\n            else: raise ValueError(MISSING_FILENAME_TEXT)\n\n        f = open(filename, \"w\")\n        try:\n            f.write(self.header)\n            now = time.time()\n            for cookie in self:\n                if not ignore_discard and cookie.discard:\n                    continue\n                if not ignore_expires and cookie.is_expired(now):\n                    continue\n                if cookie.secure: secure = \"TRUE\"\n                else: secure = \"FALSE\"\n                if cookie.domain.startswith(\".\"): initial_dot = \"TRUE\"\n                else: initial_dot = \"FALSE\"\n                if cookie.expires is not None:\n                    expires = str(cookie.expires)\n                else:\n                    expires = \"\"\n                if cookie.value is None:\n                    # cookies.txt regards 'Set-Cookie: foo' as a cookie\n                    # with no name, whereas cookielib regards it as a\n                    # cookie with no value.\n                    name = \"\"\n                    value = cookie.name\n                else:\n                    name = cookie.name\n                    value = cookie.value\n                f.write(\n                    \"\\t\".join([cookie.domain, initial_dot, cookie.path,\n                               secure, expires, name, value])+\n                    \"\\n\")\n        finally:\n            f.close()\n", 
    "__future__": "\"\"\"Record of phased-in incompatible language changes.\n\nEach line is of the form:\n\n    FeatureName = \"_Feature(\" OptionalRelease \",\" MandatoryRelease \",\"\n                              CompilerFlag \")\"\n\nwhere, normally, OptionalRelease < MandatoryRelease, and both are 5-tuples\nof the same form as sys.version_info:\n\n    (PY_MAJOR_VERSION, # the 2 in 2.1.0a3; an int\n     PY_MINOR_VERSION, # the 1; an int\n     PY_MICRO_VERSION, # the 0; an int\n     PY_RELEASE_LEVEL, # \"alpha\", \"beta\", \"candidate\" or \"final\"; string\n     PY_RELEASE_SERIAL # the 3; an int\n    )\n\nOptionalRelease records the first release in which\n\n    from __future__ import FeatureName\n\nwas accepted.\n\nIn the case of MandatoryReleases that have not yet occurred,\nMandatoryRelease predicts the release in which the feature will become part\nof the language.\n\nElse MandatoryRelease records when the feature became part of the language;\nin releases at or after that, modules no longer need\n\n    from __future__ import FeatureName\n\nto use the feature in question, but may continue to use such imports.\n\nMandatoryRelease may also be None, meaning that a planned feature got\ndropped.\n\nInstances of class _Feature have two corresponding methods,\n.getOptionalRelease() and .getMandatoryRelease().\n\nCompilerFlag is the (bitfield) flag that should be passed in the fourth\nargument to the builtin function compile() to enable the feature in\ndynamically compiled code.  This flag is stored in the .compiler_flag\nattribute on _Future instances.  These values must match the appropriate\n#defines of CO_xxx flags in Include/compile.h.\n\nNo feature line is ever to be deleted from this file.\n\"\"\"\n\nall_feature_names = [\n    \"nested_scopes\",\n    \"generators\",\n    \"division\",\n    \"absolute_import\",\n    \"with_statement\",\n    \"print_function\",\n    \"unicode_literals\",\n]\n\n__all__ = [\"all_feature_names\"] + all_feature_names\n\n# The CO_xxx symbols are defined here under the same names used by\n# compile.h, so that an editor search will find them here.  However,\n# they're not exported in __all__, because they don't really belong to\n# this module.\nCO_NESTED            = 0x0010   # nested_scopes\nCO_GENERATOR_ALLOWED = 0        # generators (obsolete, was 0x1000)\nCO_FUTURE_DIVISION   = 0x2000   # division\nCO_FUTURE_ABSOLUTE_IMPORT = 0x4000 # perform absolute imports by default\nCO_FUTURE_WITH_STATEMENT  = 0x8000   # with statement\nCO_FUTURE_PRINT_FUNCTION  = 0x10000   # print function\nCO_FUTURE_UNICODE_LITERALS = 0x20000 # unicode string literals\n\nclass _Feature:\n    def __init__(self, optionalRelease, mandatoryRelease, compiler_flag):\n        self.optional = optionalRelease\n        self.mandatory = mandatoryRelease\n        self.compiler_flag = compiler_flag\n\n    def getOptionalRelease(self):\n        \"\"\"Return first release in which this feature was recognized.\n\n        This is a 5-tuple, of the same form as sys.version_info.\n        \"\"\"\n\n        return self.optional\n\n    def getMandatoryRelease(self):\n        \"\"\"Return release in which this feature will become mandatory.\n\n        This is a 5-tuple, of the same form as sys.version_info, or, if\n        the feature was dropped, is None.\n        \"\"\"\n\n        return self.mandatory\n\n    def __repr__(self):\n        return \"_Feature\" + repr((self.optional,\n                                  self.mandatory,\n                                  self.compiler_flag))\n\nnested_scopes = _Feature((2, 1, 0, \"beta\",  1),\n                         (2, 2, 0, \"alpha\", 0),\n                         CO_NESTED)\n\ngenerators = _Feature((2, 2, 0, \"alpha\", 1),\n                      (2, 3, 0, \"final\", 0),\n                      CO_GENERATOR_ALLOWED)\n\ndivision = _Feature((2, 2, 0, \"alpha\", 2),\n                    (3, 0, 0, \"alpha\", 0),\n                    CO_FUTURE_DIVISION)\n\nabsolute_import = _Feature((2, 5, 0, \"alpha\", 1),\n                           (3, 0, 0, \"alpha\", 0),\n                           CO_FUTURE_ABSOLUTE_IMPORT)\n\nwith_statement = _Feature((2, 5, 0, \"alpha\", 1),\n                          (2, 6, 0, \"alpha\", 0),\n                          CO_FUTURE_WITH_STATEMENT)\n\nprint_function = _Feature((2, 6, 0, \"alpha\", 2),\n                          (3, 0, 0, \"alpha\", 0),\n                          CO_FUTURE_PRINT_FUNCTION)\n\nunicode_literals = _Feature((2, 6, 0, \"alpha\", 2),\n                            (3, 0, 0, \"alpha\", 0),\n                            CO_FUTURE_UNICODE_LITERALS)\n", 
    "_abcoll": "# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) for collections, according to PEP 3119.\n\nDON'T USE THIS MODULE DIRECTLY!  The classes here should be imported\nvia collections; they are defined here only to alleviate certain\nbootstrapping issues.  Unit tests are in test_collections.\n\"\"\"\n\nfrom abc import ABCMeta, abstractmethod\nimport sys\n\n__all__ = [\"Hashable\", \"Iterable\", \"Iterator\",\n           \"Sized\", \"Container\", \"Callable\",\n           \"Set\", \"MutableSet\",\n           \"Mapping\", \"MutableMapping\",\n           \"MappingView\", \"KeysView\", \"ItemsView\", \"ValuesView\",\n           \"Sequence\", \"MutableSequence\",\n           ]\n\n### ONE-TRICK PONIES ###\n\ndef _hasattr(C, attr):\n    try:\n        return any(attr in B.__dict__ for B in C.__mro__)\n    except AttributeError:\n        # Old-style class\n        return hasattr(C, attr)\n\n\nclass Hashable:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __hash__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Hashable:\n            try:\n                for B in C.__mro__:\n                    if \"__hash__\" in B.__dict__:\n                        if B.__dict__[\"__hash__\"]:\n                            return True\n                        break\n            except AttributeError:\n                # Old-style class\n                if getattr(C, \"__hash__\", None):\n                    return True\n        return NotImplemented\n\n\nclass Iterable:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __iter__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterable:\n            if _hasattr(C, \"__iter__\"):\n                return True\n        return NotImplemented\n\nIterable.register(str)\n\n\nclass Iterator(Iterable):\n\n    @abstractmethod\n    def next(self):\n        'Return the next item from the iterator. When exhausted, raise StopIteration'\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterator:\n            if _hasattr(C, \"next\") and _hasattr(C, \"__iter__\"):\n                return True\n        return NotImplemented\n\n\nclass Sized:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __len__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Sized:\n            if _hasattr(C, \"__len__\"):\n                return True\n        return NotImplemented\n\n\nclass Container:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __contains__(self, x):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Container:\n            if _hasattr(C, \"__contains__\"):\n                return True\n        return NotImplemented\n\n\nclass Callable:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __call__(self, *args, **kwds):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Callable:\n            if _hasattr(C, \"__call__\"):\n                return True\n        return NotImplemented\n\n\n### SETS ###\n\n\nclass Set(Sized, Iterable, Container):\n    \"\"\"A set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__ and __len__.\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), all you have to do is redefine __le__ and\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    def __le__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) > len(other):\n            return False\n        for elem in self:\n            if elem not in other:\n                return False\n        return True\n\n    def __lt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) < len(other) and self.__le__(other)\n\n    def __gt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) > len(other) and self.__ge__(other)\n\n    def __ge__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) < len(other):\n            return False\n        for elem in other:\n            if elem not in self:\n                return False\n        return True\n\n    def __eq__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) == len(other) and self.__le__(other)\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    @classmethod\n    def _from_iterable(cls, it):\n        '''Construct an instance of the class from any iterable input.\n\n        Must override this method if the class constructor signature\n        does not accept an iterable for an input.\n        '''\n        return cls(it)\n\n    def __and__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        return self._from_iterable(value for value in other if value in self)\n\n    __rand__ = __and__\n\n    def isdisjoint(self, other):\n        'Return True if two sets have a null intersection.'\n        for value in other:\n            if value in self:\n                return False\n        return True\n\n    def __or__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        chain = (e for s in (self, other) for e in s)\n        return self._from_iterable(chain)\n\n    __ror__ = __or__\n\n    def __sub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in self\n                                   if value not in other)\n\n    def __rsub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in other\n                                   if value not in self)\n\n    def __xor__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return (self - other) | (other - self)\n\n    __rxor__ = __xor__\n\n    # Sets are not hashable by default, but subclasses can change this\n    __hash__ = None\n\n    def _hash(self):\n        \"\"\"Compute the hash value of a set.\n\n        Note that we don't define __hash__: not all sets are hashable.\n        But if you define a hashable set type, its __hash__ should\n        call this function.\n\n        This must be compatible __eq__.\n\n        All sets ought to compare equal if they contain the same\n        elements, regardless of how they are implemented, and\n        regardless of the order of the elements; so there's not much\n        freedom for __eq__ or __hash__.  We match the algorithm used\n        by the built-in frozenset type.\n        \"\"\"\n        MAX = sys.maxint\n        MASK = 2 * MAX + 1\n        n = len(self)\n        h = 1927868237 * (n + 1)\n        h &= MASK\n        for x in self:\n            hx = hash(x)\n            h ^= (hx ^ (hx << 16) ^ 89869747)  * 3644798167\n            h &= MASK\n        h = h * 69069 + 907133923\n        h &= MASK\n        if h > MAX:\n            h -= MASK + 1\n        if h == -1:\n            h = 590923713\n        return h\n\nSet.register(frozenset)\n\n\nclass MutableSet(Set):\n    \"\"\"A mutable set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__, __len__,\n    add(), and discard().\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), all you have to do is redefine __le__ and\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    @abstractmethod\n    def add(self, value):\n        \"\"\"Add an element.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def discard(self, value):\n        \"\"\"Remove an element.  Do not raise an exception if absent.\"\"\"\n        raise NotImplementedError\n\n    def remove(self, value):\n        \"\"\"Remove an element. If not a member, raise a KeyError.\"\"\"\n        if value not in self:\n            raise KeyError(value)\n        self.discard(value)\n\n    def pop(self):\n        \"\"\"Return the popped value.  Raise KeyError if empty.\"\"\"\n        it = iter(self)\n        try:\n            value = next(it)\n        except StopIteration:\n            raise KeyError\n        self.discard(value)\n        return value\n\n    def clear(self):\n        \"\"\"This is slow (creates N new iterators!) but effective.\"\"\"\n        try:\n            while True:\n                self.pop()\n        except KeyError:\n            pass\n\n    def __ior__(self, it):\n        for value in it:\n            self.add(value)\n        return self\n\n    def __iand__(self, it):\n        for value in (self - it):\n            self.discard(value)\n        return self\n\n    def __ixor__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            if not isinstance(it, Set):\n                it = self._from_iterable(it)\n            for value in it:\n                if value in self:\n                    self.discard(value)\n                else:\n                    self.add(value)\n        return self\n\n    def __isub__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            for value in it:\n                self.discard(value)\n        return self\n\nMutableSet.register(set)\n\n\n### MAPPINGS ###\n\n\nclass Mapping(Sized, Iterable, Container):\n\n    \"\"\"A Mapping is a generic container for associating key/value\n    pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __getitem__(self, key):\n        raise KeyError\n\n    def get(self, key, default=None):\n        'D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.'\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __contains__(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        else:\n            return True\n\n    def iterkeys(self):\n        'D.iterkeys() -> an iterator over the keys of D'\n        return iter(self)\n\n    def itervalues(self):\n        'D.itervalues() -> an iterator over the values of D'\n        for key in self:\n            yield self[key]\n\n    def iteritems(self):\n        'D.iteritems() -> an iterator over the (key, value) items of D'\n        for key in self:\n            yield (key, self[key])\n\n    def keys(self):\n        \"D.keys() -> list of D's keys\"\n        return list(self)\n\n    def items(self):\n        \"D.items() -> list of D's (key, value) pairs, as 2-tuples\"\n        return [(key, self[key]) for key in self]\n\n    def values(self):\n        \"D.values() -> list of D's values\"\n        return [self[key] for key in self]\n\n    # Mappings are not hashable by default, but subclasses can change this\n    __hash__ = None\n\n    def __eq__(self, other):\n        if not isinstance(other, Mapping):\n            return NotImplemented\n        return dict(self.items()) == dict(other.items())\n\n    def __ne__(self, other):\n        return not (self == other)\n\nclass MappingView(Sized):\n\n    def __init__(self, mapping):\n        self._mapping = mapping\n\n    def __len__(self):\n        return len(self._mapping)\n\n    def __repr__(self):\n        return '{0.__class__.__name__}({0._mapping!r})'.format(self)\n\n\nclass KeysView(MappingView, Set):\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, key):\n        return key in self._mapping\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield key\n\n\nclass ItemsView(MappingView, Set):\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, item):\n        key, value = item\n        try:\n            v = self._mapping[key]\n        except KeyError:\n            return False\n        else:\n            return v == value\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield (key, self._mapping[key])\n\n\nclass ValuesView(MappingView):\n\n    def __contains__(self, value):\n        for key in self._mapping:\n            if value == self._mapping[key]:\n                return True\n        return False\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield self._mapping[key]\n\n\nclass MutableMapping(Mapping):\n\n    \"\"\"A MutableMapping is a generic container for associating\n    key/value pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __setitem__, __delitem__,\n    __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, key, value):\n        raise KeyError\n\n    @abstractmethod\n    def __delitem__(self, key):\n        raise KeyError\n\n    __marker = object()\n\n    def pop(self, key, default=__marker):\n        '''D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n          If key is not found, d is returned if given, otherwise KeyError is raised.\n        '''\n        try:\n            value = self[key]\n        except KeyError:\n            if default is self.__marker:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def popitem(self):\n        '''D.popitem() -> (k, v), remove and return some (key, value) pair\n           as a 2-tuple; but raise KeyError if D is empty.\n        '''\n        try:\n            key = next(iter(self))\n        except StopIteration:\n            raise KeyError\n        value = self[key]\n        del self[key]\n        return key, value\n\n    def clear(self):\n        'D.clear() -> None.  Remove all items from D.'\n        try:\n            while True:\n                self.popitem()\n        except KeyError:\n            pass\n\n    def update(*args, **kwds):\n        ''' D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n            If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n            If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n            In either case, this is followed by: for k, v in F.items(): D[k] = v\n        '''\n        if len(args) > 2:\n            raise TypeError(\"update() takes at most 2 positional \"\n                            \"arguments ({} given)\".format(len(args)))\n        elif not args:\n            raise TypeError(\"update() takes at least 1 argument (0 given)\")\n        self = args[0]\n        other = args[1] if len(args) >= 2 else ()\n\n        if isinstance(other, Mapping):\n            for key in other:\n                self[key] = other[key]\n        elif hasattr(other, \"keys\"):\n            for key in other.keys():\n                self[key] = other[key]\n        else:\n            for key, value in other:\n                self[key] = value\n        for key, value in kwds.items():\n            self[key] = value\n\n    def setdefault(self, key, default=None):\n        'D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D'\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n\nMutableMapping.register(dict)\n\n\n### SEQUENCES ###\n\n\nclass Sequence(Sized, Iterable, Container):\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must override __new__ or __init__,\n    __getitem__, and __len__.\n    \"\"\"\n\n    @abstractmethod\n    def __getitem__(self, index):\n        raise IndexError\n\n    def __iter__(self):\n        i = 0\n        try:\n            while True:\n                v = self[i]\n                yield v\n                i += 1\n        except IndexError:\n            return\n\n    def __contains__(self, value):\n        for v in self:\n            if v == value:\n                return True\n        return False\n\n    def __reversed__(self):\n        for i in reversed(range(len(self))):\n            yield self[i]\n\n    def index(self, value):\n        '''S.index(value) -> integer -- return first index of value.\n           Raises ValueError if the value is not present.\n        '''\n        for i, v in enumerate(self):\n            if v == value:\n                return i\n        raise ValueError\n\n    def count(self, value):\n        'S.count(value) -> integer -- return number of occurrences of value'\n        return sum(1 for v in self if v == value)\n\nSequence.register(tuple)\nSequence.register(basestring)\nSequence.register(buffer)\nSequence.register(xrange)\n\n\nclass MutableSequence(Sequence):\n\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must provide __new__ or __init__,\n    __getitem__, __setitem__, __delitem__, __len__, and insert().\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, index, value):\n        raise IndexError\n\n    @abstractmethod\n    def __delitem__(self, index):\n        raise IndexError\n\n    @abstractmethod\n    def insert(self, index, value):\n        'S.insert(index, object) -- insert object before index'\n        raise IndexError\n\n    def append(self, value):\n        'S.append(object) -- append object to the end of the sequence'\n        self.insert(len(self), value)\n\n    def reverse(self):\n        'S.reverse() -- reverse *IN PLACE*'\n        n = len(self)\n        for i in range(n//2):\n            self[i], self[n-i-1] = self[n-i-1], self[i]\n\n    def extend(self, values):\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\n        for v in values:\n            self.append(v)\n\n    def pop(self, index=-1):\n        '''S.pop([index]) -> item -- remove and return item at index (default last).\n           Raise IndexError if list is empty or index is out of range.\n        '''\n        v = self[index]\n        del self[index]\n        return v\n\n    def remove(self, value):\n        '''S.remove(value) -- remove first occurrence of value.\n           Raise ValueError if the value is not present.\n        '''\n        del self[self.index(value)]\n\n    def __iadd__(self, values):\n        self.extend(values)\n        return self\n\nMutableSequence.register(list)\n", 
    "_functools": "\"\"\" Supplies the internal functions for functools.py in the standard library \"\"\"\n\n# reduce() has moved to _functools in Python 2.6+.\nreduce = reduce\n\nclass partial(object):\n    \"\"\"\n    partial(func, *args, **keywords) - new function with partial application\n    of the given arguments and keywords.\n    \"\"\"\n\n    def __init__(self, *args, **keywords):\n        if not args:\n            raise TypeError('__init__() takes at least 2 arguments (1 given)')\n        func, args = args[0], args[1:]\n        if not callable(func):\n            raise TypeError(\"the first argument must be callable\")\n        self._func = func\n        self._args = args\n        self._keywords = keywords or None\n\n    def __delattr__(self, key):\n        if key == '__dict__':\n            raise TypeError(\"a partial object's dictionary may not be deleted\")\n        object.__delattr__(self, key)\n\n    @property\n    def func(self):\n        return self._func\n\n    @property\n    def args(self):\n        return self._args\n\n    @property\n    def keywords(self):\n        return self._keywords\n\n    def __call__(self, *fargs, **fkeywords):\n        if self.keywords is not None:\n            fkeywords = dict(self.keywords, **fkeywords)\n        return self.func(*(self.args + fargs), **fkeywords)\n\n    def __reduce__(self):\n        d = dict((k, v) for k, v in self.__dict__.iteritems() if k not in\n                ('_func', '_args', '_keywords'))\n        if len(d) == 0:\n            d = None\n        return (type(self), (self.func,),\n                (self.func, self.args, self.keywords, d))\n\n    def __setstate__(self, state):\n        self._func, self._args, self._keywords, d = state\n        if d is not None:\n            self.__dict__.update(d)\n", 
    "_marshal": "\"\"\"Internal Python object serialization\n\nThis module contains functions that can read and write Python values in a binary format. The format is specific to Python, but independent of machine architecture issues (e.g., you can write a Python value to a file on a PC, transport the file to a Sun, and read it back there). Details of the format may change between Python versions.\n\"\"\"\n\n# NOTE: This module is used in the Python3 interpreter, but also by\n# the \"sandboxed\" process.  It must work for Python2 as well.\n\nimport types\n\ntry:\n    intern\nexcept NameError:\n    from sys import intern\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n\nTYPE_NULL     = '0'\nTYPE_NONE     = 'N'\nTYPE_FALSE    = 'F'\nTYPE_TRUE     = 'T'\nTYPE_STOPITER = 'S'\nTYPE_ELLIPSIS = '.'\nTYPE_INT      = 'i'\nTYPE_INT64    = 'I'\nTYPE_FLOAT    = 'f'\nTYPE_COMPLEX  = 'x'\nTYPE_LONG     = 'l'\nTYPE_STRING   = 's'\nTYPE_INTERNED = 't'\nTYPE_STRINGREF= 'R'\nTYPE_TUPLE    = '('\nTYPE_LIST     = '['\nTYPE_DICT     = '{'\nTYPE_CODE     = 'c'\nTYPE_UNICODE  = 'u'\nTYPE_UNKNOWN  = '?'\nTYPE_SET      = '<'\nTYPE_FROZENSET= '>'\n\nclass _Marshaller:\n\n    dispatch = {}\n\n    def __init__(self, writefunc):\n        self._write = writefunc\n\n    def dump(self, x):\n        try:\n            self.dispatch[type(x)](self, x)\n        except KeyError:\n            for tp in type(x).mro():\n                func = self.dispatch.get(tp)\n                if func:\n                    break\n            else:\n                raise ValueError(\"unmarshallable object\")\n            func(self, x)\n\n    def w_long64(self, x):\n        self.w_long(x)\n        self.w_long(x>>32)\n\n    def w_long(self, x):\n        a = chr(x & 0xff)\n        x >>= 8\n        b = chr(x & 0xff)\n        x >>= 8\n        c = chr(x & 0xff)\n        x >>= 8\n        d = chr(x & 0xff)\n        self._write(a + b + c + d)\n\n    def w_short(self, x):\n        self._write(chr((x)     & 0xff))\n        self._write(chr((x>> 8) & 0xff))\n\n    def dump_none(self, x):\n        self._write(TYPE_NONE)\n    dispatch[type(None)] = dump_none\n\n    def dump_bool(self, x):\n        if x:\n            self._write(TYPE_TRUE)\n        else:\n            self._write(TYPE_FALSE)\n    dispatch[bool] = dump_bool\n\n    def dump_stopiter(self, x):\n        if x is not StopIteration:\n            raise ValueError(\"unmarshallable object\")\n        self._write(TYPE_STOPITER)\n    dispatch[type(StopIteration)] = dump_stopiter\n\n    def dump_ellipsis(self, x):\n        self._write(TYPE_ELLIPSIS)\n    \n    try:\n        dispatch[type(Ellipsis)] = dump_ellipsis\n    except NameError:\n        pass\n\n    # In Python3, this function is not used; see dump_long() below.\n    def dump_int(self, x):\n        y = x>>31\n        if y and y != -1:\n            self._write(TYPE_INT64)\n            self.w_long64(x)\n        else:\n            self._write(TYPE_INT)\n            self.w_long(x)\n    dispatch[int] = dump_int\n\n    def dump_long(self, x):\n        self._write(TYPE_LONG)\n        sign = 1\n        if x < 0:\n            sign = -1\n            x = -x\n        digits = []\n        while x:\n            digits.append(x & 0x7FFF)\n            x = x>>15\n        self.w_long(len(digits) * sign)\n        for d in digits:\n            self.w_short(d)\n    try:\n        long\n    except NameError:\n        dispatch[int] = dump_long\n    else:\n        dispatch[long] = dump_long\n\n    def dump_float(self, x):\n        write = self._write\n        write(TYPE_FLOAT)\n        s = repr(x)\n        write(chr(len(s)))\n        write(s)\n    dispatch[float] = dump_float\n\n    def dump_complex(self, x):\n        write = self._write\n        write(TYPE_COMPLEX)\n        s = repr(x.real)\n        write(chr(len(s)))\n        write(s)\n        s = repr(x.imag)\n        write(chr(len(s)))\n        write(s)\n    try:\n        dispatch[complex] = dump_complex\n    except NameError:\n        pass\n\n    def dump_string(self, x):\n        # XXX we can't check for interned strings, yet,\n        # so we (for now) never create TYPE_INTERNED or TYPE_STRINGREF\n        self._write(TYPE_STRING)\n        self.w_long(len(x))\n        self._write(x)\n    dispatch[bytes] = dump_string\n\n    def dump_unicode(self, x):\n        self._write(TYPE_UNICODE)\n        s = x.encode('utf8')\n        self.w_long(len(s))\n        self._write(s)\n    try:\n        unicode\n    except NameError:\n        dispatch[str] = dump_unicode\n    else:\n        dispatch[unicode] = dump_unicode\n\n    def dump_tuple(self, x):\n        self._write(TYPE_TUPLE)\n        self.w_long(len(x))\n        for item in x:\n            self.dump(item)\n    dispatch[tuple] = dump_tuple\n\n    def dump_list(self, x):\n        self._write(TYPE_LIST)\n        self.w_long(len(x))\n        for item in x:\n            self.dump(item)\n    dispatch[list] = dump_list\n\n    def dump_dict(self, x):\n        self._write(TYPE_DICT)\n        for key, value in x.items():\n            self.dump(key)\n            self.dump(value)\n        self._write(TYPE_NULL)\n    dispatch[dict] = dump_dict\n\n    def dump_code(self, x):\n        self._write(TYPE_CODE)\n        self.w_long(x.co_argcount)\n        self.w_long(x.co_nlocals)\n        self.w_long(x.co_stacksize)\n        self.w_long(x.co_flags)\n        self.dump(x.co_code)\n        self.dump(x.co_consts)\n        self.dump(x.co_names)\n        self.dump(x.co_varnames)\n        self.dump(x.co_freevars)\n        self.dump(x.co_cellvars)\n        self.dump(x.co_filename)\n        self.dump(x.co_name)\n        self.w_long(x.co_firstlineno)\n        self.dump(x.co_lnotab)\n    try:\n        dispatch[types.CodeType] = dump_code\n    except NameError:\n        pass\n\n    def dump_set(self, x):\n        self._write(TYPE_SET)\n        self.w_long(len(x))\n        for each in x:\n            self.dump(each)\n    try:\n        dispatch[set] = dump_set\n    except NameError:\n        pass\n\n    def dump_frozenset(self, x):\n        self._write(TYPE_FROZENSET)\n        self.w_long(len(x))\n        for each in x:\n            self.dump(each)\n    try:\n        dispatch[frozenset] = dump_frozenset\n    except NameError:\n        pass\n\nclass _NULL:\n    pass\n\nclass _StringBuffer:\n    def __init__(self, value):\n        self.bufstr = value\n        self.bufpos = 0\n\n    def read(self, n):\n        pos = self.bufpos\n        newpos = pos + n\n        ret = self.bufstr[pos : newpos]\n        self.bufpos = newpos\n        return ret\n\n\nclass _Unmarshaller:\n\n    dispatch = {}\n\n    def __init__(self, readfunc):\n        self._read = readfunc\n        self._stringtable = []\n\n    def load(self):\n        c = self._read(1)\n        if not c:\n            raise EOFError\n        try:\n            return self.dispatch[c](self)\n        except KeyError:\n            raise ValueError(\"bad marshal code: %c (%d)\" % (c, ord(c)))\n\n    def r_short(self):\n        lo = ord(self._read(1))\n        hi = ord(self._read(1))\n        x = lo | (hi<<8)\n        if x & 0x8000:\n            x = x - 0x10000\n        return x\n\n    def r_long(self):\n        s = self._read(4)\n        a = ord(s[0])\n        b = ord(s[1])\n        c = ord(s[2])\n        d = ord(s[3])\n        x = a | (b<<8) | (c<<16) | (d<<24)\n        if d & 0x80 and x > 0:\n            x = -((1<<32) - x)\n            return int(x)\n        else:\n            return x\n\n    def r_long64(self):\n        a = ord(self._read(1))\n        b = ord(self._read(1))\n        c = ord(self._read(1))\n        d = ord(self._read(1))\n        e = ord(self._read(1))\n        f = ord(self._read(1))\n        g = ord(self._read(1))\n        h = ord(self._read(1))\n        x = a | (b<<8) | (c<<16) | (d<<24)\n        x = x | (e<<32) | (f<<40) | (g<<48) | (h<<56)\n        if h & 0x80 and x > 0:\n            x = -((1<<64) - x)\n        return x\n\n    def load_null(self):\n        return _NULL\n    dispatch[TYPE_NULL] = load_null\n\n    def load_none(self):\n        return None\n    dispatch[TYPE_NONE] = load_none\n\n    def load_true(self):\n        return True\n    dispatch[TYPE_TRUE] = load_true\n\n    def load_false(self):\n        return False\n    dispatch[TYPE_FALSE] = load_false\n\n    def load_stopiter(self):\n        return StopIteration\n    dispatch[TYPE_STOPITER] = load_stopiter\n\n    def load_ellipsis(self):\n        return Ellipsis\n    dispatch[TYPE_ELLIPSIS] = load_ellipsis\n\n    dispatch[TYPE_INT] = r_long\n\n    dispatch[TYPE_INT64] = r_long64\n\n    def load_long(self):\n        size = self.r_long()\n        sign = 1\n        if size < 0:\n            sign = -1\n            size = -size\n        x = 0\n        for i in range(size):\n            d = self.r_short()\n            x = x | (d<<(i*15))\n        return x * sign\n    dispatch[TYPE_LONG] = load_long\n\n    def load_float(self):\n        n = ord(self._read(1))\n        s = self._read(n)\n        return float(s)\n    dispatch[TYPE_FLOAT] = load_float\n\n    def load_complex(self):\n        n = ord(self._read(1))\n        s = self._read(n)\n        real = float(s)\n        n = ord(self._read(1))\n        s = self._read(n)\n        imag = float(s)\n        return complex(real, imag)\n    dispatch[TYPE_COMPLEX] = load_complex\n\n    def load_string(self):\n        n = self.r_long()\n        return self._read(n)\n    dispatch[TYPE_STRING] = load_string\n\n    def load_interned(self):\n        n = self.r_long()\n        ret = intern(self._read(n))\n        self._stringtable.append(ret)\n        return ret\n    dispatch[TYPE_INTERNED] = load_interned\n\n    def load_stringref(self):\n        n = self.r_long()\n        return self._stringtable[n]\n    dispatch[TYPE_STRINGREF] = load_stringref\n\n    def load_unicode(self):\n        n = self.r_long()\n        s = self._read(n)\n        ret = s.decode('utf8')\n        return ret\n    dispatch[TYPE_UNICODE] = load_unicode\n\n    def load_tuple(self):\n        return tuple(self.load_list())\n    dispatch[TYPE_TUPLE] = load_tuple\n\n    def load_list(self):\n        n = self.r_long()\n        list = [self.load() for i in range(n)]\n        return list\n    dispatch[TYPE_LIST] = load_list\n\n    def load_dict(self):\n        d = {}\n        while 1:\n            key = self.load()\n            if key is _NULL:\n                break\n            value = self.load()\n            d[key] = value\n        return d\n    dispatch[TYPE_DICT] = load_dict\n\n    def load_code(self):\n        argcount = self.r_long()\n        nlocals = self.r_long()\n        stacksize = self.r_long()\n        flags = self.r_long()\n        code = self.load()\n        consts = self.load()\n        names = self.load()\n        varnames = self.load()\n        freevars = self.load()\n        cellvars = self.load()\n        filename = self.load()\n        name = self.load()\n        firstlineno = self.r_long()\n        lnotab = self.load()\n        return types.CodeType(argcount, nlocals, stacksize, flags, code, consts,\n                              names, varnames, filename, name, firstlineno,\n                              lnotab, freevars, cellvars)\n    dispatch[TYPE_CODE] = load_code\n\n    def load_set(self):\n        n = self.r_long()\n        args = [self.load() for i in range(n)]\n        return set(args)\n    dispatch[TYPE_SET] = load_set\n\n    def load_frozenset(self):\n        n = self.r_long()\n        args = [self.load() for i in range(n)]\n        return frozenset(args)\n    dispatch[TYPE_FROZENSET] = load_frozenset\n\n# ________________________________________________________________\n\ndef _read(self, n):\n    pos = self.bufpos\n    newpos = pos + n\n    if newpos > len(self.bufstr): raise EOFError\n    ret = self.bufstr[pos : newpos]\n    self.bufpos = newpos\n    return ret\n\ndef _read1(self):\n    ret = self.bufstr[self.bufpos]\n    self.bufpos += 1\n    return ret\n\ndef _r_short(self):\n    lo = ord(_read1(self))\n    hi = ord(_read1(self))\n    x = lo | (hi<<8)\n    if x & 0x8000:\n        x = x - 0x10000\n    return x\n\ndef _r_long(self):\n    # inlined this most common case\n    p = self.bufpos\n    s = self.bufstr\n    a = ord(s[p])\n    b = ord(s[p+1])\n    c = ord(s[p+2])\n    d = ord(s[p+3])\n    self.bufpos += 4\n    x = a | (b<<8) | (c<<16) | (d<<24)\n    if d & 0x80 and x > 0:\n        x = -((1<<32) - x)\n        return int(x)\n    else:\n        return x\n\ndef _r_long64(self):\n    a = ord(_read1(self))\n    b = ord(_read1(self))\n    c = ord(_read1(self))\n    d = ord(_read1(self))\n    e = ord(_read1(self))\n    f = ord(_read1(self))\n    g = ord(_read1(self))\n    h = ord(_read1(self))\n    x = a | (b<<8) | (c<<16) | (d<<24)\n    x = x | (e<<32) | (f<<40) | (g<<48) | (h<<56)\n    if h & 0x80 and x > 0:\n        x = -((1<<64) - x)\n    return x\n\n_load_dispatch = {}\n\nclass _FastUnmarshaller:\n\n    dispatch = {}\n\n    def __init__(self, buffer):\n        self.bufstr = buffer\n        self.bufpos = 0\n        self._stringtable = []\n\n    def load(self):\n        # make flow space happy\n        c = '?'\n        try:\n            c = self.bufstr[self.bufpos]\n            self.bufpos += 1\n            return _load_dispatch[c](self)\n        except KeyError:\n            raise ValueError(\"bad marshal code: %c (%d)\" % (c, ord(c)))\n        except IndexError:\n            raise EOFError\n\n    def load_null(self):\n        return _NULL\n    dispatch[TYPE_NULL] = load_null\n\n    def load_none(self):\n        return None\n    dispatch[TYPE_NONE] = load_none\n\n    def load_true(self):\n        return True\n    dispatch[TYPE_TRUE] = load_true\n\n    def load_false(self):\n        return False\n    dispatch[TYPE_FALSE] = load_false\n\n    def load_stopiter(self):\n        return StopIteration\n    dispatch[TYPE_STOPITER] = load_stopiter\n\n    def load_ellipsis(self):\n        return Ellipsis\n    dispatch[TYPE_ELLIPSIS] = load_ellipsis\n\n    def load_int(self):\n        return _r_long(self)\n    dispatch[TYPE_INT] = load_int\n\n    def load_int64(self):\n        return _r_long64(self)\n    dispatch[TYPE_INT64] = load_int64\n\n    def load_long(self):\n        size = _r_long(self)\n        sign = 1\n        if size < 0:\n            sign = -1\n            size = -size\n        x = 0\n        for i in range(size):\n            d = _r_short(self)\n            x = x | (d<<(i*15))\n        return x * sign\n    dispatch[TYPE_LONG] = load_long\n\n    def load_float(self):\n        n = ord(_read1(self))\n        s = _read(self, n)\n        return float(s)\n    dispatch[TYPE_FLOAT] = load_float\n\n    def load_complex(self):\n        n = ord(_read1(self))\n        s = _read(self, n)\n        real = float(s)\n        n = ord(_read1(self))\n        s = _read(self, n)\n        imag = float(s)\n        return complex(real, imag)\n    dispatch[TYPE_COMPLEX] = load_complex\n\n    def load_string(self):\n        n = _r_long(self)\n        return _read(self, n)\n    dispatch[TYPE_STRING] = load_string\n\n    def load_interned(self):\n        n = _r_long(self)\n        ret = intern(_read(self, n))\n        self._stringtable.append(ret)\n        return ret\n    dispatch[TYPE_INTERNED] = load_interned\n\n    def load_stringref(self):\n        n = _r_long(self)\n        return self._stringtable[n]\n    dispatch[TYPE_STRINGREF] = load_stringref\n\n    def load_unicode(self):\n        n = _r_long(self)\n        s = _read(self, n)\n        ret = s.decode('utf8')\n        return ret\n    dispatch[TYPE_UNICODE] = load_unicode\n\n    def load_tuple(self):\n        return tuple(self.load_list())\n    dispatch[TYPE_TUPLE] = load_tuple\n\n    def load_list(self):\n        n = _r_long(self)\n        list = []\n        for i in range(n):\n            list.append(self.load())\n        return list\n    dispatch[TYPE_LIST] = load_list\n\n    def load_dict(self):\n        d = {}\n        while 1:\n            key = self.load()\n            if key is _NULL:\n                break\n            value = self.load()\n            d[key] = value\n        return d\n    dispatch[TYPE_DICT] = load_dict\n\n    def load_code(self):\n        argcount = _r_long(self)\n        nlocals = _r_long(self)\n        stacksize = _r_long(self)\n        flags = _r_long(self)\n        code = self.load()\n        consts = self.load()\n        names = self.load()\n        varnames = self.load()\n        freevars = self.load()\n        cellvars = self.load()\n        filename = self.load()\n        name = self.load()\n        firstlineno = _r_long(self)\n        lnotab = self.load()\n        return types.CodeType(argcount, nlocals, stacksize, flags, code, consts,\n                              names, varnames, filename, name, firstlineno,\n                              lnotab, freevars, cellvars)\n    dispatch[TYPE_CODE] = load_code\n\n    def load_set(self):\n        n = _r_long(self)\n        args = [self.load() for i in range(n)]\n        return set(args)\n    dispatch[TYPE_SET] = load_set\n\n    def load_frozenset(self):\n        n = _r_long(self)\n        args = [self.load() for i in range(n)]\n        return frozenset(args)\n    dispatch[TYPE_FROZENSET] = load_frozenset\n\n_load_dispatch = _FastUnmarshaller.dispatch\n\n# _________________________________________________________________\n#\n# user interface\n\nversion = 1\n\n@builtinify\ndef dump(x, f, version=version):\n    # XXX 'version' is ignored, we always dump in a version-0-compatible format\n    m = _Marshaller(f.write)\n    m.dump(x)\n\n@builtinify\ndef load(f):\n    um = _Unmarshaller(f.read)\n    return um.load()\n\n@builtinify\ndef dumps(x, version=version):\n    # XXX 'version' is ignored, we always dump in a version-0-compatible format\n    buffer = []\n    m = _Marshaller(buffer.append)\n    m.dump(x)\n    return ''.join(buffer)\n\n@builtinify\ndef loads(s):\n    um = _FastUnmarshaller(s)\n    return um.load()\n", 
    "_md5": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note that PyPy contains also a built-in module 'md5' which will hide\n# this one if compiled in.\n\n\"\"\"A sample implementation of MD5 in pure Python.\n\nThis is an implementation of the MD5 hash function, as specified by\nRFC 1321, in pure Python. It was implemented using Bruce Schneier's\nexcellent book \"Applied Cryptography\", 2nd ed., 1996.\n\nSurely this is not meant to compete with the existing implementation\nof the Python standard library (written in C). Rather, it should be\nseen as a Python complement that is more readable than C and can be\nused more conveniently for learning and experimenting purposes in\nthe field of cryptography.\n\nThis module tries very hard to follow the API of the existing Python\nstandard library's \"md5\" module, but although it seems to work fine,\nit has not been extensively tested! (But note that there is a test\nmodule, test_md5py.py, that compares this Python implementation with\nthe C one of the Python standard library.\n\nBEWARE: this comes with no guarantee whatsoever about fitness and/or\nother properties! Specifically, do not use this in any production\ncode! License is Python License!\n\nSpecial thanks to Aurelian Coman who fixed some nasty bugs!\n\nDinu C. Gherman\n\"\"\"\n\n\n__date__    = '2004-11-17'\n__version__ = 0.91 # Modernised by J. Hall\u00e9n and L. Creighton for Pypy\n\n__metaclass__ = type # or genrpy won't work\n\nimport struct, copy\n\n\n# ======================================================================\n# Bit-Manipulation helpers\n# ======================================================================\n\ndef _bytelist2long(list):\n    \"Transform a list of characters into a list of longs.\"\n\n    imax = len(list) // 4\n    hl = [0] * imax\n\n    j = 0\n    i = 0\n    while i < imax:\n        b0 = ord(list[j])\n        b1 = ord(list[j+1]) << 8\n        b2 = ord(list[j+2]) << 16\n        b3 = ord(list[j+3]) << 24\n        hl[i] = b0 | b1 |b2 | b3\n        i = i+1\n        j = j+4\n\n    return hl\n\n\ndef _rotateLeft(x, n):\n    \"Rotate x (32 bit) left n bits circularly.\"\n\n    return (x << n) | (x >> (32-n))\n\n\n# ======================================================================\n# The real MD5 meat...\n#\n#   Implemented after \"Applied Cryptography\", 2nd ed., 1996,\n#   pp. 436-441 by Bruce Schneier.\n# ======================================================================\n\n# F, G, H and I are basic MD5 functions.\n\ndef F(x, y, z):\n    return (x & y) | ((~x) & z)\n\ndef G(x, y, z):\n    return (x & z) | (y & (~z))\n\ndef H(x, y, z):\n    return x ^ y ^ z\n\ndef I(x, y, z):\n    return y ^ (x | (~z))\n\n\ndef XX(func, a, b, c, d, x, s, ac):\n    \"\"\"Wrapper for call distribution to functions F, G, H and I.\n\n    This replaces functions FF, GG, HH and II from \"Appl. Crypto.\"\n    Rotation is separate from addition to prevent recomputation\n    (now summed-up in one function).\n    \"\"\"\n\n    res = 0\n    res = res + a + func(b, c, d)\n    res = res + x \n    res = res + ac\n    res = res & 0xffffffff\n    res = _rotateLeft(res, s)\n    res = res & 0xffffffff\n    res = res + b\n\n    return res & 0xffffffff\n\n\nclass MD5Type:\n    \"An implementation of the MD5 hash function in pure Python.\"\n\n    digest_size = digestsize = 16\n    block_size = 64\n\n    def __init__(self):\n        \"Initialisation.\"\n        \n        # Initial message length in bits(!).\n        self.length = 0\n        self.count = [0, 0]\n\n        # Initial empty message as a sequence of bytes (8 bit characters).\n        self.input = []\n\n        # Call a separate init function, that can be used repeatedly\n        # to start from scratch on the same object.\n        self.init()\n\n\n    def init(self):\n        \"Initialize the message-digest and set all fields to zero.\"\n\n        self.length = 0\n        self.count = [0, 0]\n        self.input = []\n\n        # Load magic initialization constants.\n        self.A = 0x67452301\n        self.B = 0xefcdab89\n        self.C = 0x98badcfe\n        self.D = 0x10325476\n\n\n    def _transform(self, inp):\n        \"\"\"Basic MD5 step transforming the digest based on the input.\n\n        Note that if the Mysterious Constants are arranged backwards\n        in little-endian order and decrypted with the DES they produce\n        OCCULT MESSAGES!\n        \"\"\"\n\n        a, b, c, d = A, B, C, D = self.A, self.B, self.C, self.D\n\n        # Round 1.\n\n        S11, S12, S13, S14 = 7, 12, 17, 22\n\n        a = XX(F, a, b, c, d, inp[ 0], S11, 0xD76AA478) # 1 \n        d = XX(F, d, a, b, c, inp[ 1], S12, 0xE8C7B756) # 2 \n        c = XX(F, c, d, a, b, inp[ 2], S13, 0x242070DB) # 3 \n        b = XX(F, b, c, d, a, inp[ 3], S14, 0xC1BDCEEE) # 4 \n        a = XX(F, a, b, c, d, inp[ 4], S11, 0xF57C0FAF) # 5 \n        d = XX(F, d, a, b, c, inp[ 5], S12, 0x4787C62A) # 6 \n        c = XX(F, c, d, a, b, inp[ 6], S13, 0xA8304613) # 7 \n        b = XX(F, b, c, d, a, inp[ 7], S14, 0xFD469501) # 8 \n        a = XX(F, a, b, c, d, inp[ 8], S11, 0x698098D8) # 9 \n        d = XX(F, d, a, b, c, inp[ 9], S12, 0x8B44F7AF) # 10 \n        c = XX(F, c, d, a, b, inp[10], S13, 0xFFFF5BB1) # 11 \n        b = XX(F, b, c, d, a, inp[11], S14, 0x895CD7BE) # 12 \n        a = XX(F, a, b, c, d, inp[12], S11, 0x6B901122) # 13 \n        d = XX(F, d, a, b, c, inp[13], S12, 0xFD987193) # 14 \n        c = XX(F, c, d, a, b, inp[14], S13, 0xA679438E) # 15 \n        b = XX(F, b, c, d, a, inp[15], S14, 0x49B40821) # 16 \n\n        # Round 2.\n\n        S21, S22, S23, S24 = 5, 9, 14, 20\n\n        a = XX(G, a, b, c, d, inp[ 1], S21, 0xF61E2562) # 17 \n        d = XX(G, d, a, b, c, inp[ 6], S22, 0xC040B340) # 18 \n        c = XX(G, c, d, a, b, inp[11], S23, 0x265E5A51) # 19 \n        b = XX(G, b, c, d, a, inp[ 0], S24, 0xE9B6C7AA) # 20 \n        a = XX(G, a, b, c, d, inp[ 5], S21, 0xD62F105D) # 21 \n        d = XX(G, d, a, b, c, inp[10], S22, 0x02441453) # 22 \n        c = XX(G, c, d, a, b, inp[15], S23, 0xD8A1E681) # 23 \n        b = XX(G, b, c, d, a, inp[ 4], S24, 0xE7D3FBC8) # 24 \n        a = XX(G, a, b, c, d, inp[ 9], S21, 0x21E1CDE6) # 25 \n        d = XX(G, d, a, b, c, inp[14], S22, 0xC33707D6) # 26 \n        c = XX(G, c, d, a, b, inp[ 3], S23, 0xF4D50D87) # 27 \n        b = XX(G, b, c, d, a, inp[ 8], S24, 0x455A14ED) # 28 \n        a = XX(G, a, b, c, d, inp[13], S21, 0xA9E3E905) # 29 \n        d = XX(G, d, a, b, c, inp[ 2], S22, 0xFCEFA3F8) # 30 \n        c = XX(G, c, d, a, b, inp[ 7], S23, 0x676F02D9) # 31 \n        b = XX(G, b, c, d, a, inp[12], S24, 0x8D2A4C8A) # 32 \n\n        # Round 3.\n\n        S31, S32, S33, S34 = 4, 11, 16, 23\n\n        a = XX(H, a, b, c, d, inp[ 5], S31, 0xFFFA3942) # 33 \n        d = XX(H, d, a, b, c, inp[ 8], S32, 0x8771F681) # 34 \n        c = XX(H, c, d, a, b, inp[11], S33, 0x6D9D6122) # 35 \n        b = XX(H, b, c, d, a, inp[14], S34, 0xFDE5380C) # 36 \n        a = XX(H, a, b, c, d, inp[ 1], S31, 0xA4BEEA44) # 37 \n        d = XX(H, d, a, b, c, inp[ 4], S32, 0x4BDECFA9) # 38 \n        c = XX(H, c, d, a, b, inp[ 7], S33, 0xF6BB4B60) # 39 \n        b = XX(H, b, c, d, a, inp[10], S34, 0xBEBFBC70) # 40 \n        a = XX(H, a, b, c, d, inp[13], S31, 0x289B7EC6) # 41 \n        d = XX(H, d, a, b, c, inp[ 0], S32, 0xEAA127FA) # 42 \n        c = XX(H, c, d, a, b, inp[ 3], S33, 0xD4EF3085) # 43 \n        b = XX(H, b, c, d, a, inp[ 6], S34, 0x04881D05) # 44 \n        a = XX(H, a, b, c, d, inp[ 9], S31, 0xD9D4D039) # 45 \n        d = XX(H, d, a, b, c, inp[12], S32, 0xE6DB99E5) # 46 \n        c = XX(H, c, d, a, b, inp[15], S33, 0x1FA27CF8) # 47 \n        b = XX(H, b, c, d, a, inp[ 2], S34, 0xC4AC5665) # 48 \n\n        # Round 4.\n\n        S41, S42, S43, S44 = 6, 10, 15, 21\n\n        a = XX(I, a, b, c, d, inp[ 0], S41, 0xF4292244) # 49 \n        d = XX(I, d, a, b, c, inp[ 7], S42, 0x432AFF97) # 50 \n        c = XX(I, c, d, a, b, inp[14], S43, 0xAB9423A7) # 51 \n        b = XX(I, b, c, d, a, inp[ 5], S44, 0xFC93A039) # 52 \n        a = XX(I, a, b, c, d, inp[12], S41, 0x655B59C3) # 53 \n        d = XX(I, d, a, b, c, inp[ 3], S42, 0x8F0CCC92) # 54 \n        c = XX(I, c, d, a, b, inp[10], S43, 0xFFEFF47D) # 55 \n        b = XX(I, b, c, d, a, inp[ 1], S44, 0x85845DD1) # 56 \n        a = XX(I, a, b, c, d, inp[ 8], S41, 0x6FA87E4F) # 57 \n        d = XX(I, d, a, b, c, inp[15], S42, 0xFE2CE6E0) # 58 \n        c = XX(I, c, d, a, b, inp[ 6], S43, 0xA3014314) # 59 \n        b = XX(I, b, c, d, a, inp[13], S44, 0x4E0811A1) # 60 \n        a = XX(I, a, b, c, d, inp[ 4], S41, 0xF7537E82) # 61 \n        d = XX(I, d, a, b, c, inp[11], S42, 0xBD3AF235) # 62 \n        c = XX(I, c, d, a, b, inp[ 2], S43, 0x2AD7D2BB) # 63 \n        b = XX(I, b, c, d, a, inp[ 9], S44, 0xEB86D391) # 64 \n\n        A = (A + a) & 0xffffffff\n        B = (B + b) & 0xffffffff\n        C = (C + c) & 0xffffffff\n        D = (D + d) & 0xffffffff\n\n        self.A, self.B, self.C, self.D = A, B, C, D\n\n\n    # Down from here all methods follow the Python Standard Library\n    # API of the md5 module.\n\n    def update(self, inBuf):\n        \"\"\"Add to the current message.\n\n        Update the md5 object with the string arg. Repeated calls\n        are equivalent to a single call with the concatenation of all\n        the arguments, i.e. m.update(a); m.update(b) is equivalent\n        to m.update(a+b).\n\n        The hash is immediately calculated for all full blocks. The final\n        calculation is made in digest(). This allows us to keep an\n        intermediate value for the hash, so that we only need to make\n        minimal recalculation if we call update() to add moredata to\n        the hashed string.\n        \"\"\"\n\n        leninBuf = len(inBuf)\n\n        # Compute number of bytes mod 64.\n        index = (self.count[0] >> 3) & 0x3F\n\n        # Update number of bits.\n        self.count[0] = self.count[0] + (leninBuf << 3)\n        if self.count[0] < (leninBuf << 3):\n            self.count[1] = self.count[1] + 1\n        self.count[1] = self.count[1] + (leninBuf >> 29)\n\n        partLen = 64 - index\n\n        if leninBuf >= partLen:\n            self.input[index:] = list(inBuf[:partLen])\n            self._transform(_bytelist2long(self.input))\n            i = partLen\n            while i + 63 < leninBuf:\n                self._transform(_bytelist2long(list(inBuf[i:i+64])))\n                i = i + 64\n            else:\n                self.input = list(inBuf[i:leninBuf])\n        else:\n            i = 0\n            self.input = self.input + list(inBuf)\n\n\n    def digest(self):\n        \"\"\"Terminate the message-digest computation and return digest.\n\n        Return the digest of the strings passed to the update()\n        method so far. This is a 16-byte string which may contain\n        non-ASCII characters, including null bytes.\n        \"\"\"\n\n        A = self.A\n        B = self.B\n        C = self.C\n        D = self.D\n        input = [] + self.input\n        count = [] + self.count\n\n        index = (self.count[0] >> 3) & 0x3f\n\n        if index < 56:\n            padLen = 56 - index\n        else:\n            padLen = 120 - index\n\n        padding = [b'\\200'] + [b'\\000'] * 63\n        self.update(padding[:padLen])\n\n        # Append length (before padding).\n        bits = _bytelist2long(self.input[:56]) + count\n\n        self._transform(bits)\n\n        # Store state in digest.\n        digest = struct.pack(\"<IIII\", self.A, self.B, self.C, self.D)\n\n        self.A = A \n        self.B = B\n        self.C = C\n        self.D = D\n        self.input = input \n        self.count = count \n\n        return digest\n\n\n    def hexdigest(self):\n        \"\"\"Terminate and return digest in HEX form.\n\n        Like digest() except the digest is returned as a string of\n        length 32, containing only hexadecimal digits. This may be\n        used to exchange the value safely in email or other non-\n        binary environments.\n        \"\"\"\n\n        return ''.join(['%02x' % ord(c) for c in self.digest()])\n\n    def copy(self):\n        \"\"\"Return a clone object.\n\n        Return a copy ('clone') of the md5 object. This can be used\n        to efficiently compute the digests of strings that share\n        a common initial substring.\n        \"\"\"\n        if 0: # set this to 1 to make the flow space crash\n            return copy.deepcopy(self)\n        clone = self.__class__()\n        clone.length = self.length\n        clone.count  = [] + self.count[:]\n        clone.input  = [] + self.input\n        clone.A = self.A\n        clone.B = self.B\n        clone.C = self.C\n        clone.D = self.D\n        return clone\n\n\n# ======================================================================\n# Mimic Python top-level functions from standard library API\n# for consistency with the _md5 module of the standard library.\n# ======================================================================\n\ndigest_size = 16\n\ndef new(arg=None):\n    \"\"\"Return a new md5 crypto object.\n    If arg is present, the method call update(arg) is made.\n    \"\"\"\n\n    crypto = MD5Type()\n    if arg:\n        crypto.update(arg)\n\n    return crypto\n\n", 
    "_scproxy": "\"\"\"Helper methods for urllib to fetch the proxy configuration settings using\nthe SystemConfiguration framework.\n\n\"\"\"\nimport sys\nif sys.platform != 'darwin':\n    raise ImportError('Requires Mac OS X')\n\nfrom ctypes import c_int32, c_int64, c_void_p, c_char_p, c_int, cdll\nfrom ctypes import pointer, create_string_buffer\nfrom ctypes.util import find_library\n\nkCFNumberSInt32Type = 3\nkCFStringEncodingUTF8 = 134217984\n\ndef _CFSetup():\n    sc = cdll.LoadLibrary(find_library(\"SystemConfiguration\"))\n    cf = cdll.LoadLibrary(find_library(\"CoreFoundation\"))\n    sctable = [\n        ('SCDynamicStoreCopyProxies', [c_void_p], c_void_p),\n    ]\n    cftable = [\n        ('CFArrayGetCount', [c_void_p], c_int64),\n        ('CFArrayGetValueAtIndex', [c_void_p, c_int64], c_void_p),\n        ('CFDictionaryGetValue', [c_void_p, c_void_p], c_void_p),\n        ('CFStringCreateWithCString', [c_void_p, c_char_p, c_int32], c_void_p),\n        ('CFStringGetLength', [c_void_p], c_int32),\n        ('CFStringGetCString', [c_void_p, c_char_p, c_int32, c_int32], c_int32),\n        ('CFNumberGetValue', [c_void_p, c_int, c_void_p], c_int32),\n        ('CFRelease', [c_void_p], None),\n    ]\n    scconst = [\n        'kSCPropNetProxiesExceptionsList',\n        'kSCPropNetProxiesExcludeSimpleHostnames',\n        'kSCPropNetProxiesHTTPEnable',\n        'kSCPropNetProxiesHTTPProxy',\n        'kSCPropNetProxiesHTTPPort',\n        'kSCPropNetProxiesHTTPSEnable',\n        'kSCPropNetProxiesHTTPSProxy',\n        'kSCPropNetProxiesHTTPSPort',\n        'kSCPropNetProxiesFTPEnable',\n        'kSCPropNetProxiesFTPProxy',\n        'kSCPropNetProxiesFTPPort',\n        'kSCPropNetProxiesGopherEnable',\n        'kSCPropNetProxiesGopherProxy',\n        'kSCPropNetProxiesGopherPort',\n    ]\n    class CFProxy(object):\n        def __init__(self):\n            for mod, table in [(sc, sctable), (cf, cftable)]:\n                for fname, argtypes, restype in table:\n                    func = getattr(mod, fname)\n                    func.argtypes = argtypes\n                    func.restype = restype\n                    setattr(self, fname, func)\n            for k in scconst:\n                v = None\n                try:\n                    v = c_void_p.in_dll(sc, k)\n                except ValueError:\n                    v = None\n                setattr(self, k, v)\n    return CFProxy()\nffi = _CFSetup()\n\ndef cfstring_to_pystring(value):\n    length = (ffi.CFStringGetLength(value) * 4) + 1\n    buff = create_string_buffer(length)\n    ffi.CFStringGetCString(value, buff, length * 4, kCFStringEncodingUTF8)\n    return unicode(buff.value, 'utf8')\n\ndef cfnum_to_int32(num):\n    result_ptr = pointer(c_int32(0))\n    ffi.CFNumberGetValue(num, kCFNumberSInt32Type, result_ptr)\n    return result_ptr[0]\n\ndef _get_proxy_settings():\n    result = {'exclude_simple': False}\n    cfdct = ffi.SCDynamicStoreCopyProxies(None)\n    if not cfdct:\n        return result\n    try:\n        k = ffi.kSCPropNetProxiesExcludeSimpleHostnames\n        if k:\n            cfnum = ffi.CFDictionaryGetValue(cfdct, k)\n            if cfnum:\n                result['exclude_simple'] = bool(cfnum_to_int32(cfnum))\n        k = ffi.kSCPropNetProxiesExceptionsList\n        if k:\n            cfarr = ffi.CFDictionaryGetValue(cfdct, k)\n            if cfarr:\n                lst = []\n                for i in range(ffi.CFArrayGetCount(cfarr)):\n                    cfstr = ffi.CFArrayGetValueAtIndex(cfarr, i)\n                    if cfstr:\n                        v = cfstring_to_pystring(cfstr)\n                    else:\n                        v = None\n                    lst.append(v)\n                result['exceptions'] = lst\n        return result\n    finally:\n        ffi.CFRelease(cfdct)\n\ndef _get_proxies():\n    result = {}\n    cfdct = ffi.SCDynamicStoreCopyProxies(None)\n    if not cfdct:\n        return result\n    try:\n        for proto in 'HTTP', 'HTTPS', 'FTP', 'Gopher':\n            enabled_key = getattr(ffi, 'kSCPropNetProxies' + proto + 'Enable')\n            proxy_key = getattr(ffi, 'kSCPropNetProxies' + proto + 'Proxy')\n            port_key = getattr(ffi, 'kSCPropNetProxies' + proto + 'Port')\n            cfnum = ffi.CFDictionaryGetValue(cfdct, enabled_key)\n            if cfnum and cfnum_to_int32(cfnum):\n                cfhoststr = ffi.CFDictionaryGetValue(cfdct, proxy_key)\n                cfportnum = ffi.CFDictionaryGetValue(cfdct, port_key)\n                if cfhoststr:\n                    host = cfstring_to_pystring(cfhoststr)\n                    if host:\n                        if cfportnum:\n                            port = cfnum_to_int32(cfportnum)\n                            v = u'http://%s:%d' % (host, port)\n                        else:\n                            v = u'http://%s' % (host,)\n                        result[proto.lower()] = v\n        return result\n    finally:\n        ffi.CFRelease(cfdct)\n", 
    "_sha": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note that PyPy contains also a built-in module 'sha' which will hide\n# this one if compiled in.\n\n\"\"\"A sample implementation of SHA-1 in pure Python.\n\n   Framework adapted from Dinu Gherman's MD5 implementation by\n   J. Hall\u00e9n and L. Creighton. SHA-1 implementation based directly on\n   the text of the NIST standard FIPS PUB 180-1.\n\"\"\"\n\n\n__date__    = '2004-11-17'\n__version__ = 0.91 # Modernised by J. Hall\u00e9n and L. Creighton for Pypy\n\n\nimport struct, copy\n\n\n# ======================================================================\n# Bit-Manipulation helpers\n#\n#   _long2bytes() was contributed by Barry Warsaw\n#   and is reused here with tiny modifications.\n# ======================================================================\n\ndef _long2bytesBigEndian(n, blocksize=0):\n    \"\"\"Convert a long integer to a byte string.\n\n    If optional blocksize is given and greater than zero, pad the front\n    of the byte string with binary zeros so that the length is a multiple\n    of blocksize.\n    \"\"\"\n\n    # After much testing, this algorithm was deemed to be the fastest.\n    s = b''\n    pack = struct.pack\n    while n > 0:\n        s = pack('>I', n & 0xffffffff) + s\n        n = n >> 32\n\n    # Strip off leading zeros.\n    for i in range(len(s)):\n        if s[i] != '\\000':\n            break\n    else:\n        # Only happens when n == 0.\n        s = '\\000'\n        i = 0\n\n    s = s[i:]\n\n    # Add back some pad bytes. This could be done more efficiently\n    # w.r.t. the de-padding being done above, but sigh...\n    if blocksize > 0 and len(s) % blocksize:\n        s = (blocksize - len(s) % blocksize) * '\\000' + s\n\n    return s\n\n\ndef _bytelist2longBigEndian(list):\n    \"Transform a list of characters into a list of longs.\"\n\n    imax = len(list) // 4\n    hl = [0] * imax\n\n    j = 0\n    i = 0\n    while i < imax:\n        b0 = ord(list[j]) << 24\n        b1 = ord(list[j+1]) << 16\n        b2 = ord(list[j+2]) << 8\n        b3 = ord(list[j+3])\n        hl[i] = b0 | b1 | b2 | b3\n        i = i+1\n        j = j+4\n\n    return hl\n\n\ndef _rotateLeft(x, n):\n    \"Rotate x (32 bit) left n bits circularly.\"\n\n    return (x << n) | (x >> (32-n))\n\n\n# ======================================================================\n# The SHA transformation functions\n#\n# ======================================================================\n\ndef f0_19(B, C, D):\n    return (B & C) | ((~ B) & D)\n\ndef f20_39(B, C, D):\n    return B ^ C ^ D\n\ndef f40_59(B, C, D):\n    return (B & C) | (B & D) | (C & D)\n\ndef f60_79(B, C, D):\n    return B ^ C ^ D\n\n\nf = [f0_19, f20_39, f40_59, f60_79]\n\n# Constants to be used\nK = [\n    0x5A827999, # ( 0 <= t <= 19)\n    0x6ED9EBA1, # (20 <= t <= 39)\n    0x8F1BBCDC, # (40 <= t <= 59)\n    0xCA62C1D6  # (60 <= t <= 79)\n    ]\n\nclass sha:\n    \"An implementation of the SHA hash function in pure Python.\"\n\n    digest_size = digestsize = 20\n    block_size = 512 // 8\n\n    def __init__(self):\n        \"Initialisation.\"\n\n        # Initial message length in bits(!).\n        self.length = 0\n        self.count = [0, 0]\n\n        # Initial empty message as a sequence of bytes (8 bit characters).\n        self.input = []\n\n        # Call a separate init function, that can be used repeatedly\n        # to start from scratch on the same object.\n        self.init()\n\n\n    def init(self):\n        \"Initialize the message-digest and set all fields to zero.\"\n\n        self.length = 0\n        self.input = []\n\n        # Initial 160 bit message digest (5 times 32 bit).\n        self.H0 = 0x67452301\n        self.H1 = 0xEFCDAB89\n        self.H2 = 0x98BADCFE\n        self.H3 = 0x10325476\n        self.H4 = 0xC3D2E1F0\n\n    def _transform(self, W):\n\n        for t in range(16, 80):\n            W.append(_rotateLeft(\n                W[t-3] ^ W[t-8] ^ W[t-14] ^ W[t-16], 1) & 0xffffffff)\n\n        A = self.H0\n        B = self.H1\n        C = self.H2\n        D = self.H3\n        E = self.H4\n\n        \"\"\"\n        This loop was unrolled to gain about 10% in speed\n        for t in range(0, 80):\n            TEMP = _rotateLeft(A, 5) + f[t/20] + E + W[t] + K[t/20]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n        \"\"\"\n\n        for t in range(0, 20):\n            TEMP = _rotateLeft(A, 5) + ((B & C) | ((~ B) & D)) + E + W[t] + K[0]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(20, 40):\n            TEMP = _rotateLeft(A, 5) + (B ^ C ^ D) + E + W[t] + K[1]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(40, 60):\n            TEMP = _rotateLeft(A, 5) + ((B & C) | (B & D) | (C & D)) + E + W[t] + K[2]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(60, 80):\n            TEMP = _rotateLeft(A, 5) + (B ^ C ^ D)  + E + W[t] + K[3]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n\n        self.H0 = (self.H0 + A) & 0xffffffff\n        self.H1 = (self.H1 + B) & 0xffffffff\n        self.H2 = (self.H2 + C) & 0xffffffff\n        self.H3 = (self.H3 + D) & 0xffffffff\n        self.H4 = (self.H4 + E) & 0xffffffff\n\n\n    # Down from here all methods follow the Python Standard Library\n    # API of the sha module.\n\n    def update(self, inBuf):\n        \"\"\"Add to the current message.\n\n        Update the md5 object with the string arg. Repeated calls\n        are equivalent to a single call with the concatenation of all\n        the arguments, i.e. m.update(a); m.update(b) is equivalent\n        to m.update(a+b).\n\n        The hash is immediately calculated for all full blocks. The final\n        calculation is made in digest(). It will calculate 1-2 blocks,\n        depending on how much padding we have to add. This allows us to\n        keep an intermediate value for the hash, so that we only need to\n        make minimal recalculation if we call update() to add more data\n        to the hashed string.\n        \"\"\"\n\n        leninBuf = len(inBuf)\n\n        # Compute number of bytes mod 64.\n        index = (self.count[1] >> 3) & 0x3F\n\n        # Update number of bits.\n        self.count[1] = self.count[1] + (leninBuf << 3)\n        if self.count[1] < (leninBuf << 3):\n            self.count[0] = self.count[0] + 1\n        self.count[0] = self.count[0] + (leninBuf >> 29)\n\n        partLen = 64 - index\n\n        if leninBuf >= partLen:\n            self.input[index:] = list(inBuf[:partLen])\n            self._transform(_bytelist2longBigEndian(self.input))\n            i = partLen\n            while i + 63 < leninBuf:\n                self._transform(_bytelist2longBigEndian(list(inBuf[i:i+64])))\n                i = i + 64\n            else:\n                self.input = list(inBuf[i:leninBuf])\n        else:\n            i = 0\n            self.input = self.input + list(inBuf)\n\n\n    def digest(self):\n        \"\"\"Terminate the message-digest computation and return digest.\n\n        Return the digest of the strings passed to the update()\n        method so far. This is a 16-byte string which may contain\n        non-ASCII characters, including null bytes.\n        \"\"\"\n\n        H0 = self.H0\n        H1 = self.H1\n        H2 = self.H2\n        H3 = self.H3\n        H4 = self.H4\n        input = [] + self.input\n        count = [] + self.count\n\n        index = (self.count[1] >> 3) & 0x3f\n\n        if index < 56:\n            padLen = 56 - index\n        else:\n            padLen = 120 - index\n\n        padding = ['\\200'] + ['\\000'] * 63\n        self.update(padding[:padLen])\n\n        # Append length (before padding).\n        bits = _bytelist2longBigEndian(self.input[:56]) + count\n\n        self._transform(bits)\n\n        # Store state in digest.\n        digest = _long2bytesBigEndian(self.H0, 4) + \\\n                 _long2bytesBigEndian(self.H1, 4) + \\\n                 _long2bytesBigEndian(self.H2, 4) + \\\n                 _long2bytesBigEndian(self.H3, 4) + \\\n                 _long2bytesBigEndian(self.H4, 4)\n\n        self.H0 = H0\n        self.H1 = H1\n        self.H2 = H2\n        self.H3 = H3\n        self.H4 = H4\n        self.input = input\n        self.count = count\n\n        return digest\n\n\n    def hexdigest(self):\n        \"\"\"Terminate and return digest in HEX form.\n\n        Like digest() except the digest is returned as a string of\n        length 32, containing only hexadecimal digits. This may be\n        used to exchange the value safely in email or other non-\n        binary environments.\n        \"\"\"\n        return ''.join(['%02x' % ord(c) for c in self.digest()])\n\n    def copy(self):\n        \"\"\"Return a clone object.\n\n        Return a copy ('clone') of the md5 object. This can be used\n        to efficiently compute the digests of strings that share\n        a common initial substring.\n        \"\"\"\n\n        return copy.deepcopy(self)\n\n\n# ======================================================================\n# Mimic Python top-level functions from standard library API\n# for consistency with the _sha module of the standard library.\n# ======================================================================\n\n# These are mandatory variables in the module. They have constant values\n# in the SHA standard.\n\ndigest_size = 20\ndigestsize = 20\nblocksize = 1\n\ndef new(arg=None):\n    \"\"\"Return a new sha crypto object.\n\n    If arg is present, the method call update(arg) is made.\n    \"\"\"\n\n    crypto = sha()\n    if arg:\n        crypto.update(arg)\n\n    return crypto\n", 
    "_sha256": "import struct\n\nSHA_BLOCKSIZE = 64\nSHA_DIGESTSIZE = 32\n\n\ndef new_shaobject():\n    return {\n        'digest': [0]*8,\n        'count_lo': 0,\n        'count_hi': 0,\n        'data': [0]* SHA_BLOCKSIZE,\n        'local': 0,\n        'digestsize': 0\n    }\n\nROR = lambda x, y: (((x & 0xffffffff) >> (y & 31)) | (x << (32 - (y & 31)))) & 0xffffffff\nCh = lambda x, y, z: (z ^ (x & (y ^ z)))\nMaj = lambda x, y, z: (((x | y) & z) | (x & y))\nS = lambda x, n: ROR(x, n)\nR = lambda x, n: (x & 0xffffffff) >> n\nSigma0 = lambda x: (S(x, 2) ^ S(x, 13) ^ S(x, 22))\nSigma1 = lambda x: (S(x, 6) ^ S(x, 11) ^ S(x, 25))\nGamma0 = lambda x: (S(x, 7) ^ S(x, 18) ^ R(x, 3))\nGamma1 = lambda x: (S(x, 17) ^ S(x, 19) ^ R(x, 10))\n\ndef sha_transform(sha_info):\n    W = []\n    \n    d = sha_info['data']\n    for i in xrange(0,16):\n        W.append( (d[4*i]<<24) + (d[4*i+1]<<16) + (d[4*i+2]<<8) + d[4*i+3])\n    \n    for i in xrange(16,64):\n        W.append( (Gamma1(W[i - 2]) + W[i - 7] + Gamma0(W[i - 15]) + W[i - 16]) & 0xffffffff )\n    \n    ss = sha_info['digest'][:]\n    \n    def RND(a,b,c,d,e,f,g,h,i,ki):\n        t0 = h + Sigma1(e) + Ch(e, f, g) + ki + W[i];\n        t1 = Sigma0(a) + Maj(a, b, c);\n        d += t0;\n        h  = t0 + t1;\n        return d & 0xffffffff, h & 0xffffffff\n    \n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],0,0x428a2f98);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],1,0x71374491);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],2,0xb5c0fbcf);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],3,0xe9b5dba5);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],4,0x3956c25b);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],5,0x59f111f1);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],6,0x923f82a4);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],7,0xab1c5ed5);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],8,0xd807aa98);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],9,0x12835b01);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],10,0x243185be);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],11,0x550c7dc3);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],12,0x72be5d74);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],13,0x80deb1fe);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],14,0x9bdc06a7);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],15,0xc19bf174);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],16,0xe49b69c1);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],17,0xefbe4786);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],18,0x0fc19dc6);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],19,0x240ca1cc);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],20,0x2de92c6f);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],21,0x4a7484aa);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],22,0x5cb0a9dc);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],23,0x76f988da);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],24,0x983e5152);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],25,0xa831c66d);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],26,0xb00327c8);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],27,0xbf597fc7);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],28,0xc6e00bf3);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],29,0xd5a79147);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],30,0x06ca6351);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],31,0x14292967);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],32,0x27b70a85);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],33,0x2e1b2138);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],34,0x4d2c6dfc);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],35,0x53380d13);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],36,0x650a7354);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],37,0x766a0abb);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],38,0x81c2c92e);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],39,0x92722c85);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],40,0xa2bfe8a1);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],41,0xa81a664b);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],42,0xc24b8b70);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],43,0xc76c51a3);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],44,0xd192e819);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],45,0xd6990624);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],46,0xf40e3585);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],47,0x106aa070);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],48,0x19a4c116);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],49,0x1e376c08);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],50,0x2748774c);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],51,0x34b0bcb5);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],52,0x391c0cb3);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],53,0x4ed8aa4a);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],54,0x5b9cca4f);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],55,0x682e6ff3);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],56,0x748f82ee);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],57,0x78a5636f);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],58,0x84c87814);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],59,0x8cc70208);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],60,0x90befffa);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],61,0xa4506ceb);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],62,0xbef9a3f7);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],63,0xc67178f2);\n    \n    dig = []\n    for i, x in enumerate(sha_info['digest']):\n        dig.append( (x + ss[i]) & 0xffffffff )\n    sha_info['digest'] = dig\n\ndef sha_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [0x6A09E667, 0xBB67AE85, 0x3C6EF372, 0xA54FF53A, 0x510E527F, 0x9B05688C, 0x1F83D9AB, 0x5BE0CD19]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 32\n    return sha_info\n\ndef sha224_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [0xc1059ed8, 0x367cd507, 0x3070dd17, 0xf70e5939, 0xffc00b31, 0x68581511, 0x64f98fa7, 0xbefa4fa4]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 28\n    return sha_info\n\ndef getbuf(s):\n    if isinstance(s, str):\n        return s\n    elif isinstance(s, unicode):\n        return str(s)\n    else:\n        return buffer(s)\n\ndef sha_update(sha_info, buffer):\n    count = len(buffer)\n    buffer_idx = 0\n    clo = (sha_info['count_lo'] + (count << 3)) & 0xffffffff\n    if clo < sha_info['count_lo']:\n        sha_info['count_hi'] += 1\n    sha_info['count_lo'] = clo\n    \n    sha_info['count_hi'] += (count >> 29)\n    \n    if sha_info['local']:\n        i = SHA_BLOCKSIZE - sha_info['local']\n        if i > count:\n            i = count\n        \n        # copy buffer\n        for x in enumerate(buffer[buffer_idx:buffer_idx+i]):\n            sha_info['data'][sha_info['local']+x[0]] = struct.unpack('B', x[1])[0]\n        \n        count -= i\n        buffer_idx += i\n        \n        sha_info['local'] += i\n        if sha_info['local'] == SHA_BLOCKSIZE:\n            sha_transform(sha_info)\n            sha_info['local'] = 0\n        else:\n            return\n    \n    while count >= SHA_BLOCKSIZE:\n        # copy buffer\n        sha_info['data'] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + SHA_BLOCKSIZE]]\n        count -= SHA_BLOCKSIZE\n        buffer_idx += SHA_BLOCKSIZE\n        sha_transform(sha_info)\n        \n    \n    # copy buffer\n    pos = sha_info['local']\n    sha_info['data'][pos:pos+count] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + count]]\n    sha_info['local'] = count\n\ndef sha_final(sha_info):\n    lo_bit_count = sha_info['count_lo']\n    hi_bit_count = sha_info['count_hi']\n    count = (lo_bit_count >> 3) & 0x3f\n    sha_info['data'][count] = 0x80;\n    count += 1\n    if count > SHA_BLOCKSIZE - 8:\n        # zero the bytes in data after the count\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n        sha_transform(sha_info)\n        # zero bytes in data\n        sha_info['data'] = [0] * SHA_BLOCKSIZE\n    else:\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n    \n    sha_info['data'][56] = (hi_bit_count >> 24) & 0xff\n    sha_info['data'][57] = (hi_bit_count >> 16) & 0xff\n    sha_info['data'][58] = (hi_bit_count >>  8) & 0xff\n    sha_info['data'][59] = (hi_bit_count >>  0) & 0xff\n    sha_info['data'][60] = (lo_bit_count >> 24) & 0xff\n    sha_info['data'][61] = (lo_bit_count >> 16) & 0xff\n    sha_info['data'][62] = (lo_bit_count >>  8) & 0xff\n    sha_info['data'][63] = (lo_bit_count >>  0) & 0xff\n    \n    sha_transform(sha_info)\n    \n    dig = []\n    for i in sha_info['digest']:\n        dig.extend([ ((i>>24) & 0xff), ((i>>16) & 0xff), ((i>>8) & 0xff), (i & 0xff) ])\n    return ''.join([chr(i) for i in dig])\n\nclass sha256(object):\n    digest_size = digestsize = SHA_DIGESTSIZE\n    block_size = SHA_BLOCKSIZE\n\n    def __init__(self, s=None):\n        self._sha = sha_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n    \n    def update(self, s):\n        sha_update(self._sha, getbuf(s))\n    \n    def digest(self):\n        return sha_final(self._sha.copy())[:self._sha['digestsize']]\n    \n    def hexdigest(self):\n        return ''.join(['%.2x' % ord(i) for i in self.digest()])\n\n    def copy(self):\n        new = sha256.__new__(sha256)\n        new._sha = self._sha.copy()\n        return new\n\nclass sha224(sha256):\n    digest_size = digestsize = 28\n\n    def __init__(self, s=None):\n        self._sha = sha224_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def copy(self):\n        new = sha224.__new__(sha224)\n        new._sha = self._sha.copy()\n        return new\n\ndef test():\n    a_str = \"just a test string\"\n    \n    assert 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' == sha256().hexdigest()\n    assert 'd7b553c6f09ac85d142415f857c5310f3bbbe7cdd787cce4b985acedd585266f' == sha256(a_str).hexdigest()\n    assert '8113ebf33c97daa9998762aacafe750c7cefc2b2f173c90c59663a57fe626f21' == sha256(a_str*7).hexdigest()\n    \n    s = sha256(a_str)\n    s.update(a_str)\n    assert '03d9963e05a094593190b6fc794cb1a3e1ac7d7883f0b5855268afeccc70d461' == s.hexdigest()\n\nif __name__ == \"__main__\":\n    test()\n\n\n", 
    "_sha512": "\"\"\"\nThis code was Ported from CPython's sha512module.c\n\"\"\"\n\nimport struct\n\nSHA_BLOCKSIZE = 128\nSHA_DIGESTSIZE = 64\n\n\ndef new_shaobject():\n    return {\n        'digest': [0]*8,\n        'count_lo': 0,\n        'count_hi': 0,\n        'data': [0]* SHA_BLOCKSIZE,\n        'local': 0,\n        'digestsize': 0\n    }\n\nROR64 = lambda x, y: (((x & 0xffffffffffffffff) >> (y & 63)) | (x << (64 - (y & 63)))) & 0xffffffffffffffff\nCh = lambda x, y, z: (z ^ (x & (y ^ z)))\nMaj = lambda x, y, z: (((x | y) & z) | (x & y))\nS = lambda x, n: ROR64(x, n)\nR = lambda x, n: (x & 0xffffffffffffffff) >> n\nSigma0 = lambda x: (S(x, 28) ^ S(x, 34) ^ S(x, 39))\nSigma1 = lambda x: (S(x, 14) ^ S(x, 18) ^ S(x, 41))\nGamma0 = lambda x: (S(x, 1) ^ S(x, 8) ^ R(x, 7))\nGamma1 = lambda x: (S(x, 19) ^ S(x, 61) ^ R(x, 6))\n\ndef sha_transform(sha_info):\n    W = []\n\n    d = sha_info['data']\n    for i in xrange(0,16):\n        W.append( (d[8*i]<<56) + (d[8*i+1]<<48) + (d[8*i+2]<<40) + (d[8*i+3]<<32) + (d[8*i+4]<<24) + (d[8*i+5]<<16) + (d[8*i+6]<<8) + d[8*i+7])\n\n    for i in xrange(16,80):\n        W.append( (Gamma1(W[i - 2]) + W[i - 7] + Gamma0(W[i - 15]) + W[i - 16]) & 0xffffffffffffffff )\n\n    ss = sha_info['digest'][:]\n\n    def RND(a,b,c,d,e,f,g,h,i,ki):\n        t0 = (h + Sigma1(e) + Ch(e, f, g) + ki + W[i]) & 0xffffffffffffffff\n        t1 = (Sigma0(a) + Maj(a, b, c)) & 0xffffffffffffffff\n        d = (d + t0) & 0xffffffffffffffff\n        h = (t0 + t1) & 0xffffffffffffffff\n        return d & 0xffffffffffffffff, h & 0xffffffffffffffff\n\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],0,0x428a2f98d728ae22)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],1,0x7137449123ef65cd)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],2,0xb5c0fbcfec4d3b2f)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],3,0xe9b5dba58189dbbc)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],4,0x3956c25bf348b538)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],5,0x59f111f1b605d019)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],6,0x923f82a4af194f9b)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],7,0xab1c5ed5da6d8118)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],8,0xd807aa98a3030242)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],9,0x12835b0145706fbe)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],10,0x243185be4ee4b28c)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],11,0x550c7dc3d5ffb4e2)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],12,0x72be5d74f27b896f)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],13,0x80deb1fe3b1696b1)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],14,0x9bdc06a725c71235)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],15,0xc19bf174cf692694)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],16,0xe49b69c19ef14ad2)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],17,0xefbe4786384f25e3)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],18,0x0fc19dc68b8cd5b5)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],19,0x240ca1cc77ac9c65)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],20,0x2de92c6f592b0275)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],21,0x4a7484aa6ea6e483)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],22,0x5cb0a9dcbd41fbd4)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],23,0x76f988da831153b5)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],24,0x983e5152ee66dfab)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],25,0xa831c66d2db43210)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],26,0xb00327c898fb213f)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],27,0xbf597fc7beef0ee4)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],28,0xc6e00bf33da88fc2)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],29,0xd5a79147930aa725)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],30,0x06ca6351e003826f)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],31,0x142929670a0e6e70)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],32,0x27b70a8546d22ffc)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],33,0x2e1b21385c26c926)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],34,0x4d2c6dfc5ac42aed)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],35,0x53380d139d95b3df)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],36,0x650a73548baf63de)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],37,0x766a0abb3c77b2a8)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],38,0x81c2c92e47edaee6)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],39,0x92722c851482353b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],40,0xa2bfe8a14cf10364)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],41,0xa81a664bbc423001)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],42,0xc24b8b70d0f89791)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],43,0xc76c51a30654be30)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],44,0xd192e819d6ef5218)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],45,0xd69906245565a910)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],46,0xf40e35855771202a)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],47,0x106aa07032bbd1b8)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],48,0x19a4c116b8d2d0c8)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],49,0x1e376c085141ab53)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],50,0x2748774cdf8eeb99)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],51,0x34b0bcb5e19b48a8)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],52,0x391c0cb3c5c95a63)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],53,0x4ed8aa4ae3418acb)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],54,0x5b9cca4f7763e373)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],55,0x682e6ff3d6b2b8a3)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],56,0x748f82ee5defb2fc)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],57,0x78a5636f43172f60)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],58,0x84c87814a1f0ab72)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],59,0x8cc702081a6439ec)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],60,0x90befffa23631e28)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],61,0xa4506cebde82bde9)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],62,0xbef9a3f7b2c67915)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],63,0xc67178f2e372532b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],64,0xca273eceea26619c)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],65,0xd186b8c721c0c207)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],66,0xeada7dd6cde0eb1e)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],67,0xf57d4f7fee6ed178)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],68,0x06f067aa72176fba)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],69,0x0a637dc5a2c898a6)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],70,0x113f9804bef90dae)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],71,0x1b710b35131c471b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],72,0x28db77f523047d84)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],73,0x32caab7b40c72493)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],74,0x3c9ebe0a15c9bebc)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],75,0x431d67c49c100d4c)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],76,0x4cc5d4becb3e42b6)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],77,0x597f299cfc657e2a)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],78,0x5fcb6fab3ad6faec)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],79,0x6c44198c4a475817)\n\n    dig = []\n    for i, x in enumerate(sha_info['digest']):\n        dig.append( (x + ss[i]) & 0xffffffffffffffff )\n    sha_info['digest'] = dig\n\ndef sha_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [ 0x6a09e667f3bcc908, 0xbb67ae8584caa73b, 0x3c6ef372fe94f82b, 0xa54ff53a5f1d36f1, 0x510e527fade682d1, 0x9b05688c2b3e6c1f, 0x1f83d9abfb41bd6b, 0x5be0cd19137e2179]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 64\n    return sha_info\n\ndef sha384_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [ 0xcbbb9d5dc1059ed8, 0x629a292a367cd507, 0x9159015a3070dd17, 0x152fecd8f70e5939, 0x67332667ffc00b31, 0x8eb44a8768581511, 0xdb0c2e0d64f98fa7, 0x47b5481dbefa4fa4]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 48\n    return sha_info\n\ndef getbuf(s):\n    if isinstance(s, str):\n        return s\n    elif isinstance(s, unicode):\n        return str(s)\n    else:\n        return buffer(s)\n\ndef sha_update(sha_info, buffer):\n    count = len(buffer)\n    buffer_idx = 0\n    clo = (sha_info['count_lo'] + (count << 3)) & 0xffffffff\n    if clo < sha_info['count_lo']:\n        sha_info['count_hi'] += 1\n    sha_info['count_lo'] = clo\n\n    sha_info['count_hi'] += (count >> 29)\n\n    if sha_info['local']:\n        i = SHA_BLOCKSIZE - sha_info['local']\n        if i > count:\n            i = count\n\n        # copy buffer\n        for x in enumerate(buffer[buffer_idx:buffer_idx+i]):\n            sha_info['data'][sha_info['local']+x[0]] = struct.unpack('B', x[1])[0]\n\n        count -= i\n        buffer_idx += i\n\n        sha_info['local'] += i\n        if sha_info['local'] == SHA_BLOCKSIZE:\n            sha_transform(sha_info)\n            sha_info['local'] = 0\n        else:\n            return\n\n    while count >= SHA_BLOCKSIZE:\n        # copy buffer\n        sha_info['data'] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + SHA_BLOCKSIZE]]\n        count -= SHA_BLOCKSIZE\n        buffer_idx += SHA_BLOCKSIZE\n        sha_transform(sha_info)\n\n    # copy buffer\n    pos = sha_info['local']\n    sha_info['data'][pos:pos+count] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + count]]\n    sha_info['local'] = count\n\ndef sha_final(sha_info):\n    lo_bit_count = sha_info['count_lo']\n    hi_bit_count = sha_info['count_hi']\n    count = (lo_bit_count >> 3) & 0x7f\n    sha_info['data'][count] = 0x80;\n    count += 1\n    if count > SHA_BLOCKSIZE - 16:\n        # zero the bytes in data after the count\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n        sha_transform(sha_info)\n        # zero bytes in data\n        sha_info['data'] = [0] * SHA_BLOCKSIZE\n    else:\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n\n    sha_info['data'][112] = 0;\n    sha_info['data'][113] = 0;\n    sha_info['data'][114] = 0;\n    sha_info['data'][115] = 0;\n    sha_info['data'][116] = 0;\n    sha_info['data'][117] = 0;\n    sha_info['data'][118] = 0;\n    sha_info['data'][119] = 0;\n\n    sha_info['data'][120] = (hi_bit_count >> 24) & 0xff\n    sha_info['data'][121] = (hi_bit_count >> 16) & 0xff\n    sha_info['data'][122] = (hi_bit_count >>  8) & 0xff\n    sha_info['data'][123] = (hi_bit_count >>  0) & 0xff\n    sha_info['data'][124] = (lo_bit_count >> 24) & 0xff\n    sha_info['data'][125] = (lo_bit_count >> 16) & 0xff\n    sha_info['data'][126] = (lo_bit_count >>  8) & 0xff\n    sha_info['data'][127] = (lo_bit_count >>  0) & 0xff\n\n    sha_transform(sha_info)\n\n    dig = []\n    for i in sha_info['digest']:\n        dig.extend([ ((i>>56) & 0xff), ((i>>48) & 0xff), ((i>>40) & 0xff), ((i>>32) & 0xff), ((i>>24) & 0xff), ((i>>16) & 0xff), ((i>>8) & 0xff), (i & 0xff) ])\n    return ''.join([chr(i) for i in dig])\n\nclass sha512(object):\n    digest_size = digestsize = SHA_DIGESTSIZE\n    block_size = SHA_BLOCKSIZE\n\n    def __init__(self, s=None):\n        self._sha = sha_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def update(self, s):\n        sha_update(self._sha, getbuf(s))\n\n    def digest(self):\n        return sha_final(self._sha.copy())[:self._sha['digestsize']]\n\n    def hexdigest(self):\n        return ''.join(['%.2x' % ord(i) for i in self.digest()])\n\n    def copy(self):\n        new = sha512.__new__(sha512)\n        new._sha = self._sha.copy()\n        return new\n\nclass sha384(sha512):\n    digest_size = digestsize = 48\n\n    def __init__(self, s=None):\n        self._sha = sha384_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def copy(self):\n        new = sha384.__new__(sha384)\n        new._sha = self._sha.copy()\n        return new\n\ndef test():\n    import _sha512\n\n    a_str = \"just a test string\"\n\n    assert _sha512.sha512().hexdigest() == sha512().hexdigest()\n    assert _sha512.sha512(a_str).hexdigest() == sha512(a_str).hexdigest()\n    assert _sha512.sha512(a_str*7).hexdigest() == sha512(a_str*7).hexdigest()\n\n    s = sha512(a_str)\n    s.update(a_str)\n    assert _sha512.sha512(a_str+a_str).hexdigest() == s.hexdigest()\n\nif __name__ == \"__main__\":\n    test()\n", 
    "_strptime": "\"\"\"Strptime-related classes and functions.\n\nCLASSES:\n    LocaleTime -- Discovers and stores locale-specific time information\n    TimeRE -- Creates regexes for pattern matching a string of text containing\n                time information\n\nFUNCTIONS:\n    _getlang -- Figure out what language is being used for the locale\n    strptime -- Calculates the time struct represented by the passed-in string\n\n\"\"\"\nimport time\nimport locale\nimport calendar\nfrom re import compile as re_compile\nfrom re import IGNORECASE\nfrom re import escape as re_escape\nfrom datetime import date as datetime_date\ntry:\n    from thread import allocate_lock as _thread_allocate_lock\nexcept:\n    from dummy_thread import allocate_lock as _thread_allocate_lock\n\n__all__ = []\n\ndef _getlang():\n    # Figure out what the current language is set to.\n    return locale.getlocale(locale.LC_TIME)\n\nclass LocaleTime(object):\n    \"\"\"Stores and handles locale-specific information related to time.\n\n    ATTRIBUTES:\n        f_weekday -- full weekday names (7-item list)\n        a_weekday -- abbreviated weekday names (7-item list)\n        f_month -- full month names (13-item list; dummy value in [0], which\n                    is added by code)\n        a_month -- abbreviated month names (13-item list, dummy value in\n                    [0], which is added by code)\n        am_pm -- AM/PM representation (2-item list)\n        LC_date_time -- format string for date/time representation (string)\n        LC_date -- format string for date representation (string)\n        LC_time -- format string for time representation (string)\n        timezone -- daylight- and non-daylight-savings timezone representation\n                    (2-item list of sets)\n        lang -- Language used by instance (2-item tuple)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Set all attributes.\n\n        Order of methods called matters for dependency reasons.\n\n        The locale language is set at the offset and then checked again before\n        exiting.  This is to make sure that the attributes were not set with a\n        mix of information from more than one locale.  This would most likely\n        happen when using threads where one thread calls a locale-dependent\n        function while another thread changes the locale while the function in\n        the other thread is still running.  Proper coding would call for\n        locks to prevent changing the locale while locale-dependent code is\n        running.  The check here is done in case someone does not think about\n        doing this.\n\n        Only other possible issue is if someone changed the timezone and did\n        not call tz.tzset .  That is an issue for the programmer, though,\n        since changing the timezone is worthless without that call.\n\n        \"\"\"\n        self.lang = _getlang()\n        self.__calc_weekday()\n        self.__calc_month()\n        self.__calc_am_pm()\n        self.__calc_timezone()\n        self.__calc_date_time()\n        if _getlang() != self.lang:\n            raise ValueError(\"locale changed during initialization\")\n\n    def __pad(self, seq, front):\n        # Add '' to seq to either the front (is True), else the back.\n        seq = list(seq)\n        if front:\n            seq.insert(0, '')\n        else:\n            seq.append('')\n        return seq\n\n    def __calc_weekday(self):\n        # Set self.a_weekday and self.f_weekday using the calendar\n        # module.\n        a_weekday = [calendar.day_abbr[i].lower() for i in range(7)]\n        f_weekday = [calendar.day_name[i].lower() for i in range(7)]\n        self.a_weekday = a_weekday\n        self.f_weekday = f_weekday\n\n    def __calc_month(self):\n        # Set self.f_month and self.a_month using the calendar module.\n        a_month = [calendar.month_abbr[i].lower() for i in range(13)]\n        f_month = [calendar.month_name[i].lower() for i in range(13)]\n        self.a_month = a_month\n        self.f_month = f_month\n\n    def __calc_am_pm(self):\n        # Set self.am_pm by using time.strftime().\n\n        # The magic date (1999,3,17,hour,44,55,2,76,0) is not really that\n        # magical; just happened to have used it everywhere else where a\n        # static date was needed.\n        am_pm = []\n        for hour in (01,22):\n            time_tuple = time.struct_time((1999,3,17,hour,44,55,2,76,0))\n            am_pm.append(time.strftime(\"%p\", time_tuple).lower())\n        self.am_pm = am_pm\n\n    def __calc_date_time(self):\n        # Set self.date_time, self.date, & self.time by using\n        # time.strftime().\n\n        # Use (1999,3,17,22,44,55,2,76,0) for magic date because the amount of\n        # overloaded numbers is minimized.  The order in which searches for\n        # values within the format string is very important; it eliminates\n        # possible ambiguity for what something represents.\n        time_tuple = time.struct_time((1999,3,17,22,44,55,2,76,0))\n        date_time = [None, None, None]\n        date_time[0] = time.strftime(\"%c\", time_tuple).lower()\n        date_time[1] = time.strftime(\"%x\", time_tuple).lower()\n        date_time[2] = time.strftime(\"%X\", time_tuple).lower()\n        replacement_pairs = [('%', '%%'), (self.f_weekday[2], '%A'),\n                    (self.f_month[3], '%B'), (self.a_weekday[2], '%a'),\n                    (self.a_month[3], '%b'), (self.am_pm[1], '%p'),\n                    ('1999', '%Y'), ('99', '%y'), ('22', '%H'),\n                    ('44', '%M'), ('55', '%S'), ('76', '%j'),\n                    ('17', '%d'), ('03', '%m'), ('3', '%m'),\n                    # '3' needed for when no leading zero.\n                    ('2', '%w'), ('10', '%I')]\n        replacement_pairs.extend([(tz, \"%Z\") for tz_values in self.timezone\n                                                for tz in tz_values])\n        for offset,directive in ((0,'%c'), (1,'%x'), (2,'%X')):\n            current_format = date_time[offset]\n            for old, new in replacement_pairs:\n                # Must deal with possible lack of locale info\n                # manifesting itself as the empty string (e.g., Swedish's\n                # lack of AM/PM info) or a platform returning a tuple of empty\n                # strings (e.g., MacOS 9 having timezone as ('','')).\n                if old:\n                    current_format = current_format.replace(old, new)\n            # If %W is used, then Sunday, 2005-01-03 will fall on week 0 since\n            # 2005-01-03 occurs before the first Monday of the year.  Otherwise\n            # %U is used.\n            time_tuple = time.struct_time((1999,1,3,1,1,1,6,3,0))\n            if '00' in time.strftime(directive, time_tuple):\n                U_W = '%W'\n            else:\n                U_W = '%U'\n            date_time[offset] = current_format.replace('11', U_W)\n        self.LC_date_time = date_time[0]\n        self.LC_date = date_time[1]\n        self.LC_time = date_time[2]\n\n    def __calc_timezone(self):\n        # Set self.timezone by using time.tzname.\n        # Do not worry about possibility of time.tzname[0] == timetzname[1]\n        # and time.daylight; handle that in strptime .\n        try:\n            time.tzset()\n        except AttributeError:\n            pass\n        no_saving = frozenset([\"utc\", \"gmt\", time.tzname[0].lower()])\n        if time.daylight:\n            has_saving = frozenset([time.tzname[1].lower()])\n        else:\n            has_saving = frozenset()\n        self.timezone = (no_saving, has_saving)\n\n\nclass TimeRE(dict):\n    \"\"\"Handle conversion from format directives to regexes.\"\"\"\n\n    def __init__(self, locale_time=None):\n        \"\"\"Create keys/values.\n\n        Order of execution is important for dependency reasons.\n\n        \"\"\"\n        if locale_time:\n            self.locale_time = locale_time\n        else:\n            self.locale_time = LocaleTime()\n        base = super(TimeRE, self)\n        base.__init__({\n            # The \" \\d\" part of the regex is to make %c from ANSI C work\n            'd': r\"(?P<d>3[0-1]|[1-2]\\d|0[1-9]|[1-9]| [1-9])\",\n            'f': r\"(?P<f>[0-9]{1,6})\",\n            'H': r\"(?P<H>2[0-3]|[0-1]\\d|\\d)\",\n            'I': r\"(?P<I>1[0-2]|0[1-9]|[1-9])\",\n            'j': r\"(?P<j>36[0-6]|3[0-5]\\d|[1-2]\\d\\d|0[1-9]\\d|00[1-9]|[1-9]\\d|0[1-9]|[1-9])\",\n            'm': r\"(?P<m>1[0-2]|0[1-9]|[1-9])\",\n            'M': r\"(?P<M>[0-5]\\d|\\d)\",\n            'S': r\"(?P<S>6[0-1]|[0-5]\\d|\\d)\",\n            'U': r\"(?P<U>5[0-3]|[0-4]\\d|\\d)\",\n            'w': r\"(?P<w>[0-6])\",\n            # W is set below by using 'U'\n            'y': r\"(?P<y>\\d\\d)\",\n            #XXX: Does 'Y' need to worry about having less or more than\n            #     4 digits?\n            'Y': r\"(?P<Y>\\d\\d\\d\\d)\",\n            'A': self.__seqToRE(self.locale_time.f_weekday, 'A'),\n            'a': self.__seqToRE(self.locale_time.a_weekday, 'a'),\n            'B': self.__seqToRE(self.locale_time.f_month[1:], 'B'),\n            'b': self.__seqToRE(self.locale_time.a_month[1:], 'b'),\n            'p': self.__seqToRE(self.locale_time.am_pm, 'p'),\n            'Z': self.__seqToRE((tz for tz_names in self.locale_time.timezone\n                                        for tz in tz_names),\n                                'Z'),\n            '%': '%'})\n        base.__setitem__('W', base.__getitem__('U').replace('U', 'W'))\n        base.__setitem__('c', self.pattern(self.locale_time.LC_date_time))\n        base.__setitem__('x', self.pattern(self.locale_time.LC_date))\n        base.__setitem__('X', self.pattern(self.locale_time.LC_time))\n\n    def __seqToRE(self, to_convert, directive):\n        \"\"\"Convert a list to a regex string for matching a directive.\n\n        Want possible matching values to be from longest to shortest.  This\n        prevents the possibility of a match occurring for a value that also\n        a substring of a larger value that should have matched (e.g., 'abc'\n        matching when 'abcdef' should have been the match).\n\n        \"\"\"\n        to_convert = sorted(to_convert, key=len, reverse=True)\n        for value in to_convert:\n            if value != '':\n                break\n        else:\n            return ''\n        regex = '|'.join(re_escape(stuff) for stuff in to_convert)\n        regex = '(?P<%s>%s' % (directive, regex)\n        return '%s)' % regex\n\n    def pattern(self, format):\n        \"\"\"Return regex pattern for the format string.\n\n        Need to make sure that any characters that might be interpreted as\n        regex syntax are escaped.\n\n        \"\"\"\n        processed_format = ''\n        # The sub() call escapes all characters that might be misconstrued\n        # as regex syntax.  Cannot use re.escape since we have to deal with\n        # format directives (%m, etc.).\n        regex_chars = re_compile(r\"([\\\\.^$*+?\\(\\){}\\[\\]|])\")\n        format = regex_chars.sub(r\"\\\\\\1\", format)\n        whitespace_replacement = re_compile('\\s+')\n        format = whitespace_replacement.sub('\\s+', format)\n        while '%' in format:\n            directive_index = format.index('%')+1\n            processed_format = \"%s%s%s\" % (processed_format,\n                                           format[:directive_index-1],\n                                           self[format[directive_index]])\n            format = format[directive_index+1:]\n        return \"%s%s\" % (processed_format, format)\n\n    def compile(self, format):\n        \"\"\"Return a compiled re object for the format string.\"\"\"\n        return re_compile(self.pattern(format), IGNORECASE)\n\n_cache_lock = _thread_allocate_lock()\n# DO NOT modify _TimeRE_cache or _regex_cache without acquiring the cache lock\n# first!\n_TimeRE_cache = TimeRE()\n_CACHE_MAX_SIZE = 5 # Max number of regexes stored in _regex_cache\n_regex_cache = {}\n\ndef _calc_julian_from_U_or_W(year, week_of_year, day_of_week, week_starts_Mon):\n    \"\"\"Calculate the Julian day based on the year, week of the year, and day of\n    the week, with week_start_day representing whether the week of the year\n    assumes the week starts on Sunday or Monday (6 or 0).\"\"\"\n    first_weekday = datetime_date(year, 1, 1).weekday()\n    # If we are dealing with the %U directive (week starts on Sunday), it's\n    # easier to just shift the view to Sunday being the first day of the\n    # week.\n    if not week_starts_Mon:\n        first_weekday = (first_weekday + 1) % 7\n        day_of_week = (day_of_week + 1) % 7\n    # Need to watch out for a week 0 (when the first day of the year is not\n    # the same as that specified by %U or %W).\n    week_0_length = (7 - first_weekday) % 7\n    if week_of_year == 0:\n        return 1 + day_of_week - first_weekday\n    else:\n        days_to_week = week_0_length + (7 * (week_of_year - 1))\n        return 1 + days_to_week + day_of_week\n\n\ndef _strptime(data_string, format=\"%a %b %d %H:%M:%S %Y\"):\n    \"\"\"Return a time struct based on the input string and the format string.\"\"\"\n    global _TimeRE_cache, _regex_cache\n    with _cache_lock:\n        if _getlang() != _TimeRE_cache.locale_time.lang:\n            _TimeRE_cache = TimeRE()\n            _regex_cache.clear()\n        if len(_regex_cache) > _CACHE_MAX_SIZE:\n            _regex_cache.clear()\n        locale_time = _TimeRE_cache.locale_time\n        format_regex = _regex_cache.get(format)\n        if not format_regex:\n            try:\n                format_regex = _TimeRE_cache.compile(format)\n            # KeyError raised when a bad format is found; can be specified as\n            # \\\\, in which case it was a stray % but with a space after it\n            except KeyError, err:\n                bad_directive = err.args[0]\n                if bad_directive == \"\\\\\":\n                    bad_directive = \"%\"\n                del err\n                raise ValueError(\"'%s' is a bad directive in format '%s'\" %\n                                    (bad_directive, format))\n            # IndexError only occurs when the format string is \"%\"\n            except IndexError:\n                raise ValueError(\"stray %% in format '%s'\" % format)\n            _regex_cache[format] = format_regex\n    found = format_regex.match(data_string)\n    if not found:\n        raise ValueError(\"time data %r does not match format %r\" %\n                         (data_string, format))\n    if len(data_string) != found.end():\n        raise ValueError(\"unconverted data remains: %s\" %\n                          data_string[found.end():])\n\n    year = None\n    month = day = 1\n    hour = minute = second = fraction = 0\n    tz = -1\n    # Default to -1 to signify that values not known; not critical to have,\n    # though\n    week_of_year = -1\n    week_of_year_start = -1\n    # weekday and julian defaulted to -1 so as to signal need to calculate\n    # values\n    weekday = julian = -1\n    found_dict = found.groupdict()\n    for group_key in found_dict.iterkeys():\n        # Directives not explicitly handled below:\n        #   c, x, X\n        #      handled by making out of other directives\n        #   U, W\n        #      worthless without day of the week\n        if group_key == 'y':\n            year = int(found_dict['y'])\n            # Open Group specification for strptime() states that a %y\n            #value in the range of [00, 68] is in the century 2000, while\n            #[69,99] is in the century 1900\n            if year <= 68:\n                year += 2000\n            else:\n                year += 1900\n        elif group_key == 'Y':\n            year = int(found_dict['Y'])\n        elif group_key == 'm':\n            month = int(found_dict['m'])\n        elif group_key == 'B':\n            month = locale_time.f_month.index(found_dict['B'].lower())\n        elif group_key == 'b':\n            month = locale_time.a_month.index(found_dict['b'].lower())\n        elif group_key == 'd':\n            day = int(found_dict['d'])\n        elif group_key == 'H':\n            hour = int(found_dict['H'])\n        elif group_key == 'I':\n            hour = int(found_dict['I'])\n            ampm = found_dict.get('p', '').lower()\n            # If there was no AM/PM indicator, we'll treat this like AM\n            if ampm in ('', locale_time.am_pm[0]):\n                # We're in AM so the hour is correct unless we're\n                # looking at 12 midnight.\n                # 12 midnight == 12 AM == hour 0\n                if hour == 12:\n                    hour = 0\n            elif ampm == locale_time.am_pm[1]:\n                # We're in PM so we need to add 12 to the hour unless\n                # we're looking at 12 noon.\n                # 12 noon == 12 PM == hour 12\n                if hour != 12:\n                    hour += 12\n        elif group_key == 'M':\n            minute = int(found_dict['M'])\n        elif group_key == 'S':\n            second = int(found_dict['S'])\n        elif group_key == 'f':\n            s = found_dict['f']\n            # Pad to always return microseconds.\n            s += \"0\" * (6 - len(s))\n            fraction = int(s)\n        elif group_key == 'A':\n            weekday = locale_time.f_weekday.index(found_dict['A'].lower())\n        elif group_key == 'a':\n            weekday = locale_time.a_weekday.index(found_dict['a'].lower())\n        elif group_key == 'w':\n            weekday = int(found_dict['w'])\n            if weekday == 0:\n                weekday = 6\n            else:\n                weekday -= 1\n        elif group_key == 'j':\n            julian = int(found_dict['j'])\n        elif group_key in ('U', 'W'):\n            week_of_year = int(found_dict[group_key])\n            if group_key == 'U':\n                # U starts week on Sunday.\n                week_of_year_start = 6\n            else:\n                # W starts week on Monday.\n                week_of_year_start = 0\n        elif group_key == 'Z':\n            # Since -1 is default value only need to worry about setting tz if\n            # it can be something other than -1.\n            found_zone = found_dict['Z'].lower()\n            for value, tz_values in enumerate(locale_time.timezone):\n                if found_zone in tz_values:\n                    # Deal with bad locale setup where timezone names are the\n                    # same and yet time.daylight is true; too ambiguous to\n                    # be able to tell what timezone has daylight savings\n                    if (time.tzname[0] == time.tzname[1] and\n                       time.daylight and found_zone not in (\"utc\", \"gmt\")):\n                        break\n                    else:\n                        tz = value\n                        break\n    leap_year_fix = False\n    if year is None and month == 2 and day == 29:\n        year = 1904  # 1904 is first leap year of 20th century\n        leap_year_fix = True\n    elif year is None:\n        year = 1900\n    # If we know the week of the year and what day of that week, we can figure\n    # out the Julian day of the year.\n    if julian == -1 and week_of_year != -1 and weekday != -1:\n        week_starts_Mon = True if week_of_year_start == 0 else False\n        julian = _calc_julian_from_U_or_W(year, week_of_year, weekday,\n                                            week_starts_Mon)\n    # Cannot pre-calculate datetime_date() since can change in Julian\n    # calculation and thus could have different value for the day of the week\n    # calculation.\n    if julian == -1:\n        # Need to add 1 to result since first day of the year is 1, not 0.\n        julian = datetime_date(year, month, day).toordinal() - \\\n                  datetime_date(year, 1, 1).toordinal() + 1\n    else:  # Assume that if they bothered to include Julian day it will\n           # be accurate.\n        datetime_result = datetime_date.fromordinal((julian - 1) + datetime_date(year, 1, 1).toordinal())\n        year = datetime_result.year\n        month = datetime_result.month\n        day = datetime_result.day\n    if weekday == -1:\n        weekday = datetime_date(year, month, day).weekday()\n    if leap_year_fix:\n        # the caller didn't supply a year but asked for Feb 29th. We couldn't\n        # use the default of 1900 for computations. We set it back to ensure\n        # that February 29th is smaller than March 1st.\n        year = 1900\n\n    return (time.struct_time((year, month, day,\n                              hour, minute, second,\n                              weekday, julian, tz)), fraction)\n\ndef _strptime_time(data_string, format=\"%a %b %d %H:%M:%S %Y\"):\n    return _strptime(data_string, format)[0]\n", 
    "_structseq": "\"\"\"\nImplementation helper: a struct that looks like a tuple.  See timemodule\nand posixmodule for example uses.\n\"\"\"\n\nclass structseqfield(object):\n    \"\"\"Definition of field of a structseq.  The 'index' is for positional\n    tuple-like indexing.  Fields whose index is after a gap in the numbers\n    cannot be accessed like this, but only by name.\n    \"\"\"\n    def __init__(self, index, doc=None, default=lambda self: None):\n        self.__name__ = '?'\n        self.index    = index    # patched to None if not positional\n        self._index   = index\n        self.__doc__  = doc\n        self._default = default\n\n    def __repr__(self):\n        return '<field %s (%s)>' % (self.__name__,\n                                    self.__doc__ or 'undocumented')\n\n    def __get__(self, obj, typ=None):\n        if obj is None:\n            return self\n        if self.index is None:\n            return obj.__dict__[self.__name__]\n        else:\n            return obj[self.index]\n\n    def __set__(self, obj, value):\n        raise TypeError(\"readonly attribute\")\n\n\nclass structseqtype(type):\n\n    def __new__(metacls, classname, bases, dict):\n        assert not bases\n        fields_by_index = {}\n        for name, field in dict.items():\n            if isinstance(field, structseqfield):\n                assert field._index not in fields_by_index\n                fields_by_index[field._index] = field\n                field.__name__ = name\n        dict['n_fields'] = len(fields_by_index)\n\n        extra_fields = sorted(fields_by_index.iteritems())\n        n_sequence_fields = 0\n        while extra_fields and extra_fields[0][0] == n_sequence_fields:\n            extra_fields.pop(0)\n            n_sequence_fields += 1\n        dict['n_sequence_fields'] = n_sequence_fields\n        dict['n_unnamed_fields'] = 0     # no fully anonymous fields in PyPy\n\n        extra_fields = [field for index, field in extra_fields]\n        for field in extra_fields:\n            field.index = None     # no longer relevant\n\n        assert '__new__' not in dict\n        dict['_extra_fields'] = tuple(extra_fields)\n        dict['__new__'] = structseq_new\n        dict['__reduce__'] = structseq_reduce\n        dict['__setattr__'] = structseq_setattr\n        dict['__repr__'] = structseq_repr\n        dict['_name'] = dict.get('name', '')\n        return type.__new__(metacls, classname, (tuple,), dict)\n\n\nbuiltin_dict = dict\n\ndef structseq_new(cls, sequence, dict={}):\n    sequence = tuple(sequence)\n    dict = builtin_dict(dict)\n    N = cls.n_sequence_fields\n    if len(sequence) < N:\n        if N < cls.n_fields:\n            msg = \"at least\"\n        else:\n            msg = \"exactly\"\n        raise TypeError(\"expected a sequence with %s %d items\" % (\n            msg, N))\n    if len(sequence) > N:\n        if len(sequence) > cls.n_fields:\n            if N < cls.n_fields:\n                msg = \"at most\"\n            else:\n                msg = \"exactly\"\n            raise TypeError(\"expected a sequence with %s %d items\" % (\n                msg, cls.n_fields))\n        for field, value in zip(cls._extra_fields, sequence[N:]):\n            name = field.__name__\n            if name in dict:\n                raise TypeError(\"duplicate value for %r\" % (name,))\n            dict[name] = value\n        sequence = sequence[:N]\n    result = tuple.__new__(cls, sequence)\n    object.__setattr__(result, '__dict__', dict)\n    for field in cls._extra_fields:\n        name = field.__name__\n        if name not in dict:\n            dict[name] = field._default(result)\n    return result\n\ndef structseq_reduce(self):\n    return type(self), (tuple(self), self.__dict__)\n\ndef structseq_setattr(self, attr, value):\n    raise AttributeError(\"%r object has no attribute %r\" % (\n        self.__class__.__name__, attr))\n\ndef structseq_repr(self):\n    fields = {}\n    for field in type(self).__dict__.values():\n        if isinstance(field, structseqfield):\n            fields[field._index] = field\n    parts = [\"%s=%r\" % (fields[index].__name__, value)\n             for index, value in enumerate(self)]\n    return \"%s(%s)\" % (self._name, \", \".join(parts))\n", 
    "_weakrefset": "# Access WeakSet through the weakref module.\n# This code is separated-out because it is needed\n# by abc.py to load everything else at startup.\n\nfrom _weakref import ref\n\n__all__ = ['WeakSet']\n\n\nclass _IterationGuard(object):\n    # This context manager registers itself in the current iterators of the\n    # weak container, such as to delay all removals until the context manager\n    # exits.\n    # This technique should be relatively thread-safe (since sets are).\n\n    def __init__(self, weakcontainer):\n        # Don't create cycles\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\n\nclass WeakSet(object):\n    def __init__(self, data=None):\n        self.data = set()\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n        self._remove = _remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        discard = self.data.discard\n        while l:\n            discard(l.pop())\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    # Caveat: the iterator will keep a strong reference to\n                    # `item` until it is resumed or closed.\n                    yield item\n\n    def __len__(self):\n        return len(self.data) - len(self._pending_removals)\n\n    def __contains__(self, item):\n        try:\n            wr = ref(item)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def __reduce__(self):\n        return (self.__class__, (list(self),),\n                getattr(self, '__dict__', None))\n\n    __hash__ = None\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet')\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        for element in other:\n            self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def difference(self, other):\n        newset = self.copy()\n        newset.difference_update(other)\n        return newset\n    __sub__ = difference\n\n    def difference_update(self, other):\n        self.__isub__(other)\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n        return self\n\n    def intersection(self, other):\n        return self.__class__(item for item in other if item in self)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        self.__iand__(other)\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset(ref(item) for item in other)\n    __le__ = issubset\n\n    def __lt__(self, other):\n        return self.data < set(ref(item) for item in other)\n\n    def issuperset(self, other):\n        return self.data.issuperset(ref(item) for item in other)\n    __ge__ = issuperset\n\n    def __gt__(self, other):\n        return self.data > set(ref(item) for item in other)\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set(ref(item) for item in other)\n\n    def __ne__(self, other):\n        opposite = self.__eq__(other)\n        if opposite is NotImplemented:\n            return NotImplemented\n        return not opposite\n\n    def symmetric_difference(self, other):\n        newset = self.copy()\n        newset.symmetric_difference_update(other)\n        return newset\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        self.__ixor__(other)\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\n        return self\n\n    def union(self, other):\n        return self.__class__(e for s in (self, other) for e in s)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n", 
    "abc": "# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) according to PEP 3119.\"\"\"\n\nimport types\n\nfrom _weakrefset import WeakSet\n\n# Instance of old-style class\nclass _C: pass\n_InstanceType = type(_C())\n\n\ndef abstractmethod(funcobj):\n    \"\"\"A decorator indicating abstract methods.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract methods are overridden.\n    The abstract methods can be called using any of the normal\n    'super' call mechanisms.\n\n    Usage:\n\n        class C:\n            __metaclass__ = ABCMeta\n            @abstractmethod\n            def my_abstract_method(self, ...):\n                ...\n    \"\"\"\n    funcobj.__isabstractmethod__ = True\n    return funcobj\n\n\nclass abstractproperty(property):\n    \"\"\"A decorator indicating abstract properties.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract properties are overridden.\n    The abstract properties can be called using any of the normal\n    'super' call mechanisms.\n\n    Usage:\n\n        class C:\n            __metaclass__ = ABCMeta\n            @abstractproperty\n            def my_abstract_property(self):\n                ...\n\n    This defines a read-only property; you can also define a read-write\n    abstract property using the 'long' form of property declaration:\n\n        class C:\n            __metaclass__ = ABCMeta\n            def getx(self): ...\n            def setx(self, value): ...\n            x = abstractproperty(getx, setx)\n    \"\"\"\n    __isabstractmethod__ = True\n\n\nclass ABCMeta(type):\n\n    \"\"\"Metaclass for defining Abstract Base Classes (ABCs).\n\n    Use this metaclass to create an ABC.  An ABC can be subclassed\n    directly, and then acts as a mix-in class.  You can also register\n    unrelated concrete classes (even built-in classes) and unrelated\n    ABCs as 'virtual subclasses' -- these and their descendants will\n    be considered subclasses of the registering ABC by the built-in\n    issubclass() function, but the registering ABC won't show up in\n    their MRO (Method Resolution Order) nor will method\n    implementations defined by the registering ABC be callable (not\n    even via super()).\n\n    \"\"\"\n\n    # A global counter that is incremented each time a class is\n    # registered as a virtual subclass of anything.  It forces the\n    # negative cache to be cleared before its next use.\n    _abc_invalidation_counter = 0\n\n    def __new__(mcls, name, bases, namespace):\n        cls = super(ABCMeta, mcls).__new__(mcls, name, bases, namespace)\n        # Compute set of abstract method names\n        abstracts = set(name\n                     for name, value in namespace.items()\n                     if getattr(value, \"__isabstractmethod__\", False))\n        for base in bases:\n            for name in getattr(base, \"__abstractmethods__\", set()):\n                value = getattr(cls, name, None)\n                if getattr(value, \"__isabstractmethod__\", False):\n                    abstracts.add(name)\n        cls.__abstractmethods__ = frozenset(abstracts)\n        # Set up inheritance registry\n        cls._abc_registry = WeakSet()\n        cls._abc_cache = WeakSet()\n        cls._abc_negative_cache = WeakSet()\n        cls._abc_negative_cache_version = ABCMeta._abc_invalidation_counter\n        return cls\n\n    def register(cls, subclass):\n        \"\"\"Register a virtual subclass of an ABC.\"\"\"\n        if not isinstance(subclass, (type, types.ClassType)):\n            raise TypeError(\"Can only register classes\")\n        if issubclass(subclass, cls):\n            return  # Already a subclass\n        # Subtle: test for cycles *after* testing for \"already a subclass\";\n        # this means we allow X.register(X) and interpret it as a no-op.\n        if issubclass(cls, subclass):\n            # This would create a cycle, which is bad for the algorithm below\n            raise RuntimeError(\"Refusing to create an inheritance cycle\")\n        cls._abc_registry.add(subclass)\n        ABCMeta._abc_invalidation_counter += 1  # Invalidate negative cache\n\n    def _dump_registry(cls, file=None):\n        \"\"\"Debug helper to print the ABC registry.\"\"\"\n        print >> file, \"Class: %s.%s\" % (cls.__module__, cls.__name__)\n        print >> file, \"Inv.counter: %s\" % ABCMeta._abc_invalidation_counter\n        for name in sorted(cls.__dict__.keys()):\n            if name.startswith(\"_abc_\"):\n                value = getattr(cls, name)\n                print >> file, \"%s: %r\" % (name, value)\n\n    def __instancecheck__(cls, instance):\n        \"\"\"Override for isinstance(instance, cls).\"\"\"\n        # Inline the cache checking when it's simple.\n        subclass = getattr(instance, '__class__', None)\n        if subclass is not None and subclass in cls._abc_cache:\n            return True\n        subtype = type(instance)\n        # Old-style instances\n        if subtype is _InstanceType:\n            subtype = subclass\n        if subtype is subclass or subclass is None:\n            if (cls._abc_negative_cache_version ==\n                ABCMeta._abc_invalidation_counter and\n                subtype in cls._abc_negative_cache):\n                return False\n            # Fall back to the subclass check.\n            return cls.__subclasscheck__(subtype)\n        return (cls.__subclasscheck__(subclass) or\n                cls.__subclasscheck__(subtype))\n\n    def __subclasscheck__(cls, subclass):\n        \"\"\"Override for issubclass(subclass, cls).\"\"\"\n        # Check cache\n        if subclass in cls._abc_cache:\n            return True\n        # Check negative cache; may have to invalidate\n        if cls._abc_negative_cache_version < ABCMeta._abc_invalidation_counter:\n            # Invalidate the negative cache\n            cls._abc_negative_cache = WeakSet()\n            cls._abc_negative_cache_version = ABCMeta._abc_invalidation_counter\n        elif subclass in cls._abc_negative_cache:\n            return False\n        # Check the subclass hook\n        ok = cls.__subclasshook__(subclass)\n        if ok is not NotImplemented:\n            assert isinstance(ok, bool)\n            if ok:\n                cls._abc_cache.add(subclass)\n            else:\n                cls._abc_negative_cache.add(subclass)\n            return ok\n        # Check if it's a direct subclass\n        if cls in getattr(subclass, '__mro__', ()):\n            cls._abc_cache.add(subclass)\n            return True\n        # Check if it's a subclass of a registered class (recursive)\n        for rcls in cls._abc_registry:\n            if issubclass(subclass, rcls):\n                cls._abc_cache.add(subclass)\n                return True\n        # Check if it's a subclass of a subclass (recursive)\n        for scls in cls.__subclasses__():\n            if issubclass(subclass, scls):\n                cls._abc_cache.add(subclass)\n                return True\n        # No dice; update negative cache\n        cls._abc_negative_cache.add(subclass)\n        return False\n", 
    "atexit": "\"\"\"\natexit.py - allow programmer to define multiple exit functions to be executed\nupon normal program termination.\n\nOne public function, register, is defined.\n\"\"\"\n\n__all__ = [\"register\"]\n\nimport sys\n\n_exithandlers = []\ndef _run_exitfuncs():\n    \"\"\"run any registered exit functions\n\n    _exithandlers is traversed in reverse order so functions are executed\n    last in, first out.\n    \"\"\"\n\n    exc_info = None\n    while _exithandlers:\n        func, targs, kargs = _exithandlers.pop()\n        try:\n            func(*targs, **kargs)\n        except SystemExit:\n            exc_info = sys.exc_info()\n        except:\n            import traceback\n            print >> sys.stderr, \"Error in atexit._run_exitfuncs:\"\n            traceback.print_exc()\n            exc_info = sys.exc_info()\n\n    if exc_info is not None:\n        raise exc_info[0], exc_info[1], exc_info[2]\n\n\ndef register(func, *targs, **kargs):\n    \"\"\"register a function to be executed upon normal program termination\n\n    func - function to be called at exit\n    targs - optional arguments to pass to func\n    kargs - optional keyword arguments to pass to func\n\n    func is returned to facilitate usage as a decorator.\n    \"\"\"\n    _exithandlers.append((func, targs, kargs))\n    return func\n\nif hasattr(sys, \"exitfunc\"):\n    # Assume it's another registered exit function - append it to our list\n    register(sys.exitfunc)\nsys.exitfunc = _run_exitfuncs\n\nif __name__ == \"__main__\":\n    def x1():\n        print \"running x1\"\n    def x2(n):\n        print \"running x2(%r)\" % (n,)\n    def x3(n, kwd=None):\n        print \"running x3(%r, kwd=%r)\" % (n, kwd)\n\n    register(x1)\n    register(x2, 12)\n    register(x3, 5, \"bar\")\n    register(x3, \"no kwd args\")\n", 
    "base64": "#! /usr/bin/env python\n\n\"\"\"RFC 3548: Base16, Base32, Base64 Data Encodings\"\"\"\n\n# Modified 04-Oct-1995 by Jack Jansen to use binascii module\n# Modified 30-Dec-2003 by Barry Warsaw to add full RFC 3548 support\n\nimport re\nimport struct\nimport binascii\n\n\n__all__ = [\n    # Legacy interface exports traditional RFC 1521 Base64 encodings\n    'encode', 'decode', 'encodestring', 'decodestring',\n    # Generalized interface for other encodings\n    'b64encode', 'b64decode', 'b32encode', 'b32decode',\n    'b16encode', 'b16decode',\n    # Standard Base64 encoding\n    'standard_b64encode', 'standard_b64decode',\n    # Some common Base64 alternatives.  As referenced by RFC 3458, see thread\n    # starting at:\n    #\n    # http://zgp.org/pipermail/p2p-hackers/2001-September/000316.html\n    'urlsafe_b64encode', 'urlsafe_b64decode',\n    ]\n\n_translation = [chr(_x) for _x in range(256)]\nEMPTYSTRING = ''\n\n\ndef _translate(s, altchars):\n    translation = _translation[:]\n    for k, v in altchars.items():\n        translation[ord(k)] = v\n    return s.translate(''.join(translation))\n\n\n\f\n# Base64 encoding/decoding uses binascii\n\ndef b64encode(s, altchars=None):\n    \"\"\"Encode a string using Base64.\n\n    s is the string to encode.  Optional altchars must be a string of at least\n    length 2 (additional characters are ignored) which specifies an\n    alternative alphabet for the '+' and '/' characters.  This allows an\n    application to e.g. generate url or filesystem safe Base64 strings.\n\n    The encoded string is returned.\n    \"\"\"\n    # Strip off the trailing newline\n    encoded = binascii.b2a_base64(s)[:-1]\n    if altchars is not None:\n        return _translate(encoded, {'+': altchars[0], '/': altchars[1]})\n    return encoded\n\n\ndef b64decode(s, altchars=None):\n    \"\"\"Decode a Base64 encoded string.\n\n    s is the string to decode.  Optional altchars must be a string of at least\n    length 2 (additional characters are ignored) which specifies the\n    alternative alphabet used instead of the '+' and '/' characters.\n\n    The decoded string is returned.  A TypeError is raised if s were\n    incorrectly padded or if there are non-alphabet characters present in the\n    string.\n    \"\"\"\n    if altchars is not None:\n        s = _translate(s, {altchars[0]: '+', altchars[1]: '/'})\n    try:\n        return binascii.a2b_base64(s)\n    except binascii.Error, msg:\n        # Transform this exception for consistency\n        raise TypeError(msg)\n\n\ndef standard_b64encode(s):\n    \"\"\"Encode a string using the standard Base64 alphabet.\n\n    s is the string to encode.  The encoded string is returned.\n    \"\"\"\n    return b64encode(s)\n\ndef standard_b64decode(s):\n    \"\"\"Decode a string encoded with the standard Base64 alphabet.\n\n    s is the string to decode.  The decoded string is returned.  A TypeError\n    is raised if the string is incorrectly padded or if there are non-alphabet\n    characters present in the string.\n    \"\"\"\n    return b64decode(s)\n\ndef urlsafe_b64encode(s):\n    \"\"\"Encode a string using a url-safe Base64 alphabet.\n\n    s is the string to encode.  The encoded string is returned.  The alphabet\n    uses '-' instead of '+' and '_' instead of '/'.\n    \"\"\"\n    return b64encode(s, '-_')\n\ndef urlsafe_b64decode(s):\n    \"\"\"Decode a string encoded with the standard Base64 alphabet.\n\n    s is the string to decode.  The decoded string is returned.  A TypeError\n    is raised if the string is incorrectly padded or if there are non-alphabet\n    characters present in the string.\n\n    The alphabet uses '-' instead of '+' and '_' instead of '/'.\n    \"\"\"\n    return b64decode(s, '-_')\n\n\n\f\n# Base32 encoding/decoding must be done in Python\n_b32alphabet = {\n    0: 'A',  9: 'J', 18: 'S', 27: '3',\n    1: 'B', 10: 'K', 19: 'T', 28: '4',\n    2: 'C', 11: 'L', 20: 'U', 29: '5',\n    3: 'D', 12: 'M', 21: 'V', 30: '6',\n    4: 'E', 13: 'N', 22: 'W', 31: '7',\n    5: 'F', 14: 'O', 23: 'X',\n    6: 'G', 15: 'P', 24: 'Y',\n    7: 'H', 16: 'Q', 25: 'Z',\n    8: 'I', 17: 'R', 26: '2',\n    }\n\n_b32tab = _b32alphabet.items()\n_b32tab.sort()\n_b32tab = [v for k, v in _b32tab]\n_b32rev = dict([(v, long(k)) for k, v in _b32alphabet.items()])\n\n\ndef b32encode(s):\n    \"\"\"Encode a string using Base32.\n\n    s is the string to encode.  The encoded string is returned.\n    \"\"\"\n    parts = []\n    quanta, leftover = divmod(len(s), 5)\n    # Pad the last quantum with zero bits if necessary\n    if leftover:\n        s += ('\\0' * (5 - leftover))\n        quanta += 1\n    for i in range(quanta):\n        # c1 and c2 are 16 bits wide, c3 is 8 bits wide.  The intent of this\n        # code is to process the 40 bits in units of 5 bits.  So we take the 1\n        # leftover bit of c1 and tack it onto c2.  Then we take the 2 leftover\n        # bits of c2 and tack them onto c3.  The shifts and masks are intended\n        # to give us values of exactly 5 bits in width.\n        c1, c2, c3 = struct.unpack('!HHB', s[i*5:(i+1)*5])\n        c2 += (c1 & 1) << 16 # 17 bits wide\n        c3 += (c2 & 3) << 8  # 10 bits wide\n        parts.extend([_b32tab[c1 >> 11],         # bits 1 - 5\n                      _b32tab[(c1 >> 6) & 0x1f], # bits 6 - 10\n                      _b32tab[(c1 >> 1) & 0x1f], # bits 11 - 15\n                      _b32tab[c2 >> 12],         # bits 16 - 20 (1 - 5)\n                      _b32tab[(c2 >> 7) & 0x1f], # bits 21 - 25 (6 - 10)\n                      _b32tab[(c2 >> 2) & 0x1f], # bits 26 - 30 (11 - 15)\n                      _b32tab[c3 >> 5],          # bits 31 - 35 (1 - 5)\n                      _b32tab[c3 & 0x1f],        # bits 36 - 40 (1 - 5)\n                      ])\n    encoded = EMPTYSTRING.join(parts)\n    # Adjust for any leftover partial quanta\n    if leftover == 1:\n        return encoded[:-6] + '======'\n    elif leftover == 2:\n        return encoded[:-4] + '===='\n    elif leftover == 3:\n        return encoded[:-3] + '==='\n    elif leftover == 4:\n        return encoded[:-1] + '='\n    return encoded\n\n\ndef b32decode(s, casefold=False, map01=None):\n    \"\"\"Decode a Base32 encoded string.\n\n    s is the string to decode.  Optional casefold is a flag specifying whether\n    a lowercase alphabet is acceptable as input.  For security purposes, the\n    default is False.\n\n    RFC 3548 allows for optional mapping of the digit 0 (zero) to the letter O\n    (oh), and for optional mapping of the digit 1 (one) to either the letter I\n    (eye) or letter L (el).  The optional argument map01 when not None,\n    specifies which letter the digit 1 should be mapped to (when map01 is not\n    None, the digit 0 is always mapped to the letter O).  For security\n    purposes the default is None, so that 0 and 1 are not allowed in the\n    input.\n\n    The decoded string is returned.  A TypeError is raised if s were\n    incorrectly padded or if there are non-alphabet characters present in the\n    string.\n    \"\"\"\n    quanta, leftover = divmod(len(s), 8)\n    if leftover:\n        raise TypeError('Incorrect padding')\n    # Handle section 2.4 zero and one mapping.  The flag map01 will be either\n    # False, or the character to map the digit 1 (one) to.  It should be\n    # either L (el) or I (eye).\n    if map01:\n        s = _translate(s, {'0': 'O', '1': map01})\n    if casefold:\n        s = s.upper()\n    # Strip off pad characters from the right.  We need to count the pad\n    # characters because this will tell us how many null bytes to remove from\n    # the end of the decoded string.\n    padchars = 0\n    mo = re.search('(?P<pad>[=]*)$', s)\n    if mo:\n        padchars = len(mo.group('pad'))\n        if padchars > 0:\n            s = s[:-padchars]\n    # Now decode the full quanta\n    parts = []\n    acc = 0\n    shift = 35\n    for c in s:\n        val = _b32rev.get(c)\n        if val is None:\n            raise TypeError('Non-base32 digit found')\n        acc += _b32rev[c] << shift\n        shift -= 5\n        if shift < 0:\n            parts.append(binascii.unhexlify('%010x' % acc))\n            acc = 0\n            shift = 35\n    # Process the last, partial quanta\n    last = binascii.unhexlify('%010x' % acc)\n    if padchars == 0:\n        last = ''                       # No characters\n    elif padchars == 1:\n        last = last[:-1]\n    elif padchars == 3:\n        last = last[:-2]\n    elif padchars == 4:\n        last = last[:-3]\n    elif padchars == 6:\n        last = last[:-4]\n    else:\n        raise TypeError('Incorrect padding')\n    parts.append(last)\n    return EMPTYSTRING.join(parts)\n\n\n\f\n# RFC 3548, Base 16 Alphabet specifies uppercase, but hexlify() returns\n# lowercase.  The RFC also recommends against accepting input case\n# insensitively.\ndef b16encode(s):\n    \"\"\"Encode a string using Base16.\n\n    s is the string to encode.  The encoded string is returned.\n    \"\"\"\n    return binascii.hexlify(s).upper()\n\n\ndef b16decode(s, casefold=False):\n    \"\"\"Decode a Base16 encoded string.\n\n    s is the string to decode.  Optional casefold is a flag specifying whether\n    a lowercase alphabet is acceptable as input.  For security purposes, the\n    default is False.\n\n    The decoded string is returned.  A TypeError is raised if s were\n    incorrectly padded or if there are non-alphabet characters present in the\n    string.\n    \"\"\"\n    if casefold:\n        s = s.upper()\n    if re.search('[^0-9A-F]', s):\n        raise TypeError('Non-base16 digit found')\n    return binascii.unhexlify(s)\n\n\n\f\n# Legacy interface.  This code could be cleaned up since I don't believe\n# binascii has any line length limitations.  It just doesn't seem worth it\n# though.\n\nMAXLINESIZE = 76 # Excluding the CRLF\nMAXBINSIZE = (MAXLINESIZE//4)*3\n\ndef encode(input, output):\n    \"\"\"Encode a file.\"\"\"\n    while True:\n        s = input.read(MAXBINSIZE)\n        if not s:\n            break\n        while len(s) < MAXBINSIZE:\n            ns = input.read(MAXBINSIZE-len(s))\n            if not ns:\n                break\n            s += ns\n        line = binascii.b2a_base64(s)\n        output.write(line)\n\n\ndef decode(input, output):\n    \"\"\"Decode a file.\"\"\"\n    while True:\n        line = input.readline()\n        if not line:\n            break\n        s = binascii.a2b_base64(line)\n        output.write(s)\n\n\ndef encodestring(s):\n    \"\"\"Encode a string into multiple lines of base-64 data.\"\"\"\n    pieces = []\n    for i in range(0, len(s), MAXBINSIZE):\n        chunk = s[i : i + MAXBINSIZE]\n        pieces.append(binascii.b2a_base64(chunk))\n    return \"\".join(pieces)\n\n\ndef decodestring(s):\n    \"\"\"Decode a string.\"\"\"\n    return binascii.a2b_base64(s)\n\n\n\f\n# Useable as a script...\ndef test():\n    \"\"\"Small test program\"\"\"\n    import sys, getopt\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], 'deut')\n    except getopt.error, msg:\n        sys.stdout = sys.stderr\n        print msg\n        print \"\"\"usage: %s [-d|-e|-u|-t] [file|-]\n        -d, -u: decode\n        -e: encode (default)\n        -t: encode and decode string 'Aladdin:open sesame'\"\"\"%sys.argv[0]\n        sys.exit(2)\n    func = encode\n    for o, a in opts:\n        if o == '-e': func = encode\n        if o == '-d': func = decode\n        if o == '-u': func = decode\n        if o == '-t': test1(); return\n    if args and args[0] != '-':\n        with open(args[0], 'rb') as f:\n            func(f, sys.stdout)\n    else:\n        func(sys.stdin, sys.stdout)\n\n\ndef test1():\n    s0 = \"Aladdin:open sesame\"\n    s1 = encodestring(s0)\n    s2 = decodestring(s1)\n    print s0, repr(s1), s2\n\n\nif __name__ == '__main__':\n    test()\n", 
    "bdb": "\"\"\"Debugger basics\"\"\"\n\nimport fnmatch\nimport sys\nimport os\nimport types\n\n__all__ = [\"BdbQuit\",\"Bdb\",\"Breakpoint\"]\n\nclass BdbQuit(Exception):\n    \"\"\"Exception to give up completely\"\"\"\n\n\nclass Bdb:\n\n    \"\"\"Generic Python debugger base class.\n\n    This class takes care of details of the trace facility;\n    a derived class should implement user interaction.\n    The standard debugger class (pdb.Pdb) is an example.\n    \"\"\"\n\n    def __init__(self, skip=None):\n        self.skip = set(skip) if skip else None\n        self.breaks = {}\n        self.fncache = {}\n        self.frame_returning = None\n\n    def canonic(self, filename):\n        if filename == \"<\" + filename[1:-1] + \">\":\n            return filename\n        canonic = self.fncache.get(filename)\n        if not canonic:\n            canonic = os.path.abspath(filename)\n            canonic = os.path.normcase(canonic)\n            self.fncache[filename] = canonic\n        return canonic\n\n    def reset(self):\n        import linecache\n        linecache.checkcache()\n        self.botframe = None\n        self._set_stopinfo(None, None)\n\n    def trace_dispatch(self, frame, event, arg):\n        if self.quitting:\n            return # None\n        if event == 'line':\n            return self.dispatch_line(frame)\n        if event == 'call':\n            return self.dispatch_call(frame, arg)\n        if event == 'return':\n            return self.dispatch_return(frame, arg)\n        if event == 'exception':\n            return self.dispatch_exception(frame, arg)\n        if event == 'c_call':\n            return self.trace_dispatch\n        if event == 'c_exception':\n            return self.trace_dispatch\n        if event == 'c_return':\n            return self.trace_dispatch\n        print 'bdb.Bdb.dispatch: unknown debugging event:', repr(event)\n        return self.trace_dispatch\n\n    def dispatch_line(self, frame):\n        if self.stop_here(frame) or self.break_here(frame):\n            self.user_line(frame)\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_call(self, frame, arg):\n        # XXX 'arg' is no longer used\n        if self.botframe is None:\n            # First call of dispatch since reset()\n            self.botframe = frame.f_back # (CT) Note that this may also be None!\n            return self.trace_dispatch\n        if not (self.stop_here(frame) or self.break_anywhere(frame)):\n            # No need to trace this function\n            return # None\n        self.user_call(frame, arg)\n        if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_return(self, frame, arg):\n        if self.stop_here(frame) or frame == self.returnframe:\n            try:\n                self.frame_returning = frame\n                self.user_return(frame, arg)\n            finally:\n                self.frame_returning = None\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_exception(self, frame, arg):\n        if self.stop_here(frame):\n            self.user_exception(frame, arg)\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    # Normally derived classes don't override the following\n    # methods, but they may if they want to redefine the\n    # definition of stopping and breakpoints.\n\n    def is_skipped_module(self, module_name):\n        for pattern in self.skip:\n            if fnmatch.fnmatch(module_name, pattern):\n                return True\n        return False\n\n    def stop_here(self, frame):\n        # (CT) stopframe may now also be None, see dispatch_call.\n        # (CT) the former test for None is therefore removed from here.\n        if self.skip and \\\n               self.is_skipped_module(frame.f_globals.get('__name__')):\n            return False\n        if frame is self.stopframe:\n            if self.stoplineno == -1:\n                return False\n            return frame.f_lineno >= self.stoplineno\n        while frame is not None and frame is not self.stopframe:\n            if frame is self.botframe:\n                return True\n            frame = frame.f_back\n        return False\n\n    def break_here(self, frame):\n        filename = self.canonic(frame.f_code.co_filename)\n        if not filename in self.breaks:\n            return False\n        lineno = frame.f_lineno\n        if not lineno in self.breaks[filename]:\n            # The line itself has no breakpoint, but maybe the line is the\n            # first line of a function with breakpoint set by function name.\n            lineno = frame.f_code.co_firstlineno\n            if not lineno in self.breaks[filename]:\n                return False\n\n        # flag says ok to delete temp. bp\n        (bp, flag) = effective(filename, lineno, frame)\n        if bp:\n            self.currentbp = bp.number\n            if (flag and bp.temporary):\n                self.do_clear(str(bp.number))\n            return True\n        else:\n            return False\n\n    def do_clear(self, arg):\n        raise NotImplementedError, \"subclass of bdb must implement do_clear()\"\n\n    def break_anywhere(self, frame):\n        return self.canonic(frame.f_code.co_filename) in self.breaks\n\n    # Derived classes should override the user_* methods\n    # to gain control.\n\n    def user_call(self, frame, argument_list):\n        \"\"\"This method is called when there is the remote possibility\n        that we ever need to stop in this function.\"\"\"\n        pass\n\n    def user_line(self, frame):\n        \"\"\"This method is called when we stop or break at this line.\"\"\"\n        pass\n\n    def user_return(self, frame, return_value):\n        \"\"\"This method is called when a return trap is set here.\"\"\"\n        pass\n\n    def user_exception(self, frame, exc_info):\n        exc_type, exc_value, exc_traceback = exc_info\n        \"\"\"This method is called if an exception occurs,\n        but only if we are to stop at or just below this level.\"\"\"\n        pass\n\n    def _set_stopinfo(self, stopframe, returnframe, stoplineno=0):\n        self.stopframe = stopframe\n        self.returnframe = returnframe\n        self.quitting = 0\n        # stoplineno >= 0 means: stop at line >= the stoplineno\n        # stoplineno -1 means: don't stop at all\n        self.stoplineno = stoplineno\n\n    # Derived classes and clients can call the following methods\n    # to affect the stepping state.\n\n    def set_until(self, frame): #the name \"until\" is borrowed from gdb\n        \"\"\"Stop when the line with the line no greater than the current one is\n        reached or when returning from current frame\"\"\"\n        self._set_stopinfo(frame, frame, frame.f_lineno+1)\n\n    def set_step(self):\n        \"\"\"Stop after one line of code.\"\"\"\n        # Issue #13183: pdb skips frames after hitting a breakpoint and running\n        # step commands.\n        # Restore the trace function in the caller (that may not have been set\n        # for performance reasons) when returning from the current frame.\n        if self.frame_returning:\n            caller_frame = self.frame_returning.f_back\n            if caller_frame and not caller_frame.f_trace:\n                caller_frame.f_trace = self.trace_dispatch\n        self._set_stopinfo(None, None)\n\n    def set_next(self, frame):\n        \"\"\"Stop on the next line in or below the given frame.\"\"\"\n        self._set_stopinfo(frame, None)\n\n    def set_return(self, frame):\n        \"\"\"Stop when returning from the given frame.\"\"\"\n        self._set_stopinfo(frame.f_back, frame)\n\n    def set_trace(self, frame=None):\n        \"\"\"Start debugging from `frame`.\n\n        If frame is not specified, debugging starts from caller's frame.\n        \"\"\"\n        if frame is None:\n            frame = sys._getframe().f_back\n        self.reset()\n        while frame:\n            frame.f_trace = self.trace_dispatch\n            self.botframe = frame\n            frame = frame.f_back\n        self.set_step()\n        sys.settrace(self.trace_dispatch)\n\n    def set_continue(self):\n        # Don't stop except at breakpoints or when finished\n        self._set_stopinfo(self.botframe, None, -1)\n        if not self.breaks:\n            # no breakpoints; run without debugger overhead\n            sys.settrace(None)\n            frame = sys._getframe().f_back\n            while frame and frame is not self.botframe:\n                del frame.f_trace\n                frame = frame.f_back\n\n    def set_quit(self):\n        self.stopframe = self.botframe\n        self.returnframe = None\n        self.quitting = 1\n        sys.settrace(None)\n\n    # Derived classes and clients can call the following methods\n    # to manipulate breakpoints.  These methods return an\n    # error message is something went wrong, None if all is well.\n    # Set_break prints out the breakpoint line and file:lineno.\n    # Call self.get_*break*() to see the breakpoints or better\n    # for bp in Breakpoint.bpbynumber: if bp: bp.bpprint().\n\n    def set_break(self, filename, lineno, temporary=0, cond = None,\n                  funcname=None):\n        filename = self.canonic(filename)\n        import linecache # Import as late as possible\n        line = linecache.getline(filename, lineno)\n        if not line:\n            return 'Line %s:%d does not exist' % (filename,\n                                   lineno)\n        if not filename in self.breaks:\n            self.breaks[filename] = []\n        list = self.breaks[filename]\n        if not lineno in list:\n            list.append(lineno)\n        bp = Breakpoint(filename, lineno, temporary, cond, funcname)\n\n    def _prune_breaks(self, filename, lineno):\n        if (filename, lineno) not in Breakpoint.bplist:\n            self.breaks[filename].remove(lineno)\n        if not self.breaks[filename]:\n            del self.breaks[filename]\n\n    def clear_break(self, filename, lineno):\n        filename = self.canonic(filename)\n        if not filename in self.breaks:\n            return 'There are no breakpoints in %s' % filename\n        if lineno not in self.breaks[filename]:\n            return 'There is no breakpoint at %s:%d' % (filename,\n                                    lineno)\n        # If there's only one bp in the list for that file,line\n        # pair, then remove the breaks entry\n        for bp in Breakpoint.bplist[filename, lineno][:]:\n            bp.deleteMe()\n        self._prune_breaks(filename, lineno)\n\n    def clear_bpbynumber(self, arg):\n        try:\n            number = int(arg)\n        except:\n            return 'Non-numeric breakpoint number (%s)' % arg\n        try:\n            bp = Breakpoint.bpbynumber[number]\n        except IndexError:\n            return 'Breakpoint number (%d) out of range' % number\n        if not bp:\n            return 'Breakpoint (%d) already deleted' % number\n        bp.deleteMe()\n        self._prune_breaks(bp.file, bp.line)\n\n    def clear_all_file_breaks(self, filename):\n        filename = self.canonic(filename)\n        if not filename in self.breaks:\n            return 'There are no breakpoints in %s' % filename\n        for line in self.breaks[filename]:\n            blist = Breakpoint.bplist[filename, line]\n            for bp in blist:\n                bp.deleteMe()\n        del self.breaks[filename]\n\n    def clear_all_breaks(self):\n        if not self.breaks:\n            return 'There are no breakpoints'\n        for bp in Breakpoint.bpbynumber:\n            if bp:\n                bp.deleteMe()\n        self.breaks = {}\n\n    def get_break(self, filename, lineno):\n        filename = self.canonic(filename)\n        return filename in self.breaks and \\\n            lineno in self.breaks[filename]\n\n    def get_breaks(self, filename, lineno):\n        filename = self.canonic(filename)\n        return filename in self.breaks and \\\n            lineno in self.breaks[filename] and \\\n            Breakpoint.bplist[filename, lineno] or []\n\n    def get_file_breaks(self, filename):\n        filename = self.canonic(filename)\n        if filename in self.breaks:\n            return self.breaks[filename]\n        else:\n            return []\n\n    def get_all_breaks(self):\n        return self.breaks\n\n    # Derived classes and clients can call the following method\n    # to get a data structure representing a stack trace.\n\n    def get_stack(self, f, t):\n        stack = []\n        if t and t.tb_frame is f:\n            t = t.tb_next\n        while f is not None:\n            stack.append((f, f.f_lineno))\n            if f is self.botframe:\n                break\n            f = f.f_back\n        stack.reverse()\n        i = max(0, len(stack) - 1)\n        while t is not None:\n            stack.append((t.tb_frame, t.tb_lineno))\n            t = t.tb_next\n        if f is None:\n            i = max(0, len(stack) - 1)\n        return stack, i\n\n    #\n\n    def format_stack_entry(self, frame_lineno, lprefix=': '):\n        import linecache, repr\n        frame, lineno = frame_lineno\n        filename = self.canonic(frame.f_code.co_filename)\n        s = '%s(%r)' % (filename, lineno)\n        if frame.f_code.co_name:\n            s = s + frame.f_code.co_name\n        else:\n            s = s + \"<lambda>\"\n        if '__args__' in frame.f_locals:\n            args = frame.f_locals['__args__']\n        else:\n            args = None\n        if args:\n            s = s + repr.repr(args)\n        else:\n            s = s + '()'\n        if '__return__' in frame.f_locals:\n            rv = frame.f_locals['__return__']\n            s = s + '->'\n            s = s + repr.repr(rv)\n        line = linecache.getline(filename, lineno, frame.f_globals)\n        if line: s = s + lprefix + line.strip()\n        return s\n\n    # The following two methods can be called by clients to use\n    # a debugger to debug a statement, given as a string.\n\n    def run(self, cmd, globals=None, locals=None):\n        if globals is None:\n            import __main__\n            globals = __main__.__dict__\n        if locals is None:\n            locals = globals\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        if not isinstance(cmd, types.CodeType):\n            cmd = cmd+'\\n'\n        try:\n            exec cmd in globals, locals\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n\n    def runeval(self, expr, globals=None, locals=None):\n        if globals is None:\n            import __main__\n            globals = __main__.__dict__\n        if locals is None:\n            locals = globals\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        if not isinstance(expr, types.CodeType):\n            expr = expr+'\\n'\n        try:\n            return eval(expr, globals, locals)\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n\n    def runctx(self, cmd, globals, locals):\n        # B/W compatibility\n        self.run(cmd, globals, locals)\n\n    # This method is more useful to debug a single function call.\n\n    def runcall(self, func, *args, **kwds):\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        res = None\n        try:\n            res = func(*args, **kwds)\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n        return res\n\n\ndef set_trace():\n    Bdb().set_trace()\n\n\nclass Breakpoint:\n\n    \"\"\"Breakpoint class\n\n    Implements temporary breakpoints, ignore counts, disabling and\n    (re)-enabling, and conditionals.\n\n    Breakpoints are indexed by number through bpbynumber and by\n    the file,line tuple using bplist.  The former points to a\n    single instance of class Breakpoint.  The latter points to a\n    list of such instances since there may be more than one\n    breakpoint per line.\n\n    \"\"\"\n\n    # XXX Keeping state in the class is a mistake -- this means\n    # you cannot have more than one active Bdb instance.\n\n    next = 1        # Next bp to be assigned\n    bplist = {}     # indexed by (file, lineno) tuple\n    bpbynumber = [None] # Each entry is None or an instance of Bpt\n                # index 0 is unused, except for marking an\n                # effective break .... see effective()\n\n    def __init__(self, file, line, temporary=0, cond=None, funcname=None):\n        self.funcname = funcname\n        # Needed if funcname is not None.\n        self.func_first_executable_line = None\n        self.file = file    # This better be in canonical form!\n        self.line = line\n        self.temporary = temporary\n        self.cond = cond\n        self.enabled = 1\n        self.ignore = 0\n        self.hits = 0\n        self.number = Breakpoint.next\n        Breakpoint.next = Breakpoint.next + 1\n        # Build the two lists\n        self.bpbynumber.append(self)\n        if (file, line) in self.bplist:\n            self.bplist[file, line].append(self)\n        else:\n            self.bplist[file, line] = [self]\n\n\n    def deleteMe(self):\n        index = (self.file, self.line)\n        self.bpbynumber[self.number] = None   # No longer in list\n        self.bplist[index].remove(self)\n        if not self.bplist[index]:\n            # No more bp for this f:l combo\n            del self.bplist[index]\n\n    def enable(self):\n        self.enabled = 1\n\n    def disable(self):\n        self.enabled = 0\n\n    def bpprint(self, out=None):\n        if out is None:\n            out = sys.stdout\n        if self.temporary:\n            disp = 'del  '\n        else:\n            disp = 'keep '\n        if self.enabled:\n            disp = disp + 'yes  '\n        else:\n            disp = disp + 'no   '\n        print >>out, '%-4dbreakpoint   %s at %s:%d' % (self.number, disp,\n                                                       self.file, self.line)\n        if self.cond:\n            print >>out, '\\tstop only if %s' % (self.cond,)\n        if self.ignore:\n            print >>out, '\\tignore next %d hits' % (self.ignore)\n        if (self.hits):\n            if (self.hits > 1): ss = 's'\n            else: ss = ''\n            print >>out, ('\\tbreakpoint already hit %d time%s' %\n                          (self.hits, ss))\n\n# -----------end of Breakpoint class----------\n\ndef checkfuncname(b, frame):\n    \"\"\"Check whether we should break here because of `b.funcname`.\"\"\"\n    if not b.funcname:\n        # Breakpoint was set via line number.\n        if b.line != frame.f_lineno:\n            # Breakpoint was set at a line with a def statement and the function\n            # defined is called: don't break.\n            return False\n        return True\n\n    # Breakpoint set via function name.\n\n    if frame.f_code.co_name != b.funcname:\n        # It's not a function call, but rather execution of def statement.\n        return False\n\n    # We are in the right frame.\n    if not b.func_first_executable_line:\n        # The function is entered for the 1st time.\n        b.func_first_executable_line = frame.f_lineno\n\n    if  b.func_first_executable_line != frame.f_lineno:\n        # But we are not at the first line number: don't break.\n        return False\n    return True\n\n# Determines if there is an effective (active) breakpoint at this\n# line of code.  Returns breakpoint number or 0 if none\ndef effective(file, line, frame):\n    \"\"\"Determine which breakpoint for this file:line is to be acted upon.\n\n    Called only if we know there is a bpt at this\n    location.  Returns breakpoint that was triggered and a flag\n    that indicates if it is ok to delete a temporary bp.\n\n    \"\"\"\n    possibles = Breakpoint.bplist[file,line]\n    for i in range(0, len(possibles)):\n        b = possibles[i]\n        if b.enabled == 0:\n            continue\n        if not checkfuncname(b, frame):\n            continue\n        # Count every hit when bp is enabled\n        b.hits = b.hits + 1\n        if not b.cond:\n            # If unconditional, and ignoring,\n            # go on to next, else break\n            if b.ignore > 0:\n                b.ignore = b.ignore -1\n                continue\n            else:\n                # breakpoint and marker that's ok\n                # to delete if temporary\n                return (b,1)\n        else:\n            # Conditional bp.\n            # Ignore count applies only to those bpt hits where the\n            # condition evaluates to true.\n            try:\n                val = eval(b.cond, frame.f_globals,\n                       frame.f_locals)\n                if val:\n                    if b.ignore > 0:\n                        b.ignore = b.ignore -1\n                        # continue\n                    else:\n                        return (b,1)\n                # else:\n                #   continue\n            except:\n                # if eval fails, most conservative\n                # thing is to stop on breakpoint\n                # regardless of ignore count.\n                # Don't delete temporary,\n                # as another hint to user.\n                return (b,0)\n    return (None, None)\n\n# -------------------- testing --------------------\n\nclass Tdb(Bdb):\n    def user_call(self, frame, args):\n        name = frame.f_code.co_name\n        if not name: name = '???'\n        print '+++ call', name, args\n    def user_line(self, frame):\n        import linecache\n        name = frame.f_code.co_name\n        if not name: name = '???'\n        fn = self.canonic(frame.f_code.co_filename)\n        line = linecache.getline(fn, frame.f_lineno, frame.f_globals)\n        print '+++', fn, frame.f_lineno, name, ':', line.strip()\n    def user_return(self, frame, retval):\n        print '+++ return', retval\n    def user_exception(self, frame, exc_stuff):\n        print '+++ exception', exc_stuff\n        self.set_continue()\n\ndef foo(n):\n    print 'foo(', n, ')'\n    x = bar(n*10)\n    print 'bar returned', x\n\ndef bar(a):\n    print 'bar(', a, ')'\n    return a/2\n\ndef test():\n    t = Tdb()\n    t.run('import bdb; bdb.foo(10)')\n\n# end\n", 
    "bisect": "\"\"\"Bisection algorithms.\"\"\"\n\ndef insort_right(a, x, lo=0, hi=None):\n    \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted.\n\n    If x is already in a, insert it to the right of the rightmost x.\n\n    Optional args lo (default 0) and hi (default len(a)) bound the\n    slice of a to be searched.\n    \"\"\"\n\n    if lo < 0:\n        raise ValueError('lo must be non-negative')\n    if hi is None:\n        hi = len(a)\n    while lo < hi:\n        mid = (lo+hi)//2\n        if x < a[mid]: hi = mid\n        else: lo = mid+1\n    a.insert(lo, x)\n\ninsort = insort_right   # backward compatibility\n\ndef bisect_right(a, x, lo=0, hi=None):\n    \"\"\"Return the index where to insert item x in list a, assuming a is sorted.\n\n    The return value i is such that all e in a[:i] have e <= x, and all e in\n    a[i:] have e > x.  So if x already appears in the list, a.insert(x) will\n    insert just after the rightmost x already there.\n\n    Optional args lo (default 0) and hi (default len(a)) bound the\n    slice of a to be searched.\n    \"\"\"\n\n    if lo < 0:\n        raise ValueError('lo must be non-negative')\n    if hi is None:\n        hi = len(a)\n    while lo < hi:\n        mid = (lo+hi)//2\n        if x < a[mid]: hi = mid\n        else: lo = mid+1\n    return lo\n\nbisect = bisect_right   # backward compatibility\n\ndef insort_left(a, x, lo=0, hi=None):\n    \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted.\n\n    If x is already in a, insert it to the left of the leftmost x.\n\n    Optional args lo (default 0) and hi (default len(a)) bound the\n    slice of a to be searched.\n    \"\"\"\n\n    if lo < 0:\n        raise ValueError('lo must be non-negative')\n    if hi is None:\n        hi = len(a)\n    while lo < hi:\n        mid = (lo+hi)//2\n        if a[mid] < x: lo = mid+1\n        else: hi = mid\n    a.insert(lo, x)\n\n\ndef bisect_left(a, x, lo=0, hi=None):\n    \"\"\"Return the index where to insert item x in list a, assuming a is sorted.\n\n    The return value i is such that all e in a[:i] have e < x, and all e in\n    a[i:] have e >= x.  So if x already appears in the list, a.insert(x) will\n    insert just before the leftmost x already there.\n\n    Optional args lo (default 0) and hi (default len(a)) bound the\n    slice of a to be searched.\n    \"\"\"\n\n    if lo < 0:\n        raise ValueError('lo must be non-negative')\n    if hi is None:\n        hi = len(a)\n    while lo < hi:\n        mid = (lo+hi)//2\n        if a[mid] < x: lo = mid+1\n        else: hi = mid\n    return lo\n\n# Overwrite above definitions with a fast C implementation\ntry:\n    from _bisect import *\nexcept ImportError:\n    pass\n", 
    "cPickle": "#\n# Reimplementation of cPickle, mostly as a copy of pickle.py\n#\n\nfrom pickle import Pickler, dump, dumps, PickleError, PicklingError, UnpicklingError, _EmptyClass\nfrom pickle import __doc__, __version__, format_version, compatible_formats\nfrom types import *\nfrom copy_reg import dispatch_table\nfrom copy_reg import _extension_registry, _inverted_registry, _extension_cache\nimport marshal, struct, sys\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n# These are purely informational; no code uses these.\nformat_version = \"2.0\"                  # File format version we write\ncompatible_formats = [\"1.0\",            # Original protocol 0\n                      \"1.1\",            # Protocol 0 with INST added\n                      \"1.2\",            # Original protocol 1\n                      \"1.3\",            # Protocol 1 with BINFLOAT added\n                      \"2.0\",            # Protocol 2\n                      ]                 # Old format versions we can read\n\n# Keep in synch with cPickle.  This is the highest protocol number we\n# know how to read.\nHIGHEST_PROTOCOL = 2\n\nBadPickleGet = KeyError\nUnpickleableError = PicklingError\n\nMARK            = ord('(')   # push special markobject on stack\nSTOP            = ord('.')   # every pickle ends with STOP\nPOP             = ord('0')   # discard topmost stack item\nPOP_MARK        = ord('1')   # discard stack top through topmost markobject\nDUP             = ord('2')   # duplicate top stack item\nFLOAT           = ord('F')   # push float object; decimal string argument\nINT             = ord('I')   # push integer or bool; decimal string argument\nBININT          = ord('J')   # push four-byte signed int\nBININT1         = ord('K')   # push 1-byte unsigned int\nLONG            = ord('L')   # push long; decimal string argument\nBININT2         = ord('M')   # push 2-byte unsigned int\nNONE            = ord('N')   # push None\nPERSID          = ord('P')   # push persistent object; id is taken from string arg\nBINPERSID       = ord('Q')   #  \"       \"         \"  ;  \"  \"   \"     \"  stack\nREDUCE          = ord('R')   # apply callable to argtuple, both on stack\nSTRING          = ord('S')   # push string; NL-terminated string argument\nBINSTRING       = ord('T')   # push string; counted binary string argument\nSHORT_BINSTRING = ord('U')   #  \"     \"   ;    \"      \"       \"      \" < 256 bytes\nUNICODE         = ord('V')   # push Unicode string; raw-unicode-escaped'd argument\nBINUNICODE      = ord('X')   #   \"     \"       \"  ; counted UTF-8 string argument\nAPPEND          = ord('a')   # append stack top to list below it\nBUILD           = ord('b')   # call __setstate__ or __dict__.update()\nGLOBAL          = ord('c')   # push self.find_class(modname, name); 2 string args\nDICT            = ord('d')   # build a dict from stack items\nEMPTY_DICT      = ord('}')   # push empty dict\nAPPENDS         = ord('e')   # extend list on stack by topmost stack slice\nGET             = ord('g')   # push item from memo on stack; index is string arg\nBINGET          = ord('h')   #   \"    \"    \"    \"   \"   \"  ;   \"    \" 1-byte arg\nINST            = ord('i')   # build & push class instance\nLONG_BINGET     = ord('j')   # push item from memo on stack; index is 4-byte arg\nLIST            = ord('l')   # build list from topmost stack items\nEMPTY_LIST      = ord(']')   # push empty list\nOBJ             = ord('o')   # build & push class instance\nPUT             = ord('p')   # store stack top in memo; index is string arg\nBINPUT          = ord('q')   #   \"     \"    \"   \"   \" ;   \"    \" 1-byte arg\nLONG_BINPUT     = ord('r')   #   \"     \"    \"   \"   \" ;   \"    \" 4-byte arg\nSETITEM         = ord('s')   # add key+value pair to dict\nTUPLE           = ord('t')   # build tuple from topmost stack items\nEMPTY_TUPLE     = ord(')')   # push empty tuple\nSETITEMS        = ord('u')   # modify dict by adding topmost key+value pairs\nBINFLOAT        = ord('G')   # push float; arg is 8-byte float encoding\n\nTRUE            = 'I01\\n'  # not an opcode; see INT docs in pickletools.py\nFALSE           = 'I00\\n'  # not an opcode; see INT docs in pickletools.py\n\n# Protocol 2\n\nPROTO           = ord('\\x80')  # identify pickle protocol\nNEWOBJ          = ord('\\x81')  # build object by applying cls.__new__ to argtuple\nEXT1            = ord('\\x82')  # push object from extension registry; 1-byte index\nEXT2            = ord('\\x83')  # ditto, but 2-byte index\nEXT4            = ord('\\x84')  # ditto, but 4-byte index\nTUPLE1          = ord('\\x85')  # build 1-tuple from stack top\nTUPLE2          = ord('\\x86')  # build 2-tuple from two topmost stack items\nTUPLE3          = ord('\\x87')  # build 3-tuple from three topmost stack items\nNEWTRUE         = ord('\\x88')  # push True\nNEWFALSE        = ord('\\x89')  # push False\nLONG1           = ord('\\x8a')  # push long from < 256 bytes\nLONG4           = ord('\\x8b')  # push really big long\n\n_tuplesize2code = [EMPTY_TUPLE, TUPLE1, TUPLE2, TUPLE3]\n\n\n# ____________________________________________________________\n# XXX some temporary dark magic to produce pickled dumps that are\n#     closer to the ones produced by cPickle in CPython\n\nfrom pickle import StringIO\n\nPythonPickler = Pickler\nclass Pickler(PythonPickler):\n    def __init__(self, *args, **kw):\n        self.__f = None\n        if len(args) == 1 and isinstance(args[0], int):\n            self.__f = StringIO()\n            PythonPickler.__init__(self, self.__f, args[0], **kw)\n        else:\n            PythonPickler.__init__(self, *args, **kw)\n\n    def memoize(self, obj):\n        self.memo[id(None)] = None   # cPickle starts counting at one\n        return PythonPickler.memoize(self, obj)\n\n    def getvalue(self):\n        return self.__f and self.__f.getvalue()\n\n@builtinify\ndef dump(obj, file, protocol=None):\n    Pickler(file, protocol).dump(obj)\n\n@builtinify\ndef dumps(obj, protocol=None):\n    file = StringIO()\n    Pickler(file, protocol).dump(obj)\n    return file.getvalue()\n\n# Why use struct.pack() for pickling but marshal.loads() for\n# unpickling?  struct.pack() is 40% faster than marshal.dumps(), but\n# marshal.loads() is twice as fast as struct.unpack()!\nmloads = marshal.loads\n\n# Unpickling machinery\n\nclass _Stack(list):\n    def pop(self, index=-1):\n        try:\n            return list.pop(self, index)\n        except IndexError:\n            raise UnpicklingError(\"unpickling stack underflow\")\n\nclass Unpickler(object):\n\n    def __init__(self, file):\n        \"\"\"This takes a file-like object for reading a pickle data stream.\n\n        The protocol version of the pickle is detected automatically, so no\n        proto argument is needed.\n\n        The file-like object must have two methods, a read() method that\n        takes an integer argument, and a readline() method that requires no\n        arguments.  Both methods should return a string.  Thus file-like\n        object can be a file object opened for reading, a StringIO object,\n        or any other custom object that meets this interface.\n        \"\"\"\n        self.readline = file.readline\n        self.read = file.read\n        self.memo = {}\n\n    def load(self):\n        \"\"\"Read a pickled object representation from the open file.\n\n        Return the reconstituted object hierarchy specified in the file.\n        \"\"\"\n        self.mark = object() # any new unique object\n        self.stack = _Stack()\n        self.append = self.stack.append\n        try:\n            key = ord(self.read(1))\n            while key != STOP:\n                self.dispatch[key](self)\n                key = ord(self.read(1))\n        except TypeError:\n            if self.read(1) == '':\n                raise EOFError\n            raise\n        return self.stack.pop()\n\n    # Return largest index k such that self.stack[k] is self.mark.\n    # If the stack doesn't contain a mark, eventually raises IndexError.\n    # This could be sped by maintaining another stack, of indices at which\n    # the mark appears.  For that matter, the latter stack would suffice,\n    # and we wouldn't need to push mark objects on self.stack at all.\n    # Doing so is probably a good thing, though, since if the pickle is\n    # corrupt (or hostile) we may get a clue from finding self.mark embedded\n    # in unpickled objects.\n    def marker(self):\n        k = len(self.stack)-1\n        while self.stack[k] is not self.mark: k -= 1\n        return k\n\n    dispatch = {}\n\n    def load_proto(self):\n        proto = ord(self.read(1))\n        if not 0 <= proto <= 2:\n            raise ValueError, \"unsupported pickle protocol: %d\" % proto\n    dispatch[PROTO] = load_proto\n\n    def load_persid(self):\n        pid = self.readline()[:-1]\n        self.append(self.persistent_load(pid))\n    dispatch[PERSID] = load_persid\n\n    def load_binpersid(self):\n        pid = self.stack.pop()\n        self.append(self.persistent_load(pid))\n    dispatch[BINPERSID] = load_binpersid\n\n    def load_none(self):\n        self.append(None)\n    dispatch[NONE] = load_none\n\n    def load_false(self):\n        self.append(False)\n    dispatch[NEWFALSE] = load_false\n\n    def load_true(self):\n        self.append(True)\n    dispatch[NEWTRUE] = load_true\n\n    def load_int(self):\n        data = self.readline()\n        if data == FALSE[1:]:\n            val = False\n        elif data == TRUE[1:]:\n            val = True\n        else:\n            val = int(data)\n        self.append(val)\n    dispatch[INT] = load_int\n\n    def load_binint(self):\n        self.append(mloads('i' + self.read(4)))\n    dispatch[BININT] = load_binint\n\n    def load_binint1(self):\n        self.append(ord(self.read(1)))\n    dispatch[BININT1] = load_binint1\n\n    def load_binint2(self):\n        self.append(mloads('i' + self.read(2) + '\\000\\000'))\n    dispatch[BININT2] = load_binint2\n\n    def load_long(self):\n        self.append(long(self.readline()[:-1], 0))\n    dispatch[LONG] = load_long\n\n    def load_long1(self):\n        n = ord(self.read(1))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG1] = load_long1\n\n    def load_long4(self):\n        n = mloads('i' + self.read(4))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG4] = load_long4\n\n    def load_float(self):\n        self.append(float(self.readline()[:-1]))\n    dispatch[FLOAT] = load_float\n\n    def load_binfloat(self, unpack=struct.unpack):\n        self.append(unpack('>d', self.read(8))[0])\n    dispatch[BINFLOAT] = load_binfloat\n\n    def load_string(self):\n        rep = self.readline()\n        if len(rep) < 3:\n            raise ValueError, \"insecure string pickle\"\n        if rep[0] == \"'\" == rep[-2]:\n            rep = rep[1:-2]\n        elif rep[0] == '\"' == rep[-2]:\n            rep = rep[1:-2]\n        else:\n            raise ValueError, \"insecure string pickle\"\n        self.append(rep.decode(\"string-escape\"))\n    dispatch[STRING] = load_string\n\n    def load_binstring(self):\n        L = mloads('i' + self.read(4))\n        self.append(self.read(L))\n    dispatch[BINSTRING] = load_binstring\n\n    def load_unicode(self):\n        self.append(unicode(self.readline()[:-1],'raw-unicode-escape'))\n    dispatch[UNICODE] = load_unicode\n\n    def load_binunicode(self):\n        L = mloads('i' + self.read(4))\n        self.append(unicode(self.read(L),'utf-8'))\n    dispatch[BINUNICODE] = load_binunicode\n\n    def load_short_binstring(self):\n        L = ord(self.read(1))\n        self.append(self.read(L))\n    dispatch[SHORT_BINSTRING] = load_short_binstring\n\n    def load_tuple(self):\n        k = self.marker()\n        self.stack[k:] = [tuple(self.stack[k+1:])]\n    dispatch[TUPLE] = load_tuple\n\n    def load_empty_tuple(self):\n        self.stack.append(())\n    dispatch[EMPTY_TUPLE] = load_empty_tuple\n\n    def load_tuple1(self):\n        self.stack[-1] = (self.stack[-1],)\n    dispatch[TUPLE1] = load_tuple1\n\n    def load_tuple2(self):\n        self.stack[-2:] = [(self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE2] = load_tuple2\n\n    def load_tuple3(self):\n        self.stack[-3:] = [(self.stack[-3], self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE3] = load_tuple3\n\n    def load_empty_list(self):\n        self.stack.append([])\n    dispatch[EMPTY_LIST] = load_empty_list\n\n    def load_empty_dictionary(self):\n        self.stack.append({})\n    dispatch[EMPTY_DICT] = load_empty_dictionary\n\n    def load_list(self):\n        k = self.marker()\n        self.stack[k:] = [self.stack[k+1:]]\n    dispatch[LIST] = load_list\n\n    def load_dict(self):\n        k = self.marker()\n        d = {}\n        items = self.stack[k+1:]\n        for i in range(0, len(items), 2):\n            key = items[i]\n            value = items[i+1]\n            d[key] = value\n        self.stack[k:] = [d]\n    dispatch[DICT] = load_dict\n\n    # INST and OBJ differ only in how they get a class object.  It's not\n    # only sensible to do the rest in a common routine, the two routines\n    # previously diverged and grew different bugs.\n    # klass is the class to instantiate, and k points to the topmost mark\n    # object, following which are the arguments for klass.__init__.\n    def _instantiate(self, klass, k):\n        args = tuple(self.stack[k+1:])\n        del self.stack[k:]\n        instantiated = 0\n        if (not args and\n                type(klass) is ClassType and\n                not hasattr(klass, \"__getinitargs__\")):\n            try:\n                value = _EmptyClass()\n                value.__class__ = klass\n                instantiated = 1\n            except RuntimeError:\n                # In restricted execution, assignment to inst.__class__ is\n                # prohibited\n                pass\n        if not instantiated:\n            try:\n                value = klass(*args)\n            except TypeError, err:\n                raise TypeError, \"in constructor for %s: %s\" % (\n                    klass.__name__, str(err)), sys.exc_info()[2]\n        self.append(value)\n\n    def load_inst(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self._instantiate(klass, self.marker())\n    dispatch[INST] = load_inst\n\n    def load_obj(self):\n        # Stack is ... markobject classobject arg1 arg2 ...\n        k = self.marker()\n        klass = self.stack.pop(k+1)\n        self._instantiate(klass, k)\n    dispatch[OBJ] = load_obj\n\n    def load_newobj(self):\n        args = self.stack.pop()\n        cls = self.stack[-1]\n        obj = cls.__new__(cls, *args)\n        self.stack[-1] = obj\n    dispatch[NEWOBJ] = load_newobj\n\n    def load_global(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self.append(klass)\n    dispatch[GLOBAL] = load_global\n\n    def load_ext1(self):\n        code = ord(self.read(1))\n        self.get_extension(code)\n    dispatch[EXT1] = load_ext1\n\n    def load_ext2(self):\n        code = mloads('i' + self.read(2) + '\\000\\000')\n        self.get_extension(code)\n    dispatch[EXT2] = load_ext2\n\n    def load_ext4(self):\n        code = mloads('i' + self.read(4))\n        self.get_extension(code)\n    dispatch[EXT4] = load_ext4\n\n    def get_extension(self, code):\n        nil = []\n        obj = _extension_cache.get(code, nil)\n        if obj is not nil:\n            self.append(obj)\n            return\n        key = _inverted_registry.get(code)\n        if not key:\n            raise ValueError(\"unregistered extension code %d\" % code)\n        obj = self.find_class(*key)\n        _extension_cache[code] = obj\n        self.append(obj)\n\n    def find_class(self, module, name):\n        # Subclasses may override this\n        __import__(module)\n        mod = sys.modules[module]\n        klass = getattr(mod, name)\n        return klass\n\n    def load_reduce(self):\n        args = self.stack.pop()\n        func = self.stack[-1]\n        value = self.stack[-1](*args)\n        self.stack[-1] = value\n    dispatch[REDUCE] = load_reduce\n\n    def load_pop(self):\n        del self.stack[-1]\n    dispatch[POP] = load_pop\n\n    def load_pop_mark(self):\n        k = self.marker()\n        del self.stack[k:]\n    dispatch[POP_MARK] = load_pop_mark\n\n    def load_dup(self):\n        self.append(self.stack[-1])\n    dispatch[DUP] = load_dup\n\n    def load_get(self):\n        self.append(self.memo[self.readline()[:-1]])\n    dispatch[GET] = load_get\n\n    def load_binget(self):\n        i = ord(self.read(1))\n        self.append(self.memo[repr(i)])\n    dispatch[BINGET] = load_binget\n\n    def load_long_binget(self):\n        i = mloads('i' + self.read(4))\n        self.append(self.memo[repr(i)])\n    dispatch[LONG_BINGET] = load_long_binget\n\n    def load_put(self):\n        self.memo[self.readline()[:-1]] = self.stack[-1]\n    dispatch[PUT] = load_put\n\n    def load_binput(self):\n        i = ord(self.read(1))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[BINPUT] = load_binput\n\n    def load_long_binput(self):\n        i = mloads('i' + self.read(4))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[LONG_BINPUT] = load_long_binput\n\n    def load_append(self):\n        value = self.stack.pop()\n        self.stack[-1].append(value)\n    dispatch[APPEND] = load_append\n\n    def load_appends(self):\n        stack = self.stack\n        mark = self.marker()\n        lst = stack[mark - 1]\n        lst.extend(stack[mark + 1:])\n        del stack[mark:]\n    dispatch[APPENDS] = load_appends\n\n    def load_setitem(self):\n        stack = self.stack\n        value = stack.pop()\n        key = stack.pop()\n        dict = stack[-1]\n        dict[key] = value\n    dispatch[SETITEM] = load_setitem\n\n    def load_setitems(self):\n        stack = self.stack\n        mark = self.marker()\n        dict = stack[mark - 1]\n        for i in range(mark + 1, len(stack), 2):\n            dict[stack[i]] = stack[i + 1]\n\n        del stack[mark:]\n    dispatch[SETITEMS] = load_setitems\n\n    def load_build(self):\n        stack = self.stack\n        state = stack.pop()\n        inst = stack[-1]\n        setstate = getattr(inst, \"__setstate__\", None)\n        if setstate:\n            setstate(state)\n            return\n        slotstate = None\n        if isinstance(state, tuple) and len(state) == 2:\n            state, slotstate = state\n        if state:\n            try:\n                d = inst.__dict__\n                try:\n                    for k, v in state.iteritems():\n                        d[intern(k)] = v\n                # keys in state don't have to be strings\n                # don't blow up, but don't go out of our way\n                except TypeError:\n                    d.update(state)\n\n            except RuntimeError:\n                # XXX In restricted execution, the instance's __dict__\n                # is not accessible.  Use the old way of unpickling\n                # the instance variables.  This is a semantic\n                # difference when unpickling in restricted\n                # vs. unrestricted modes.\n                # Note, however, that cPickle has never tried to do the\n                # .update() business, and always uses\n                #     PyObject_SetItem(inst.__dict__, key, value) in a\n                # loop over state.items().\n                for k, v in state.items():\n                    setattr(inst, k, v)\n        if slotstate:\n            for k, v in slotstate.items():\n                setattr(inst, k, v)\n    dispatch[BUILD] = load_build\n\n    def load_mark(self):\n        self.append(self.mark)\n    dispatch[MARK] = load_mark\n\n#from pickle import decode_long\n\ndef decode_long(data):\n    r\"\"\"Decode a long from a two's complement little-endian binary string.\n\n    >>> decode_long('')\n    0L\n    >>> decode_long(\"\\xff\\x00\")\n    255L\n    >>> decode_long(\"\\xff\\x7f\")\n    32767L\n    >>> decode_long(\"\\x00\\xff\")\n    -256L\n    >>> decode_long(\"\\x00\\x80\")\n    -32768L\n    >>> decode_long(\"\\x80\")\n    -128L\n    >>> decode_long(\"\\x7f\")\n    127L\n    \"\"\"\n\n    nbytes = len(data)\n    if nbytes == 0:\n        return 0L\n    ind = nbytes - 1\n    while ind and ord(data[ind]) == 0:\n        ind -= 1\n    n = ord(data[ind])\n    while ind:\n        n <<= 8\n        ind -= 1\n        if ord(data[ind]):\n            n += ord(data[ind])\n    if ord(data[nbytes - 1]) >= 128:\n        n -= 1L << (nbytes << 3)\n    return n\n\ndef load(f):\n    return Unpickler(f).load()\n\ndef loads(str):\n    f = StringIO(str)\n    return Unpickler(f).load()\n", 
    "cStringIO": "#\n# StringIO-based cStringIO implementation.\n#\n\n# Note that PyPy also contains a built-in module 'cStringIO' which will hide\n# this one if compiled in.\n\nfrom StringIO import *\nfrom StringIO import __doc__\n\nclass StringIO(StringIO):\n    def reset(self):\n        \"\"\"\n        reset() -- Reset the file position to the beginning\n        \"\"\"\n        self.seek(0, 0)\n", 
    "calendar": "\"\"\"Calendar printing functions\n\nNote when comparing these calendars to the ones printed by cal(1): By\ndefault, these calendars have Monday as the first day of the week, and\nSunday as the last (the European convention). Use setfirstweekday() to\nset the first day of the week (0=Monday, 6=Sunday).\"\"\"\n\nimport sys\nimport datetime\nimport locale as _locale\n\n__all__ = [\"IllegalMonthError\", \"IllegalWeekdayError\", \"setfirstweekday\",\n           \"firstweekday\", \"isleap\", \"leapdays\", \"weekday\", \"monthrange\",\n           \"monthcalendar\", \"prmonth\", \"month\", \"prcal\", \"calendar\",\n           \"timegm\", \"month_name\", \"month_abbr\", \"day_name\", \"day_abbr\"]\n\n# Exception raised for bad input (with string parameter for details)\nerror = ValueError\n\n# Exceptions raised for bad input\nclass IllegalMonthError(ValueError):\n    def __init__(self, month):\n        self.month = month\n    def __str__(self):\n        return \"bad month number %r; must be 1-12\" % self.month\n\n\nclass IllegalWeekdayError(ValueError):\n    def __init__(self, weekday):\n        self.weekday = weekday\n    def __str__(self):\n        return \"bad weekday number %r; must be 0 (Monday) to 6 (Sunday)\" % self.weekday\n\n\n# Constants for months referenced later\nJanuary = 1\nFebruary = 2\n\n# Number of days per month (except for February in leap years)\nmdays = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n\n# This module used to have hard-coded lists of day and month names, as\n# English strings.  The classes following emulate a read-only version of\n# that, but supply localized names.  Note that the values are computed\n# fresh on each call, in case the user changes locale between calls.\n\nclass _localized_month:\n\n    _months = [datetime.date(2001, i+1, 1).strftime for i in range(12)]\n    _months.insert(0, lambda x: \"\")\n\n    def __init__(self, format):\n        self.format = format\n\n    def __getitem__(self, i):\n        funcs = self._months[i]\n        if isinstance(i, slice):\n            return [f(self.format) for f in funcs]\n        else:\n            return funcs(self.format)\n\n    def __len__(self):\n        return 13\n\n\nclass _localized_day:\n\n    # January 1, 2001, was a Monday.\n    _days = [datetime.date(2001, 1, i+1).strftime for i in range(7)]\n\n    def __init__(self, format):\n        self.format = format\n\n    def __getitem__(self, i):\n        funcs = self._days[i]\n        if isinstance(i, slice):\n            return [f(self.format) for f in funcs]\n        else:\n            return funcs(self.format)\n\n    def __len__(self):\n        return 7\n\n\n# Full and abbreviated names of weekdays\nday_name = _localized_day('%A')\nday_abbr = _localized_day('%a')\n\n# Full and abbreviated names of months (1-based arrays!!!)\nmonth_name = _localized_month('%B')\nmonth_abbr = _localized_month('%b')\n\n# Constants for weekdays\n(MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, SUNDAY) = range(7)\n\n\ndef isleap(year):\n    \"\"\"Return True for leap years, False for non-leap years.\"\"\"\n    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n\n\ndef leapdays(y1, y2):\n    \"\"\"Return number of leap years in range [y1, y2).\n       Assume y1 <= y2.\"\"\"\n    y1 -= 1\n    y2 -= 1\n    return (y2//4 - y1//4) - (y2//100 - y1//100) + (y2//400 - y1//400)\n\n\ndef weekday(year, month, day):\n    \"\"\"Return weekday (0-6 ~ Mon-Sun) for year (1970-...), month (1-12),\n       day (1-31).\"\"\"\n    return datetime.date(year, month, day).weekday()\n\n\ndef monthrange(year, month):\n    \"\"\"Return weekday (0-6 ~ Mon-Sun) and number of days (28-31) for\n       year, month.\"\"\"\n    if not 1 <= month <= 12:\n        raise IllegalMonthError(month)\n    day1 = weekday(year, month, 1)\n    ndays = mdays[month] + (month == February and isleap(year))\n    return day1, ndays\n\n\nclass Calendar(object):\n    \"\"\"\n    Base calendar class. This class doesn't do any formatting. It simply\n    provides data to subclasses.\n    \"\"\"\n\n    def __init__(self, firstweekday=0):\n        self.firstweekday = firstweekday # 0 = Monday, 6 = Sunday\n\n    def getfirstweekday(self):\n        return self._firstweekday % 7\n\n    def setfirstweekday(self, firstweekday):\n        self._firstweekday = firstweekday\n\n    firstweekday = property(getfirstweekday, setfirstweekday)\n\n    def iterweekdays(self):\n        \"\"\"\n        Return a iterator for one week of weekday numbers starting with the\n        configured first one.\n        \"\"\"\n        for i in range(self.firstweekday, self.firstweekday + 7):\n            yield i%7\n\n    def itermonthdates(self, year, month):\n        \"\"\"\n        Return an iterator for one month. The iterator will yield datetime.date\n        values and will always iterate through complete weeks, so it will yield\n        dates outside the specified month.\n        \"\"\"\n        date = datetime.date(year, month, 1)\n        # Go back to the beginning of the week\n        days = (date.weekday() - self.firstweekday) % 7\n        date -= datetime.timedelta(days=days)\n        oneday = datetime.timedelta(days=1)\n        while True:\n            yield date\n            try:\n                date += oneday\n            except OverflowError:\n                # Adding one day could fail after datetime.MAXYEAR\n                break\n            if date.month != month and date.weekday() == self.firstweekday:\n                break\n\n    def itermonthdays2(self, year, month):\n        \"\"\"\n        Like itermonthdates(), but will yield (day number, weekday number)\n        tuples. For days outside the specified month the day number is 0.\n        \"\"\"\n        for date in self.itermonthdates(year, month):\n            if date.month != month:\n                yield (0, date.weekday())\n            else:\n                yield (date.day, date.weekday())\n\n    def itermonthdays(self, year, month):\n        \"\"\"\n        Like itermonthdates(), but will yield day numbers. For days outside\n        the specified month the day number is 0.\n        \"\"\"\n        for date in self.itermonthdates(year, month):\n            if date.month != month:\n                yield 0\n            else:\n                yield date.day\n\n    def monthdatescalendar(self, year, month):\n        \"\"\"\n        Return a matrix (list of lists) representing a month's calendar.\n        Each row represents a week; week entries are datetime.date values.\n        \"\"\"\n        dates = list(self.itermonthdates(year, month))\n        return [ dates[i:i+7] for i in range(0, len(dates), 7) ]\n\n    def monthdays2calendar(self, year, month):\n        \"\"\"\n        Return a matrix representing a month's calendar.\n        Each row represents a week; week entries are\n        (day number, weekday number) tuples. Day numbers outside this month\n        are zero.\n        \"\"\"\n        days = list(self.itermonthdays2(year, month))\n        return [ days[i:i+7] for i in range(0, len(days), 7) ]\n\n    def monthdayscalendar(self, year, month):\n        \"\"\"\n        Return a matrix representing a month's calendar.\n        Each row represents a week; days outside this month are zero.\n        \"\"\"\n        days = list(self.itermonthdays(year, month))\n        return [ days[i:i+7] for i in range(0, len(days), 7) ]\n\n    def yeardatescalendar(self, year, width=3):\n        \"\"\"\n        Return the data for the specified year ready for formatting. The return\n        value is a list of month rows. Each month row contains up to width months.\n        Each month contains between 4 and 6 weeks and each week contains 1-7\n        days. Days are datetime.date objects.\n        \"\"\"\n        months = [\n            self.monthdatescalendar(year, i)\n            for i in range(January, January+12)\n        ]\n        return [months[i:i+width] for i in range(0, len(months), width) ]\n\n    def yeardays2calendar(self, year, width=3):\n        \"\"\"\n        Return the data for the specified year ready for formatting (similar to\n        yeardatescalendar()). Entries in the week lists are\n        (day number, weekday number) tuples. Day numbers outside this month are\n        zero.\n        \"\"\"\n        months = [\n            self.monthdays2calendar(year, i)\n            for i in range(January, January+12)\n        ]\n        return [months[i:i+width] for i in range(0, len(months), width) ]\n\n    def yeardayscalendar(self, year, width=3):\n        \"\"\"\n        Return the data for the specified year ready for formatting (similar to\n        yeardatescalendar()). Entries in the week lists are day numbers.\n        Day numbers outside this month are zero.\n        \"\"\"\n        months = [\n            self.monthdayscalendar(year, i)\n            for i in range(January, January+12)\n        ]\n        return [months[i:i+width] for i in range(0, len(months), width) ]\n\n\nclass TextCalendar(Calendar):\n    \"\"\"\n    Subclass of Calendar that outputs a calendar as a simple plain text\n    similar to the UNIX program cal.\n    \"\"\"\n\n    def prweek(self, theweek, width):\n        \"\"\"\n        Print a single week (no newline).\n        \"\"\"\n        print self.formatweek(theweek, width),\n\n    def formatday(self, day, weekday, width):\n        \"\"\"\n        Returns a formatted day.\n        \"\"\"\n        if day == 0:\n            s = ''\n        else:\n            s = '%2i' % day             # right-align single-digit days\n        return s.center(width)\n\n    def formatweek(self, theweek, width):\n        \"\"\"\n        Returns a single week in a string (no newline).\n        \"\"\"\n        return ' '.join(self.formatday(d, wd, width) for (d, wd) in theweek)\n\n    def formatweekday(self, day, width):\n        \"\"\"\n        Returns a formatted week day name.\n        \"\"\"\n        if width >= 9:\n            names = day_name\n        else:\n            names = day_abbr\n        return names[day][:width].center(width)\n\n    def formatweekheader(self, width):\n        \"\"\"\n        Return a header for a week.\n        \"\"\"\n        return ' '.join(self.formatweekday(i, width) for i in self.iterweekdays())\n\n    def formatmonthname(self, theyear, themonth, width, withyear=True):\n        \"\"\"\n        Return a formatted month name.\n        \"\"\"\n        s = month_name[themonth]\n        if withyear:\n            s = \"%s %r\" % (s, theyear)\n        return s.center(width)\n\n    def prmonth(self, theyear, themonth, w=0, l=0):\n        \"\"\"\n        Print a month's calendar.\n        \"\"\"\n        print self.formatmonth(theyear, themonth, w, l),\n\n    def formatmonth(self, theyear, themonth, w=0, l=0):\n        \"\"\"\n        Return a month's calendar string (multi-line).\n        \"\"\"\n        w = max(2, w)\n        l = max(1, l)\n        s = self.formatmonthname(theyear, themonth, 7 * (w + 1) - 1)\n        s = s.rstrip()\n        s += '\\n' * l\n        s += self.formatweekheader(w).rstrip()\n        s += '\\n' * l\n        for week in self.monthdays2calendar(theyear, themonth):\n            s += self.formatweek(week, w).rstrip()\n            s += '\\n' * l\n        return s\n\n    def formatyear(self, theyear, w=2, l=1, c=6, m=3):\n        \"\"\"\n        Returns a year's calendar as a multi-line string.\n        \"\"\"\n        w = max(2, w)\n        l = max(1, l)\n        c = max(2, c)\n        colwidth = (w + 1) * 7 - 1\n        v = []\n        a = v.append\n        a(repr(theyear).center(colwidth*m+c*(m-1)).rstrip())\n        a('\\n'*l)\n        header = self.formatweekheader(w)\n        for (i, row) in enumerate(self.yeardays2calendar(theyear, m)):\n            # months in this row\n            months = range(m*i+1, min(m*(i+1)+1, 13))\n            a('\\n'*l)\n            names = (self.formatmonthname(theyear, k, colwidth, False)\n                     for k in months)\n            a(formatstring(names, colwidth, c).rstrip())\n            a('\\n'*l)\n            headers = (header for k in months)\n            a(formatstring(headers, colwidth, c).rstrip())\n            a('\\n'*l)\n            # max number of weeks for this row\n            height = max(len(cal) for cal in row)\n            for j in range(height):\n                weeks = []\n                for cal in row:\n                    if j >= len(cal):\n                        weeks.append('')\n                    else:\n                        weeks.append(self.formatweek(cal[j], w))\n                a(formatstring(weeks, colwidth, c).rstrip())\n                a('\\n' * l)\n        return ''.join(v)\n\n    def pryear(self, theyear, w=0, l=0, c=6, m=3):\n        \"\"\"Print a year's calendar.\"\"\"\n        print self.formatyear(theyear, w, l, c, m)\n\n\nclass HTMLCalendar(Calendar):\n    \"\"\"\n    This calendar returns complete HTML pages.\n    \"\"\"\n\n    # CSS classes for the day <td>s\n    cssclasses = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n\n    def formatday(self, day, weekday):\n        \"\"\"\n        Return a day as a table cell.\n        \"\"\"\n        if day == 0:\n            return '<td class=\"noday\">&nbsp;</td>' # day outside month\n        else:\n            return '<td class=\"%s\">%d</td>' % (self.cssclasses[weekday], day)\n\n    def formatweek(self, theweek):\n        \"\"\"\n        Return a complete week as a table row.\n        \"\"\"\n        s = ''.join(self.formatday(d, wd) for (d, wd) in theweek)\n        return '<tr>%s</tr>' % s\n\n    def formatweekday(self, day):\n        \"\"\"\n        Return a weekday name as a table header.\n        \"\"\"\n        return '<th class=\"%s\">%s</th>' % (self.cssclasses[day], day_abbr[day])\n\n    def formatweekheader(self):\n        \"\"\"\n        Return a header for a week as a table row.\n        \"\"\"\n        s = ''.join(self.formatweekday(i) for i in self.iterweekdays())\n        return '<tr>%s</tr>' % s\n\n    def formatmonthname(self, theyear, themonth, withyear=True):\n        \"\"\"\n        Return a month name as a table row.\n        \"\"\"\n        if withyear:\n            s = '%s %s' % (month_name[themonth], theyear)\n        else:\n            s = '%s' % month_name[themonth]\n        return '<tr><th colspan=\"7\" class=\"month\">%s</th></tr>' % s\n\n    def formatmonth(self, theyear, themonth, withyear=True):\n        \"\"\"\n        Return a formatted month as a table.\n        \"\"\"\n        v = []\n        a = v.append\n        a('<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"month\">')\n        a('\\n')\n        a(self.formatmonthname(theyear, themonth, withyear=withyear))\n        a('\\n')\n        a(self.formatweekheader())\n        a('\\n')\n        for week in self.monthdays2calendar(theyear, themonth):\n            a(self.formatweek(week))\n            a('\\n')\n        a('</table>')\n        a('\\n')\n        return ''.join(v)\n\n    def formatyear(self, theyear, width=3):\n        \"\"\"\n        Return a formatted year as a table of tables.\n        \"\"\"\n        v = []\n        a = v.append\n        width = max(width, 1)\n        a('<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"year\">')\n        a('\\n')\n        a('<tr><th colspan=\"%d\" class=\"year\">%s</th></tr>' % (width, theyear))\n        for i in range(January, January+12, width):\n            # months in this row\n            months = range(i, min(i+width, 13))\n            a('<tr>')\n            for m in months:\n                a('<td>')\n                a(self.formatmonth(theyear, m, withyear=False))\n                a('</td>')\n            a('</tr>')\n        a('</table>')\n        return ''.join(v)\n\n    def formatyearpage(self, theyear, width=3, css='calendar.css', encoding=None):\n        \"\"\"\n        Return a formatted year as a complete HTML page.\n        \"\"\"\n        if encoding is None:\n            encoding = sys.getdefaultencoding()\n        v = []\n        a = v.append\n        a('<?xml version=\"1.0\" encoding=\"%s\"?>\\n' % encoding)\n        a('<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\\n')\n        a('<html>\\n')\n        a('<head>\\n')\n        a('<meta http-equiv=\"Content-Type\" content=\"text/html; charset=%s\" />\\n' % encoding)\n        if css is not None:\n            a('<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />\\n' % css)\n        a('<title>Calendar for %d</title>\\n' % theyear)\n        a('</head>\\n')\n        a('<body>\\n')\n        a(self.formatyear(theyear, width))\n        a('</body>\\n')\n        a('</html>\\n')\n        return ''.join(v).encode(encoding, \"xmlcharrefreplace\")\n\n\nclass TimeEncoding:\n    def __init__(self, locale):\n        self.locale = locale\n\n    def __enter__(self):\n        self.oldlocale = _locale.getlocale(_locale.LC_TIME)\n        _locale.setlocale(_locale.LC_TIME, self.locale)\n        return _locale.getlocale(_locale.LC_TIME)[1]\n\n    def __exit__(self, *args):\n        _locale.setlocale(_locale.LC_TIME, self.oldlocale)\n\n\nclass LocaleTextCalendar(TextCalendar):\n    \"\"\"\n    This class can be passed a locale name in the constructor and will return\n    month and weekday names in the specified locale. If this locale includes\n    an encoding all strings containing month and weekday names will be returned\n    as unicode.\n    \"\"\"\n\n    def __init__(self, firstweekday=0, locale=None):\n        TextCalendar.__init__(self, firstweekday)\n        if locale is None:\n            locale = _locale.getdefaultlocale()\n        self.locale = locale\n\n    def formatweekday(self, day, width):\n        with TimeEncoding(self.locale) as encoding:\n            if width >= 9:\n                names = day_name\n            else:\n                names = day_abbr\n            name = names[day]\n            if encoding is not None:\n                name = name.decode(encoding)\n            return name[:width].center(width)\n\n    def formatmonthname(self, theyear, themonth, width, withyear=True):\n        with TimeEncoding(self.locale) as encoding:\n            s = month_name[themonth]\n            if encoding is not None:\n                s = s.decode(encoding)\n            if withyear:\n                s = \"%s %r\" % (s, theyear)\n            return s.center(width)\n\n\nclass LocaleHTMLCalendar(HTMLCalendar):\n    \"\"\"\n    This class can be passed a locale name in the constructor and will return\n    month and weekday names in the specified locale. If this locale includes\n    an encoding all strings containing month and weekday names will be returned\n    as unicode.\n    \"\"\"\n    def __init__(self, firstweekday=0, locale=None):\n        HTMLCalendar.__init__(self, firstweekday)\n        if locale is None:\n            locale = _locale.getdefaultlocale()\n        self.locale = locale\n\n    def formatweekday(self, day):\n        with TimeEncoding(self.locale) as encoding:\n            s = day_abbr[day]\n            if encoding is not None:\n                s = s.decode(encoding)\n            return '<th class=\"%s\">%s</th>' % (self.cssclasses[day], s)\n\n    def formatmonthname(self, theyear, themonth, withyear=True):\n        with TimeEncoding(self.locale) as encoding:\n            s = month_name[themonth]\n            if encoding is not None:\n                s = s.decode(encoding)\n            if withyear:\n                s = '%s %s' % (s, theyear)\n            return '<tr><th colspan=\"7\" class=\"month\">%s</th></tr>' % s\n\n\n# Support for old module level interface\nc = TextCalendar()\n\nfirstweekday = c.getfirstweekday\n\ndef setfirstweekday(firstweekday):\n    try:\n        firstweekday.__index__\n    except AttributeError:\n        raise IllegalWeekdayError(firstweekday)\n    if not MONDAY <= firstweekday <= SUNDAY:\n        raise IllegalWeekdayError(firstweekday)\n    c.firstweekday = firstweekday\n\nmonthcalendar = c.monthdayscalendar\nprweek = c.prweek\nweek = c.formatweek\nweekheader = c.formatweekheader\nprmonth = c.prmonth\nmonth = c.formatmonth\ncalendar = c.formatyear\nprcal = c.pryear\n\n\n# Spacing of month columns for multi-column year calendar\n_colwidth = 7*3 - 1         # Amount printed by prweek()\n_spacing = 6                # Number of spaces between columns\n\n\ndef format(cols, colwidth=_colwidth, spacing=_spacing):\n    \"\"\"Prints multi-column formatting for year calendars\"\"\"\n    print formatstring(cols, colwidth, spacing)\n\n\ndef formatstring(cols, colwidth=_colwidth, spacing=_spacing):\n    \"\"\"Returns a string formatted from n strings, centered within n columns.\"\"\"\n    spacing *= ' '\n    return spacing.join(c.center(colwidth) for c in cols)\n\n\nEPOCH = 1970\n_EPOCH_ORD = datetime.date(EPOCH, 1, 1).toordinal()\n\n\ndef timegm(tuple):\n    \"\"\"Unrelated but handy function to calculate Unix timestamp from GMT.\"\"\"\n    year, month, day, hour, minute, second = tuple[:6]\n    days = datetime.date(year, month, 1).toordinal() - _EPOCH_ORD + day - 1\n    hours = days*24 + hour\n    minutes = hours*60 + minute\n    seconds = minutes*60 + second\n    return seconds\n\n\ndef main(args):\n    import optparse\n    parser = optparse.OptionParser(usage=\"usage: %prog [options] [year [month]]\")\n    parser.add_option(\n        \"-w\", \"--width\",\n        dest=\"width\", type=\"int\", default=2,\n        help=\"width of date column (default 2, text only)\"\n    )\n    parser.add_option(\n        \"-l\", \"--lines\",\n        dest=\"lines\", type=\"int\", default=1,\n        help=\"number of lines for each week (default 1, text only)\"\n    )\n    parser.add_option(\n        \"-s\", \"--spacing\",\n        dest=\"spacing\", type=\"int\", default=6,\n        help=\"spacing between months (default 6, text only)\"\n    )\n    parser.add_option(\n        \"-m\", \"--months\",\n        dest=\"months\", type=\"int\", default=3,\n        help=\"months per row (default 3, text only)\"\n    )\n    parser.add_option(\n        \"-c\", \"--css\",\n        dest=\"css\", default=\"calendar.css\",\n        help=\"CSS to use for page (html only)\"\n    )\n    parser.add_option(\n        \"-L\", \"--locale\",\n        dest=\"locale\", default=None,\n        help=\"locale to be used from month and weekday names\"\n    )\n    parser.add_option(\n        \"-e\", \"--encoding\",\n        dest=\"encoding\", default=None,\n        help=\"Encoding to use for output\"\n    )\n    parser.add_option(\n        \"-t\", \"--type\",\n        dest=\"type\", default=\"text\",\n        choices=(\"text\", \"html\"),\n        help=\"output type (text or html)\"\n    )\n\n    (options, args) = parser.parse_args(args)\n\n    if options.locale and not options.encoding:\n        parser.error(\"if --locale is specified --encoding is required\")\n        sys.exit(1)\n\n    locale = options.locale, options.encoding\n\n    if options.type == \"html\":\n        if options.locale:\n            cal = LocaleHTMLCalendar(locale=locale)\n        else:\n            cal = HTMLCalendar()\n        encoding = options.encoding\n        if encoding is None:\n            encoding = sys.getdefaultencoding()\n        optdict = dict(encoding=encoding, css=options.css)\n        if len(args) == 1:\n            print cal.formatyearpage(datetime.date.today().year, **optdict)\n        elif len(args) == 2:\n            print cal.formatyearpage(int(args[1]), **optdict)\n        else:\n            parser.error(\"incorrect number of arguments\")\n            sys.exit(1)\n    else:\n        if options.locale:\n            cal = LocaleTextCalendar(locale=locale)\n        else:\n            cal = TextCalendar()\n        optdict = dict(w=options.width, l=options.lines)\n        if len(args) != 3:\n            optdict[\"c\"] = options.spacing\n            optdict[\"m\"] = options.months\n        if len(args) == 1:\n            result = cal.formatyear(datetime.date.today().year, **optdict)\n        elif len(args) == 2:\n            result = cal.formatyear(int(args[1]), **optdict)\n        elif len(args) == 3:\n            result = cal.formatmonth(int(args[1]), int(args[2]), **optdict)\n        else:\n            parser.error(\"incorrect number of arguments\")\n            sys.exit(1)\n        if options.encoding:\n            result = result.encode(options.encoding)\n        print result\n\n\nif __name__ == \"__main__\":\n    main(sys.argv)\n", 
    "cmd": "\"\"\"A generic class to build line-oriented command interpreters.\n\nInterpreters constructed with this class obey the following conventions:\n\n1. End of file on input is processed as the command 'EOF'.\n2. A command is parsed out of each line by collecting the prefix composed\n   of characters in the identchars member.\n3. A command `foo' is dispatched to a method 'do_foo()'; the do_ method\n   is passed a single argument consisting of the remainder of the line.\n4. Typing an empty line repeats the last command.  (Actually, it calls the\n   method `emptyline', which may be overridden in a subclass.)\n5. There is a predefined `help' method.  Given an argument `topic', it\n   calls the command `help_topic'.  With no arguments, it lists all topics\n   with defined help_ functions, broken into up to three topics; documented\n   commands, miscellaneous help topics, and undocumented commands.\n6. The command '?' is a synonym for `help'.  The command '!' is a synonym\n   for `shell', if a do_shell method exists.\n7. If completion is enabled, completing commands will be done automatically,\n   and completing of commands args is done by calling complete_foo() with\n   arguments text, line, begidx, endidx.  text is string we are matching\n   against, all returned matches must begin with it.  line is the current\n   input line (lstripped), begidx and endidx are the beginning and end\n   indexes of the text being matched, which could be used to provide\n   different completion depending upon which position the argument is in.\n\nThe `default' method may be overridden to intercept commands for which there\nis no do_ method.\n\nThe `completedefault' method may be overridden to intercept completions for\ncommands that have no complete_ method.\n\nThe data member `self.ruler' sets the character used to draw separator lines\nin the help messages.  If empty, no ruler line is drawn.  It defaults to \"=\".\n\nIf the value of `self.intro' is nonempty when the cmdloop method is called,\nit is printed out on interpreter startup.  This value may be overridden\nvia an optional argument to the cmdloop() method.\n\nThe data members `self.doc_header', `self.misc_header', and\n`self.undoc_header' set the headers used for the help function's\nlistings of documented functions, miscellaneous topics, and undocumented\nfunctions respectively.\n\nThese interpreters use raw_input; thus, if the readline module is loaded,\nthey automatically support Emacs-like command history and editing features.\n\"\"\"\n\nimport string\n\n__all__ = [\"Cmd\"]\n\nPROMPT = '(Cmd) '\nIDENTCHARS = string.ascii_letters + string.digits + '_'\n\nclass Cmd:\n    \"\"\"A simple framework for writing line-oriented command interpreters.\n\n    These are often useful for test harnesses, administrative tools, and\n    prototypes that will later be wrapped in a more sophisticated interface.\n\n    A Cmd instance or subclass instance is a line-oriented interpreter\n    framework.  There is no good reason to instantiate Cmd itself; rather,\n    it's useful as a superclass of an interpreter class you define yourself\n    in order to inherit Cmd's methods and encapsulate action methods.\n\n    \"\"\"\n    prompt = PROMPT\n    identchars = IDENTCHARS\n    ruler = '='\n    lastcmd = ''\n    intro = None\n    doc_leader = \"\"\n    doc_header = \"Documented commands (type help <topic>):\"\n    misc_header = \"Miscellaneous help topics:\"\n    undoc_header = \"Undocumented commands:\"\n    nohelp = \"*** No help on %s\"\n    use_rawinput = 1\n\n    def __init__(self, completekey='tab', stdin=None, stdout=None):\n        \"\"\"Instantiate a line-oriented interpreter framework.\n\n        The optional argument 'completekey' is the readline name of a\n        completion key; it defaults to the Tab key. If completekey is\n        not None and the readline module is available, command completion\n        is done automatically. The optional arguments stdin and stdout\n        specify alternate input and output file objects; if not specified,\n        sys.stdin and sys.stdout are used.\n\n        \"\"\"\n        import sys\n        if stdin is not None:\n            self.stdin = stdin\n        else:\n            self.stdin = sys.stdin\n        if stdout is not None:\n            self.stdout = stdout\n        else:\n            self.stdout = sys.stdout\n        self.cmdqueue = []\n        self.completekey = completekey\n\n    def cmdloop(self, intro=None):\n        \"\"\"Repeatedly issue a prompt, accept input, parse an initial prefix\n        off the received input, and dispatch to action methods, passing them\n        the remainder of the line as argument.\n\n        \"\"\"\n\n        self.preloop()\n        if self.use_rawinput and self.completekey:\n            try:\n                import readline\n                self.old_completer = readline.get_completer()\n                readline.set_completer(self.complete)\n                readline.parse_and_bind(self.completekey+\": complete\")\n            except ImportError:\n                pass\n        try:\n            if intro is not None:\n                self.intro = intro\n            if self.intro:\n                self.stdout.write(str(self.intro)+\"\\n\")\n            stop = None\n            while not stop:\n                if self.cmdqueue:\n                    line = self.cmdqueue.pop(0)\n                else:\n                    if self.use_rawinput:\n                        try:\n                            line = raw_input(self.prompt)\n                        except EOFError:\n                            line = 'EOF'\n                    else:\n                        self.stdout.write(self.prompt)\n                        self.stdout.flush()\n                        line = self.stdin.readline()\n                        if not len(line):\n                            line = 'EOF'\n                        else:\n                            line = line.rstrip('\\r\\n')\n                line = self.precmd(line)\n                stop = self.onecmd(line)\n                stop = self.postcmd(stop, line)\n            self.postloop()\n        finally:\n            if self.use_rawinput and self.completekey:\n                try:\n                    import readline\n                    readline.set_completer(self.old_completer)\n                except ImportError:\n                    pass\n\n\n    def precmd(self, line):\n        \"\"\"Hook method executed just before the command line is\n        interpreted, but after the input prompt is generated and issued.\n\n        \"\"\"\n        return line\n\n    def postcmd(self, stop, line):\n        \"\"\"Hook method executed just after a command dispatch is finished.\"\"\"\n        return stop\n\n    def preloop(self):\n        \"\"\"Hook method executed once when the cmdloop() method is called.\"\"\"\n        pass\n\n    def postloop(self):\n        \"\"\"Hook method executed once when the cmdloop() method is about to\n        return.\n\n        \"\"\"\n        pass\n\n    def parseline(self, line):\n        \"\"\"Parse the line into a command name and a string containing\n        the arguments.  Returns a tuple containing (command, args, line).\n        'command' and 'args' may be None if the line couldn't be parsed.\n        \"\"\"\n        line = line.strip()\n        if not line:\n            return None, None, line\n        elif line[0] == '?':\n            line = 'help ' + line[1:]\n        elif line[0] == '!':\n            if hasattr(self, 'do_shell'):\n                line = 'shell ' + line[1:]\n            else:\n                return None, None, line\n        i, n = 0, len(line)\n        while i < n and line[i] in self.identchars: i = i+1\n        cmd, arg = line[:i], line[i:].strip()\n        return cmd, arg, line\n\n    def onecmd(self, line):\n        \"\"\"Interpret the argument as though it had been typed in response\n        to the prompt.\n\n        This may be overridden, but should not normally need to be;\n        see the precmd() and postcmd() methods for useful execution hooks.\n        The return value is a flag indicating whether interpretation of\n        commands by the interpreter should stop.\n\n        \"\"\"\n        cmd, arg, line = self.parseline(line)\n        if not line:\n            return self.emptyline()\n        if cmd is None:\n            return self.default(line)\n        self.lastcmd = line\n        if line == 'EOF' :\n            self.lastcmd = ''\n        if cmd == '':\n            return self.default(line)\n        else:\n            try:\n                func = getattr(self, 'do_' + cmd)\n            except AttributeError:\n                return self.default(line)\n            return func(arg)\n\n    def emptyline(self):\n        \"\"\"Called when an empty line is entered in response to the prompt.\n\n        If this method is not overridden, it repeats the last nonempty\n        command entered.\n\n        \"\"\"\n        if self.lastcmd:\n            return self.onecmd(self.lastcmd)\n\n    def default(self, line):\n        \"\"\"Called on an input line when the command prefix is not recognized.\n\n        If this method is not overridden, it prints an error message and\n        returns.\n\n        \"\"\"\n        self.stdout.write('*** Unknown syntax: %s\\n'%line)\n\n    def completedefault(self, *ignored):\n        \"\"\"Method called to complete an input line when no command-specific\n        complete_*() method is available.\n\n        By default, it returns an empty list.\n\n        \"\"\"\n        return []\n\n    def completenames(self, text, *ignored):\n        dotext = 'do_'+text\n        return [a[3:] for a in self.get_names() if a.startswith(dotext)]\n\n    def complete(self, text, state):\n        \"\"\"Return the next possible completion for 'text'.\n\n        If a command has not been entered, then complete against command list.\n        Otherwise try to call complete_<command> to get list of completions.\n        \"\"\"\n        if state == 0:\n            import readline\n            origline = readline.get_line_buffer()\n            line = origline.lstrip()\n            stripped = len(origline) - len(line)\n            begidx = readline.get_begidx() - stripped\n            endidx = readline.get_endidx() - stripped\n            if begidx>0:\n                cmd, args, foo = self.parseline(line)\n                if cmd == '':\n                    compfunc = self.completedefault\n                else:\n                    try:\n                        compfunc = getattr(self, 'complete_' + cmd)\n                    except AttributeError:\n                        compfunc = self.completedefault\n            else:\n                compfunc = self.completenames\n            self.completion_matches = compfunc(text, line, begidx, endidx)\n        try:\n            return self.completion_matches[state]\n        except IndexError:\n            return None\n\n    def get_names(self):\n        # This method used to pull in base class attributes\n        # at a time dir() didn't do it yet.\n        return dir(self.__class__)\n\n    def complete_help(self, *args):\n        commands = set(self.completenames(*args))\n        topics = set(a[5:] for a in self.get_names()\n                     if a.startswith('help_' + args[0]))\n        return list(commands | topics)\n\n    def do_help(self, arg):\n        'List available commands with \"help\" or detailed help with \"help cmd\".'\n        if arg:\n            # XXX check arg syntax\n            try:\n                func = getattr(self, 'help_' + arg)\n            except AttributeError:\n                try:\n                    doc=getattr(self, 'do_' + arg).__doc__\n                    if doc:\n                        self.stdout.write(\"%s\\n\"%str(doc))\n                        return\n                except AttributeError:\n                    pass\n                self.stdout.write(\"%s\\n\"%str(self.nohelp % (arg,)))\n                return\n            func()\n        else:\n            names = self.get_names()\n            cmds_doc = []\n            cmds_undoc = []\n            help = {}\n            for name in names:\n                if name[:5] == 'help_':\n                    help[name[5:]]=1\n            names.sort()\n            # There can be duplicates if routines overridden\n            prevname = ''\n            for name in names:\n                if name[:3] == 'do_':\n                    if name == prevname:\n                        continue\n                    prevname = name\n                    cmd=name[3:]\n                    if cmd in help:\n                        cmds_doc.append(cmd)\n                        del help[cmd]\n                    elif getattr(self, name).__doc__:\n                        cmds_doc.append(cmd)\n                    else:\n                        cmds_undoc.append(cmd)\n            self.stdout.write(\"%s\\n\"%str(self.doc_leader))\n            self.print_topics(self.doc_header,   cmds_doc,   15,80)\n            self.print_topics(self.misc_header,  help.keys(),15,80)\n            self.print_topics(self.undoc_header, cmds_undoc, 15,80)\n\n    def print_topics(self, header, cmds, cmdlen, maxcol):\n        if cmds:\n            self.stdout.write(\"%s\\n\"%str(header))\n            if self.ruler:\n                self.stdout.write(\"%s\\n\"%str(self.ruler * len(header)))\n            self.columnize(cmds, maxcol-1)\n            self.stdout.write(\"\\n\")\n\n    def columnize(self, list, displaywidth=80):\n        \"\"\"Display a list of strings as a compact set of columns.\n\n        Each column is only as wide as necessary.\n        Columns are separated by two spaces (one was not legible enough).\n        \"\"\"\n        if not list:\n            self.stdout.write(\"<empty>\\n\")\n            return\n        nonstrings = [i for i in range(len(list))\n                        if not isinstance(list[i], str)]\n        if nonstrings:\n            raise TypeError, (\"list[i] not a string for i in %s\" %\n                              \", \".join(map(str, nonstrings)))\n        size = len(list)\n        if size == 1:\n            self.stdout.write('%s\\n'%str(list[0]))\n            return\n        # Try every row count from 1 upwards\n        for nrows in range(1, len(list)):\n            ncols = (size+nrows-1) // nrows\n            colwidths = []\n            totwidth = -2\n            for col in range(ncols):\n                colwidth = 0\n                for row in range(nrows):\n                    i = row + nrows*col\n                    if i >= size:\n                        break\n                    x = list[i]\n                    colwidth = max(colwidth, len(x))\n                colwidths.append(colwidth)\n                totwidth += colwidth + 2\n                if totwidth > displaywidth:\n                    break\n            if totwidth <= displaywidth:\n                break\n        else:\n            nrows = len(list)\n            ncols = 1\n            colwidths = [0]\n        for row in range(nrows):\n            texts = []\n            for col in range(ncols):\n                i = row + nrows*col\n                if i >= size:\n                    x = \"\"\n                else:\n                    x = list[i]\n                texts.append(x)\n            while texts and not texts[-1]:\n                del texts[-1]\n            for col in range(len(texts)):\n                texts[col] = texts[col].ljust(colwidths[col])\n            self.stdout.write(\"%s\\n\"%str(\"  \".join(texts)))\n", 
    "code": "\"\"\"Utilities needed to emulate Python's interactive interpreter.\n\n\"\"\"\n\n# Inspired by similar code by Jeff Epler and Fredrik Lundh.\n\n\nimport sys\nimport traceback\nfrom codeop import CommandCompiler, compile_command\n\n__all__ = [\"InteractiveInterpreter\", \"InteractiveConsole\", \"interact\",\n           \"compile_command\"]\n\ndef softspace(file, newvalue):\n    oldvalue = 0\n    try:\n        oldvalue = file.softspace\n    except AttributeError:\n        pass\n    try:\n        file.softspace = newvalue\n    except (AttributeError, TypeError):\n        # \"attribute-less object\" or \"read-only attributes\"\n        pass\n    return oldvalue\n\nclass InteractiveInterpreter:\n    \"\"\"Base class for InteractiveConsole.\n\n    This class deals with parsing and interpreter state (the user's\n    namespace); it doesn't deal with input buffering or prompting or\n    input file naming (the filename is always passed in explicitly).\n\n    \"\"\"\n\n    def __init__(self, locals=None):\n        \"\"\"Constructor.\n\n        The optional 'locals' argument specifies the dictionary in\n        which code will be executed; it defaults to a newly created\n        dictionary with key \"__name__\" set to \"__console__\" and key\n        \"__doc__\" set to None.\n\n        \"\"\"\n        if locals is None:\n            locals = {\"__name__\": \"__console__\", \"__doc__\": None}\n        self.locals = locals\n        self.compile = CommandCompiler()\n\n    def runsource(self, source, filename=\"<input>\", symbol=\"single\"):\n        \"\"\"Compile and run some source in the interpreter.\n\n        Arguments are as for compile_command().\n\n        One several things can happen:\n\n        1) The input is incorrect; compile_command() raised an\n        exception (SyntaxError or OverflowError).  A syntax traceback\n        will be printed by calling the showsyntaxerror() method.\n\n        2) The input is incomplete, and more input is required;\n        compile_command() returned None.  Nothing happens.\n\n        3) The input is complete; compile_command() returned a code\n        object.  The code is executed by calling self.runcode() (which\n        also handles run-time exceptions, except for SystemExit).\n\n        The return value is True in case 2, False in the other cases (unless\n        an exception is raised).  The return value can be used to\n        decide whether to use sys.ps1 or sys.ps2 to prompt the next\n        line.\n\n        \"\"\"\n        try:\n            code = self.compile(source, filename, symbol)\n        except (OverflowError, SyntaxError, ValueError):\n            # Case 1\n            self.showsyntaxerror(filename)\n            return False\n\n        if code is None:\n            # Case 2\n            return True\n\n        # Case 3\n        self.runcode(code)\n        return False\n\n    def runcode(self, code):\n        \"\"\"Execute a code object.\n\n        When an exception occurs, self.showtraceback() is called to\n        display a traceback.  All exceptions are caught except\n        SystemExit, which is reraised.\n\n        A note about KeyboardInterrupt: this exception may occur\n        elsewhere in this code, and may not always be caught.  The\n        caller should be prepared to deal with it.\n\n        \"\"\"\n        try:\n            exec code in self.locals\n        except SystemExit:\n            raise\n        except:\n            self.showtraceback()\n        else:\n            if softspace(sys.stdout, 0):\n                print\n\n    def showsyntaxerror(self, filename=None):\n        \"\"\"Display the syntax error that just occurred.\n\n        This doesn't display a stack trace because there isn't one.\n\n        If a filename is given, it is stuffed in the exception instead\n        of what was there before (because Python's parser always uses\n        \"<string>\" when reading from a string).\n\n        The output is written by self.write(), below.\n\n        \"\"\"\n        type, value, sys.last_traceback = sys.exc_info()\n        sys.last_type = type\n        sys.last_value = value\n        if filename and type is SyntaxError:\n            # Work hard to stuff the correct filename in the exception\n            try:\n                msg, (dummy_filename, lineno, offset, line) = value\n            except:\n                # Not the format we expect; leave it alone\n                pass\n            else:\n                # Stuff in the right filename\n                value = SyntaxError(msg, (filename, lineno, offset, line))\n                sys.last_value = value\n        list = traceback.format_exception_only(type, value)\n        map(self.write, list)\n\n    def showtraceback(self):\n        \"\"\"Display the exception that just occurred.\n\n        We remove the first stack item because it is our own code.\n\n        The output is written by self.write(), below.\n\n        \"\"\"\n        try:\n            type, value, tb = sys.exc_info()\n            sys.last_type = type\n            sys.last_value = value\n            sys.last_traceback = tb\n            tblist = traceback.extract_tb(tb)\n            del tblist[:1]\n            list = traceback.format_list(tblist)\n            if list:\n                list.insert(0, \"Traceback (most recent call last):\\n\")\n            list[len(list):] = traceback.format_exception_only(type, value)\n        finally:\n            tblist = tb = None\n        map(self.write, list)\n\n    def write(self, data):\n        \"\"\"Write a string.\n\n        The base implementation writes to sys.stderr; a subclass may\n        replace this with a different implementation.\n\n        \"\"\"\n        sys.stderr.write(data)\n\n\nclass InteractiveConsole(InteractiveInterpreter):\n    \"\"\"Closely emulate the behavior of the interactive Python interpreter.\n\n    This class builds on InteractiveInterpreter and adds prompting\n    using the familiar sys.ps1 and sys.ps2, and input buffering.\n\n    \"\"\"\n\n    def __init__(self, locals=None, filename=\"<console>\"):\n        \"\"\"Constructor.\n\n        The optional locals argument will be passed to the\n        InteractiveInterpreter base class.\n\n        The optional filename argument should specify the (file)name\n        of the input stream; it will show up in tracebacks.\n\n        \"\"\"\n        InteractiveInterpreter.__init__(self, locals)\n        self.filename = filename\n        self.resetbuffer()\n\n    def resetbuffer(self):\n        \"\"\"Reset the input buffer.\"\"\"\n        self.buffer = []\n\n    def interact(self, banner=None):\n        \"\"\"Closely emulate the interactive Python console.\n\n        The optional banner argument specify the banner to print\n        before the first interaction; by default it prints a banner\n        similar to the one printed by the real Python interpreter,\n        followed by the current class name in parentheses (so as not\n        to confuse this with the real interpreter -- since it's so\n        close!).\n\n        \"\"\"\n        try:\n            sys.ps1\n        except AttributeError:\n            sys.ps1 = \">>> \"\n        try:\n            sys.ps2\n        except AttributeError:\n            sys.ps2 = \"... \"\n        cprt = 'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.'\n        if banner is None:\n            self.write(\"Python %s on %s\\n%s\\n(%s)\\n\" %\n                       (sys.version, sys.platform, cprt,\n                        self.__class__.__name__))\n        else:\n            self.write(\"%s\\n\" % str(banner))\n        more = 0\n        while 1:\n            try:\n                if more:\n                    prompt = sys.ps2\n                else:\n                    prompt = sys.ps1\n                try:\n                    line = self.raw_input(prompt)\n                    # Can be None if sys.stdin was redefined\n                    encoding = getattr(sys.stdin, \"encoding\", None)\n                    if encoding and not isinstance(line, unicode):\n                        line = line.decode(encoding)\n                except EOFError:\n                    self.write(\"\\n\")\n                    break\n                else:\n                    more = self.push(line)\n            except KeyboardInterrupt:\n                self.write(\"\\nKeyboardInterrupt\\n\")\n                self.resetbuffer()\n                more = 0\n\n    def push(self, line):\n        \"\"\"Push a line to the interpreter.\n\n        The line should not have a trailing newline; it may have\n        internal newlines.  The line is appended to a buffer and the\n        interpreter's runsource() method is called with the\n        concatenated contents of the buffer as source.  If this\n        indicates that the command was executed or invalid, the buffer\n        is reset; otherwise, the command is incomplete, and the buffer\n        is left as it was after the line was appended.  The return\n        value is 1 if more input is required, 0 if the line was dealt\n        with in some way (this is the same as runsource()).\n\n        \"\"\"\n        self.buffer.append(line)\n        source = \"\\n\".join(self.buffer)\n        more = self.runsource(source, self.filename)\n        if not more:\n            self.resetbuffer()\n        return more\n\n    def raw_input(self, prompt=\"\"):\n        \"\"\"Write a prompt and read a line.\n\n        The returned line does not include the trailing newline.\n        When the user enters the EOF key sequence, EOFError is raised.\n\n        The base implementation uses the built-in function\n        raw_input(); a subclass may replace this with a different\n        implementation.\n\n        \"\"\"\n        return raw_input(prompt)\n\n\ndef interact(banner=None, readfunc=None, local=None):\n    \"\"\"Closely emulate the interactive Python interpreter.\n\n    This is a backwards compatible interface to the InteractiveConsole\n    class.  When readfunc is not specified, it attempts to import the\n    readline module to enable GNU readline if it is available.\n\n    Arguments (all optional, all default to None):\n\n    banner -- passed to InteractiveConsole.interact()\n    readfunc -- if not None, replaces InteractiveConsole.raw_input()\n    local -- passed to InteractiveInterpreter.__init__()\n\n    \"\"\"\n    console = InteractiveConsole(local)\n    if readfunc is not None:\n        console.raw_input = readfunc\n    else:\n        try:\n            import readline\n        except ImportError:\n            pass\n    console.interact(banner)\n\n\nif __name__ == \"__main__\":\n    interact()\n", 
    "codecs": "\"\"\" codecs -- Python Codec Registry, API and helpers.\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"#\"\n\nimport __builtin__, sys\n\n### Registry and builtin stateless codec functions\n\ntry:\n    from _codecs import *\nexcept ImportError, why:\n    raise SystemError('Failed to load the builtin codecs: %s' % why)\n\n__all__ = [\"register\", \"lookup\", \"open\", \"EncodedFile\", \"BOM\", \"BOM_BE\",\n           \"BOM_LE\", \"BOM32_BE\", \"BOM32_LE\", \"BOM64_BE\", \"BOM64_LE\",\n           \"BOM_UTF8\", \"BOM_UTF16\", \"BOM_UTF16_LE\", \"BOM_UTF16_BE\",\n           \"BOM_UTF32\", \"BOM_UTF32_LE\", \"BOM_UTF32_BE\",\n           \"strict_errors\", \"ignore_errors\", \"replace_errors\",\n           \"xmlcharrefreplace_errors\",\n           \"register_error\", \"lookup_error\"]\n\n### Constants\n\n#\n# Byte Order Mark (BOM = ZERO WIDTH NO-BREAK SPACE = U+FEFF)\n# and its possible byte string values\n# for UTF8/UTF16/UTF32 output and little/big endian machines\n#\n\n# UTF-8\nBOM_UTF8 = '\\xef\\xbb\\xbf'\n\n# UTF-16, little endian\nBOM_LE = BOM_UTF16_LE = '\\xff\\xfe'\n\n# UTF-16, big endian\nBOM_BE = BOM_UTF16_BE = '\\xfe\\xff'\n\n# UTF-32, little endian\nBOM_UTF32_LE = '\\xff\\xfe\\x00\\x00'\n\n# UTF-32, big endian\nBOM_UTF32_BE = '\\x00\\x00\\xfe\\xff'\n\nif sys.byteorder == 'little':\n\n    # UTF-16, native endianness\n    BOM = BOM_UTF16 = BOM_UTF16_LE\n\n    # UTF-32, native endianness\n    BOM_UTF32 = BOM_UTF32_LE\n\nelse:\n\n    # UTF-16, native endianness\n    BOM = BOM_UTF16 = BOM_UTF16_BE\n\n    # UTF-32, native endianness\n    BOM_UTF32 = BOM_UTF32_BE\n\n# Old broken names (don't use in new code)\nBOM32_LE = BOM_UTF16_LE\nBOM32_BE = BOM_UTF16_BE\nBOM64_LE = BOM_UTF32_LE\nBOM64_BE = BOM_UTF32_BE\n\n\n### Codec base classes (defining the API)\n\nclass CodecInfo(tuple):\n\n    def __new__(cls, encode, decode, streamreader=None, streamwriter=None,\n        incrementalencoder=None, incrementaldecoder=None, name=None):\n        self = tuple.__new__(cls, (encode, decode, streamreader, streamwriter))\n        self.name = name\n        self.encode = encode\n        self.decode = decode\n        self.incrementalencoder = incrementalencoder\n        self.incrementaldecoder = incrementaldecoder\n        self.streamwriter = streamwriter\n        self.streamreader = streamreader\n        return self\n\n    def __repr__(self):\n        return \"<%s.%s object for encoding %s at 0x%x>\" % (self.__class__.__module__, self.__class__.__name__, self.name, id(self))\n\nclass Codec:\n\n    \"\"\" Defines the interface for stateless encoders/decoders.\n\n        The .encode()/.decode() methods may use different error\n        handling schemes by providing the errors argument. These\n        string values are predefined:\n\n         'strict' - raise a ValueError error (or a subclass)\n         'ignore' - ignore the character and continue with the next\n         'replace' - replace with a suitable replacement character;\n                    Python will use the official U+FFFD REPLACEMENT\n                    CHARACTER for the builtin Unicode codecs on\n                    decoding and '?' on encoding.\n         'xmlcharrefreplace' - Replace with the appropriate XML\n                               character reference (only for encoding).\n         'backslashreplace'  - Replace with backslashed escape sequences\n                               (only for encoding).\n\n        The set of allowed values can be extended via register_error.\n\n    \"\"\"\n    def encode(self, input, errors='strict'):\n\n        \"\"\" Encodes the object input and returns a tuple (output\n            object, length consumed).\n\n            errors defines the error handling to apply. It defaults to\n            'strict' handling.\n\n            The method may not store state in the Codec instance. Use\n            StreamCodec for codecs which have to keep state in order to\n            make encoding/decoding efficient.\n\n            The encoder must be able to handle zero length input and\n            return an empty object of the output object type in this\n            situation.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def decode(self, input, errors='strict'):\n\n        \"\"\" Decodes the object input and returns a tuple (output\n            object, length consumed).\n\n            input must be an object which provides the bf_getreadbuf\n            buffer slot. Python strings, buffer objects and memory\n            mapped files are examples of objects providing this slot.\n\n            errors defines the error handling to apply. It defaults to\n            'strict' handling.\n\n            The method may not store state in the Codec instance. Use\n            StreamCodec for codecs which have to keep state in order to\n            make encoding/decoding efficient.\n\n            The decoder must be able to handle zero length input and\n            return an empty object of the output object type in this\n            situation.\n\n        \"\"\"\n        raise NotImplementedError\n\nclass IncrementalEncoder(object):\n    \"\"\"\n    An IncrementalEncoder encodes an input in multiple steps. The input can be\n    passed piece by piece to the encode() method. The IncrementalEncoder remembers\n    the state of the Encoding process between calls to encode().\n    \"\"\"\n    def __init__(self, errors='strict'):\n        \"\"\"\n        Creates an IncrementalEncoder instance.\n\n        The IncrementalEncoder may use different error handling schemes by\n        providing the errors keyword argument. See the module docstring\n        for a list of possible values.\n        \"\"\"\n        self.errors = errors\n        self.buffer = \"\"\n\n    def encode(self, input, final=False):\n        \"\"\"\n        Encodes input and returns the resulting object.\n        \"\"\"\n        raise NotImplementedError\n\n    def reset(self):\n        \"\"\"\n        Resets the encoder to the initial state.\n        \"\"\"\n\n    def getstate(self):\n        \"\"\"\n        Return the current state of the encoder.\n        \"\"\"\n        return 0\n\n    def setstate(self, state):\n        \"\"\"\n        Set the current state of the encoder. state must have been\n        returned by getstate().\n        \"\"\"\n\nclass BufferedIncrementalEncoder(IncrementalEncoder):\n    \"\"\"\n    This subclass of IncrementalEncoder can be used as the baseclass for an\n    incremental encoder if the encoder must keep some of the output in a\n    buffer between calls to encode().\n    \"\"\"\n    def __init__(self, errors='strict'):\n        IncrementalEncoder.__init__(self, errors)\n        self.buffer = \"\" # unencoded input that is kept between calls to encode()\n\n    def _buffer_encode(self, input, errors, final):\n        # Overwrite this method in subclasses: It must encode input\n        # and return an (output, length consumed) tuple\n        raise NotImplementedError\n\n    def encode(self, input, final=False):\n        # encode input (taking the buffer into account)\n        data = self.buffer + input\n        (result, consumed) = self._buffer_encode(data, self.errors, final)\n        # keep unencoded input until the next call\n        self.buffer = data[consumed:]\n        return result\n\n    def reset(self):\n        IncrementalEncoder.reset(self)\n        self.buffer = \"\"\n\n    def getstate(self):\n        return self.buffer or 0\n\n    def setstate(self, state):\n        self.buffer = state or \"\"\n\nclass IncrementalDecoder(object):\n    \"\"\"\n    An IncrementalDecoder decodes an input in multiple steps. The input can be\n    passed piece by piece to the decode() method. The IncrementalDecoder\n    remembers the state of the decoding process between calls to decode().\n    \"\"\"\n    def __init__(self, errors='strict'):\n        \"\"\"\n        Creates a IncrementalDecoder instance.\n\n        The IncrementalDecoder may use different error handling schemes by\n        providing the errors keyword argument. See the module docstring\n        for a list of possible values.\n        \"\"\"\n        self.errors = errors\n\n    def decode(self, input, final=False):\n        \"\"\"\n        Decodes input and returns the resulting object.\n        \"\"\"\n        raise NotImplementedError\n\n    def reset(self):\n        \"\"\"\n        Resets the decoder to the initial state.\n        \"\"\"\n\n    def getstate(self):\n        \"\"\"\n        Return the current state of the decoder.\n\n        This must be a (buffered_input, additional_state_info) tuple.\n        buffered_input must be a bytes object containing bytes that\n        were passed to decode() that have not yet been converted.\n        additional_state_info must be a non-negative integer\n        representing the state of the decoder WITHOUT yet having\n        processed the contents of buffered_input.  In the initial state\n        and after reset(), getstate() must return (b\"\", 0).\n        \"\"\"\n        return (b\"\", 0)\n\n    def setstate(self, state):\n        \"\"\"\n        Set the current state of the decoder.\n\n        state must have been returned by getstate().  The effect of\n        setstate((b\"\", 0)) must be equivalent to reset().\n        \"\"\"\n\nclass BufferedIncrementalDecoder(IncrementalDecoder):\n    \"\"\"\n    This subclass of IncrementalDecoder can be used as the baseclass for an\n    incremental decoder if the decoder must be able to handle incomplete byte\n    sequences.\n    \"\"\"\n    def __init__(self, errors='strict'):\n        IncrementalDecoder.__init__(self, errors)\n        self.buffer = \"\" # undecoded input that is kept between calls to decode()\n\n    def _buffer_decode(self, input, errors, final):\n        # Overwrite this method in subclasses: It must decode input\n        # and return an (output, length consumed) tuple\n        raise NotImplementedError\n\n    def decode(self, input, final=False):\n        # decode input (taking the buffer into account)\n        data = self.buffer + input\n        (result, consumed) = self._buffer_decode(data, self.errors, final)\n        # keep undecoded input until the next call\n        self.buffer = data[consumed:]\n        return result\n\n    def reset(self):\n        IncrementalDecoder.reset(self)\n        self.buffer = \"\"\n\n    def getstate(self):\n        # additional state info is always 0\n        return (self.buffer, 0)\n\n    def setstate(self, state):\n        # ignore additional state info\n        self.buffer = state[0]\n\n#\n# The StreamWriter and StreamReader class provide generic working\n# interfaces which can be used to implement new encoding submodules\n# very easily. See encodings/utf_8.py for an example on how this is\n# done.\n#\n\nclass StreamWriter(Codec):\n\n    def __init__(self, stream, errors='strict'):\n\n        \"\"\" Creates a StreamWriter instance.\n\n            stream must be a file-like object open for writing\n            (binary) data.\n\n            The StreamWriter may use different error handling\n            schemes by providing the errors keyword argument. These\n            parameters are predefined:\n\n             'strict' - raise a ValueError (or a subclass)\n             'ignore' - ignore the character and continue with the next\n             'replace'- replace with a suitable replacement character\n             'xmlcharrefreplace' - Replace with the appropriate XML\n                                   character reference.\n             'backslashreplace'  - Replace with backslashed escape\n                                   sequences (only for encoding).\n\n            The set of allowed parameter values can be extended via\n            register_error.\n        \"\"\"\n        self.stream = stream\n        self.errors = errors\n\n    def write(self, object):\n\n        \"\"\" Writes the object's contents encoded to self.stream.\n        \"\"\"\n        data, consumed = self.encode(object, self.errors)\n        self.stream.write(data)\n\n    def writelines(self, list):\n\n        \"\"\" Writes the concatenated list of strings to the stream\n            using .write().\n        \"\"\"\n        self.write(''.join(list))\n\n    def reset(self):\n\n        \"\"\" Flushes and resets the codec buffers used for keeping state.\n\n            Calling this method should ensure that the data on the\n            output is put into a clean state, that allows appending\n            of new fresh data without having to rescan the whole\n            stream to recover state.\n\n        \"\"\"\n        pass\n\n    def seek(self, offset, whence=0):\n        self.stream.seek(offset, whence)\n        if whence == 0 and offset == 0:\n            self.reset()\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n###\n\nclass StreamReader(Codec):\n\n    def __init__(self, stream, errors='strict'):\n\n        \"\"\" Creates a StreamReader instance.\n\n            stream must be a file-like object open for reading\n            (binary) data.\n\n            The StreamReader may use different error handling\n            schemes by providing the errors keyword argument. These\n            parameters are predefined:\n\n             'strict' - raise a ValueError (or a subclass)\n             'ignore' - ignore the character and continue with the next\n             'replace'- replace with a suitable replacement character;\n\n            The set of allowed parameter values can be extended via\n            register_error.\n        \"\"\"\n        self.stream = stream\n        self.errors = errors\n        self.bytebuffer = \"\"\n        # For str->str decoding this will stay a str\n        # For str->unicode decoding the first read will promote it to unicode\n        self.charbuffer = \"\"\n        self.linebuffer = None\n\n    def decode(self, input, errors='strict'):\n        raise NotImplementedError\n\n    def read(self, size=-1, chars=-1, firstline=False):\n\n        \"\"\" Decodes data from the stream self.stream and returns the\n            resulting object.\n\n            chars indicates the number of characters to read from the\n            stream. read() will never return more than chars\n            characters, but it might return less, if there are not enough\n            characters available.\n\n            size indicates the approximate maximum number of bytes to\n            read from the stream for decoding purposes. The decoder\n            can modify this setting as appropriate. The default value\n            -1 indicates to read and decode as much as possible.  size\n            is intended to prevent having to decode huge files in one\n            step.\n\n            If firstline is true, and a UnicodeDecodeError happens\n            after the first line terminator in the input only the first line\n            will be returned, the rest of the input will be kept until the\n            next call to read().\n\n            The method should use a greedy read strategy meaning that\n            it should read as much data as is allowed within the\n            definition of the encoding and the given size, e.g.  if\n            optional encoding endings or state markers are available\n            on the stream, these should be read too.\n        \"\"\"\n        # If we have lines cached, first merge them back into characters\n        if self.linebuffer:\n            self.charbuffer = \"\".join(self.linebuffer)\n            self.linebuffer = None\n\n        # read until we get the required number of characters (if available)\n        while True:\n            # can the request be satisfied from the character buffer?\n            if chars >= 0:\n                if len(self.charbuffer) >= chars:\n                    break\n            elif size >= 0:\n                if len(self.charbuffer) >= size:\n                    break\n            # we need more data\n            if size < 0:\n                newdata = self.stream.read()\n            else:\n                newdata = self.stream.read(size)\n            # decode bytes (those remaining from the last call included)\n            data = self.bytebuffer + newdata\n            try:\n                newchars, decodedbytes = self.decode(data, self.errors)\n            except UnicodeDecodeError, exc:\n                if firstline:\n                    newchars, decodedbytes = self.decode(data[:exc.start], self.errors)\n                    lines = newchars.splitlines(True)\n                    if len(lines)<=1:\n                        raise\n                else:\n                    raise\n            # keep undecoded bytes until the next call\n            self.bytebuffer = data[decodedbytes:]\n            # put new characters in the character buffer\n            self.charbuffer += newchars\n            # there was no data available\n            if not newdata:\n                break\n        if chars < 0:\n            # Return everything we've got\n            result = self.charbuffer\n            self.charbuffer = \"\"\n        else:\n            # Return the first chars characters\n            result = self.charbuffer[:chars]\n            self.charbuffer = self.charbuffer[chars:]\n        return result\n\n    def readline(self, size=None, keepends=True):\n\n        \"\"\" Read one line from the input stream and return the\n            decoded data.\n\n            size, if given, is passed as size argument to the\n            read() method.\n\n        \"\"\"\n        # If we have lines cached from an earlier read, return\n        # them unconditionally\n        if self.linebuffer:\n            line = self.linebuffer[0]\n            del self.linebuffer[0]\n            if len(self.linebuffer) == 1:\n                # revert to charbuffer mode; we might need more data\n                # next time\n                self.charbuffer = self.linebuffer[0]\n                self.linebuffer = None\n            if not keepends:\n                line = line.splitlines(False)[0]\n            return line\n\n        readsize = size or 72\n        line = \"\"\n        # If size is given, we call read() only once\n        while True:\n            data = self.read(readsize, firstline=True)\n            if data:\n                # If we're at a \"\\r\" read one extra character (which might\n                # be a \"\\n\") to get a proper line ending. If the stream is\n                # temporarily exhausted we return the wrong line ending.\n                if data.endswith(\"\\r\"):\n                    data += self.read(size=1, chars=1)\n\n            line += data\n            lines = line.splitlines(True)\n            if lines:\n                if len(lines) > 1:\n                    # More than one line result; the first line is a full line\n                    # to return\n                    line = lines[0]\n                    del lines[0]\n                    if len(lines) > 1:\n                        # cache the remaining lines\n                        lines[-1] += self.charbuffer\n                        self.linebuffer = lines\n                        self.charbuffer = None\n                    else:\n                        # only one remaining line, put it back into charbuffer\n                        self.charbuffer = lines[0] + self.charbuffer\n                    if not keepends:\n                        line = line.splitlines(False)[0]\n                    break\n                line0withend = lines[0]\n                line0withoutend = lines[0].splitlines(False)[0]\n                if line0withend != line0withoutend: # We really have a line end\n                    # Put the rest back together and keep it until the next call\n                    self.charbuffer = \"\".join(lines[1:]) + self.charbuffer\n                    if keepends:\n                        line = line0withend\n                    else:\n                        line = line0withoutend\n                    break\n            # we didn't get anything or this was our only try\n            if not data or size is not None:\n                if line and not keepends:\n                    line = line.splitlines(False)[0]\n                break\n            if readsize<8000:\n                readsize *= 2\n        return line\n\n    def readlines(self, sizehint=None, keepends=True):\n\n        \"\"\" Read all lines available on the input stream\n            and return them as list of lines.\n\n            Line breaks are implemented using the codec's decoder\n            method and are included in the list entries.\n\n            sizehint, if given, is ignored since there is no efficient\n            way to finding the true end-of-line.\n\n        \"\"\"\n        data = self.read()\n        return data.splitlines(keepends)\n\n    def reset(self):\n\n        \"\"\" Resets the codec buffers used for keeping state.\n\n            Note that no stream repositioning should take place.\n            This method is primarily intended to be able to recover\n            from decoding errors.\n\n        \"\"\"\n        self.bytebuffer = \"\"\n        self.charbuffer = u\"\"\n        self.linebuffer = None\n\n    def seek(self, offset, whence=0):\n        \"\"\" Set the input stream's current position.\n\n            Resets the codec buffers used for keeping state.\n        \"\"\"\n        self.stream.seek(offset, whence)\n        self.reset()\n\n    def next(self):\n\n        \"\"\" Return the next decoded line from the input stream.\"\"\"\n        line = self.readline()\n        if line:\n            return line\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n###\n\nclass StreamReaderWriter:\n\n    \"\"\" StreamReaderWriter instances allow wrapping streams which\n        work in both read and write modes.\n\n        The design is such that one can use the factory functions\n        returned by the codec.lookup() function to construct the\n        instance.\n\n    \"\"\"\n    # Optional attributes set by the file wrappers below\n    encoding = 'unknown'\n\n    def __init__(self, stream, Reader, Writer, errors='strict'):\n\n        \"\"\" Creates a StreamReaderWriter instance.\n\n            stream must be a Stream-like object.\n\n            Reader, Writer must be factory functions or classes\n            providing the StreamReader, StreamWriter interface resp.\n\n            Error handling is done in the same way as defined for the\n            StreamWriter/Readers.\n\n        \"\"\"\n        self.stream = stream\n        self.reader = Reader(stream, errors)\n        self.writer = Writer(stream, errors)\n        self.errors = errors\n\n    def read(self, size=-1):\n\n        return self.reader.read(size)\n\n    def readline(self, size=None):\n\n        return self.reader.readline(size)\n\n    def readlines(self, sizehint=None):\n\n        return self.reader.readlines(sizehint)\n\n    def next(self):\n\n        \"\"\" Return the next decoded line from the input stream.\"\"\"\n        return self.reader.next()\n\n    def __iter__(self):\n        return self\n\n    def write(self, data):\n\n        return self.writer.write(data)\n\n    def writelines(self, list):\n\n        return self.writer.writelines(list)\n\n    def reset(self):\n\n        self.reader.reset()\n        self.writer.reset()\n\n    def seek(self, offset, whence=0):\n        self.stream.seek(offset, whence)\n        self.reader.reset()\n        if whence == 0 and offset == 0:\n            self.writer.reset()\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    # these are needed to make \"with codecs.open(...)\" work properly\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n###\n\nclass StreamRecoder:\n\n    \"\"\" StreamRecoder instances provide a frontend - backend\n        view of encoding data.\n\n        They use the complete set of APIs returned by the\n        codecs.lookup() function to implement their task.\n\n        Data written to the stream is first decoded into an\n        intermediate format (which is dependent on the given codec\n        combination) and then written to the stream using an instance\n        of the provided Writer class.\n\n        In the other direction, data is read from the stream using a\n        Reader instance and then return encoded data to the caller.\n\n    \"\"\"\n    # Optional attributes set by the file wrappers below\n    data_encoding = 'unknown'\n    file_encoding = 'unknown'\n\n    def __init__(self, stream, encode, decode, Reader, Writer,\n                 errors='strict'):\n\n        \"\"\" Creates a StreamRecoder instance which implements a two-way\n            conversion: encode and decode work on the frontend (the\n            input to .read() and output of .write()) while\n            Reader and Writer work on the backend (reading and\n            writing to the stream).\n\n            You can use these objects to do transparent direct\n            recodings from e.g. latin-1 to utf-8 and back.\n\n            stream must be a file-like object.\n\n            encode, decode must adhere to the Codec interface, Reader,\n            Writer must be factory functions or classes providing the\n            StreamReader, StreamWriter interface resp.\n\n            encode and decode are needed for the frontend translation,\n            Reader and Writer for the backend translation. Unicode is\n            used as intermediate encoding.\n\n            Error handling is done in the same way as defined for the\n            StreamWriter/Readers.\n\n        \"\"\"\n        self.stream = stream\n        self.encode = encode\n        self.decode = decode\n        self.reader = Reader(stream, errors)\n        self.writer = Writer(stream, errors)\n        self.errors = errors\n\n    def read(self, size=-1):\n\n        data = self.reader.read(size)\n        data, bytesencoded = self.encode(data, self.errors)\n        return data\n\n    def readline(self, size=None):\n\n        if size is None:\n            data = self.reader.readline()\n        else:\n            data = self.reader.readline(size)\n        data, bytesencoded = self.encode(data, self.errors)\n        return data\n\n    def readlines(self, sizehint=None):\n\n        data = self.reader.read()\n        data, bytesencoded = self.encode(data, self.errors)\n        return data.splitlines(1)\n\n    def next(self):\n\n        \"\"\" Return the next decoded line from the input stream.\"\"\"\n        data = self.reader.next()\n        data, bytesencoded = self.encode(data, self.errors)\n        return data\n\n    def __iter__(self):\n        return self\n\n    def write(self, data):\n\n        data, bytesdecoded = self.decode(data, self.errors)\n        return self.writer.write(data)\n\n    def writelines(self, list):\n\n        data = ''.join(list)\n        data, bytesdecoded = self.decode(data, self.errors)\n        return self.writer.write(data)\n\n    def reset(self):\n\n        self.reader.reset()\n        self.writer.reset()\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n### Shortcuts\n\ndef open(filename, mode='rb', encoding=None, errors='strict', buffering=1):\n\n    \"\"\" Open an encoded file using the given mode and return\n        a wrapped version providing transparent encoding/decoding.\n\n        Note: The wrapped version will only accept the object format\n        defined by the codecs, i.e. Unicode objects for most builtin\n        codecs. Output is also codec dependent and will usually be\n        Unicode as well.\n\n        Files are always opened in binary mode, even if no binary mode\n        was specified. This is done to avoid data loss due to encodings\n        using 8-bit values. The default file mode is 'rb' meaning to\n        open the file in binary read mode.\n\n        encoding specifies the encoding which is to be used for the\n        file.\n\n        errors may be given to define the error handling. It defaults\n        to 'strict' which causes ValueErrors to be raised in case an\n        encoding error occurs.\n\n        buffering has the same meaning as for the builtin open() API.\n        It defaults to line buffered.\n\n        The returned wrapped file object provides an extra attribute\n        .encoding which allows querying the used encoding. This\n        attribute is only available if an encoding was specified as\n        parameter.\n\n    \"\"\"\n    if encoding is not None:\n        if 'U' in mode:\n            # No automatic conversion of '\\n' is done on reading and writing\n            mode = mode.strip().replace('U', '')\n            if mode[:1] not in set('rwa'):\n                mode = 'r' + mode\n        if 'b' not in mode:\n            # Force opening of the file in binary mode\n            mode = mode + 'b'\n    file = __builtin__.open(filename, mode, buffering)\n    if encoding is None:\n        return file\n    info = lookup(encoding)\n    srw = StreamReaderWriter(file, info.streamreader, info.streamwriter, errors)\n    # Add attributes to simplify introspection\n    srw.encoding = encoding\n    return srw\n\ndef EncodedFile(file, data_encoding, file_encoding=None, errors='strict'):\n\n    \"\"\" Return a wrapped version of file which provides transparent\n        encoding translation.\n\n        Strings written to the wrapped file are interpreted according\n        to the given data_encoding and then written to the original\n        file as string using file_encoding. The intermediate encoding\n        will usually be Unicode but depends on the specified codecs.\n\n        Strings are read from the file using file_encoding and then\n        passed back to the caller as string using data_encoding.\n\n        If file_encoding is not given, it defaults to data_encoding.\n\n        errors may be given to define the error handling. It defaults\n        to 'strict' which causes ValueErrors to be raised in case an\n        encoding error occurs.\n\n        The returned wrapped file object provides two extra attributes\n        .data_encoding and .file_encoding which reflect the given\n        parameters of the same name. The attributes can be used for\n        introspection by Python programs.\n\n    \"\"\"\n    if file_encoding is None:\n        file_encoding = data_encoding\n    data_info = lookup(data_encoding)\n    file_info = lookup(file_encoding)\n    sr = StreamRecoder(file, data_info.encode, data_info.decode,\n                       file_info.streamreader, file_info.streamwriter, errors)\n    # Add attributes to simplify introspection\n    sr.data_encoding = data_encoding\n    sr.file_encoding = file_encoding\n    return sr\n\n### Helpers for codec lookup\n\ndef getencoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its encoder function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).encode\n\ndef getdecoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its decoder function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).decode\n\ndef getincrementalencoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its IncrementalEncoder class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found\n        or the codecs doesn't provide an incremental encoder.\n\n    \"\"\"\n    encoder = lookup(encoding).incrementalencoder\n    if encoder is None:\n        raise LookupError(encoding)\n    return encoder\n\ndef getincrementaldecoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its IncrementalDecoder class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found\n        or the codecs doesn't provide an incremental decoder.\n\n    \"\"\"\n    decoder = lookup(encoding).incrementaldecoder\n    if decoder is None:\n        raise LookupError(encoding)\n    return decoder\n\ndef getreader(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its StreamReader class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).streamreader\n\ndef getwriter(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its StreamWriter class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).streamwriter\n\ndef iterencode(iterator, encoding, errors='strict', **kwargs):\n    \"\"\"\n    Encoding iterator.\n\n    Encodes the input strings from the iterator using a IncrementalEncoder.\n\n    errors and kwargs are passed through to the IncrementalEncoder\n    constructor.\n    \"\"\"\n    encoder = getincrementalencoder(encoding)(errors, **kwargs)\n    for input in iterator:\n        output = encoder.encode(input)\n        if output:\n            yield output\n    output = encoder.encode(\"\", True)\n    if output:\n        yield output\n\ndef iterdecode(iterator, encoding, errors='strict', **kwargs):\n    \"\"\"\n    Decoding iterator.\n\n    Decodes the input strings from the iterator using a IncrementalDecoder.\n\n    errors and kwargs are passed through to the IncrementalDecoder\n    constructor.\n    \"\"\"\n    decoder = getincrementaldecoder(encoding)(errors, **kwargs)\n    for input in iterator:\n        output = decoder.decode(input)\n        if output:\n            yield output\n    output = decoder.decode(\"\", True)\n    if output:\n        yield output\n\n### Helpers for charmap-based codecs\n\ndef make_identity_dict(rng):\n\n    \"\"\" make_identity_dict(rng) -> dict\n\n        Return a dictionary where elements of the rng sequence are\n        mapped to themselves.\n\n    \"\"\"\n    res = {}\n    for i in rng:\n        res[i]=i\n    return res\n\ndef make_encoding_map(decoding_map):\n\n    \"\"\" Creates an encoding map from a decoding map.\n\n        If a target mapping in the decoding map occurs multiple\n        times, then that target is mapped to None (undefined mapping),\n        causing an exception when encountered by the charmap codec\n        during translation.\n\n        One example where this happens is cp875.py which decodes\n        multiple character to \\u001a.\n\n    \"\"\"\n    m = {}\n    for k,v in decoding_map.items():\n        if not v in m:\n            m[v] = k\n        else:\n            m[v] = None\n    return m\n\n### error handlers\n\ntry:\n    strict_errors = lookup_error(\"strict\")\n    ignore_errors = lookup_error(\"ignore\")\n    replace_errors = lookup_error(\"replace\")\n    xmlcharrefreplace_errors = lookup_error(\"xmlcharrefreplace\")\n    backslashreplace_errors = lookup_error(\"backslashreplace\")\nexcept LookupError:\n    # In --disable-unicode builds, these error handler are missing\n    strict_errors = None\n    ignore_errors = None\n    replace_errors = None\n    xmlcharrefreplace_errors = None\n    backslashreplace_errors = None\n\n# Tell modulefinder that using codecs probably needs the encodings\n# package\n_false = 0\nif _false:\n    import encodings\n\n### Tests\n\nif __name__ == '__main__':\n\n    # Make stdout translate Latin-1 output into UTF-8 output\n    sys.stdout = EncodedFile(sys.stdout, 'latin-1', 'utf-8')\n\n    # Have stdin translate Latin-1 input into UTF-8 input\n    sys.stdin = EncodedFile(sys.stdin, 'utf-8', 'latin-1')\n", 
    "codeop": "r\"\"\"Utilities to compile possibly incomplete Python source code.\n\nThis module provides two interfaces, broadly similar to the builtin\nfunction compile(), which take program text, a filename and a 'mode'\nand:\n\n- Return code object if the command is complete and valid\n- Return None if the command is incomplete\n- Raise SyntaxError, ValueError or OverflowError if the command is a\n  syntax error (OverflowError and ValueError can be produced by\n  malformed literals).\n\nApproach:\n\nFirst, check if the source consists entirely of blank lines and\ncomments; if so, replace it with 'pass', because the built-in\nparser doesn't always do the right thing for these.\n\nCompile three times: as is, with \\n, and with \\n\\n appended.  If it\ncompiles as is, it's complete.  If it compiles with one \\n appended,\nwe expect more.  If it doesn't compile either way, we compare the\nerror we get when compiling with \\n or \\n\\n appended.  If the errors\nare the same, the code is broken.  But if the errors are different, we\nexpect more.  Not intuitive; not even guaranteed to hold in future\nreleases; but this matches the compiler's behavior from Python 1.4\nthrough 2.2, at least.\n\nCaveat:\n\nIt is possible (but not likely) that the parser stops parsing with a\nsuccessful outcome before reaching the end of the source; in this\ncase, trailing symbols may be ignored instead of causing an error.\nFor example, a backslash followed by two newlines may be followed by\narbitrary garbage.  This will be fixed once the API for the parser is\nbetter.\n\nThe two interfaces are:\n\ncompile_command(source, filename, symbol):\n\n    Compiles a single command in the manner described above.\n\nCommandCompiler():\n\n    Instances of this class have __call__ methods identical in\n    signature to compile_command; the difference is that if the\n    instance compiles program text containing a __future__ statement,\n    the instance 'remembers' and compiles all subsequent program texts\n    with the statement in force.\n\nThe module also provides another class:\n\nCompile():\n\n    Instances of this class act like the built-in function compile,\n    but with 'memory' in the sense described above.\n\"\"\"\n\nimport __future__\n\n_features = [getattr(__future__, fname)\n             for fname in __future__.all_feature_names]\n\n__all__ = [\"compile_command\", \"Compile\", \"CommandCompiler\"]\n\nPyCF_DONT_IMPLY_DEDENT = 0x200          # Matches pythonrun.h\n\ndef _maybe_compile(compiler, source, filename, symbol):\n    # Check for source consisting of only blank lines and comments\n    for line in source.split(\"\\n\"):\n        line = line.strip()\n        if line and line[0] != '#':\n            break               # Leave it alone\n    else:\n        if symbol != \"eval\":\n            source = \"pass\"     # Replace it with a 'pass' statement\n\n    err = err1 = err2 = None\n    code = code1 = code2 = None\n\n    try:\n        code = compiler(source, filename, symbol)\n    except SyntaxError, err:\n        pass\n\n    try:\n        code1 = compiler(source + \"\\n\", filename, symbol)\n    except SyntaxError, err1:\n        pass\n\n    try:\n        code2 = compiler(source + \"\\n\\n\", filename, symbol)\n    except SyntaxError, err2:\n        pass\n\n    if code:\n        return code\n    if not code1 and repr(err1) == repr(err2):\n        raise SyntaxError, err1\n\ndef _compile(source, filename, symbol):\n    return compile(source, filename, symbol, PyCF_DONT_IMPLY_DEDENT)\n\ndef compile_command(source, filename=\"<input>\", symbol=\"single\"):\n    r\"\"\"Compile a command and determine whether it is incomplete.\n\n    Arguments:\n\n    source -- the source string; may contain \\n characters\n    filename -- optional filename from which source was read; default\n                \"<input>\"\n    symbol -- optional grammar start symbol; \"single\" (default) or \"eval\"\n\n    Return value / exceptions raised:\n\n    - Return a code object if the command is complete and valid\n    - Return None if the command is incomplete\n    - Raise SyntaxError, ValueError or OverflowError if the command is a\n      syntax error (OverflowError and ValueError can be produced by\n      malformed literals).\n    \"\"\"\n    return _maybe_compile(_compile, source, filename, symbol)\n\nclass Compile:\n    \"\"\"Instances of this class behave much like the built-in compile\n    function, but if one is used to compile text containing a future\n    statement, it \"remembers\" and compiles all subsequent program texts\n    with the statement in force.\"\"\"\n    def __init__(self):\n        self.flags = PyCF_DONT_IMPLY_DEDENT\n\n    def __call__(self, source, filename, symbol):\n        codeob = compile(source, filename, symbol, self.flags, 1)\n        for feature in _features:\n            if codeob.co_flags & feature.compiler_flag:\n                self.flags |= feature.compiler_flag\n        return codeob\n\nclass CommandCompiler:\n    \"\"\"Instances of this class have __call__ methods identical in\n    signature to compile_command; the difference is that if the\n    instance compiles program text containing a __future__ statement,\n    the instance 'remembers' and compiles all subsequent program texts\n    with the statement in force.\"\"\"\n\n    def __init__(self,):\n        self.compiler = Compile()\n\n    def __call__(self, source, filename=\"<input>\", symbol=\"single\"):\n        r\"\"\"Compile a command and determine whether it is incomplete.\n\n        Arguments:\n\n        source -- the source string; may contain \\n characters\n        filename -- optional filename from which source was read;\n                    default \"<input>\"\n        symbol -- optional grammar start symbol; \"single\" (default) or\n                  \"eval\"\n\n        Return value / exceptions raised:\n\n        - Return a code object if the command is complete and valid\n        - Return None if the command is incomplete\n        - Raise SyntaxError, ValueError or OverflowError if the command is a\n          syntax error (OverflowError and ValueError can be produced by\n          malformed literals).\n        \"\"\"\n        return _maybe_compile(self.compiler, source, filename, symbol)\n", 
    "collections": "__all__ = ['Counter', 'deque', 'defaultdict', 'namedtuple', 'OrderedDict']\n# For bootstrapping reasons, the collection ABCs are defined in _abcoll.py.\n# They should however be considered an integral part of collections.py.\nfrom _abcoll import *\nimport _abcoll\n__all__ += _abcoll.__all__\n\nfrom _collections import deque, defaultdict\nfrom operator import itemgetter as _itemgetter, eq as _eq\nfrom keyword import iskeyword as _iskeyword\nimport sys as _sys\nimport heapq as _heapq\nfrom itertools import repeat as _repeat, chain as _chain, starmap as _starmap\nfrom itertools import imap as _imap\ntry:\n    from __pypy__ import newdict\nexcept ImportError:\n    assert '__pypy__' not in _sys.builtin_module_names\n    newdict = lambda _ : {}\ntry:\n    from __pypy__ import reversed_dict\nexcept ImportError:\n    reversed_dict = lambda d: reversed(d.keys())\n\ntry:\n    from thread import get_ident as _get_ident\nexcept ImportError:\n    from dummy_thread import get_ident as _get_ident\n\n\n################################################################################\n### OrderedDict\n################################################################################\n\nclass OrderedDict(dict):\n    '''Dictionary that remembers insertion order.\n\n    In PyPy all dicts are ordered anyway.  This is mostly useful as a\n    placeholder to mean \"this dict must be ordered even on CPython\".\n\n    Known difference: iterating over an OrderedDict which is being\n    concurrently modified raises RuntimeError in PyPy.  In CPython\n    instead we get some behavior that appears reasonable in some\n    cases but is nonsensical in other cases.  This is officially\n    forbidden by the CPython docs, so we forbid it explicitly for now.\n    '''\n\n    def __reversed__(self):\n        return reversed_dict(self)\n\n    def popitem(self, last=True):\n        '''od.popitem() -> (k, v), return and remove a (key, value) pair.\n        Pairs are returned in LIFO order if last is true or FIFO order if false.\n\n        '''\n        if last:\n            return dict.popitem(self)\n        else:\n            it = dict.__iter__(self)\n            try:\n                k = it.next()\n            except StopIteration:\n                raise KeyError('dictionary is empty')\n            return (k, self.pop(k))\n\n    def __repr__(self, _repr_running={}):\n        'od.__repr__() <==> repr(od)'\n        call_key = id(self), _get_ident()\n        if call_key in _repr_running:\n            return '...'\n        _repr_running[call_key] = 1\n        try:\n            if not self:\n                return '%s()' % (self.__class__.__name__,)\n            return '%s(%r)' % (self.__class__.__name__, self.items())\n        finally:\n            del _repr_running[call_key]\n\n    def __reduce__(self):\n        'Return state information for pickling'\n        items = [[k, self[k]] for k in self]\n        inst_dict = vars(self).copy()\n        if inst_dict:\n            return (self.__class__, (items,), inst_dict)\n        return self.__class__, (items,)\n\n    def copy(self):\n        'od.copy() -> a shallow copy of od'\n        return self.__class__(self)\n\n    def __eq__(self, other):\n        '''od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive\n        while comparison to a regular mapping is order-insensitive.\n\n        '''\n        if isinstance(other, OrderedDict):\n            return dict.__eq__(self, other) and all(_imap(_eq, self, other))\n        return dict.__eq__(self, other)\n\n    def __ne__(self, other):\n        'od.__ne__(y) <==> od!=y'\n        return not self == other\n\n    # -- the following methods support python 3.x style dictionary views --\n\n    def viewkeys(self):\n        \"od.viewkeys() -> a set-like object providing a view on od's keys\"\n        return KeysView(self)\n\n    def viewvalues(self):\n        \"od.viewvalues() -> an object providing a view on od's values\"\n        return ValuesView(self)\n\n    def viewitems(self):\n        \"od.viewitems() -> a set-like object providing a view on od's items\"\n        return ItemsView(self)\n\n\n################################################################################\n### namedtuple\n################################################################################\n\n_class_template = '''\\\nclass {typename}(tuple):\n    '{typename}({arg_list})'\n\n    __slots__ = ()\n\n    _fields = {field_names!r}\n\n    def __new__(_cls, {arg_list}):\n        'Create new instance of {typename}({arg_list})'\n        return _tuple.__new__(_cls, ({arg_list}))\n\n    @classmethod\n    def _make(cls, iterable, new=tuple.__new__, len=len):\n        'Make a new {typename} object from a sequence or iterable'\n        result = new(cls, iterable)\n        if len(result) != {num_fields:d}:\n            raise TypeError('Expected {num_fields:d} arguments, got %d' % len(result))\n        return result\n\n    def __repr__(self):\n        'Return a nicely formatted representation string'\n        return '{typename}({repr_fmt})' % self\n\n    def _asdict(self):\n        'Return a new OrderedDict which maps field names to their values'\n        return OrderedDict(zip(self._fields, self))\n\n    def _replace(_self, **kwds):\n        'Return a new {typename} object replacing specified fields with new values'\n        result = _self._make(map(kwds.pop, {field_names!r}, _self))\n        if kwds:\n            raise ValueError('Got unexpected field names: %r' % kwds.keys())\n        return result\n\n    def __getnewargs__(self):\n        'Return self as a plain tuple.  Used by copy and pickle.'\n        return tuple(self)\n\n    __dict__ = _property(_asdict)\n\n    def __getstate__(self):\n        'Exclude the OrderedDict from pickling'\n        pass\n\n{field_defs}\n'''\n\n_repr_template = '{name}=%r'\n\n_field_template = '''\\\n    {name} = _property(lambda self: self[{index:d}], doc='Alias for field number {index:d}')\n'''\n\ndef namedtuple(typename, field_names, verbose=False, rename=False):\n    \"\"\"Returns a new subclass of tuple with named fields.\n\n    >>> Point = namedtuple('Point', ['x', 'y'])\n    >>> Point.__doc__                   # docstring for the new class\n    'Point(x, y)'\n    >>> p = Point(11, y=22)             # instantiate with positional args or keywords\n    >>> p[0] + p[1]                     # indexable like a plain tuple\n    33\n    >>> x, y = p                        # unpack like a regular tuple\n    >>> x, y\n    (11, 22)\n    >>> p.x + p.y                       # fields also accessable by name\n    33\n    >>> d = p._asdict()                 # convert to a dictionary\n    >>> d['x']\n    11\n    >>> Point(**d)                      # convert from a dictionary\n    Point(x=11, y=22)\n    >>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields\n    Point(x=100, y=22)\n\n    \"\"\"\n\n    # Validate the field names.  At the user's option, either generate an error\n    # message or automatically replace the field name with a valid name.\n    if isinstance(field_names, basestring):\n        field_names = field_names.replace(',', ' ').split()\n    field_names = map(str, field_names)\n    typename = str(typename)\n    if rename:\n        seen = set()\n        for index, name in enumerate(field_names):\n            if (not all(c.isalnum() or c=='_' for c in name)\n                or _iskeyword(name)\n                or not name\n                or name[0].isdigit()\n                or name.startswith('_')\n                or name in seen):\n                field_names[index] = '_%d' % index\n            seen.add(name)\n    for name in [typename] + field_names:\n        if type(name) != str:\n            raise TypeError('Type names and field names must be strings')\n        if not all(c.isalnum() or c=='_' for c in name):\n            raise ValueError('Type names and field names can only contain '\n                             'alphanumeric characters and underscores: %r' % name)\n        if _iskeyword(name):\n            raise ValueError('Type names and field names cannot be a '\n                             'keyword: %r' % name)\n        if name[0].isdigit():\n            raise ValueError('Type names and field names cannot start with '\n                             'a number: %r' % name)\n    seen = set()\n    for name in field_names:\n        if name.startswith('_') and not rename:\n            raise ValueError('Field names cannot start with an underscore: '\n                             '%r' % name)\n        if name in seen:\n            raise ValueError('Encountered duplicate field name: %r' % name)\n        seen.add(name)\n\n    # Fill-in the class template\n    class_definition = _class_template.format(\n        typename = typename,\n        field_names = tuple(field_names),\n        num_fields = len(field_names),\n        arg_list = repr(tuple(field_names)).replace(\"'\", \"\")[1:-1],\n        repr_fmt = ', '.join(_repr_template.format(name=name)\n                             for name in field_names),\n        field_defs = '\\n'.join(_field_template.format(index=index, name=name)\n                               for index, name in enumerate(field_names))\n    )\n    if verbose:\n        print class_definition\n\n    # Execute the template string in a temporary namespace and support\n    # tracing utilities by setting a value for frame.f_globals['__name__']\n    namespace = newdict('module')\n    namespace['__name__'] = 'namedtuple_%s' % typename\n    namespace['OrderedDict'] = OrderedDict\n    namespace['_property'] = property\n    namespace['_tuple'] = tuple\n    try:\n        exec class_definition in namespace\n    except SyntaxError as e:\n        raise SyntaxError(e.message + ':\\n' + class_definition)\n    result = namespace[typename]\n\n    # For pickling to work, the __module__ variable needs to be set to the frame\n    # where the named tuple is created.  Bypass this step in environments where\n    # sys._getframe is not defined (Jython for example) or sys._getframe is not\n    # defined for arguments greater than 0 (IronPython).\n    try:\n        result.__module__ = _sys._getframe(1).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):\n        pass\n\n    return result\n\n\n########################################################################\n###  Counter\n########################################################################\n\nclass Counter(dict):\n    '''Dict subclass for counting hashable items.  Sometimes called a bag\n    or multiset.  Elements are stored as dictionary keys and their counts\n    are stored as dictionary values.\n\n    >>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n    >>> c.most_common(3)                # three most common elements\n    [('a', 5), ('b', 4), ('c', 3)]\n    >>> sorted(c)                       # list all unique elements\n    ['a', 'b', 'c', 'd', 'e']\n    >>> ''.join(sorted(c.elements()))   # list elements with repetitions\n    'aaaaabbbbcccdde'\n    >>> sum(c.values())                 # total of all counts\n    15\n\n    >>> c['a']                          # count of letter 'a'\n    5\n    >>> for elem in 'shazam':           # update counts from an iterable\n    ...     c[elem] += 1                # by adding 1 to each element's count\n    >>> c['a']                          # now there are seven 'a'\n    7\n    >>> del c['b']                      # remove all 'b'\n    >>> c['b']                          # now there are zero 'b'\n    0\n\n    >>> d = Counter('simsalabim')       # make another counter\n    >>> c.update(d)                     # add in the second counter\n    >>> c['a']                          # now there are nine 'a'\n    9\n\n    >>> c.clear()                       # empty the counter\n    >>> c\n    Counter()\n\n    Note:  If a count is set to zero or reduced to zero, it will remain\n    in the counter until the entry is deleted or the counter is cleared:\n\n    >>> c = Counter('aaabbc')\n    >>> c['b'] -= 2                     # reduce the count of 'b' by two\n    >>> c.most_common()                 # 'b' is still in, but its count is zero\n    [('a', 3), ('c', 1), ('b', 0)]\n\n    '''\n    # References:\n    #   http://en.wikipedia.org/wiki/Multiset\n    #   http://www.gnu.org/software/smalltalk/manual-base/html_node/Bag.html\n    #   http://www.demo2s.com/Tutorial/Cpp/0380__set-multiset/Catalog0380__set-multiset.htm\n    #   http://code.activestate.com/recipes/259174/\n    #   Knuth, TAOCP Vol. II section 4.6.3\n\n    def __init__(self, iterable=None, **kwds):\n        '''Create a new, empty Counter object.  And if given, count elements\n        from an input iterable.  Or, initialize the count from another mapping\n        of elements to their counts.\n\n        >>> c = Counter()                           # a new, empty counter\n        >>> c = Counter('gallahad')                 # a new counter from an iterable\n        >>> c = Counter({'a': 4, 'b': 2})           # a new counter from a mapping\n        >>> c = Counter(a=4, b=2)                   # a new counter from keyword args\n\n        '''\n        super(Counter, self).__init__()\n        self.update(iterable, **kwds)\n\n    def __missing__(self, key):\n        'The count of elements not in the Counter is zero.'\n        # Needed so that self[missing_item] does not raise KeyError\n        return 0\n\n    def most_common(self, n=None):\n        '''List the n most common elements and their counts from the most\n        common to the least.  If n is None, then list all element counts.\n\n        >>> Counter('abcdeabcdabcaba').most_common(3)\n        [('a', 5), ('b', 4), ('c', 3)]\n\n        '''\n        # Emulate Bag.sortedByCount from Smalltalk\n        if n is None:\n            return sorted(self.iteritems(), key=_itemgetter(1), reverse=True)\n        return _heapq.nlargest(n, self.iteritems(), key=_itemgetter(1))\n\n    def elements(self):\n        '''Iterator over elements repeating each as many times as its count.\n\n        >>> c = Counter('ABCABC')\n        >>> sorted(c.elements())\n        ['A', 'A', 'B', 'B', 'C', 'C']\n\n        # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n        >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n        >>> product = 1\n        >>> for factor in prime_factors.elements():     # loop over factors\n        ...     product *= factor                       # and multiply them\n        >>> product\n        1836\n\n        Note, if an element's count has been set to zero or is a negative\n        number, elements() will ignore it.\n\n        '''\n        # Emulate Bag.do from Smalltalk and Multiset.begin from C++.\n        return _chain.from_iterable(_starmap(_repeat, self.iteritems()))\n\n    # Override dict methods where necessary\n\n    @classmethod\n    def fromkeys(cls, iterable, v=None):\n        # There is no equivalent method for counters because setting v=1\n        # means that no element can have a count greater than one.\n        raise NotImplementedError(\n            'Counter.fromkeys() is undefined.  Use Counter(iterable) instead.')\n\n    def update(self, iterable=None, **kwds):\n        '''Like dict.update() but add counts instead of replacing them.\n\n        Source can be an iterable, a dictionary, or another Counter instance.\n\n        >>> c = Counter('which')\n        >>> c.update('witch')           # add elements from another iterable\n        >>> d = Counter('watch')\n        >>> c.update(d)                 # add elements from another counter\n        >>> c['h']                      # four 'h' in which, witch, and watch\n        4\n\n        '''\n        # The regular dict.update() operation makes no sense here because the\n        # replace behavior results in the some of original untouched counts\n        # being mixed-in with all of the other counts for a mismash that\n        # doesn't have a straight-forward interpretation in most counting\n        # contexts.  Instead, we implement straight-addition.  Both the inputs\n        # and outputs are allowed to contain zero and negative counts.\n\n        if iterable is not None:\n            if isinstance(iterable, Mapping):\n                if self:\n                    self_get = self.get\n                    for elem, count in iterable.iteritems():\n                        self[elem] = self_get(elem, 0) + count\n                else:\n                    super(Counter, self).update(iterable) # fast path when counter is empty\n            else:\n                self_get = self.get\n                for elem in iterable:\n                    self[elem] = self_get(elem, 0) + 1\n        if kwds:\n            self.update(kwds)\n\n    def subtract(self, iterable=None, **kwds):\n        '''Like dict.update() but subtracts counts instead of replacing them.\n        Counts can be reduced below zero.  Both the inputs and outputs are\n        allowed to contain zero and negative counts.\n\n        Source can be an iterable, a dictionary, or another Counter instance.\n\n        >>> c = Counter('which')\n        >>> c.subtract('witch')             # subtract elements from another iterable\n        >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n        >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n        0\n        >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n        -1\n\n        '''\n        if iterable is not None:\n            self_get = self.get\n            if isinstance(iterable, Mapping):\n                for elem, count in iterable.items():\n                    self[elem] = self_get(elem, 0) - count\n            else:\n                for elem in iterable:\n                    self[elem] = self_get(elem, 0) - 1\n        if kwds:\n            self.subtract(kwds)\n\n    def copy(self):\n        'Return a shallow copy.'\n        return self.__class__(self)\n\n    def __reduce__(self):\n        return self.__class__, (dict(self),)\n\n    def __delitem__(self, elem):\n        'Like dict.__delitem__() but does not raise KeyError for missing values.'\n        if elem in self:\n            super(Counter, self).__delitem__(elem)\n\n    def __repr__(self):\n        if not self:\n            return '%s()' % self.__class__.__name__\n        items = ', '.join(map('%r: %r'.__mod__, self.most_common()))\n        return '%s({%s})' % (self.__class__.__name__, items)\n\n    # Multiset-style mathematical operations discussed in:\n    #       Knuth TAOCP Volume II section 4.6.3 exercise 19\n    #       and at http://en.wikipedia.org/wiki/Multiset\n    #\n    # Outputs guaranteed to only include positive counts.\n    #\n    # To strip negative and zero counts, add-in an empty counter:\n    #       c += Counter()\n\n    def __add__(self, other):\n        '''Add counts from two counters.\n\n        >>> Counter('abbb') + Counter('bcc')\n        Counter({'b': 4, 'c': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            newcount = count + other[elem]\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count > 0:\n                result[elem] = count\n        return result\n\n    def __sub__(self, other):\n        ''' Subtract count, but keep only results with positive counts.\n\n        >>> Counter('abbbc') - Counter('bccd')\n        Counter({'b': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            newcount = count - other[elem]\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count < 0:\n                result[elem] = 0 - count\n        return result\n\n    def __or__(self, other):\n        '''Union is the maximum of value in either of the input counters.\n\n        >>> Counter('abbb') | Counter('bcc')\n        Counter({'b': 3, 'c': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            other_count = other[elem]\n            newcount = other_count if count < other_count else count\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count > 0:\n                result[elem] = count\n        return result\n\n    def __and__(self, other):\n        ''' Intersection is the minimum of corresponding counts.\n\n        >>> Counter('abbb') & Counter('bcc')\n        Counter({'b': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            other_count = other[elem]\n            newcount = count if count < other_count else other_count\n            if newcount > 0:\n                result[elem] = newcount\n        return result\n\n\nif __name__ == '__main__':\n    # verify that instances can be pickled\n    from cPickle import loads, dumps\n    Point = namedtuple('Point', 'x, y', True)\n    p = Point(x=10, y=20)\n    assert p == loads(dumps(p))\n\n    # test and demonstrate ability to override methods\n    class Point(namedtuple('Point', 'x y')):\n        __slots__ = ()\n        @property\n        def hypot(self):\n            return (self.x ** 2 + self.y ** 2) ** 0.5\n        def __str__(self):\n            return 'Point: x=%6.3f  y=%6.3f  hypot=%6.3f' % (self.x, self.y, self.hypot)\n\n    for p in Point(3, 4), Point(14, 5/7.):\n        print p\n\n    class Point(namedtuple('Point', 'x y')):\n        'Point class with optimized _make() and _replace() without error-checking'\n        __slots__ = ()\n        _make = classmethod(tuple.__new__)\n        def _replace(self, _map=map, **kwds):\n            return self._make(_map(kwds.get, ('x', 'y'), self))\n\n    print Point(11, 22)._replace(x=100)\n\n    Point3D = namedtuple('Point3D', Point._fields + ('z',))\n    print Point3D.__doc__\n\n    import doctest\n    TestResults = namedtuple('TestResults', 'failed attempted')\n    print TestResults(*doctest.testmod())\n", 
    "cookielib": "r\"\"\"HTTP cookie handling for web clients.\n\nThis module has (now fairly distant) origins in Gisle Aas' Perl module\nHTTP::Cookies, from the libwww-perl library.\n\nDocstrings, comments and debug strings in this code refer to the\nattributes of the HTTP cookie system as cookie-attributes, to distinguish\nthem clearly from Python attributes.\n\nClass diagram (note that BSDDBCookieJar and the MSIE* classes are not\ndistributed with the Python standard library, but are available from\nhttp://wwwsearch.sf.net/):\n\n                        CookieJar____\n                        /     \\      \\\n            FileCookieJar      \\      \\\n             /    |   \\         \\      \\\n MozillaCookieJar | LWPCookieJar \\      \\\n                  |               |      \\\n                  |   ---MSIEBase |       \\\n                  |  /      |     |        \\\n                  | /   MSIEDBCookieJar BSDDBCookieJar\n                  |/\n               MSIECookieJar\n\n\"\"\"\n\n__all__ = ['Cookie', 'CookieJar', 'CookiePolicy', 'DefaultCookiePolicy',\n           'FileCookieJar', 'LWPCookieJar', 'lwp_cookie_str', 'LoadError',\n           'MozillaCookieJar']\n\nimport re, urlparse, copy, time, urllib\ntry:\n    import threading as _threading\nexcept ImportError:\n    import dummy_threading as _threading\nimport httplib  # only for the default HTTP port\nfrom calendar import timegm\n\ndebug = False   # set to True to enable debugging via the logging module\nlogger = None\n\ndef _debug(*args):\n    if not debug:\n        return\n    global logger\n    if not logger:\n        import logging\n        logger = logging.getLogger(\"cookielib\")\n    return logger.debug(*args)\n\n\nDEFAULT_HTTP_PORT = str(httplib.HTTP_PORT)\nMISSING_FILENAME_TEXT = (\"a filename was not supplied (nor was the CookieJar \"\n                         \"instance initialised with one)\")\n\ndef _warn_unhandled_exception():\n    # There are a few catch-all except: statements in this module, for\n    # catching input that's bad in unexpected ways.  Warn if any\n    # exceptions are caught there.\n    import warnings, traceback, StringIO\n    f = StringIO.StringIO()\n    traceback.print_exc(None, f)\n    msg = f.getvalue()\n    warnings.warn(\"cookielib bug!\\n%s\" % msg, stacklevel=2)\n\n\n# Date/time conversion\n# -----------------------------------------------------------------------------\n\nEPOCH_YEAR = 1970\ndef _timegm(tt):\n    year, month, mday, hour, min, sec = tt[:6]\n    if ((year >= EPOCH_YEAR) and (1 <= month <= 12) and (1 <= mday <= 31) and\n        (0 <= hour <= 24) and (0 <= min <= 59) and (0 <= sec <= 61)):\n        return timegm(tt)\n    else:\n        return None\n\nDAYS = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\nMONTHS = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n          \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\nMONTHS_LOWER = []\nfor month in MONTHS: MONTHS_LOWER.append(month.lower())\n\ndef time2isoz(t=None):\n    \"\"\"Return a string representing time in seconds since epoch, t.\n\n    If the function is called without an argument, it will use the current\n    time.\n\n    The format of the returned string is like \"YYYY-MM-DD hh:mm:ssZ\",\n    representing Universal Time (UTC, aka GMT).  An example of this format is:\n\n    1994-11-24 08:49:37Z\n\n    \"\"\"\n    if t is None: t = time.time()\n    year, mon, mday, hour, min, sec = time.gmtime(t)[:6]\n    return \"%04d-%02d-%02d %02d:%02d:%02dZ\" % (\n        year, mon, mday, hour, min, sec)\n\ndef time2netscape(t=None):\n    \"\"\"Return a string representing time in seconds since epoch, t.\n\n    If the function is called without an argument, it will use the current\n    time.\n\n    The format of the returned string is like this:\n\n    Wed, DD-Mon-YYYY HH:MM:SS GMT\n\n    \"\"\"\n    if t is None: t = time.time()\n    year, mon, mday, hour, min, sec, wday = time.gmtime(t)[:7]\n    return \"%s %02d-%s-%04d %02d:%02d:%02d GMT\" % (\n        DAYS[wday], mday, MONTHS[mon-1], year, hour, min, sec)\n\n\nUTC_ZONES = {\"GMT\": None, \"UTC\": None, \"UT\": None, \"Z\": None}\n\nTIMEZONE_RE = re.compile(r\"^([-+])?(\\d\\d?):?(\\d\\d)?$\")\ndef offset_from_tz_string(tz):\n    offset = None\n    if tz in UTC_ZONES:\n        offset = 0\n    else:\n        m = TIMEZONE_RE.search(tz)\n        if m:\n            offset = 3600 * int(m.group(2))\n            if m.group(3):\n                offset = offset + 60 * int(m.group(3))\n            if m.group(1) == '-':\n                offset = -offset\n    return offset\n\ndef _str2time(day, mon, yr, hr, min, sec, tz):\n    # translate month name to number\n    # month numbers start with 1 (January)\n    try:\n        mon = MONTHS_LOWER.index(mon.lower())+1\n    except ValueError:\n        # maybe it's already a number\n        try:\n            imon = int(mon)\n        except ValueError:\n            return None\n        if 1 <= imon <= 12:\n            mon = imon\n        else:\n            return None\n\n    # make sure clock elements are defined\n    if hr is None: hr = 0\n    if min is None: min = 0\n    if sec is None: sec = 0\n\n    yr = int(yr)\n    day = int(day)\n    hr = int(hr)\n    min = int(min)\n    sec = int(sec)\n\n    if yr < 1000:\n        # find \"obvious\" year\n        cur_yr = time.localtime(time.time())[0]\n        m = cur_yr % 100\n        tmp = yr\n        yr = yr + cur_yr - m\n        m = m - tmp\n        if abs(m) > 50:\n            if m > 0: yr = yr + 100\n            else: yr = yr - 100\n\n    # convert UTC time tuple to seconds since epoch (not timezone-adjusted)\n    t = _timegm((yr, mon, day, hr, min, sec, tz))\n\n    if t is not None:\n        # adjust time using timezone string, to get absolute time since epoch\n        if tz is None:\n            tz = \"UTC\"\n        tz = tz.upper()\n        offset = offset_from_tz_string(tz)\n        if offset is None:\n            return None\n        t = t - offset\n\n    return t\n\nSTRICT_DATE_RE = re.compile(\n    r\"^[SMTWF][a-z][a-z], (\\d\\d) ([JFMASOND][a-z][a-z]) \"\n    \"(\\d\\d\\d\\d) (\\d\\d):(\\d\\d):(\\d\\d) GMT$\")\nWEEKDAY_RE = re.compile(\n    r\"^(?:Sun|Mon|Tue|Wed|Thu|Fri|Sat)[a-z]*,?\\s*\", re.I)\nLOOSE_HTTP_DATE_RE = re.compile(\n    r\"\"\"^\n    (\\d\\d?)            # day\n       (?:\\s+|[-\\/])\n    (\\w+)              # month\n        (?:\\s+|[-\\/])\n    (\\d+)              # year\n    (?:\n          (?:\\s+|:)    # separator before clock\n       (\\d\\d?):(\\d\\d)  # hour:min\n       (?::(\\d\\d))?    # optional seconds\n    )?                 # optional clock\n       \\s*\n    ([-+]?\\d{2,4}|(?![APap][Mm]\\b)[A-Za-z]+)? # timezone\n       \\s*\n    (?:\\(\\w+\\))?       # ASCII representation of timezone in parens.\n       \\s*$\"\"\", re.X)\ndef http2time(text):\n    \"\"\"Returns time in seconds since epoch of time represented by a string.\n\n    Return value is an integer.\n\n    None is returned if the format of str is unrecognized, the time is outside\n    the representable range, or the timezone string is not recognized.  If the\n    string contains no timezone, UTC is assumed.\n\n    The timezone in the string may be numerical (like \"-0800\" or \"+0100\") or a\n    string timezone (like \"UTC\", \"GMT\", \"BST\" or \"EST\").  Currently, only the\n    timezone strings equivalent to UTC (zero offset) are known to the function.\n\n    The function loosely parses the following formats:\n\n    Wed, 09 Feb 1994 22:23:32 GMT       -- HTTP format\n    Tuesday, 08-Feb-94 14:15:29 GMT     -- old rfc850 HTTP format\n    Tuesday, 08-Feb-1994 14:15:29 GMT   -- broken rfc850 HTTP format\n    09 Feb 1994 22:23:32 GMT            -- HTTP format (no weekday)\n    08-Feb-94 14:15:29 GMT              -- rfc850 format (no weekday)\n    08-Feb-1994 14:15:29 GMT            -- broken rfc850 format (no weekday)\n\n    The parser ignores leading and trailing whitespace.  The time may be\n    absent.\n\n    If the year is given with only 2 digits, the function will select the\n    century that makes the year closest to the current date.\n\n    \"\"\"\n    # fast exit for strictly conforming string\n    m = STRICT_DATE_RE.search(text)\n    if m:\n        g = m.groups()\n        mon = MONTHS_LOWER.index(g[1].lower()) + 1\n        tt = (int(g[2]), mon, int(g[0]),\n              int(g[3]), int(g[4]), float(g[5]))\n        return _timegm(tt)\n\n    # No, we need some messy parsing...\n\n    # clean up\n    text = text.lstrip()\n    text = WEEKDAY_RE.sub(\"\", text, 1)  # Useless weekday\n\n    # tz is time zone specifier string\n    day, mon, yr, hr, min, sec, tz = [None]*7\n\n    # loose regexp parse\n    m = LOOSE_HTTP_DATE_RE.search(text)\n    if m is not None:\n        day, mon, yr, hr, min, sec, tz = m.groups()\n    else:\n        return None  # bad format\n\n    return _str2time(day, mon, yr, hr, min, sec, tz)\n\nISO_DATE_RE = re.compile(\n    \"\"\"^\n    (\\d{4})              # year\n       [-\\/]?\n    (\\d\\d?)              # numerical month\n       [-\\/]?\n    (\\d\\d?)              # day\n   (?:\n         (?:\\s+|[-:Tt])  # separator before clock\n      (\\d\\d?):?(\\d\\d)    # hour:min\n      (?::?(\\d\\d(?:\\.\\d*)?))?  # optional seconds (and fractional)\n   )?                    # optional clock\n      \\s*\n   ([-+]?\\d\\d?:?(:?\\d\\d)?\n    |Z|z)?               # timezone  (Z is \"zero meridian\", i.e. GMT)\n      \\s*$\"\"\", re.X)\ndef iso2time(text):\n    \"\"\"\n    As for http2time, but parses the ISO 8601 formats:\n\n    1994-02-03 14:15:29 -0100    -- ISO 8601 format\n    1994-02-03 14:15:29          -- zone is optional\n    1994-02-03                   -- only date\n    1994-02-03T14:15:29          -- Use T as separator\n    19940203T141529Z             -- ISO 8601 compact format\n    19940203                     -- only date\n\n    \"\"\"\n    # clean up\n    text = text.lstrip()\n\n    # tz is time zone specifier string\n    day, mon, yr, hr, min, sec, tz = [None]*7\n\n    # loose regexp parse\n    m = ISO_DATE_RE.search(text)\n    if m is not None:\n        # XXX there's an extra bit of the timezone I'm ignoring here: is\n        #   this the right thing to do?\n        yr, mon, day, hr, min, sec, tz, _ = m.groups()\n    else:\n        return None  # bad format\n\n    return _str2time(day, mon, yr, hr, min, sec, tz)\n\n\n# Header parsing\n# -----------------------------------------------------------------------------\n\ndef unmatched(match):\n    \"\"\"Return unmatched part of re.Match object.\"\"\"\n    start, end = match.span(0)\n    return match.string[:start]+match.string[end:]\n\nHEADER_TOKEN_RE =        re.compile(r\"^\\s*([^=\\s;,]+)\")\nHEADER_QUOTED_VALUE_RE = re.compile(r\"^\\s*=\\s*\\\"([^\\\"\\\\]*(?:\\\\.[^\\\"\\\\]*)*)\\\"\")\nHEADER_VALUE_RE =        re.compile(r\"^\\s*=\\s*([^\\s;,]*)\")\nHEADER_ESCAPE_RE = re.compile(r\"\\\\(.)\")\ndef split_header_words(header_values):\n    r\"\"\"Parse header values into a list of lists containing key,value pairs.\n\n    The function knows how to deal with \",\", \";\" and \"=\" as well as quoted\n    values after \"=\".  A list of space separated tokens are parsed as if they\n    were separated by \";\".\n\n    If the header_values passed as argument contains multiple values, then they\n    are treated as if they were a single value separated by comma \",\".\n\n    This means that this function is useful for parsing header fields that\n    follow this syntax (BNF as from the HTTP/1.1 specification, but we relax\n    the requirement for tokens).\n\n      headers           = #header\n      header            = (token | parameter) *( [\";\"] (token | parameter))\n\n      token             = 1*<any CHAR except CTLs or separators>\n      separators        = \"(\" | \")\" | \"<\" | \">\" | \"@\"\n                        | \",\" | \";\" | \":\" | \"\\\" | <\">\n                        | \"/\" | \"[\" | \"]\" | \"?\" | \"=\"\n                        | \"{\" | \"}\" | SP | HT\n\n      quoted-string     = ( <\"> *(qdtext | quoted-pair ) <\"> )\n      qdtext            = <any TEXT except <\">>\n      quoted-pair       = \"\\\" CHAR\n\n      parameter         = attribute \"=\" value\n      attribute         = token\n      value             = token | quoted-string\n\n    Each header is represented by a list of key/value pairs.  The value for a\n    simple token (not part of a parameter) is None.  Syntactically incorrect\n    headers will not necessarily be parsed as you would want.\n\n    This is easier to describe with some examples:\n\n    >>> split_header_words(['foo=\"bar\"; port=\"80,81\"; discard, bar=baz'])\n    [[('foo', 'bar'), ('port', '80,81'), ('discard', None)], [('bar', 'baz')]]\n    >>> split_header_words(['text/html; charset=\"iso-8859-1\"'])\n    [[('text/html', None), ('charset', 'iso-8859-1')]]\n    >>> split_header_words([r'Basic realm=\"\\\"foo\\bar\\\"\"'])\n    [[('Basic', None), ('realm', '\"foobar\"')]]\n\n    \"\"\"\n    assert not isinstance(header_values, basestring)\n    result = []\n    for text in header_values:\n        orig_text = text\n        pairs = []\n        while text:\n            m = HEADER_TOKEN_RE.search(text)\n            if m:\n                text = unmatched(m)\n                name = m.group(1)\n                m = HEADER_QUOTED_VALUE_RE.search(text)\n                if m:  # quoted value\n                    text = unmatched(m)\n                    value = m.group(1)\n                    value = HEADER_ESCAPE_RE.sub(r\"\\1\", value)\n                else:\n                    m = HEADER_VALUE_RE.search(text)\n                    if m:  # unquoted value\n                        text = unmatched(m)\n                        value = m.group(1)\n                        value = value.rstrip()\n                    else:\n                        # no value, a lone token\n                        value = None\n                pairs.append((name, value))\n            elif text.lstrip().startswith(\",\"):\n                # concatenated headers, as per RFC 2616 section 4.2\n                text = text.lstrip()[1:]\n                if pairs: result.append(pairs)\n                pairs = []\n            else:\n                # skip junk\n                non_junk, nr_junk_chars = re.subn(\"^[=\\s;]*\", \"\", text)\n                assert nr_junk_chars > 0, (\n                    \"split_header_words bug: '%s', '%s', %s\" %\n                    (orig_text, text, pairs))\n                text = non_junk\n        if pairs: result.append(pairs)\n    return result\n\nHEADER_JOIN_ESCAPE_RE = re.compile(r\"([\\\"\\\\])\")\ndef join_header_words(lists):\n    \"\"\"Do the inverse (almost) of the conversion done by split_header_words.\n\n    Takes a list of lists of (key, value) pairs and produces a single header\n    value.  Attribute values are quoted if needed.\n\n    >>> join_header_words([[(\"text/plain\", None), (\"charset\", \"iso-8859/1\")]])\n    'text/plain; charset=\"iso-8859/1\"'\n    >>> join_header_words([[(\"text/plain\", None)], [(\"charset\", \"iso-8859/1\")]])\n    'text/plain, charset=\"iso-8859/1\"'\n\n    \"\"\"\n    headers = []\n    for pairs in lists:\n        attr = []\n        for k, v in pairs:\n            if v is not None:\n                if not re.search(r\"^\\w+$\", v):\n                    v = HEADER_JOIN_ESCAPE_RE.sub(r\"\\\\\\1\", v)  # escape \" and \\\n                    v = '\"%s\"' % v\n                k = \"%s=%s\" % (k, v)\n            attr.append(k)\n        if attr: headers.append(\"; \".join(attr))\n    return \", \".join(headers)\n\ndef _strip_quotes(text):\n    if text.startswith('\"'):\n        text = text[1:]\n    if text.endswith('\"'):\n        text = text[:-1]\n    return text\n\ndef parse_ns_headers(ns_headers):\n    \"\"\"Ad-hoc parser for Netscape protocol cookie-attributes.\n\n    The old Netscape cookie format for Set-Cookie can for instance contain\n    an unquoted \",\" in the expires field, so we have to use this ad-hoc\n    parser instead of split_header_words.\n\n    XXX This may not make the best possible effort to parse all the crap\n    that Netscape Cookie headers contain.  Ronald Tschalar's HTTPClient\n    parser is probably better, so could do worse than following that if\n    this ever gives any trouble.\n\n    Currently, this is also used for parsing RFC 2109 cookies.\n\n    \"\"\"\n    known_attrs = (\"expires\", \"domain\", \"path\", \"secure\",\n                   # RFC 2109 attrs (may turn up in Netscape cookies, too)\n                   \"version\", \"port\", \"max-age\")\n\n    result = []\n    for ns_header in ns_headers:\n        pairs = []\n        version_set = False\n        for ii, param in enumerate(re.split(r\";\\s*\", ns_header)):\n            param = param.rstrip()\n            if param == \"\": continue\n            if \"=\" not in param:\n                k, v = param, None\n            else:\n                k, v = re.split(r\"\\s*=\\s*\", param, 1)\n                k = k.lstrip()\n            if ii != 0:\n                lc = k.lower()\n                if lc in known_attrs:\n                    k = lc\n                if k == \"version\":\n                    # This is an RFC 2109 cookie.\n                    v = _strip_quotes(v)\n                    version_set = True\n                if k == \"expires\":\n                    # convert expires date to seconds since epoch\n                    v = http2time(_strip_quotes(v))  # None if invalid\n            pairs.append((k, v))\n\n        if pairs:\n            if not version_set:\n                pairs.append((\"version\", \"0\"))\n            result.append(pairs)\n\n    return result\n\n\nIPV4_RE = re.compile(r\"\\.\\d+$\")\ndef is_HDN(text):\n    \"\"\"Return True if text is a host domain name.\"\"\"\n    # XXX\n    # This may well be wrong.  Which RFC is HDN defined in, if any (for\n    #  the purposes of RFC 2965)?\n    # For the current implementation, what about IPv6?  Remember to look\n    #  at other uses of IPV4_RE also, if change this.\n    if IPV4_RE.search(text):\n        return False\n    if text == \"\":\n        return False\n    if text[0] == \".\" or text[-1] == \".\":\n        return False\n    return True\n\ndef domain_match(A, B):\n    \"\"\"Return True if domain A domain-matches domain B, according to RFC 2965.\n\n    A and B may be host domain names or IP addresses.\n\n    RFC 2965, section 1:\n\n    Host names can be specified either as an IP address or a HDN string.\n    Sometimes we compare one host name with another.  (Such comparisons SHALL\n    be case-insensitive.)  Host A's name domain-matches host B's if\n\n         *  their host name strings string-compare equal; or\n\n         * A is a HDN string and has the form NB, where N is a non-empty\n            name string, B has the form .B', and B' is a HDN string.  (So,\n            x.y.com domain-matches .Y.com but not Y.com.)\n\n    Note that domain-match is not a commutative operation: a.b.c.com\n    domain-matches .c.com, but not the reverse.\n\n    \"\"\"\n    # Note that, if A or B are IP addresses, the only relevant part of the\n    # definition of the domain-match algorithm is the direct string-compare.\n    A = A.lower()\n    B = B.lower()\n    if A == B:\n        return True\n    if not is_HDN(A):\n        return False\n    i = A.rfind(B)\n    if i == -1 or i == 0:\n        # A does not have form NB, or N is the empty string\n        return False\n    if not B.startswith(\".\"):\n        return False\n    if not is_HDN(B[1:]):\n        return False\n    return True\n\ndef liberal_is_HDN(text):\n    \"\"\"Return True if text is a sort-of-like a host domain name.\n\n    For accepting/blocking domains.\n\n    \"\"\"\n    if IPV4_RE.search(text):\n        return False\n    return True\n\ndef user_domain_match(A, B):\n    \"\"\"For blocking/accepting domains.\n\n    A and B may be host domain names or IP addresses.\n\n    \"\"\"\n    A = A.lower()\n    B = B.lower()\n    if not (liberal_is_HDN(A) and liberal_is_HDN(B)):\n        if A == B:\n            # equal IP addresses\n            return True\n        return False\n    initial_dot = B.startswith(\".\")\n    if initial_dot and A.endswith(B):\n        return True\n    if not initial_dot and A == B:\n        return True\n    return False\n\ncut_port_re = re.compile(r\":\\d+$\")\ndef request_host(request):\n    \"\"\"Return request-host, as defined by RFC 2965.\n\n    Variation from RFC: returned value is lowercased, for convenient\n    comparison.\n\n    \"\"\"\n    url = request.get_full_url()\n    host = urlparse.urlparse(url)[1]\n    if host == \"\":\n        host = request.get_header(\"Host\", \"\")\n\n    # remove port, if present\n    host = cut_port_re.sub(\"\", host, 1)\n    return host.lower()\n\ndef eff_request_host(request):\n    \"\"\"Return a tuple (request-host, effective request-host name).\n\n    As defined by RFC 2965, except both are lowercased.\n\n    \"\"\"\n    erhn = req_host = request_host(request)\n    if req_host.find(\".\") == -1 and not IPV4_RE.search(req_host):\n        erhn = req_host + \".local\"\n    return req_host, erhn\n\ndef request_path(request):\n    \"\"\"Path component of request-URI, as defined by RFC 2965.\"\"\"\n    url = request.get_full_url()\n    parts = urlparse.urlsplit(url)\n    path = escape_path(parts.path)\n    if not path.startswith(\"/\"):\n        # fix bad RFC 2396 absoluteURI\n        path = \"/\" + path\n    return path\n\ndef request_port(request):\n    host = request.get_host()\n    i = host.find(':')\n    if i >= 0:\n        port = host[i+1:]\n        try:\n            int(port)\n        except ValueError:\n            _debug(\"nonnumeric port: '%s'\", port)\n            return None\n    else:\n        port = DEFAULT_HTTP_PORT\n    return port\n\n# Characters in addition to A-Z, a-z, 0-9, '_', '.', and '-' that don't\n# need to be escaped to form a valid HTTP URL (RFCs 2396 and 1738).\nHTTP_PATH_SAFE = \"%/;:@&=+$,!~*'()\"\nESCAPED_CHAR_RE = re.compile(r\"%([0-9a-fA-F][0-9a-fA-F])\")\ndef uppercase_escaped_char(match):\n    return \"%%%s\" % match.group(1).upper()\ndef escape_path(path):\n    \"\"\"Escape any invalid characters in HTTP URL, and uppercase all escapes.\"\"\"\n    # There's no knowing what character encoding was used to create URLs\n    # containing %-escapes, but since we have to pick one to escape invalid\n    # path characters, we pick UTF-8, as recommended in the HTML 4.0\n    # specification:\n    # http://www.w3.org/TR/REC-html40/appendix/notes.html#h-B.2.1\n    # And here, kind of: draft-fielding-uri-rfc2396bis-03\n    # (And in draft IRI specification: draft-duerst-iri-05)\n    # (And here, for new URI schemes: RFC 2718)\n    if isinstance(path, unicode):\n        path = path.encode(\"utf-8\")\n    path = urllib.quote(path, HTTP_PATH_SAFE)\n    path = ESCAPED_CHAR_RE.sub(uppercase_escaped_char, path)\n    return path\n\ndef reach(h):\n    \"\"\"Return reach of host h, as defined by RFC 2965, section 1.\n\n    The reach R of a host name H is defined as follows:\n\n       *  If\n\n          -  H is the host domain name of a host; and,\n\n          -  H has the form A.B; and\n\n          -  A has no embedded (that is, interior) dots; and\n\n          -  B has at least one embedded dot, or B is the string \"local\".\n             then the reach of H is .B.\n\n       *  Otherwise, the reach of H is H.\n\n    >>> reach(\"www.acme.com\")\n    '.acme.com'\n    >>> reach(\"acme.com\")\n    'acme.com'\n    >>> reach(\"acme.local\")\n    '.local'\n\n    \"\"\"\n    i = h.find(\".\")\n    if i >= 0:\n        #a = h[:i]  # this line is only here to show what a is\n        b = h[i+1:]\n        i = b.find(\".\")\n        if is_HDN(h) and (i >= 0 or b == \"local\"):\n            return \".\"+b\n    return h\n\ndef is_third_party(request):\n    \"\"\"\n\n    RFC 2965, section 3.3.6:\n\n        An unverifiable transaction is to a third-party host if its request-\n        host U does not domain-match the reach R of the request-host O in the\n        origin transaction.\n\n    \"\"\"\n    req_host = request_host(request)\n    if not domain_match(req_host, reach(request.get_origin_req_host())):\n        return True\n    else:\n        return False\n\n\nclass Cookie:\n    \"\"\"HTTP Cookie.\n\n    This class represents both Netscape and RFC 2965 cookies.\n\n    This is deliberately a very simple class.  It just holds attributes.  It's\n    possible to construct Cookie instances that don't comply with the cookie\n    standards.  CookieJar.make_cookies is the factory function for Cookie\n    objects -- it deals with cookie parsing, supplying defaults, and\n    normalising to the representation used in this class.  CookiePolicy is\n    responsible for checking them to see whether they should be accepted from\n    and returned to the server.\n\n    Note that the port may be present in the headers, but unspecified (\"Port\"\n    rather than\"Port=80\", for example); if this is the case, port is None.\n\n    \"\"\"\n\n    def __init__(self, version, name, value,\n                 port, port_specified,\n                 domain, domain_specified, domain_initial_dot,\n                 path, path_specified,\n                 secure,\n                 expires,\n                 discard,\n                 comment,\n                 comment_url,\n                 rest,\n                 rfc2109=False,\n                 ):\n\n        if version is not None: version = int(version)\n        if expires is not None: expires = int(expires)\n        if port is None and port_specified is True:\n            raise ValueError(\"if port is None, port_specified must be false\")\n\n        self.version = version\n        self.name = name\n        self.value = value\n        self.port = port\n        self.port_specified = port_specified\n        # normalise case, as per RFC 2965 section 3.3.3\n        self.domain = domain.lower()\n        self.domain_specified = domain_specified\n        # Sigh.  We need to know whether the domain given in the\n        # cookie-attribute had an initial dot, in order to follow RFC 2965\n        # (as clarified in draft errata).  Needed for the returned $Domain\n        # value.\n        self.domain_initial_dot = domain_initial_dot\n        self.path = path\n        self.path_specified = path_specified\n        self.secure = secure\n        self.expires = expires\n        self.discard = discard\n        self.comment = comment\n        self.comment_url = comment_url\n        self.rfc2109 = rfc2109\n\n        self._rest = copy.copy(rest)\n\n    def has_nonstandard_attr(self, name):\n        return name in self._rest\n    def get_nonstandard_attr(self, name, default=None):\n        return self._rest.get(name, default)\n    def set_nonstandard_attr(self, name, value):\n        self._rest[name] = value\n\n    def is_expired(self, now=None):\n        if now is None: now = time.time()\n        if (self.expires is not None) and (self.expires <= now):\n            return True\n        return False\n\n    def __str__(self):\n        if self.port is None: p = \"\"\n        else: p = \":\"+self.port\n        limit = self.domain + p + self.path\n        if self.value is not None:\n            namevalue = \"%s=%s\" % (self.name, self.value)\n        else:\n            namevalue = self.name\n        return \"<Cookie %s for %s>\" % (namevalue, limit)\n\n    def __repr__(self):\n        args = []\n        for name in (\"version\", \"name\", \"value\",\n                     \"port\", \"port_specified\",\n                     \"domain\", \"domain_specified\", \"domain_initial_dot\",\n                     \"path\", \"path_specified\",\n                     \"secure\", \"expires\", \"discard\", \"comment\", \"comment_url\",\n                     ):\n            attr = getattr(self, name)\n            args.append(\"%s=%s\" % (name, repr(attr)))\n        args.append(\"rest=%s\" % repr(self._rest))\n        args.append(\"rfc2109=%s\" % repr(self.rfc2109))\n        return \"Cookie(%s)\" % \", \".join(args)\n\n\nclass CookiePolicy:\n    \"\"\"Defines which cookies get accepted from and returned to server.\n\n    May also modify cookies, though this is probably a bad idea.\n\n    The subclass DefaultCookiePolicy defines the standard rules for Netscape\n    and RFC 2965 cookies -- override that if you want a customised policy.\n\n    \"\"\"\n    def set_ok(self, cookie, request):\n        \"\"\"Return true if (and only if) cookie should be accepted from server.\n\n        Currently, pre-expired cookies never get this far -- the CookieJar\n        class deletes such cookies itself.\n\n        \"\"\"\n        raise NotImplementedError()\n\n    def return_ok(self, cookie, request):\n        \"\"\"Return true if (and only if) cookie should be returned to server.\"\"\"\n        raise NotImplementedError()\n\n    def domain_return_ok(self, domain, request):\n        \"\"\"Return false if cookies should not be returned, given cookie domain.\n        \"\"\"\n        return True\n\n    def path_return_ok(self, path, request):\n        \"\"\"Return false if cookies should not be returned, given cookie path.\n        \"\"\"\n        return True\n\n\nclass DefaultCookiePolicy(CookiePolicy):\n    \"\"\"Implements the standard rules for accepting and returning cookies.\"\"\"\n\n    DomainStrictNoDots = 1\n    DomainStrictNonDomain = 2\n    DomainRFC2965Match = 4\n\n    DomainLiberal = 0\n    DomainStrict = DomainStrictNoDots|DomainStrictNonDomain\n\n    def __init__(self,\n                 blocked_domains=None, allowed_domains=None,\n                 netscape=True, rfc2965=False,\n                 rfc2109_as_netscape=None,\n                 hide_cookie2=False,\n                 strict_domain=False,\n                 strict_rfc2965_unverifiable=True,\n                 strict_ns_unverifiable=False,\n                 strict_ns_domain=DomainLiberal,\n                 strict_ns_set_initial_dollar=False,\n                 strict_ns_set_path=False,\n                 ):\n        \"\"\"Constructor arguments should be passed as keyword arguments only.\"\"\"\n        self.netscape = netscape\n        self.rfc2965 = rfc2965\n        self.rfc2109_as_netscape = rfc2109_as_netscape\n        self.hide_cookie2 = hide_cookie2\n        self.strict_domain = strict_domain\n        self.strict_rfc2965_unverifiable = strict_rfc2965_unverifiable\n        self.strict_ns_unverifiable = strict_ns_unverifiable\n        self.strict_ns_domain = strict_ns_domain\n        self.strict_ns_set_initial_dollar = strict_ns_set_initial_dollar\n        self.strict_ns_set_path = strict_ns_set_path\n\n        if blocked_domains is not None:\n            self._blocked_domains = tuple(blocked_domains)\n        else:\n            self._blocked_domains = ()\n\n        if allowed_domains is not None:\n            allowed_domains = tuple(allowed_domains)\n        self._allowed_domains = allowed_domains\n\n    def blocked_domains(self):\n        \"\"\"Return the sequence of blocked domains (as a tuple).\"\"\"\n        return self._blocked_domains\n    def set_blocked_domains(self, blocked_domains):\n        \"\"\"Set the sequence of blocked domains.\"\"\"\n        self._blocked_domains = tuple(blocked_domains)\n\n    def is_blocked(self, domain):\n        for blocked_domain in self._blocked_domains:\n            if user_domain_match(domain, blocked_domain):\n                return True\n        return False\n\n    def allowed_domains(self):\n        \"\"\"Return None, or the sequence of allowed domains (as a tuple).\"\"\"\n        return self._allowed_domains\n    def set_allowed_domains(self, allowed_domains):\n        \"\"\"Set the sequence of allowed domains, or None.\"\"\"\n        if allowed_domains is not None:\n            allowed_domains = tuple(allowed_domains)\n        self._allowed_domains = allowed_domains\n\n    def is_not_allowed(self, domain):\n        if self._allowed_domains is None:\n            return False\n        for allowed_domain in self._allowed_domains:\n            if user_domain_match(domain, allowed_domain):\n                return False\n        return True\n\n    def set_ok(self, cookie, request):\n        \"\"\"\n        If you override .set_ok(), be sure to call this method.  If it returns\n        false, so should your subclass (assuming your subclass wants to be more\n        strict about which cookies to accept).\n\n        \"\"\"\n        _debug(\" - checking cookie %s=%s\", cookie.name, cookie.value)\n\n        assert cookie.name is not None\n\n        for n in \"version\", \"verifiability\", \"name\", \"path\", \"domain\", \"port\":\n            fn_name = \"set_ok_\"+n\n            fn = getattr(self, fn_name)\n            if not fn(cookie, request):\n                return False\n\n        return True\n\n    def set_ok_version(self, cookie, request):\n        if cookie.version is None:\n            # Version is always set to 0 by parse_ns_headers if it's a Netscape\n            # cookie, so this must be an invalid RFC 2965 cookie.\n            _debug(\"   Set-Cookie2 without version attribute (%s=%s)\",\n                   cookie.name, cookie.value)\n            return False\n        if cookie.version > 0 and not self.rfc2965:\n            _debug(\"   RFC 2965 cookies are switched off\")\n            return False\n        elif cookie.version == 0 and not self.netscape:\n            _debug(\"   Netscape cookies are switched off\")\n            return False\n        return True\n\n    def set_ok_verifiability(self, cookie, request):\n        if request.is_unverifiable() and is_third_party(request):\n            if cookie.version > 0 and self.strict_rfc2965_unverifiable:\n                _debug(\"   third-party RFC 2965 cookie during \"\n                             \"unverifiable transaction\")\n                return False\n            elif cookie.version == 0 and self.strict_ns_unverifiable:\n                _debug(\"   third-party Netscape cookie during \"\n                             \"unverifiable transaction\")\n                return False\n        return True\n\n    def set_ok_name(self, cookie, request):\n        # Try and stop servers setting V0 cookies designed to hack other\n        # servers that know both V0 and V1 protocols.\n        if (cookie.version == 0 and self.strict_ns_set_initial_dollar and\n            cookie.name.startswith(\"$\")):\n            _debug(\"   illegal name (starts with '$'): '%s'\", cookie.name)\n            return False\n        return True\n\n    def set_ok_path(self, cookie, request):\n        if cookie.path_specified:\n            req_path = request_path(request)\n            if ((cookie.version > 0 or\n                 (cookie.version == 0 and self.strict_ns_set_path)) and\n                not req_path.startswith(cookie.path)):\n                _debug(\"   path attribute %s is not a prefix of request \"\n                       \"path %s\", cookie.path, req_path)\n                return False\n        return True\n\n    def set_ok_domain(self, cookie, request):\n        if self.is_blocked(cookie.domain):\n            _debug(\"   domain %s is in user block-list\", cookie.domain)\n            return False\n        if self.is_not_allowed(cookie.domain):\n            _debug(\"   domain %s is not in user allow-list\", cookie.domain)\n            return False\n        if cookie.domain_specified:\n            req_host, erhn = eff_request_host(request)\n            domain = cookie.domain\n            if self.strict_domain and (domain.count(\".\") >= 2):\n                # XXX This should probably be compared with the Konqueror\n                # (kcookiejar.cpp) and Mozilla implementations, but it's a\n                # losing battle.\n                i = domain.rfind(\".\")\n                j = domain.rfind(\".\", 0, i)\n                if j == 0:  # domain like .foo.bar\n                    tld = domain[i+1:]\n                    sld = domain[j+1:i]\n                    if sld.lower() in (\"co\", \"ac\", \"com\", \"edu\", \"org\", \"net\",\n                       \"gov\", \"mil\", \"int\", \"aero\", \"biz\", \"cat\", \"coop\",\n                       \"info\", \"jobs\", \"mobi\", \"museum\", \"name\", \"pro\",\n                       \"travel\", \"eu\") and len(tld) == 2:\n                        # domain like .co.uk\n                        _debug(\"   country-code second level domain %s\", domain)\n                        return False\n            if domain.startswith(\".\"):\n                undotted_domain = domain[1:]\n            else:\n                undotted_domain = domain\n            embedded_dots = (undotted_domain.find(\".\") >= 0)\n            if not embedded_dots and domain != \".local\":\n                _debug(\"   non-local domain %s contains no embedded dot\",\n                       domain)\n                return False\n            if cookie.version == 0:\n                if (not erhn.endswith(domain) and\n                    (not erhn.startswith(\".\") and\n                     not (\".\"+erhn).endswith(domain))):\n                    _debug(\"   effective request-host %s (even with added \"\n                           \"initial dot) does not end with %s\",\n                           erhn, domain)\n                    return False\n            if (cookie.version > 0 or\n                (self.strict_ns_domain & self.DomainRFC2965Match)):\n                if not domain_match(erhn, domain):\n                    _debug(\"   effective request-host %s does not domain-match \"\n                           \"%s\", erhn, domain)\n                    return False\n            if (cookie.version > 0 or\n                (self.strict_ns_domain & self.DomainStrictNoDots)):\n                host_prefix = req_host[:-len(domain)]\n                if (host_prefix.find(\".\") >= 0 and\n                    not IPV4_RE.search(req_host)):\n                    _debug(\"   host prefix %s for domain %s contains a dot\",\n                           host_prefix, domain)\n                    return False\n        return True\n\n    def set_ok_port(self, cookie, request):\n        if cookie.port_specified:\n            req_port = request_port(request)\n            if req_port is None:\n                req_port = \"80\"\n            else:\n                req_port = str(req_port)\n            for p in cookie.port.split(\",\"):\n                try:\n                    int(p)\n                except ValueError:\n                    _debug(\"   bad port %s (not numeric)\", p)\n                    return False\n                if p == req_port:\n                    break\n            else:\n                _debug(\"   request port (%s) not found in %s\",\n                       req_port, cookie.port)\n                return False\n        return True\n\n    def return_ok(self, cookie, request):\n        \"\"\"\n        If you override .return_ok(), be sure to call this method.  If it\n        returns false, so should your subclass (assuming your subclass wants to\n        be more strict about which cookies to return).\n\n        \"\"\"\n        # Path has already been checked by .path_return_ok(), and domain\n        # blocking done by .domain_return_ok().\n        _debug(\" - checking cookie %s=%s\", cookie.name, cookie.value)\n\n        for n in \"version\", \"verifiability\", \"secure\", \"expires\", \"port\", \"domain\":\n            fn_name = \"return_ok_\"+n\n            fn = getattr(self, fn_name)\n            if not fn(cookie, request):\n                return False\n        return True\n\n    def return_ok_version(self, cookie, request):\n        if cookie.version > 0 and not self.rfc2965:\n            _debug(\"   RFC 2965 cookies are switched off\")\n            return False\n        elif cookie.version == 0 and not self.netscape:\n            _debug(\"   Netscape cookies are switched off\")\n            return False\n        return True\n\n    def return_ok_verifiability(self, cookie, request):\n        if request.is_unverifiable() and is_third_party(request):\n            if cookie.version > 0 and self.strict_rfc2965_unverifiable:\n                _debug(\"   third-party RFC 2965 cookie during unverifiable \"\n                       \"transaction\")\n                return False\n            elif cookie.version == 0 and self.strict_ns_unverifiable:\n                _debug(\"   third-party Netscape cookie during unverifiable \"\n                       \"transaction\")\n                return False\n        return True\n\n    def return_ok_secure(self, cookie, request):\n        if cookie.secure and request.get_type() != \"https\":\n            _debug(\"   secure cookie with non-secure request\")\n            return False\n        return True\n\n    def return_ok_expires(self, cookie, request):\n        if cookie.is_expired(self._now):\n            _debug(\"   cookie expired\")\n            return False\n        return True\n\n    def return_ok_port(self, cookie, request):\n        if cookie.port:\n            req_port = request_port(request)\n            if req_port is None:\n                req_port = \"80\"\n            for p in cookie.port.split(\",\"):\n                if p == req_port:\n                    break\n            else:\n                _debug(\"   request port %s does not match cookie port %s\",\n                       req_port, cookie.port)\n                return False\n        return True\n\n    def return_ok_domain(self, cookie, request):\n        req_host, erhn = eff_request_host(request)\n        domain = cookie.domain\n\n        # strict check of non-domain cookies: Mozilla does this, MSIE5 doesn't\n        if (cookie.version == 0 and\n            (self.strict_ns_domain & self.DomainStrictNonDomain) and\n            not cookie.domain_specified and domain != erhn):\n            _debug(\"   cookie with unspecified domain does not string-compare \"\n                   \"equal to request domain\")\n            return False\n\n        if cookie.version > 0 and not domain_match(erhn, domain):\n            _debug(\"   effective request-host name %s does not domain-match \"\n                   \"RFC 2965 cookie domain %s\", erhn, domain)\n            return False\n        if cookie.version == 0 and not (\".\"+erhn).endswith(domain):\n            _debug(\"   request-host %s does not match Netscape cookie domain \"\n                   \"%s\", req_host, domain)\n            return False\n        return True\n\n    def domain_return_ok(self, domain, request):\n        # Liberal check of.  This is here as an optimization to avoid\n        # having to load lots of MSIE cookie files unless necessary.\n        req_host, erhn = eff_request_host(request)\n        if not req_host.startswith(\".\"):\n            req_host = \".\"+req_host\n        if not erhn.startswith(\".\"):\n            erhn = \".\"+erhn\n        if not (req_host.endswith(domain) or erhn.endswith(domain)):\n            #_debug(\"   request domain %s does not match cookie domain %s\",\n            #       req_host, domain)\n            return False\n\n        if self.is_blocked(domain):\n            _debug(\"   domain %s is in user block-list\", domain)\n            return False\n        if self.is_not_allowed(domain):\n            _debug(\"   domain %s is not in user allow-list\", domain)\n            return False\n\n        return True\n\n    def path_return_ok(self, path, request):\n        _debug(\"- checking cookie path=%s\", path)\n        req_path = request_path(request)\n        if not req_path.startswith(path):\n            _debug(\"  %s does not path-match %s\", req_path, path)\n            return False\n        return True\n\n\ndef vals_sorted_by_key(adict):\n    keys = adict.keys()\n    keys.sort()\n    return map(adict.get, keys)\n\ndef deepvalues(mapping):\n    \"\"\"Iterates over nested mapping, depth-first, in sorted order by key.\"\"\"\n    values = vals_sorted_by_key(mapping)\n    for obj in values:\n        mapping = False\n        try:\n            obj.items\n        except AttributeError:\n            pass\n        else:\n            mapping = True\n            for subobj in deepvalues(obj):\n                yield subobj\n        if not mapping:\n            yield obj\n\n\n# Used as second parameter to dict.get() method, to distinguish absent\n# dict key from one with a None value.\nclass Absent: pass\n\nclass CookieJar:\n    \"\"\"Collection of HTTP cookies.\n\n    You may not need to know about this class: try\n    urllib2.build_opener(HTTPCookieProcessor).open(url).\n\n    \"\"\"\n\n    non_word_re = re.compile(r\"\\W\")\n    quote_re = re.compile(r\"([\\\"\\\\])\")\n    strict_domain_re = re.compile(r\"\\.?[^.]*\")\n    domain_re = re.compile(r\"[^.]*\")\n    dots_re = re.compile(r\"^\\.+\")\n\n    magic_re = r\"^\\#LWP-Cookies-(\\d+\\.\\d+)\"\n\n    def __init__(self, policy=None):\n        if policy is None:\n            policy = DefaultCookiePolicy()\n        self._policy = policy\n\n        self._cookies_lock = _threading.RLock()\n        self._cookies = {}\n\n    def set_policy(self, policy):\n        self._policy = policy\n\n    def _cookies_for_domain(self, domain, request):\n        cookies = []\n        if not self._policy.domain_return_ok(domain, request):\n            return []\n        _debug(\"Checking %s for cookies to return\", domain)\n        cookies_by_path = self._cookies[domain]\n        for path in cookies_by_path.keys():\n            if not self._policy.path_return_ok(path, request):\n                continue\n            cookies_by_name = cookies_by_path[path]\n            for cookie in cookies_by_name.values():\n                if not self._policy.return_ok(cookie, request):\n                    _debug(\"   not returning cookie\")\n                    continue\n                _debug(\"   it's a match\")\n                cookies.append(cookie)\n        return cookies\n\n    def _cookies_for_request(self, request):\n        \"\"\"Return a list of cookies to be returned to server.\"\"\"\n        cookies = []\n        for domain in self._cookies.keys():\n            cookies.extend(self._cookies_for_domain(domain, request))\n        return cookies\n\n    def _cookie_attrs(self, cookies):\n        \"\"\"Return a list of cookie-attributes to be returned to server.\n\n        like ['foo=\"bar\"; $Path=\"/\"', ...]\n\n        The $Version attribute is also added when appropriate (currently only\n        once per request).\n\n        \"\"\"\n        # add cookies in order of most specific (ie. longest) path first\n        cookies.sort(key=lambda arg: len(arg.path), reverse=True)\n\n        version_set = False\n\n        attrs = []\n        for cookie in cookies:\n            # set version of Cookie header\n            # XXX\n            # What should it be if multiple matching Set-Cookie headers have\n            #  different versions themselves?\n            # Answer: there is no answer; was supposed to be settled by\n            #  RFC 2965 errata, but that may never appear...\n            version = cookie.version\n            if not version_set:\n                version_set = True\n                if version > 0:\n                    attrs.append(\"$Version=%s\" % version)\n\n            # quote cookie value if necessary\n            # (not for Netscape protocol, which already has any quotes\n            #  intact, due to the poorly-specified Netscape Cookie: syntax)\n            if ((cookie.value is not None) and\n                self.non_word_re.search(cookie.value) and version > 0):\n                value = self.quote_re.sub(r\"\\\\\\1\", cookie.value)\n            else:\n                value = cookie.value\n\n            # add cookie-attributes to be returned in Cookie header\n            if cookie.value is None:\n                attrs.append(cookie.name)\n            else:\n                attrs.append(\"%s=%s\" % (cookie.name, value))\n            if version > 0:\n                if cookie.path_specified:\n                    attrs.append('$Path=\"%s\"' % cookie.path)\n                if cookie.domain.startswith(\".\"):\n                    domain = cookie.domain\n                    if (not cookie.domain_initial_dot and\n                        domain.startswith(\".\")):\n                        domain = domain[1:]\n                    attrs.append('$Domain=\"%s\"' % domain)\n                if cookie.port is not None:\n                    p = \"$Port\"\n                    if cookie.port_specified:\n                        p = p + ('=\"%s\"' % cookie.port)\n                    attrs.append(p)\n\n        return attrs\n\n    def add_cookie_header(self, request):\n        \"\"\"Add correct Cookie: header to request (urllib2.Request object).\n\n        The Cookie2 header is also added unless policy.hide_cookie2 is true.\n\n        \"\"\"\n        _debug(\"add_cookie_header\")\n        self._cookies_lock.acquire()\n        try:\n\n            self._policy._now = self._now = int(time.time())\n\n            cookies = self._cookies_for_request(request)\n\n            attrs = self._cookie_attrs(cookies)\n            if attrs:\n                if not request.has_header(\"Cookie\"):\n                    request.add_unredirected_header(\n                        \"Cookie\", \"; \".join(attrs))\n\n            # if necessary, advertise that we know RFC 2965\n            if (self._policy.rfc2965 and not self._policy.hide_cookie2 and\n                not request.has_header(\"Cookie2\")):\n                for cookie in cookies:\n                    if cookie.version != 1:\n                        request.add_unredirected_header(\"Cookie2\", '$Version=\"1\"')\n                        break\n\n        finally:\n            self._cookies_lock.release()\n\n        self.clear_expired_cookies()\n\n    def _normalized_cookie_tuples(self, attrs_set):\n        \"\"\"Return list of tuples containing normalised cookie information.\n\n        attrs_set is the list of lists of key,value pairs extracted from\n        the Set-Cookie or Set-Cookie2 headers.\n\n        Tuples are name, value, standard, rest, where name and value are the\n        cookie name and value, standard is a dictionary containing the standard\n        cookie-attributes (discard, secure, version, expires or max-age,\n        domain, path and port) and rest is a dictionary containing the rest of\n        the cookie-attributes.\n\n        \"\"\"\n        cookie_tuples = []\n\n        boolean_attrs = \"discard\", \"secure\"\n        value_attrs = (\"version\",\n                       \"expires\", \"max-age\",\n                       \"domain\", \"path\", \"port\",\n                       \"comment\", \"commenturl\")\n\n        for cookie_attrs in attrs_set:\n            name, value = cookie_attrs[0]\n\n            # Build dictionary of standard cookie-attributes (standard) and\n            # dictionary of other cookie-attributes (rest).\n\n            # Note: expiry time is normalised to seconds since epoch.  V0\n            # cookies should have the Expires cookie-attribute, and V1 cookies\n            # should have Max-Age, but since V1 includes RFC 2109 cookies (and\n            # since V0 cookies may be a mish-mash of Netscape and RFC 2109), we\n            # accept either (but prefer Max-Age).\n            max_age_set = False\n\n            bad_cookie = False\n\n            standard = {}\n            rest = {}\n            for k, v in cookie_attrs[1:]:\n                lc = k.lower()\n                # don't lose case distinction for unknown fields\n                if lc in value_attrs or lc in boolean_attrs:\n                    k = lc\n                if k in boolean_attrs and v is None:\n                    # boolean cookie-attribute is present, but has no value\n                    # (like \"discard\", rather than \"port=80\")\n                    v = True\n                if k in standard:\n                    # only first value is significant\n                    continue\n                if k == \"domain\":\n                    if v is None:\n                        _debug(\"   missing value for domain attribute\")\n                        bad_cookie = True\n                        break\n                    # RFC 2965 section 3.3.3\n                    v = v.lower()\n                if k == \"expires\":\n                    if max_age_set:\n                        # Prefer max-age to expires (like Mozilla)\n                        continue\n                    if v is None:\n                        _debug(\"   missing or invalid value for expires \"\n                              \"attribute: treating as session cookie\")\n                        continue\n                if k == \"max-age\":\n                    max_age_set = True\n                    try:\n                        v = int(v)\n                    except ValueError:\n                        _debug(\"   missing or invalid (non-numeric) value for \"\n                              \"max-age attribute\")\n                        bad_cookie = True\n                        break\n                    # convert RFC 2965 Max-Age to seconds since epoch\n                    # XXX Strictly you're supposed to follow RFC 2616\n                    #   age-calculation rules.  Remember that zero Max-Age is a\n                    #   is a request to discard (old and new) cookie, though.\n                    k = \"expires\"\n                    v = self._now + v\n                if (k in value_attrs) or (k in boolean_attrs):\n                    if (v is None and\n                        k not in (\"port\", \"comment\", \"commenturl\")):\n                        _debug(\"   missing value for %s attribute\" % k)\n                        bad_cookie = True\n                        break\n                    standard[k] = v\n                else:\n                    rest[k] = v\n\n            if bad_cookie:\n                continue\n\n            cookie_tuples.append((name, value, standard, rest))\n\n        return cookie_tuples\n\n    def _cookie_from_cookie_tuple(self, tup, request):\n        # standard is dict of standard cookie-attributes, rest is dict of the\n        # rest of them\n        name, value, standard, rest = tup\n\n        domain = standard.get(\"domain\", Absent)\n        path = standard.get(\"path\", Absent)\n        port = standard.get(\"port\", Absent)\n        expires = standard.get(\"expires\", Absent)\n\n        # set the easy defaults\n        version = standard.get(\"version\", None)\n        if version is not None:\n            try:\n                version = int(version)\n            except ValueError:\n                return None  # invalid version, ignore cookie\n        secure = standard.get(\"secure\", False)\n        # (discard is also set if expires is Absent)\n        discard = standard.get(\"discard\", False)\n        comment = standard.get(\"comment\", None)\n        comment_url = standard.get(\"commenturl\", None)\n\n        # set default path\n        if path is not Absent and path != \"\":\n            path_specified = True\n            path = escape_path(path)\n        else:\n            path_specified = False\n            path = request_path(request)\n            i = path.rfind(\"/\")\n            if i != -1:\n                if version == 0:\n                    # Netscape spec parts company from reality here\n                    path = path[:i]\n                else:\n                    path = path[:i+1]\n            if len(path) == 0: path = \"/\"\n\n        # set default domain\n        domain_specified = domain is not Absent\n        # but first we have to remember whether it starts with a dot\n        domain_initial_dot = False\n        if domain_specified:\n            domain_initial_dot = bool(domain.startswith(\".\"))\n        if domain is Absent:\n            req_host, erhn = eff_request_host(request)\n            domain = erhn\n        elif not domain.startswith(\".\"):\n            domain = \".\"+domain\n\n        # set default port\n        port_specified = False\n        if port is not Absent:\n            if port is None:\n                # Port attr present, but has no value: default to request port.\n                # Cookie should then only be sent back on that port.\n                port = request_port(request)\n            else:\n                port_specified = True\n                port = re.sub(r\"\\s+\", \"\", port)\n        else:\n            # No port attr present.  Cookie can be sent back on any port.\n            port = None\n\n        # set default expires and discard\n        if expires is Absent:\n            expires = None\n            discard = True\n        elif expires <= self._now:\n            # Expiry date in past is request to delete cookie.  This can't be\n            # in DefaultCookiePolicy, because can't delete cookies there.\n            try:\n                self.clear(domain, path, name)\n            except KeyError:\n                pass\n            _debug(\"Expiring cookie, domain='%s', path='%s', name='%s'\",\n                   domain, path, name)\n            return None\n\n        return Cookie(version,\n                      name, value,\n                      port, port_specified,\n                      domain, domain_specified, domain_initial_dot,\n                      path, path_specified,\n                      secure,\n                      expires,\n                      discard,\n                      comment,\n                      comment_url,\n                      rest)\n\n    def _cookies_from_attrs_set(self, attrs_set, request):\n        cookie_tuples = self._normalized_cookie_tuples(attrs_set)\n\n        cookies = []\n        for tup in cookie_tuples:\n            cookie = self._cookie_from_cookie_tuple(tup, request)\n            if cookie: cookies.append(cookie)\n        return cookies\n\n    def _process_rfc2109_cookies(self, cookies):\n        rfc2109_as_ns = getattr(self._policy, 'rfc2109_as_netscape', None)\n        if rfc2109_as_ns is None:\n            rfc2109_as_ns = not self._policy.rfc2965\n        for cookie in cookies:\n            if cookie.version == 1:\n                cookie.rfc2109 = True\n                if rfc2109_as_ns:\n                    # treat 2109 cookies as Netscape cookies rather than\n                    # as RFC2965 cookies\n                    cookie.version = 0\n\n    def make_cookies(self, response, request):\n        \"\"\"Return sequence of Cookie objects extracted from response object.\"\"\"\n        # get cookie-attributes for RFC 2965 and Netscape protocols\n        headers = response.info()\n        rfc2965_hdrs = headers.getheaders(\"Set-Cookie2\")\n        ns_hdrs = headers.getheaders(\"Set-Cookie\")\n\n        rfc2965 = self._policy.rfc2965\n        netscape = self._policy.netscape\n\n        if ((not rfc2965_hdrs and not ns_hdrs) or\n            (not ns_hdrs and not rfc2965) or\n            (not rfc2965_hdrs and not netscape) or\n            (not netscape and not rfc2965)):\n            return []  # no relevant cookie headers: quick exit\n\n        try:\n            cookies = self._cookies_from_attrs_set(\n                split_header_words(rfc2965_hdrs), request)\n        except Exception:\n            _warn_unhandled_exception()\n            cookies = []\n\n        if ns_hdrs and netscape:\n            try:\n                # RFC 2109 and Netscape cookies\n                ns_cookies = self._cookies_from_attrs_set(\n                    parse_ns_headers(ns_hdrs), request)\n            except Exception:\n                _warn_unhandled_exception()\n                ns_cookies = []\n            self._process_rfc2109_cookies(ns_cookies)\n\n            # Look for Netscape cookies (from Set-Cookie headers) that match\n            # corresponding RFC 2965 cookies (from Set-Cookie2 headers).\n            # For each match, keep the RFC 2965 cookie and ignore the Netscape\n            # cookie (RFC 2965 section 9.1).  Actually, RFC 2109 cookies are\n            # bundled in with the Netscape cookies for this purpose, which is\n            # reasonable behaviour.\n            if rfc2965:\n                lookup = {}\n                for cookie in cookies:\n                    lookup[(cookie.domain, cookie.path, cookie.name)] = None\n\n                def no_matching_rfc2965(ns_cookie, lookup=lookup):\n                    key = ns_cookie.domain, ns_cookie.path, ns_cookie.name\n                    return key not in lookup\n                ns_cookies = filter(no_matching_rfc2965, ns_cookies)\n\n            if ns_cookies:\n                cookies.extend(ns_cookies)\n\n        return cookies\n\n    def set_cookie_if_ok(self, cookie, request):\n        \"\"\"Set a cookie if policy says it's OK to do so.\"\"\"\n        self._cookies_lock.acquire()\n        try:\n            self._policy._now = self._now = int(time.time())\n\n            if self._policy.set_ok(cookie, request):\n                self.set_cookie(cookie)\n\n\n        finally:\n            self._cookies_lock.release()\n\n    def set_cookie(self, cookie):\n        \"\"\"Set a cookie, without checking whether or not it should be set.\"\"\"\n        c = self._cookies\n        self._cookies_lock.acquire()\n        try:\n            if cookie.domain not in c: c[cookie.domain] = {}\n            c2 = c[cookie.domain]\n            if cookie.path not in c2: c2[cookie.path] = {}\n            c3 = c2[cookie.path]\n            c3[cookie.name] = cookie\n        finally:\n            self._cookies_lock.release()\n\n    def extract_cookies(self, response, request):\n        \"\"\"Extract cookies from response, where allowable given the request.\"\"\"\n        _debug(\"extract_cookies: %s\", response.info())\n        self._cookies_lock.acquire()\n        try:\n            self._policy._now = self._now = int(time.time())\n\n            for cookie in self.make_cookies(response, request):\n                if self._policy.set_ok(cookie, request):\n                    _debug(\" setting cookie: %s\", cookie)\n                    self.set_cookie(cookie)\n        finally:\n            self._cookies_lock.release()\n\n    def clear(self, domain=None, path=None, name=None):\n        \"\"\"Clear some cookies.\n\n        Invoking this method without arguments will clear all cookies.  If\n        given a single argument, only cookies belonging to that domain will be\n        removed.  If given two arguments, cookies belonging to the specified\n        path within that domain are removed.  If given three arguments, then\n        the cookie with the specified name, path and domain is removed.\n\n        Raises KeyError if no matching cookie exists.\n\n        \"\"\"\n        if name is not None:\n            if (domain is None) or (path is None):\n                raise ValueError(\n                    \"domain and path must be given to remove a cookie by name\")\n            del self._cookies[domain][path][name]\n        elif path is not None:\n            if domain is None:\n                raise ValueError(\n                    \"domain must be given to remove cookies by path\")\n            del self._cookies[domain][path]\n        elif domain is not None:\n            del self._cookies[domain]\n        else:\n            self._cookies = {}\n\n    def clear_session_cookies(self):\n        \"\"\"Discard all session cookies.\n\n        Note that the .save() method won't save session cookies anyway, unless\n        you ask otherwise by passing a true ignore_discard argument.\n\n        \"\"\"\n        self._cookies_lock.acquire()\n        try:\n            for cookie in self:\n                if cookie.discard:\n                    self.clear(cookie.domain, cookie.path, cookie.name)\n        finally:\n            self._cookies_lock.release()\n\n    def clear_expired_cookies(self):\n        \"\"\"Discard all expired cookies.\n\n        You probably don't need to call this method: expired cookies are never\n        sent back to the server (provided you're using DefaultCookiePolicy),\n        this method is called by CookieJar itself every so often, and the\n        .save() method won't save expired cookies anyway (unless you ask\n        otherwise by passing a true ignore_expires argument).\n\n        \"\"\"\n        self._cookies_lock.acquire()\n        try:\n            now = time.time()\n            for cookie in self:\n                if cookie.is_expired(now):\n                    self.clear(cookie.domain, cookie.path, cookie.name)\n        finally:\n            self._cookies_lock.release()\n\n    def __iter__(self):\n        return deepvalues(self._cookies)\n\n    def __len__(self):\n        \"\"\"Return number of contained cookies.\"\"\"\n        i = 0\n        for cookie in self: i = i + 1\n        return i\n\n    def __repr__(self):\n        r = []\n        for cookie in self: r.append(repr(cookie))\n        return \"<%s[%s]>\" % (self.__class__, \", \".join(r))\n\n    def __str__(self):\n        r = []\n        for cookie in self: r.append(str(cookie))\n        return \"<%s[%s]>\" % (self.__class__, \", \".join(r))\n\n\n# derives from IOError for backwards-compatibility with Python 2.4.0\nclass LoadError(IOError): pass\n\nclass FileCookieJar(CookieJar):\n    \"\"\"CookieJar that can be loaded from and saved to a file.\"\"\"\n\n    def __init__(self, filename=None, delayload=False, policy=None):\n        \"\"\"\n        Cookies are NOT loaded from the named file until either the .load() or\n        .revert() method is called.\n\n        \"\"\"\n        CookieJar.__init__(self, policy)\n        if filename is not None:\n            try:\n                filename+\"\"\n            except:\n                raise ValueError(\"filename must be string-like\")\n        self.filename = filename\n        self.delayload = bool(delayload)\n\n    def save(self, filename=None, ignore_discard=False, ignore_expires=False):\n        \"\"\"Save cookies to a file.\"\"\"\n        raise NotImplementedError()\n\n    def load(self, filename=None, ignore_discard=False, ignore_expires=False):\n        \"\"\"Load cookies from a file.\"\"\"\n        if filename is None:\n            if self.filename is not None: filename = self.filename\n            else: raise ValueError(MISSING_FILENAME_TEXT)\n\n        f = open(filename)\n        try:\n            self._really_load(f, filename, ignore_discard, ignore_expires)\n        finally:\n            f.close()\n\n    def revert(self, filename=None,\n               ignore_discard=False, ignore_expires=False):\n        \"\"\"Clear all cookies and reload cookies from a saved file.\n\n        Raises LoadError (or IOError) if reversion is not successful; the\n        object's state will not be altered if this happens.\n\n        \"\"\"\n        if filename is None:\n            if self.filename is not None: filename = self.filename\n            else: raise ValueError(MISSING_FILENAME_TEXT)\n\n        self._cookies_lock.acquire()\n        try:\n\n            old_state = copy.deepcopy(self._cookies)\n            self._cookies = {}\n            try:\n                self.load(filename, ignore_discard, ignore_expires)\n            except (LoadError, IOError):\n                self._cookies = old_state\n                raise\n\n        finally:\n            self._cookies_lock.release()\n\nfrom _LWPCookieJar import LWPCookieJar, lwp_cookie_str\nfrom _MozillaCookieJar import MozillaCookieJar\n", 
    "copy": "\"\"\"Generic (shallow and deep) copying operations.\n\nInterface summary:\n\n        import copy\n\n        x = copy.copy(y)        # make a shallow copy of y\n        x = copy.deepcopy(y)    # make a deep copy of y\n\nFor module specific errors, copy.Error is raised.\n\nThe difference between shallow and deep copying is only relevant for\ncompound objects (objects that contain other objects, like lists or\nclass instances).\n\n- A shallow copy constructs a new compound object and then (to the\n  extent possible) inserts *the same objects* into it that the\n  original contains.\n\n- A deep copy constructs a new compound object and then, recursively,\n  inserts *copies* into it of the objects found in the original.\n\nTwo problems often exist with deep copy operations that don't exist\nwith shallow copy operations:\n\n a) recursive objects (compound objects that, directly or indirectly,\n    contain a reference to themselves) may cause a recursive loop\n\n b) because deep copy copies *everything* it may copy too much, e.g.\n    administrative data structures that should be shared even between\n    copies\n\nPython's deep copy operation avoids these problems by:\n\n a) keeping a table of objects already copied during the current\n    copying pass\n\n b) letting user-defined classes override the copying operation or the\n    set of components copied\n\nThis version does not copy types like module, class, function, method,\nnor stack trace, stack frame, nor file, socket, window, nor array, nor\nany similar types.\n\nClasses can use the same interfaces to control copying that they use\nto control pickling: they can define methods called __getinitargs__(),\n__getstate__() and __setstate__().  See the documentation for module\n\"pickle\" for information on these methods.\n\"\"\"\n\nimport types\nimport weakref\nfrom copy_reg import dispatch_table\n\nclass Error(Exception):\n    pass\nerror = Error   # backward compatibility\n\ntry:\n    from org.python.core import PyStringMap\nexcept ImportError:\n    PyStringMap = None\n\n__all__ = [\"Error\", \"copy\", \"deepcopy\"]\n\ndef copy(x):\n    \"\"\"Shallow copy operation on arbitrary Python objects.\n\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    cls = type(x)\n\n    copier = _copy_dispatch.get(cls)\n    if copier:\n        return copier(x)\n\n    copier = getattr(cls, \"__copy__\", None)\n    if copier:\n        return copier(x)\n\n    reductor = dispatch_table.get(cls)\n    if reductor:\n        rv = reductor(x)\n    else:\n        reductor = getattr(x, \"__reduce_ex__\", None)\n        if reductor:\n            rv = reductor(2)\n        else:\n            reductor = getattr(x, \"__reduce__\", None)\n            if reductor:\n                rv = reductor()\n            else:\n                raise Error(\"un(shallow)copyable object of type %s\" % cls)\n\n    return _reconstruct(x, rv, 0)\n\n\n_copy_dispatch = d = {}\n\ndef _copy_immutable(x):\n    return x\nfor t in (type(None), int, long, float, bool, str, tuple,\n          frozenset, type, xrange, types.ClassType,\n          types.BuiltinFunctionType, type(Ellipsis),\n          types.FunctionType, weakref.ref):\n    d[t] = _copy_immutable\nfor name in (\"ComplexType\", \"UnicodeType\", \"CodeType\"):\n    t = getattr(types, name, None)\n    if t is not None:\n        d[t] = _copy_immutable\n\ndef _copy_with_constructor(x):\n    return type(x)(x)\nfor t in (list, dict, set):\n    d[t] = _copy_with_constructor\n\ndef _copy_with_copy_method(x):\n    return x.copy()\nif PyStringMap is not None:\n    d[PyStringMap] = _copy_with_copy_method\n\ndef _copy_inst(x):\n    if hasattr(x, '__copy__'):\n        return x.__copy__()\n    if hasattr(x, '__getinitargs__'):\n        args = x.__getinitargs__()\n        y = x.__class__(*args)\n    else:\n        y = _EmptyClass()\n        y.__class__ = x.__class__\n    if hasattr(x, '__getstate__'):\n        state = x.__getstate__()\n    else:\n        state = x.__dict__\n    if hasattr(y, '__setstate__'):\n        y.__setstate__(state)\n    else:\n        y.__dict__.update(state)\n    return y\nd[types.InstanceType] = _copy_inst\n\ndel d\n\ndef deepcopy(x, memo=None, _nil=[]):\n    \"\"\"Deep copy operation on arbitrary Python objects.\n\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    if memo is None:\n        memo = {}\n\n    d = id(x)\n    y = memo.get(d, _nil)\n    if y is not _nil:\n        return y\n\n    cls = type(x)\n\n    copier = _deepcopy_dispatch.get(cls)\n    if copier:\n        y = copier(x, memo)\n    else:\n        try:\n            issc = issubclass(cls, type)\n        except TypeError: # cls is not a class (old Boost; see SF #502085)\n            issc = 0\n        if issc:\n            y = _deepcopy_atomic(x, memo)\n        else:\n            copier = getattr(x, \"__deepcopy__\", None)\n            if copier:\n                y = copier(memo)\n            else:\n                reductor = dispatch_table.get(cls)\n                if reductor:\n                    rv = reductor(x)\n                else:\n                    reductor = getattr(x, \"__reduce_ex__\", None)\n                    if reductor:\n                        rv = reductor(2)\n                    else:\n                        reductor = getattr(x, \"__reduce__\", None)\n                        if reductor:\n                            rv = reductor()\n                        else:\n                            raise Error(\n                                \"un(deep)copyable object of type %s\" % cls)\n                y = _reconstruct(x, rv, 1, memo)\n\n    memo[d] = y\n    _keep_alive(x, memo) # Make sure x lives at least as long as d\n    return y\n\n_deepcopy_dispatch = d = {}\n\ndef _deepcopy_atomic(x, memo):\n    return x\nd[type(None)] = _deepcopy_atomic\nd[type(Ellipsis)] = _deepcopy_atomic\nd[int] = _deepcopy_atomic\nd[long] = _deepcopy_atomic\nd[float] = _deepcopy_atomic\nd[bool] = _deepcopy_atomic\ntry:\n    d[complex] = _deepcopy_atomic\nexcept NameError:\n    pass\nd[str] = _deepcopy_atomic\ntry:\n    d[unicode] = _deepcopy_atomic\nexcept NameError:\n    pass\ntry:\n    d[types.CodeType] = _deepcopy_atomic\nexcept AttributeError:\n    pass\nd[type] = _deepcopy_atomic\nd[xrange] = _deepcopy_atomic\nd[types.ClassType] = _deepcopy_atomic\nd[types.BuiltinFunctionType] = _deepcopy_atomic\nd[types.FunctionType] = _deepcopy_atomic\nd[weakref.ref] = _deepcopy_atomic\n\ndef _deepcopy_list(x, memo):\n    y = []\n    memo[id(x)] = y\n    for a in x:\n        y.append(deepcopy(a, memo))\n    return y\nd[list] = _deepcopy_list\n\ndef _deepcopy_tuple(x, memo):\n    y = []\n    for a in x:\n        y.append(deepcopy(a, memo))\n    d = id(x)\n    try:\n        return memo[d]\n    except KeyError:\n        pass\n    for i in range(len(x)):\n        if x[i] is not y[i]:\n            y = tuple(y)\n            break\n    else:\n        y = x\n    memo[d] = y\n    return y\nd[tuple] = _deepcopy_tuple\n\ndef _deepcopy_dict(x, memo):\n    y = {}\n    memo[id(x)] = y\n    for key, value in x.iteritems():\n        y[deepcopy(key, memo)] = deepcopy(value, memo)\n    return y\nd[dict] = _deepcopy_dict\nif PyStringMap is not None:\n    d[PyStringMap] = _deepcopy_dict\n\ndef _deepcopy_method(x, memo): # Copy instance methods\n    return type(x)(x.im_func, deepcopy(x.im_self, memo), x.im_class)\n_deepcopy_dispatch[types.MethodType] = _deepcopy_method\n\ndef _keep_alive(x, memo):\n    \"\"\"Keeps a reference to the object x in the memo.\n\n    Because we remember objects by their id, we have\n    to assure that possibly temporary objects are kept\n    alive by referencing them.\n    We store a reference at the id of the memo, which should\n    normally not be used unless someone tries to deepcopy\n    the memo itself...\n    \"\"\"\n    try:\n        memo[id(memo)].append(x)\n    except KeyError:\n        # aha, this is the first one :-)\n        memo[id(memo)]=[x]\n\ndef _deepcopy_inst(x, memo):\n    if hasattr(x, '__deepcopy__'):\n        return x.__deepcopy__(memo)\n    if hasattr(x, '__getinitargs__'):\n        args = x.__getinitargs__()\n        args = deepcopy(args, memo)\n        y = x.__class__(*args)\n    else:\n        y = _EmptyClass()\n        y.__class__ = x.__class__\n    memo[id(x)] = y\n    if hasattr(x, '__getstate__'):\n        state = x.__getstate__()\n    else:\n        state = x.__dict__\n    state = deepcopy(state, memo)\n    if hasattr(y, '__setstate__'):\n        y.__setstate__(state)\n    else:\n        y.__dict__.update(state)\n    return y\nd[types.InstanceType] = _deepcopy_inst\n\ndef _reconstruct(x, info, deep, memo=None):\n    if isinstance(info, str):\n        return x\n    assert isinstance(info, tuple)\n    if memo is None:\n        memo = {}\n    n = len(info)\n    assert n in (2, 3, 4, 5)\n    callable, args = info[:2]\n    if n > 2:\n        state = info[2]\n    else:\n        state = {}\n    if n > 3:\n        listiter = info[3]\n    else:\n        listiter = None\n    if n > 4:\n        dictiter = info[4]\n    else:\n        dictiter = None\n    if deep:\n        args = deepcopy(args, memo)\n    y = callable(*args)\n    memo[id(x)] = y\n\n    if state:\n        if deep:\n            state = deepcopy(state, memo)\n        if hasattr(y, '__setstate__'):\n            y.__setstate__(state)\n        else:\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            else:\n                slotstate = None\n            if state is not None:\n                y.__dict__.update(state)\n            if slotstate is not None:\n                for key, value in slotstate.iteritems():\n                    setattr(y, key, value)\n\n    if listiter is not None:\n        for item in listiter:\n            if deep:\n                item = deepcopy(item, memo)\n            y.append(item)\n    if dictiter is not None:\n        for key, value in dictiter:\n            if deep:\n                key = deepcopy(key, memo)\n                value = deepcopy(value, memo)\n            y[key] = value\n    return y\n\ndel d\n\ndel types\n\n# Helper for instance creation without calling __init__\nclass _EmptyClass:\n    pass\n\ndef _test():\n    l = [None, 1, 2L, 3.14, 'xyzzy', (1, 2L), [3.14, 'abc'],\n         {'abc': 'ABC'}, (), [], {}]\n    l1 = copy(l)\n    print l1==l\n    l1 = map(copy, l)\n    print l1==l\n    l1 = deepcopy(l)\n    print l1==l\n    class C:\n        def __init__(self, arg=None):\n            self.a = 1\n            self.arg = arg\n            if __name__ == '__main__':\n                import sys\n                file = sys.argv[0]\n            else:\n                file = __file__\n            self.fp = open(file)\n            self.fp.close()\n        def __getstate__(self):\n            return {'a': self.a, 'arg': self.arg}\n        def __setstate__(self, state):\n            for key, value in state.iteritems():\n                setattr(self, key, value)\n        def __deepcopy__(self, memo=None):\n            new = self.__class__(deepcopy(self.arg, memo))\n            new.a = self.a\n            return new\n    c = C('argument sketch')\n    l.append(c)\n    l2 = copy(l)\n    print l == l2\n    print l\n    print l2\n    l2 = deepcopy(l)\n    print l == l2\n    print l\n    print l2\n    l.append({l[1]: l, 'xyz': l[2]})\n    l3 = copy(l)\n    import repr\n    print map(repr.repr, l)\n    print map(repr.repr, l1)\n    print map(repr.repr, l2)\n    print map(repr.repr, l3)\n    l3 = deepcopy(l)\n    import repr\n    print map(repr.repr, l)\n    print map(repr.repr, l1)\n    print map(repr.repr, l2)\n    print map(repr.repr, l3)\n    class odict(dict):\n        def __init__(self, d = {}):\n            self.a = 99\n            dict.__init__(self, d)\n        def __setitem__(self, k, i):\n            dict.__setitem__(self, k, i)\n            self.a\n    o = odict({\"A\" : \"B\"})\n    x = deepcopy(o)\n    print(o, x)\n\nif __name__ == '__main__':\n    _test()\n", 
    "copy_reg": "\"\"\"Helper to provide extensibility for pickle/cPickle.\n\nThis is only useful to add pickle support for extension types defined in\nC, not for instances of user-defined classes.\n\"\"\"\n\nfrom types import ClassType as _ClassType\n\n__all__ = [\"pickle\", \"constructor\",\n           \"add_extension\", \"remove_extension\", \"clear_extension_cache\"]\n\ndispatch_table = {}\n\ndef pickle(ob_type, pickle_function, constructor_ob=None):\n    if type(ob_type) is _ClassType:\n        raise TypeError(\"copy_reg is not intended for use with classes\")\n\n    if not hasattr(pickle_function, '__call__'):\n        raise TypeError(\"reduction functions must be callable\")\n    dispatch_table[ob_type] = pickle_function\n\n    # The constructor_ob function is a vestige of safe for unpickling.\n    # There is no reason for the caller to pass it anymore.\n    if constructor_ob is not None:\n        constructor(constructor_ob)\n\ndef constructor(object):\n    if not hasattr(object, '__call__'):\n        raise TypeError(\"constructors must be callable\")\n\n# Example: provide pickling support for complex numbers.\n\ntry:\n    complex\nexcept NameError:\n    pass\nelse:\n\n    def pickle_complex(c):\n        return complex, (c.real, c.imag)\n\n    pickle(complex, pickle_complex, complex)\n\n# Support for pickling new-style objects\n\ndef _reconstructor(cls, base, state):\n    if base is object:\n        obj = object.__new__(cls)\n    else:\n        obj = base.__new__(cls, state)\n        if base.__init__ != object.__init__:\n            base.__init__(obj, state)\n    return obj\n\n_HEAPTYPE = 1<<9\n\n# Python code for object.__reduce_ex__ for protocols 0 and 1\n\ndef _reduce_ex(self, proto):\n    assert proto < 2\n    for base in self.__class__.__mro__:\n        if hasattr(base, '__flags__') and not base.__flags__ & _HEAPTYPE:\n            break\n    else:\n        base = object # not really reachable\n    if base is object:\n        state = None\n    else:\n        if base is self.__class__:\n            raise TypeError, \"can't pickle %s objects\" % base.__name__\n        state = base(self)\n    args = (self.__class__, base, state)\n    try:\n        getstate = self.__getstate__\n    except AttributeError:\n        if getattr(self, \"__slots__\", None):\n            raise TypeError(\"a class that defines __slots__ without \"\n                            \"defining __getstate__ cannot be pickled\")\n        try:\n            dict = self.__dict__\n        except AttributeError:\n            dict = None\n    else:\n        dict = getstate()\n    if dict:\n        return _reconstructor, args, dict\n    else:\n        return _reconstructor, args\n\n# Helper for __reduce_ex__ protocol 2\n\ndef __newobj__(cls, *args):\n    return cls.__new__(cls, *args)\n\ndef _slotnames(cls):\n    \"\"\"Return a list of slot names for a given class.\n\n    This needs to find slots defined by the class and its bases, so we\n    can't simply return the __slots__ attribute.  We must walk down\n    the Method Resolution Order and concatenate the __slots__ of each\n    class found there.  (This assumes classes don't modify their\n    __slots__ attribute to misrepresent their slots after the class is\n    defined.)\n    \"\"\"\n\n    # Get the value from a cache in the class if possible\n    names = cls.__dict__.get(\"__slotnames__\")\n    if names is not None:\n        return names\n\n    # Not cached -- calculate the value\n    names = []\n    if not hasattr(cls, \"__slots__\"):\n        # This class has no slots\n        pass\n    else:\n        # Slots found -- gather slot names from all base classes\n        for c in cls.__mro__:\n            if \"__slots__\" in c.__dict__:\n                slots = c.__dict__['__slots__']\n                # if class has a single slot, it can be given as a string\n                if isinstance(slots, basestring):\n                    slots = (slots,)\n                for name in slots:\n                    # special descriptors\n                    if name in (\"__dict__\", \"__weakref__\"):\n                        continue\n                    # mangled names\n                    elif name.startswith('__') and not name.endswith('__'):\n                        names.append('_%s%s' % (c.__name__, name))\n                    else:\n                        names.append(name)\n\n    # Cache the outcome in the class if at all possible\n    try:\n        cls.__slotnames__ = names\n    except:\n        pass # But don't die if we can't\n\n    return names\n\n# A registry of extension codes.  This is an ad-hoc compression\n# mechanism.  Whenever a global reference to <module>, <name> is about\n# to be pickled, the (<module>, <name>) tuple is looked up here to see\n# if it is a registered extension code for it.  Extension codes are\n# universal, so that the meaning of a pickle does not depend on\n# context.  (There are also some codes reserved for local use that\n# don't have this restriction.)  Codes are positive ints; 0 is\n# reserved.\n\n_extension_registry = {}                # key -> code\n_inverted_registry = {}                 # code -> key\n_extension_cache = {}                   # code -> object\n# Don't ever rebind those names:  cPickle grabs a reference to them when\n# it's initialized, and won't see a rebinding.\n\ndef add_extension(module, name, code):\n    \"\"\"Register an extension code.\"\"\"\n    code = int(code)\n    if not 1 <= code <= 0x7fffffff:\n        raise ValueError, \"code out of range\"\n    key = (module, name)\n    if (_extension_registry.get(key) == code and\n        _inverted_registry.get(code) == key):\n        return # Redundant registrations are benign\n    if key in _extension_registry:\n        raise ValueError(\"key %s is already registered with code %s\" %\n                         (key, _extension_registry[key]))\n    if code in _inverted_registry:\n        raise ValueError(\"code %s is already in use for key %s\" %\n                         (code, _inverted_registry[code]))\n    _extension_registry[key] = code\n    _inverted_registry[code] = key\n\ndef remove_extension(module, name, code):\n    \"\"\"Unregister an extension code.  For testing only.\"\"\"\n    key = (module, name)\n    if (_extension_registry.get(key) != code or\n        _inverted_registry.get(code) != key):\n        raise ValueError(\"key %s is not registered with code %s\" %\n                         (key, code))\n    del _extension_registry[key]\n    del _inverted_registry[code]\n    if code in _extension_cache:\n        del _extension_cache[code]\n\ndef clear_extension_cache():\n    _extension_cache.clear()\n\n# Standard extension code assignments\n\n# Reserved ranges\n\n# First  Last Count  Purpose\n#     1   127   127  Reserved for Python standard library\n#   128   191    64  Reserved for Zope\n#   192   239    48  Reserved for 3rd parties\n#   240   255    16  Reserved for private use (will never be assigned)\n#   256   Inf   Inf  Reserved for future assignment\n\n# Extension codes are assigned by the Python Software Foundation.\n", 
    "datetime": "\"\"\"Concrete date/time and related types -- prototype implemented in Python.\n\nSee http://www.zope.org/Members/fdrake/DateTimeWiki/FrontPage\n\nSee also http://dir.yahoo.com/Reference/calendars/\n\nFor a primer on DST, including many current DST rules, see\nhttp://webexhibits.org/daylightsaving/\n\nFor more about DST than you ever wanted to know, see\nftp://elsie.nci.nih.gov/pub/\n\nSources for time zone and DST data: http://www.twinsun.com/tz/tz-link.htm\n\nThis was originally copied from the sandbox of the CPython CVS repository.\nThanks to Tim Peters for suggesting using it.\n\"\"\"\n\nfrom __future__ import division\nimport time as _time\nimport math as _math\nimport struct as _struct\n\ndef _cmp(x, y):\n    return 0 if x == y else 1 if x > y else -1\n\ndef _round(x):\n    return int(_math.floor(x + 0.5) if x >= 0.0 else _math.ceil(x - 0.5))\n\nMINYEAR = 1\nMAXYEAR = 9999\n_MINYEARFMT = 1900\n\n# Utility functions, adapted from Python's Demo/classes/Dates.py, which\n# also assumes the current Gregorian calendar indefinitely extended in\n# both directions.  Difference:  Dates.py calls January 1 of year 0 day\n# number 1.  The code here calls January 1 of year 1 day number 1.  This is\n# to match the definition of the \"proleptic Gregorian\" calendar in Dershowitz\n# and Reingold's \"Calendrical Calculations\", where it's the base calendar\n# for all computations.  See the book for algorithms for converting between\n# proleptic Gregorian ordinals and many other calendar systems.\n\n_DAYS_IN_MONTH = [-1, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n\n_DAYS_BEFORE_MONTH = [-1]\ndbm = 0\nfor dim in _DAYS_IN_MONTH[1:]:\n    _DAYS_BEFORE_MONTH.append(dbm)\n    dbm += dim\ndel dbm, dim\n\ndef _is_leap(year):\n    \"year -> 1 if leap year, else 0.\"\n    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n\ndef _days_before_year(year):\n    \"year -> number of days before January 1st of year.\"\n    y = year - 1\n    return y*365 + y//4 - y//100 + y//400\n\ndef _days_in_month(year, month):\n    \"year, month -> number of days in that month in that year.\"\n    assert 1 <= month <= 12, month\n    if month == 2 and _is_leap(year):\n        return 29\n    return _DAYS_IN_MONTH[month]\n\ndef _days_before_month(year, month):\n    \"year, month -> number of days in year preceding first day of month.\"\n    assert 1 <= month <= 12, 'month must be in 1..12'\n    return _DAYS_BEFORE_MONTH[month] + (month > 2 and _is_leap(year))\n\ndef _ymd2ord(year, month, day):\n    \"year, month, day -> ordinal, considering 01-Jan-0001 as day 1.\"\n    assert 1 <= month <= 12, 'month must be in 1..12'\n    dim = _days_in_month(year, month)\n    assert 1 <= day <= dim, ('day must be in 1..%d' % dim)\n    return (_days_before_year(year) +\n            _days_before_month(year, month) +\n            day)\n\n_DI400Y = _days_before_year(401)    # number of days in 400 years\n_DI100Y = _days_before_year(101)    #    \"    \"   \"   \" 100   \"\n_DI4Y   = _days_before_year(5)      #    \"    \"   \"   \"   4   \"\n\n# A 4-year cycle has an extra leap day over what we'd get from pasting\n# together 4 single years.\nassert _DI4Y == 4 * 365 + 1\n\n# Similarly, a 400-year cycle has an extra leap day over what we'd get from\n# pasting together 4 100-year cycles.\nassert _DI400Y == 4 * _DI100Y + 1\n\n# OTOH, a 100-year cycle has one fewer leap day than we'd get from\n# pasting together 25 4-year cycles.\nassert _DI100Y == 25 * _DI4Y - 1\n\ndef _ord2ymd(n):\n    \"ordinal -> (year, month, day), considering 01-Jan-0001 as day 1.\"\n\n    # n is a 1-based index, starting at 1-Jan-1.  The pattern of leap years\n    # repeats exactly every 400 years.  The basic strategy is to find the\n    # closest 400-year boundary at or before n, then work with the offset\n    # from that boundary to n.  Life is much clearer if we subtract 1 from\n    # n first -- then the values of n at 400-year boundaries are exactly\n    # those divisible by _DI400Y:\n    #\n    #     D  M   Y            n              n-1\n    #     -- --- ----        ----------     ----------------\n    #     31 Dec -400        -_DI400Y       -_DI400Y -1\n    #      1 Jan -399         -_DI400Y +1   -_DI400Y      400-year boundary\n    #     ...\n    #     30 Dec  000        -1             -2\n    #     31 Dec  000         0             -1\n    #      1 Jan  001         1              0            400-year boundary\n    #      2 Jan  001         2              1\n    #      3 Jan  001         3              2\n    #     ...\n    #     31 Dec  400         _DI400Y        _DI400Y -1\n    #      1 Jan  401         _DI400Y +1     _DI400Y      400-year boundary\n    n -= 1\n    n400, n = divmod(n, _DI400Y)\n    year = n400 * 400 + 1   # ..., -399, 1, 401, ...\n\n    # Now n is the (non-negative) offset, in days, from January 1 of year, to\n    # the desired date.  Now compute how many 100-year cycles precede n.\n    # Note that it's possible for n100 to equal 4!  In that case 4 full\n    # 100-year cycles precede the desired day, which implies the desired\n    # day is December 31 at the end of a 400-year cycle.\n    n100, n = divmod(n, _DI100Y)\n\n    # Now compute how many 4-year cycles precede it.\n    n4, n = divmod(n, _DI4Y)\n\n    # And now how many single years.  Again n1 can be 4, and again meaning\n    # that the desired day is December 31 at the end of the 4-year cycle.\n    n1, n = divmod(n, 365)\n\n    year += n100 * 100 + n4 * 4 + n1\n    if n1 == 4 or n100 == 4:\n        assert n == 0\n        return year-1, 12, 31\n\n    # Now the year is correct, and n is the offset from January 1.  We find\n    # the month via an estimate that's either exact or one too large.\n    leapyear = n1 == 3 and (n4 != 24 or n100 == 3)\n    assert leapyear == _is_leap(year)\n    month = (n + 50) >> 5\n    preceding = _DAYS_BEFORE_MONTH[month] + (month > 2 and leapyear)\n    if preceding > n:  # estimate is too large\n        month -= 1\n        preceding -= _DAYS_IN_MONTH[month] + (month == 2 and leapyear)\n    n -= preceding\n    assert 0 <= n < _days_in_month(year, month)\n\n    # Now the year and month are correct, and n is the offset from the\n    # start of that month:  we're done!\n    return year, month, n+1\n\n# Month and day names.  For localized versions, see the calendar module.\n_MONTHNAMES = [None, \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n                     \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n_DAYNAMES = [None, \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n\n\ndef _build_struct_time(y, m, d, hh, mm, ss, dstflag):\n    wday = (_ymd2ord(y, m, d) + 6) % 7\n    dnum = _days_before_month(y, m) + d\n    return _time.struct_time((y, m, d, hh, mm, ss, wday, dnum, dstflag))\n\ndef _format_time(hh, mm, ss, us):\n    # Skip trailing microseconds when us==0.\n    result = \"%02d:%02d:%02d\" % (hh, mm, ss)\n    if us:\n        result += \".%06d\" % us\n    return result\n\n# Correctly substitute for %z and %Z escapes in strftime formats.\ndef _wrap_strftime(object, format, timetuple):\n    year = timetuple[0]\n    if year < _MINYEARFMT:\n        raise ValueError(\"year=%d is before %d; the datetime strftime() \"\n                         \"methods require year >= %d\" %\n                         (year, _MINYEARFMT, _MINYEARFMT))\n    # Don't call utcoffset() or tzname() unless actually needed.\n    freplace = None  # the string to use for %f\n    zreplace = None  # the string to use for %z\n    Zreplace = None  # the string to use for %Z\n\n    # Scan format for %z and %Z escapes, replacing as needed.\n    newformat = []\n    push = newformat.append\n    i, n = 0, len(format)\n    while i < n:\n        ch = format[i]\n        i += 1\n        if ch == '%':\n            if i < n:\n                ch = format[i]\n                i += 1\n                if ch == 'f':\n                    if freplace is None:\n                        freplace = '%06d' % getattr(object,\n                                                    'microsecond', 0)\n                    newformat.append(freplace)\n                elif ch == 'z':\n                    if zreplace is None:\n                        zreplace = \"\"\n                        if hasattr(object, \"_utcoffset\"):\n                            offset = object._utcoffset()\n                            if offset is not None:\n                                sign = '+'\n                                if offset < 0:\n                                    offset = -offset\n                                    sign = '-'\n                                h, m = divmod(offset, 60)\n                                zreplace = '%c%02d%02d' % (sign, h, m)\n                    assert '%' not in zreplace\n                    newformat.append(zreplace)\n                elif ch == 'Z':\n                    if Zreplace is None:\n                        Zreplace = \"\"\n                        if hasattr(object, \"tzname\"):\n                            s = object.tzname()\n                            if s is not None:\n                                # strftime is going to have at this: escape %\n                                Zreplace = s.replace('%', '%%')\n                    newformat.append(Zreplace)\n                else:\n                    push('%')\n                    push(ch)\n            else:\n                push('%')\n        else:\n            push(ch)\n    newformat = \"\".join(newformat)\n    return _time.strftime(newformat, timetuple)\n\n# Just raise TypeError if the arg isn't None or a string.\ndef _check_tzname(name):\n    if name is not None and not isinstance(name, str):\n        raise TypeError(\"tzinfo.tzname() must return None or string, \"\n                        \"not '%s'\" % type(name))\n\n# name is the offset-producing method, \"utcoffset\" or \"dst\".\n# offset is what it returned.\n# If offset isn't None or timedelta, raises TypeError.\n# If offset is None, returns None.\n# Else offset is checked for being in range, and a whole # of minutes.\n# If it is, its integer value is returned.  Else ValueError is raised.\ndef _check_utc_offset(name, offset):\n    assert name in (\"utcoffset\", \"dst\")\n    if offset is None:\n        return\n    if not isinstance(offset, timedelta):\n        raise TypeError(\"tzinfo.%s() must return None \"\n                        \"or timedelta, not '%s'\" % (name, type(offset)))\n    days = offset.days\n    if days < -1 or days > 0:\n        offset = 1440  # trigger out-of-range\n    else:\n        seconds = days * 86400 + offset.seconds\n        minutes, seconds = divmod(seconds, 60)\n        if seconds or offset.microseconds:\n            raise ValueError(\"tzinfo.%s() must return a whole number \"\n                             \"of minutes\" % name)\n        offset = minutes\n    if not -1440 < offset < 1440:\n        raise ValueError(\"%s()=%d, must be in -1439..1439\" % (name, offset))\n    return offset\n\ndef _check_int_field(value):\n    if isinstance(value, int):\n        return value\n    if not isinstance(value, float):\n        try:\n            value = value.__int__()\n        except AttributeError:\n            pass\n        else:\n            if isinstance(value, (int, long)):\n                return value\n            raise TypeError('__int__ method should return an integer')\n        raise TypeError('an integer is required')\n    raise TypeError('integer argument expected, got float')\n\ndef _check_date_fields(year, month, day):\n    year = _check_int_field(year)\n    month = _check_int_field(month)\n    day = _check_int_field(day)\n    if not MINYEAR <= year <= MAXYEAR:\n        raise ValueError('year must be in %d..%d' % (MINYEAR, MAXYEAR), year)\n    if not 1 <= month <= 12:\n        raise ValueError('month must be in 1..12', month)\n    dim = _days_in_month(year, month)\n    if not 1 <= day <= dim:\n        raise ValueError('day must be in 1..%d' % dim, day)\n    return year, month, day\n\ndef _check_time_fields(hour, minute, second, microsecond):\n    hour = _check_int_field(hour)\n    minute = _check_int_field(minute)\n    second = _check_int_field(second)\n    microsecond = _check_int_field(microsecond)\n    if not 0 <= hour <= 23:\n        raise ValueError('hour must be in 0..23', hour)\n    if not 0 <= minute <= 59:\n        raise ValueError('minute must be in 0..59', minute)\n    if not 0 <= second <= 59:\n        raise ValueError('second must be in 0..59', second)\n    if not 0 <= microsecond <= 999999:\n        raise ValueError('microsecond must be in 0..999999', microsecond)\n    return hour, minute, second, microsecond\n\ndef _check_tzinfo_arg(tz):\n    if tz is not None and not isinstance(tz, tzinfo):\n        raise TypeError(\"tzinfo argument must be None or of a tzinfo subclass\")\n\n\n# Notes on comparison:  In general, datetime module comparison operators raise\n# TypeError when they don't know how to do a comparison themself.  If they\n# returned NotImplemented instead, comparison could (silently) fall back to\n# the default compare-objects-by-comparing-their-memory-addresses strategy,\n# and that's not helpful.  There are two exceptions:\n#\n# 1. For date and datetime, if the other object has a \"timetuple\" attr,\n#    NotImplemented is returned.  This is a hook to allow other kinds of\n#    datetime-like objects a chance to intercept the comparison.\n#\n# 2. Else __eq__ and __ne__ return False and True, respectively.  This is\n#    so opertaions like\n#\n#        x == y\n#        x != y\n#        x in sequence\n#        x not in sequence\n#        dict[x] = y\n#\n#    don't raise annoying TypeErrors just because a datetime object\n#    is part of a heterogeneous collection.  If there's no known way to\n#    compare X to a datetime, saying they're not equal is reasonable.\n\ndef _cmperror(x, y):\n    raise TypeError(\"can't compare '%s' to '%s'\" % (\n                    type(x).__name__, type(y).__name__))\n\n# This is a start at a struct tm workalike.  Goals:\n#\n# + Works the same way across platforms.\n# + Handles all the fields datetime needs handled, without 1970-2038 glitches.\n#\n# Note:  I suspect it's best if this flavor of tm does *not* try to\n# second-guess timezones or DST.  Instead fold whatever adjustments you want\n# into the minutes argument (and the constructor will normalize).\n\nclass _tmxxx:\n\n    ordinal = None\n\n    def __init__(self, year, month, day, hour=0, minute=0, second=0,\n                 microsecond=0):\n        # Normalize all the inputs, and store the normalized values.\n        if not 0 <= microsecond <= 999999:\n            carry, microsecond = divmod(microsecond, 1000000)\n            second += carry\n        if not 0 <= second <= 59:\n            carry, second = divmod(second, 60)\n            minute += carry\n        if not 0 <= minute <= 59:\n            carry, minute = divmod(minute, 60)\n            hour += carry\n        if not 0 <= hour <= 23:\n            carry, hour = divmod(hour, 24)\n            day += carry\n\n        # That was easy.  Now it gets muddy:  the proper range for day\n        # can't be determined without knowing the correct month and year,\n        # but if day is, e.g., plus or minus a million, the current month\n        # and year values make no sense (and may also be out of bounds\n        # themselves).\n        # Saying 12 months == 1 year should be non-controversial.\n        if not 1 <= month <= 12:\n            carry, month = divmod(month-1, 12)\n            year += carry\n            month += 1\n            assert 1 <= month <= 12\n\n        # Now only day can be out of bounds (year may also be out of bounds\n        # for a datetime object, but we don't care about that here).\n        # If day is out of bounds, what to do is arguable, but at least the\n        # method here is principled and explainable.\n        dim = _days_in_month(year, month)\n        if not 1 <= day <= dim:\n            # Move day-1 days from the first of the month.  First try to\n            # get off cheap if we're only one day out of range (adjustments\n            # for timezone alone can't be worse than that).\n            if day == 0:    # move back a day\n                month -= 1\n                if month > 0:\n                    day = _days_in_month(year, month)\n                else:\n                    year, month, day = year-1, 12, 31\n            elif day == dim + 1:    # move forward a day\n                month += 1\n                day = 1\n                if month > 12:\n                    month = 1\n                    year += 1\n            else:\n                self.ordinal = _ymd2ord(year, month, 1) + (day - 1)\n                year, month, day = _ord2ymd(self.ordinal)\n\n        self.year, self.month, self.day = year, month, day\n        self.hour, self.minute, self.second = hour, minute, second\n        self.microsecond = microsecond\n\nclass timedelta(object):\n    \"\"\"Represent the difference between two datetime objects.\n\n    Supported operators:\n\n    - add, subtract timedelta\n    - unary plus, minus, abs\n    - compare to timedelta\n    - multiply, divide by int/long\n\n    In addition, datetime supports subtraction of two datetime objects\n    returning a timedelta, and addition or subtraction of a datetime\n    and a timedelta giving a datetime.\n\n    Representation: (days, seconds, microseconds).  Why?  Because I\n    felt like it.\n    \"\"\"\n    __slots__ = '_days', '_seconds', '_microseconds', '_hashcode'\n\n    def __new__(cls, days=0, seconds=0, microseconds=0,\n                milliseconds=0, minutes=0, hours=0, weeks=0):\n        # Doing this efficiently and accurately in C is going to be difficult\n        # and error-prone, due to ubiquitous overflow possibilities, and that\n        # C double doesn't have enough bits of precision to represent\n        # microseconds over 10K years faithfully.  The code here tries to make\n        # explicit where go-fast assumptions can be relied on, in order to\n        # guide the C implementation; it's way more convoluted than speed-\n        # ignoring auto-overflow-to-long idiomatic Python could be.\n\n        # XXX Check that all inputs are ints, longs or floats.\n\n        # Final values, all integer.\n        # s and us fit in 32-bit signed ints; d isn't bounded.\n        d = s = us = 0\n\n        # Normalize everything to days, seconds, microseconds.\n        days += weeks*7\n        seconds += minutes*60 + hours*3600\n        microseconds += milliseconds*1000\n\n        # Get rid of all fractions, and normalize s and us.\n        # Take a deep breath <wink>.\n        if isinstance(days, float):\n            dayfrac, days = _math.modf(days)\n            daysecondsfrac, daysecondswhole = _math.modf(dayfrac * (24.*3600.))\n            assert daysecondswhole == int(daysecondswhole)  # can't overflow\n            s = int(daysecondswhole)\n            assert days == int(days)\n            d = int(days)\n        else:\n            daysecondsfrac = 0.0\n            d = days\n        assert isinstance(daysecondsfrac, float)\n        assert abs(daysecondsfrac) <= 1.0\n        assert isinstance(d, (int, long))\n        assert abs(s) <= 24 * 3600\n        # days isn't referenced again before redefinition\n\n        if isinstance(seconds, float):\n            secondsfrac, seconds = _math.modf(seconds)\n            assert seconds == int(seconds)\n            seconds = int(seconds)\n            secondsfrac += daysecondsfrac\n            assert abs(secondsfrac) <= 2.0\n        else:\n            secondsfrac = daysecondsfrac\n        # daysecondsfrac isn't referenced again\n        assert isinstance(secondsfrac, float)\n        assert abs(secondsfrac) <= 2.0\n\n        assert isinstance(seconds, (int, long))\n        days, seconds = divmod(seconds, 24*3600)\n        d += days\n        s += int(seconds)    # can't overflow\n        assert isinstance(s, int)\n        assert abs(s) <= 2 * 24 * 3600\n        # seconds isn't referenced again before redefinition\n\n        usdouble = secondsfrac * 1e6\n        assert abs(usdouble) < 2.1e6    # exact value not critical\n        # secondsfrac isn't referenced again\n\n        if isinstance(microseconds, float):\n            microseconds = _round(microseconds + usdouble)\n            seconds, microseconds = divmod(microseconds, 1000000)\n            days, seconds = divmod(seconds, 24*3600)\n            d += days\n            s += int(seconds)\n            microseconds = int(microseconds)\n        else:\n            microseconds = int(microseconds)\n            seconds, microseconds = divmod(microseconds, 1000000)\n            days, seconds = divmod(seconds, 24*3600)\n            d += days\n            s += int(seconds)\n            microseconds = _round(microseconds + usdouble)\n        assert isinstance(s, int)\n        assert isinstance(microseconds, int)\n        assert abs(s) <= 3 * 24 * 3600\n        assert abs(microseconds) < 3.1e6\n\n        # Just a little bit of carrying possible for microseconds and seconds.\n        seconds, us = divmod(microseconds, 1000000)\n        s += seconds\n        days, s = divmod(s, 24*3600)\n        d += days\n\n        assert isinstance(d, (int, long))\n        assert isinstance(s, int) and 0 <= s < 24*3600\n        assert isinstance(us, int) and 0 <= us < 1000000\n\n        if abs(d) > 999999999:\n            raise OverflowError(\"timedelta # of days is too large: %d\" % d)\n\n        self = object.__new__(cls)\n        self._days = d\n        self._seconds = s\n        self._microseconds = us\n        self._hashcode = -1\n        return self\n\n    def __repr__(self):\n        if self._microseconds:\n            return \"%s(%d, %d, %d)\" % ('datetime.' + self.__class__.__name__,\n                                       self._days,\n                                       self._seconds,\n                                       self._microseconds)\n        if self._seconds:\n            return \"%s(%d, %d)\" % ('datetime.' + self.__class__.__name__,\n                                   self._days,\n                                   self._seconds)\n        return \"%s(%d)\" % ('datetime.' + self.__class__.__name__, self._days)\n\n    def __str__(self):\n        mm, ss = divmod(self._seconds, 60)\n        hh, mm = divmod(mm, 60)\n        s = \"%d:%02d:%02d\" % (hh, mm, ss)\n        if self._days:\n            def plural(n):\n                return n, abs(n) != 1 and \"s\" or \"\"\n            s = (\"%d day%s, \" % plural(self._days)) + s\n        if self._microseconds:\n            s = s + \".%06d\" % self._microseconds\n        return s\n\n    def total_seconds(self):\n        \"\"\"Total seconds in the duration.\"\"\"\n        return ((self.days * 86400 + self.seconds) * 10**6 +\n                self.microseconds) / 10**6\n\n    # Read-only field accessors\n    @property\n    def days(self):\n        \"\"\"days\"\"\"\n        return self._days\n\n    @property\n    def seconds(self):\n        \"\"\"seconds\"\"\"\n        return self._seconds\n\n    @property\n    def microseconds(self):\n        \"\"\"microseconds\"\"\"\n        return self._microseconds\n\n    def __add__(self, other):\n        if isinstance(other, timedelta):\n            # for CPython compatibility, we cannot use\n            # our __class__ here, but need a real timedelta\n            return timedelta(self._days + other._days,\n                             self._seconds + other._seconds,\n                             self._microseconds + other._microseconds)\n        return NotImplemented\n\n    __radd__ = __add__\n\n    def __sub__(self, other):\n        if isinstance(other, timedelta):\n            # for CPython compatibility, we cannot use\n            # our __class__ here, but need a real timedelta\n            return timedelta(self._days - other._days,\n                             self._seconds - other._seconds,\n                             self._microseconds - other._microseconds)\n        return NotImplemented\n\n    def __rsub__(self, other):\n        if isinstance(other, timedelta):\n            return -self + other\n        return NotImplemented\n\n    def __neg__(self):\n        # for CPython compatibility, we cannot use\n        # our __class__ here, but need a real timedelta\n        return timedelta(-self._days,\n                         -self._seconds,\n                         -self._microseconds)\n\n    def __pos__(self):\n        return self\n\n    def __abs__(self):\n        if self._days < 0:\n            return -self\n        else:\n            return self\n\n    def __mul__(self, other):\n        if isinstance(other, (int, long)):\n            # for CPython compatibility, we cannot use\n            # our __class__ here, but need a real timedelta\n            return timedelta(self._days * other,\n                             self._seconds * other,\n                             self._microseconds * other)\n        return NotImplemented\n\n    __rmul__ = __mul__\n\n    def _to_microseconds(self):\n        return ((self._days * (24*3600) + self._seconds) * 1000000 +\n                self._microseconds)\n\n    def __div__(self, other):\n        if not isinstance(other, (int, long)):\n            return NotImplemented\n        usec = self._to_microseconds()\n        return timedelta(0, 0, usec // other)\n\n    __floordiv__ = __div__\n\n    # Comparisons of timedelta objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) == 0\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) != 0\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) <= 0\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) < 0\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) >= 0\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) > 0\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, timedelta)\n        return _cmp(self._getstate(), other._getstate())\n\n    def __hash__(self):\n        if self._hashcode == -1:\n            self._hashcode = hash(self._getstate())\n        return self._hashcode\n\n    def __nonzero__(self):\n        return (self._days != 0 or\n                self._seconds != 0 or\n                self._microseconds != 0)\n\n    # Pickle support.\n\n    def _getstate(self):\n        return (self._days, self._seconds, self._microseconds)\n\n    def __reduce__(self):\n        return (self.__class__, self._getstate())\n\ntimedelta.min = timedelta(-999999999)\ntimedelta.max = timedelta(days=999999999, hours=23, minutes=59, seconds=59,\n                          microseconds=999999)\ntimedelta.resolution = timedelta(microseconds=1)\n\nclass date(object):\n    \"\"\"Concrete date type.\n\n    Constructors:\n\n    __new__()\n    fromtimestamp()\n    today()\n    fromordinal()\n\n    Operators:\n\n    __repr__, __str__\n    __cmp__, __hash__\n    __add__, __radd__, __sub__ (add/radd only with timedelta arg)\n\n    Methods:\n\n    timetuple()\n    toordinal()\n    weekday()\n    isoweekday(), isocalendar(), isoformat()\n    ctime()\n    strftime()\n\n    Properties (readonly):\n    year, month, day\n    \"\"\"\n    __slots__ = '_year', '_month', '_day', '_hashcode'\n\n    def __new__(cls, year, month=None, day=None):\n        \"\"\"Constructor.\n\n        Arguments:\n\n        year, month, day (required, base 1)\n        \"\"\"\n        if month is None and isinstance(year, bytes) and len(year) == 4 and \\\n                1 <= ord(year[2]) <= 12:\n            # Pickle support\n            self = object.__new__(cls)\n            self.__setstate(year)\n            self._hashcode = -1\n            return self\n        year, month, day = _check_date_fields(year, month, day)\n        self = object.__new__(cls)\n        self._year = year\n        self._month = month\n        self._day = day\n        self._hashcode = -1\n        return self\n\n    # Additional constructors\n\n    @classmethod\n    def fromtimestamp(cls, t):\n        \"Construct a date from a POSIX timestamp (like time.time()).\"\n        y, m, d, hh, mm, ss, weekday, jday, dst = _time.localtime(t)\n        return cls(y, m, d)\n\n    @classmethod\n    def today(cls):\n        \"Construct a date from time.time().\"\n        t = _time.time()\n        return cls.fromtimestamp(t)\n\n    @classmethod\n    def fromordinal(cls, n):\n        \"\"\"Contruct a date from a proleptic Gregorian ordinal.\n\n        January 1 of year 1 is day 1.  Only the year, month and day are\n        non-zero in the result.\n        \"\"\"\n        y, m, d = _ord2ymd(n)\n        return cls(y, m, d)\n\n    # Conversions to string\n\n    def __repr__(self):\n        \"\"\"Convert to formal string, for repr().\n\n        >>> dt = datetime(2010, 1, 1)\n        >>> repr(dt)\n        'datetime.datetime(2010, 1, 1, 0, 0)'\n\n        >>> dt = datetime(2010, 1, 1, tzinfo=timezone.utc)\n        >>> repr(dt)\n        'datetime.datetime(2010, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)'\n        \"\"\"\n        return \"%s(%d, %d, %d)\" % ('datetime.' + self.__class__.__name__,\n                                   self._year,\n                                   self._month,\n                                   self._day)\n\n    # XXX These shouldn't depend on time.localtime(), because that\n    # clips the usable dates to [1970 .. 2038).  At least ctime() is\n    # easily done without using strftime() -- that's better too because\n    # strftime(\"%c\", ...) is locale specific.\n\n    def ctime(self):\n        \"Return ctime() style string.\"\n        weekday = self.toordinal() % 7 or 7\n        return \"%s %s %2d 00:00:00 %04d\" % (\n            _DAYNAMES[weekday],\n            _MONTHNAMES[self._month],\n            self._day, self._year)\n\n    def strftime(self, fmt):\n        \"Format using strftime().\"\n        return _wrap_strftime(self, fmt, self.timetuple())\n\n    def __format__(self, fmt):\n        if not isinstance(fmt, (str, unicode)):\n            raise ValueError(\"__format__ expects str or unicode, not %s\" %\n                             fmt.__class__.__name__)\n        if len(fmt) != 0:\n            return self.strftime(fmt)\n        return str(self)\n\n    def isoformat(self):\n        \"\"\"Return the date formatted according to ISO.\n\n        This is 'YYYY-MM-DD'.\n\n        References:\n        - http://www.w3.org/TR/NOTE-datetime\n        - http://www.cl.cam.ac.uk/~mgk25/iso-time.html\n        \"\"\"\n        return \"%04d-%02d-%02d\" % (self._year, self._month, self._day)\n\n    __str__ = isoformat\n\n    # Read-only field accessors\n    @property\n    def year(self):\n        \"\"\"year (1-9999)\"\"\"\n        return self._year\n\n    @property\n    def month(self):\n        \"\"\"month (1-12)\"\"\"\n        return self._month\n\n    @property\n    def day(self):\n        \"\"\"day (1-31)\"\"\"\n        return self._day\n\n    # Standard conversions, __cmp__, __hash__ (and helpers)\n\n    def timetuple(self):\n        \"Return local time tuple compatible with time.localtime().\"\n        return _build_struct_time(self._year, self._month, self._day,\n                                  0, 0, 0, -1)\n\n    def toordinal(self):\n        \"\"\"Return proleptic Gregorian ordinal for the year, month and day.\n\n        January 1 of year 1 is day 1.  Only the year, month and day values\n        contribute to the result.\n        \"\"\"\n        return _ymd2ord(self._year, self._month, self._day)\n\n    def replace(self, year=None, month=None, day=None):\n        \"\"\"Return a new date with new values for the specified fields.\"\"\"\n        if year is None:\n            year = self._year\n        if month is None:\n            month = self._month\n        if day is None:\n            day = self._day\n        return date(year, month, day)\n\n    # Comparisons of date objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) == 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) != 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) <= 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) < 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) >= 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) > 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, date)\n        y, m, d = self._year, self._month, self._day\n        y2, m2, d2 = other._year, other._month, other._day\n        return _cmp((y, m, d), (y2, m2, d2))\n\n    def __hash__(self):\n        \"Hash.\"\n        if self._hashcode == -1:\n            self._hashcode = hash(self._getstate())\n        return self._hashcode\n\n    # Computations\n\n    def _checkOverflow(self, year):\n        if not MINYEAR <= year <= MAXYEAR:\n            raise OverflowError(\"date +/-: result year %d not in %d..%d\" %\n                                (year, MINYEAR, MAXYEAR))\n\n    def __add__(self, other):\n        \"Add a date to a timedelta.\"\n        if isinstance(other, timedelta):\n            t = _tmxxx(self._year,\n                      self._month,\n                      self._day + other.days)\n            self._checkOverflow(t.year)\n            result = date(t.year, t.month, t.day)\n            return result\n        return NotImplemented\n\n    __radd__ = __add__\n\n    def __sub__(self, other):\n        \"\"\"Subtract two dates, or a date and a timedelta.\"\"\"\n        if isinstance(other, timedelta):\n            return self + timedelta(-other.days)\n        if isinstance(other, date):\n            days1 = self.toordinal()\n            days2 = other.toordinal()\n            return timedelta(days1 - days2)\n        return NotImplemented\n\n    def weekday(self):\n        \"Return day of the week, where Monday == 0 ... Sunday == 6.\"\n        return (self.toordinal() + 6) % 7\n\n    # Day-of-the-week and week-of-the-year, according to ISO\n\n    def isoweekday(self):\n        \"Return day of the week, where Monday == 1 ... Sunday == 7.\"\n        # 1-Jan-0001 is a Monday\n        return self.toordinal() % 7 or 7\n\n    def isocalendar(self):\n        \"\"\"Return a 3-tuple containing ISO year, week number, and weekday.\n\n        The first ISO week of the year is the (Mon-Sun) week\n        containing the year's first Thursday; everything else derives\n        from that.\n\n        The first week is 1; Monday is 1 ... Sunday is 7.\n\n        ISO calendar algorithm taken from\n        http://www.phys.uu.nl/~vgent/calendar/isocalendar.htm\n        \"\"\"\n        year = self._year\n        week1monday = _isoweek1monday(year)\n        today = _ymd2ord(self._year, self._month, self._day)\n        # Internally, week and day have origin 0\n        week, day = divmod(today - week1monday, 7)\n        if week < 0:\n            year -= 1\n            week1monday = _isoweek1monday(year)\n            week, day = divmod(today - week1monday, 7)\n        elif week >= 52:\n            if today >= _isoweek1monday(year+1):\n                year += 1\n                week = 0\n        return year, week+1, day+1\n\n    # Pickle support.\n\n    def _getstate(self):\n        yhi, ylo = divmod(self._year, 256)\n        return (_struct.pack('4B', yhi, ylo, self._month, self._day),)\n\n    def __setstate(self, string):\n        yhi, ylo, self._month, self._day = (ord(string[0]), ord(string[1]),\n                                            ord(string[2]), ord(string[3]))\n        self._year = yhi * 256 + ylo\n\n    def __reduce__(self):\n        return (self.__class__, self._getstate())\n\n_date_class = date  # so functions w/ args named \"date\" can get at the class\n\ndate.min = date(1, 1, 1)\ndate.max = date(9999, 12, 31)\ndate.resolution = timedelta(days=1)\n\nclass tzinfo(object):\n    \"\"\"Abstract base class for time zone info classes.\n\n    Subclasses must override the name(), utcoffset() and dst() methods.\n    \"\"\"\n    __slots__ = ()\n\n    def tzname(self, dt):\n        \"datetime -> string name of time zone.\"\n        raise NotImplementedError(\"tzinfo subclass must override tzname()\")\n\n    def utcoffset(self, dt):\n        \"datetime -> minutes east of UTC (negative for west of UTC)\"\n        raise NotImplementedError(\"tzinfo subclass must override utcoffset()\")\n\n    def dst(self, dt):\n        \"\"\"datetime -> DST offset in minutes east of UTC.\n\n        Return 0 if DST not in effect.  utcoffset() must include the DST\n        offset.\n        \"\"\"\n        raise NotImplementedError(\"tzinfo subclass must override dst()\")\n\n    def fromutc(self, dt):\n        \"datetime in UTC -> datetime in local time.\"\n\n        if not isinstance(dt, datetime):\n            raise TypeError(\"fromutc() requires a datetime argument\")\n        if dt.tzinfo is not self:\n            raise ValueError(\"dt.tzinfo is not self\")\n\n        dtoff = dt.utcoffset()\n        if dtoff is None:\n            raise ValueError(\"fromutc() requires a non-None utcoffset() \"\n                             \"result\")\n\n        # See the long comment block at the end of this file for an\n        # explanation of this algorithm.\n        dtdst = dt.dst()\n        if dtdst is None:\n            raise ValueError(\"fromutc() requires a non-None dst() result\")\n        delta = dtoff - dtdst\n        if delta:\n            dt += delta\n            dtdst = dt.dst()\n            if dtdst is None:\n                raise ValueError(\"fromutc(): dt.dst gave inconsistent \"\n                                 \"results; cannot convert\")\n        if dtdst:\n            return dt + dtdst\n        else:\n            return dt\n\n    # Pickle support.\n\n    def __reduce__(self):\n        getinitargs = getattr(self, \"__getinitargs__\", None)\n        if getinitargs:\n            args = getinitargs()\n        else:\n            args = ()\n        getstate = getattr(self, \"__getstate__\", None)\n        if getstate:\n            state = getstate()\n        else:\n            state = getattr(self, \"__dict__\", None) or None\n        if state is None:\n            return (self.__class__, args)\n        else:\n            return (self.__class__, args, state)\n\n_tzinfo_class = tzinfo\n\nclass time(object):\n    \"\"\"Time with time zone.\n\n    Constructors:\n\n    __new__()\n\n    Operators:\n\n    __repr__, __str__\n    __cmp__, __hash__\n\n    Methods:\n\n    strftime()\n    isoformat()\n    utcoffset()\n    tzname()\n    dst()\n\n    Properties (readonly):\n    hour, minute, second, microsecond, tzinfo\n    \"\"\"\n    __slots__ = '_hour', '_minute', '_second', '_microsecond', '_tzinfo', '_hashcode'\n\n    def __new__(cls, hour=0, minute=0, second=0, microsecond=0, tzinfo=None):\n        \"\"\"Constructor.\n\n        Arguments:\n\n        hour, minute (required)\n        second, microsecond (default to zero)\n        tzinfo (default to None)\n        \"\"\"\n        if isinstance(hour, bytes) and len(hour) == 6 and ord(hour[0]) < 24:\n            # Pickle support\n            self = object.__new__(cls)\n            self.__setstate(hour, minute or None)\n            self._hashcode = -1\n            return self\n        hour, minute, second, microsecond = _check_time_fields(\n            hour, minute, second, microsecond)\n        _check_tzinfo_arg(tzinfo)\n        self = object.__new__(cls)\n        self._hour = hour\n        self._minute = minute\n        self._second = second\n        self._microsecond = microsecond\n        self._tzinfo = tzinfo\n        self._hashcode = -1\n        return self\n\n    # Read-only field accessors\n    @property\n    def hour(self):\n        \"\"\"hour (0-23)\"\"\"\n        return self._hour\n\n    @property\n    def minute(self):\n        \"\"\"minute (0-59)\"\"\"\n        return self._minute\n\n    @property\n    def second(self):\n        \"\"\"second (0-59)\"\"\"\n        return self._second\n\n    @property\n    def microsecond(self):\n        \"\"\"microsecond (0-999999)\"\"\"\n        return self._microsecond\n\n    @property\n    def tzinfo(self):\n        \"\"\"timezone info object\"\"\"\n        return self._tzinfo\n\n    # Standard conversions, __hash__ (and helpers)\n\n    # Comparisons of time objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) == 0\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) != 0\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) <= 0\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) < 0\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) >= 0\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) > 0\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, time)\n        mytz = self._tzinfo\n        ottz = other._tzinfo\n        myoff = otoff = None\n\n        if mytz is ottz:\n            base_compare = True\n        else:\n            myoff = self._utcoffset()\n            otoff = other._utcoffset()\n            base_compare = myoff == otoff\n\n        if base_compare:\n            return _cmp((self._hour, self._minute, self._second,\n                         self._microsecond),\n                        (other._hour, other._minute, other._second,\n                         other._microsecond))\n        if myoff is None or otoff is None:\n            raise TypeError(\"can't compare offset-naive and offset-aware times\")\n        myhhmm = self._hour * 60 + self._minute - myoff\n        othhmm = other._hour * 60 + other._minute - otoff\n        return _cmp((myhhmm, self._second, self._microsecond),\n                    (othhmm, other._second, other._microsecond))\n\n    def __hash__(self):\n        \"\"\"Hash.\"\"\"\n        if self._hashcode == -1:\n            tzoff = self._utcoffset()\n            if not tzoff:  # zero or None\n                self._hashcode = hash(self._getstate()[0])\n            else:\n                h, m = divmod(self.hour * 60 + self.minute - tzoff, 60)\n                if 0 <= h < 24:\n                    self._hashcode = hash(time(h, m, self.second, self.microsecond))\n                else:\n                    self._hashcode = hash((h, m, self.second, self.microsecond))\n        return self._hashcode\n\n    # Conversion to string\n\n    def _tzstr(self, sep=\":\"):\n        \"\"\"Return formatted timezone offset (+xx:xx) or None.\"\"\"\n        off = self._utcoffset()\n        if off is not None:\n            if off < 0:\n                sign = \"-\"\n                off = -off\n            else:\n                sign = \"+\"\n            hh, mm = divmod(off, 60)\n            assert 0 <= hh < 24\n            off = \"%s%02d%s%02d\" % (sign, hh, sep, mm)\n        return off\n\n    def __repr__(self):\n        \"\"\"Convert to formal string, for repr().\"\"\"\n        if self._microsecond != 0:\n            s = \", %d, %d\" % (self._second, self._microsecond)\n        elif self._second != 0:\n            s = \", %d\" % self._second\n        else:\n            s = \"\"\n        s= \"%s(%d, %d%s)\" % ('datetime.' + self.__class__.__name__,\n                             self._hour, self._minute, s)\n        if self._tzinfo is not None:\n            assert s[-1:] == \")\"\n            s = s[:-1] + \", tzinfo=%r\" % self._tzinfo + \")\"\n        return s\n\n    def isoformat(self):\n        \"\"\"Return the time formatted according to ISO.\n\n        This is 'HH:MM:SS.mmmmmm+zz:zz', or 'HH:MM:SS+zz:zz' if\n        self.microsecond == 0.\n        \"\"\"\n        s = _format_time(self._hour, self._minute, self._second,\n                         self._microsecond)\n        tz = self._tzstr()\n        if tz:\n            s += tz\n        return s\n\n    __str__ = isoformat\n\n    def strftime(self, fmt):\n        \"\"\"Format using strftime().  The date part of the timestamp passed\n        to underlying strftime should not be used.\n        \"\"\"\n        # The year must be >= _MINYEARFMT else Python's strftime implementation\n        # can raise a bogus exception.\n        timetuple = (1900, 1, 1,\n                     self._hour, self._minute, self._second,\n                     0, 1, -1)\n        return _wrap_strftime(self, fmt, timetuple)\n\n    def __format__(self, fmt):\n        if not isinstance(fmt, (str, unicode)):\n            raise ValueError(\"__format__ expects str or unicode, not %s\" %\n                             fmt.__class__.__name__)\n        if len(fmt) != 0:\n            return self.strftime(fmt)\n        return str(self)\n\n    # Timezone functions\n\n    def utcoffset(self):\n        \"\"\"Return the timezone offset in minutes east of UTC (negative west of\n        UTC).\"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(None)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _utcoffset(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(None)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        return offset\n\n    def tzname(self):\n        \"\"\"Return the timezone name.\n\n        Note that the name is 100% informational -- there's no requirement that\n        it mean anything in particular. For example, \"GMT\", \"UTC\", \"-500\",\n        \"-5:00\", \"EDT\", \"US/Eastern\", \"America/New York\" are all valid replies.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        name = self._tzinfo.tzname(None)\n        _check_tzname(name)\n        return name\n\n    def dst(self):\n        \"\"\"Return 0 if DST is not in effect, or the DST offset (in minutes\n        eastward) if DST is in effect.\n\n        This is purely informational; the DST offset has already been added to\n        the UTC offset returned by utcoffset() if applicable, so there's no\n        need to consult dst() unless you're interested in displaying the DST\n        info.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(None)\n        offset = _check_utc_offset(\"dst\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _dst(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(None)\n        offset = _check_utc_offset(\"dst\", offset)\n        return offset\n\n    def replace(self, hour=None, minute=None, second=None, microsecond=None,\n                tzinfo=True):\n        \"\"\"Return a new time with new values for the specified fields.\"\"\"\n        if hour is None:\n            hour = self.hour\n        if minute is None:\n            minute = self.minute\n        if second is None:\n            second = self.second\n        if microsecond is None:\n            microsecond = self.microsecond\n        if tzinfo is True:\n            tzinfo = self.tzinfo\n        return time(hour, minute, second, microsecond, tzinfo)\n\n    def __nonzero__(self):\n        if self.second or self.microsecond:\n            return True\n        offset = self._utcoffset() or 0\n        return self.hour * 60 + self.minute != offset\n\n    # Pickle support.\n\n    def _getstate(self):\n        us2, us3 = divmod(self._microsecond, 256)\n        us1, us2 = divmod(us2, 256)\n        basestate = _struct.pack('6B', self._hour, self._minute, self._second,\n                                       us1, us2, us3)\n        if self._tzinfo is None:\n            return (basestate,)\n        else:\n            return (basestate, self._tzinfo)\n\n    def __setstate(self, string, tzinfo):\n        if tzinfo is not None and not isinstance(tzinfo, _tzinfo_class):\n            raise TypeError(\"bad tzinfo state arg\")\n        self._hour, self._minute, self._second, us1, us2, us3 = (\n            ord(string[0]), ord(string[1]), ord(string[2]),\n            ord(string[3]), ord(string[4]), ord(string[5]))\n        self._microsecond = (((us1 << 8) | us2) << 8) | us3\n        self._tzinfo = tzinfo\n\n    def __reduce__(self):\n        return (time, self._getstate())\n\n_time_class = time  # so functions w/ args named \"time\" can get at the class\n\ntime.min = time(0, 0, 0)\ntime.max = time(23, 59, 59, 999999)\ntime.resolution = timedelta(microseconds=1)\n\nclass datetime(date):\n    \"\"\"datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\n    The year, month and day arguments are required. tzinfo may be None, or an\n    instance of a tzinfo subclass. The remaining arguments may be ints or longs.\n    \"\"\"\n    __slots__ = date.__slots__ + time.__slots__\n\n    def __new__(cls, year, month=None, day=None, hour=0, minute=0, second=0,\n                microsecond=0, tzinfo=None):\n        if isinstance(year, bytes) and len(year) == 10 and \\\n                1 <= ord(year[2]) <= 12:\n            # Pickle support\n            self = object.__new__(cls)\n            self.__setstate(year, month)\n            self._hashcode = -1\n            return self\n        year, month, day = _check_date_fields(year, month, day)\n        hour, minute, second, microsecond = _check_time_fields(\n            hour, minute, second, microsecond)\n        _check_tzinfo_arg(tzinfo)\n        self = object.__new__(cls)\n        self._year = year\n        self._month = month\n        self._day = day\n        self._hour = hour\n        self._minute = minute\n        self._second = second\n        self._microsecond = microsecond\n        self._tzinfo = tzinfo\n        self._hashcode = -1\n        return self\n\n    # Read-only field accessors\n    @property\n    def hour(self):\n        \"\"\"hour (0-23)\"\"\"\n        return self._hour\n\n    @property\n    def minute(self):\n        \"\"\"minute (0-59)\"\"\"\n        return self._minute\n\n    @property\n    def second(self):\n        \"\"\"second (0-59)\"\"\"\n        return self._second\n\n    @property\n    def microsecond(self):\n        \"\"\"microsecond (0-999999)\"\"\"\n        return self._microsecond\n\n    @property\n    def tzinfo(self):\n        \"\"\"timezone info object\"\"\"\n        return self._tzinfo\n\n    @classmethod\n    def fromtimestamp(cls, t, tz=None):\n        \"\"\"Construct a datetime from a POSIX timestamp (like time.time()).\n\n        A timezone info object may be passed in as well.\n        \"\"\"\n\n        _check_tzinfo_arg(tz)\n\n        converter = _time.localtime if tz is None else _time.gmtime\n\n        t, frac = divmod(t, 1.0)\n        us = _round(frac * 1e6)\n\n        # If timestamp is less than one microsecond smaller than a\n        # full second, us can be rounded up to 1000000.  In this case,\n        # roll over to seconds, otherwise, ValueError is raised\n        # by the constructor.\n        if us == 1000000:\n            t += 1\n            us = 0\n        y, m, d, hh, mm, ss, weekday, jday, dst = converter(t)\n        ss = min(ss, 59)    # clamp out leap seconds if the platform has them\n        result = cls(y, m, d, hh, mm, ss, us, tz)\n        if tz is not None:\n            result = tz.fromutc(result)\n        return result\n\n    @classmethod\n    def utcfromtimestamp(cls, t):\n        \"Construct a UTC datetime from a POSIX timestamp (like time.time()).\"\n        t, frac = divmod(t, 1.0)\n        us = _round(frac * 1e6)\n\n        # If timestamp is less than one microsecond smaller than a\n        # full second, us can be rounded up to 1000000.  In this case,\n        # roll over to seconds, otherwise, ValueError is raised\n        # by the constructor.\n        if us == 1000000:\n            t += 1\n            us = 0\n        y, m, d, hh, mm, ss, weekday, jday, dst = _time.gmtime(t)\n        ss = min(ss, 59)    # clamp out leap seconds if the platform has them\n        return cls(y, m, d, hh, mm, ss, us)\n\n    @classmethod\n    def now(cls, tz=None):\n        \"Construct a datetime from time.time() and optional time zone info.\"\n        t = _time.time()\n        return cls.fromtimestamp(t, tz)\n\n    @classmethod\n    def utcnow(cls):\n        \"Construct a UTC datetime from time.time().\"\n        t = _time.time()\n        return cls.utcfromtimestamp(t)\n\n    @classmethod\n    def combine(cls, date, time):\n        \"Construct a datetime from a given date and a given time.\"\n        if not isinstance(date, _date_class):\n            raise TypeError(\"date argument must be a date instance\")\n        if not isinstance(time, _time_class):\n            raise TypeError(\"time argument must be a time instance\")\n        return cls(date.year, date.month, date.day,\n                   time.hour, time.minute, time.second, time.microsecond,\n                   time.tzinfo)\n\n    def timetuple(self):\n        \"Return local time tuple compatible with time.localtime().\"\n        dst = self._dst()\n        if dst is None:\n            dst = -1\n        elif dst:\n            dst = 1\n        return _build_struct_time(self.year, self.month, self.day,\n                                  self.hour, self.minute, self.second,\n                                  dst)\n\n    def utctimetuple(self):\n        \"Return UTC time tuple compatible with time.gmtime().\"\n        y, m, d = self.year, self.month, self.day\n        hh, mm, ss = self.hour, self.minute, self.second\n        offset = self._utcoffset()\n        if offset:  # neither None nor 0\n            tm = _tmxxx(y, m, d, hh, mm - offset)\n            y, m, d = tm.year, tm.month, tm.day\n            hh, mm = tm.hour, tm.minute\n        return _build_struct_time(y, m, d, hh, mm, ss, 0)\n\n    def date(self):\n        \"Return the date part.\"\n        return date(self._year, self._month, self._day)\n\n    def time(self):\n        \"Return the time part, with tzinfo None.\"\n        return time(self.hour, self.minute, self.second, self.microsecond)\n\n    def timetz(self):\n        \"Return the time part, with same tzinfo.\"\n        return time(self.hour, self.minute, self.second, self.microsecond,\n                    self._tzinfo)\n\n    def replace(self, year=None, month=None, day=None, hour=None,\n                minute=None, second=None, microsecond=None, tzinfo=True):\n        \"\"\"Return a new datetime with new values for the specified fields.\"\"\"\n        if year is None:\n            year = self.year\n        if month is None:\n            month = self.month\n        if day is None:\n            day = self.day\n        if hour is None:\n            hour = self.hour\n        if minute is None:\n            minute = self.minute\n        if second is None:\n            second = self.second\n        if microsecond is None:\n            microsecond = self.microsecond\n        if tzinfo is True:\n            tzinfo = self.tzinfo\n        return datetime(year, month, day, hour, minute, second, microsecond,\n                        tzinfo)\n\n    def astimezone(self, tz):\n        if not isinstance(tz, tzinfo):\n            raise TypeError(\"tz argument must be an instance of tzinfo\")\n\n        mytz = self.tzinfo\n        if mytz is None:\n            raise ValueError(\"astimezone() requires an aware datetime\")\n\n        if tz is mytz:\n            return self\n\n        # Convert self to UTC, and attach the new time zone object.\n        myoffset = self.utcoffset()\n        if myoffset is None:\n            raise ValueError(\"astimezone() requires an aware datetime\")\n        utc = (self - myoffset).replace(tzinfo=tz)\n\n        # Convert from UTC to tz's local time.\n        return tz.fromutc(utc)\n\n    # Ways to produce a string.\n\n    def ctime(self):\n        \"Return ctime() style string.\"\n        weekday = self.toordinal() % 7 or 7\n        return \"%s %s %2d %02d:%02d:%02d %04d\" % (\n            _DAYNAMES[weekday],\n            _MONTHNAMES[self._month],\n            self._day,\n            self._hour, self._minute, self._second,\n            self._year)\n\n    def isoformat(self, sep='T'):\n        \"\"\"Return the time formatted according to ISO.\n\n        This is 'YYYY-MM-DD HH:MM:SS.mmmmmm', or 'YYYY-MM-DD HH:MM:SS' if\n        self.microsecond == 0.\n\n        If self.tzinfo is not None, the UTC offset is also attached, giving\n        'YYYY-MM-DD HH:MM:SS.mmmmmm+HH:MM' or 'YYYY-MM-DD HH:MM:SS+HH:MM'.\n\n        Optional argument sep specifies the separator between date and\n        time, default 'T'.\n        \"\"\"\n        s = (\"%04d-%02d-%02d%c\" % (self._year, self._month, self._day, sep) +\n             _format_time(self._hour, self._minute, self._second,\n                          self._microsecond))\n        off = self._utcoffset()\n        if off is not None:\n            if off < 0:\n                sign = \"-\"\n                off = -off\n            else:\n                sign = \"+\"\n            hh, mm = divmod(off, 60)\n            s += \"%s%02d:%02d\" % (sign, hh, mm)\n        return s\n\n    def __repr__(self):\n        \"\"\"Convert to formal string, for repr().\"\"\"\n        L = [self._year, self._month, self._day,  # These are never zero\n             self._hour, self._minute, self._second, self._microsecond]\n        if L[-1] == 0:\n            del L[-1]\n        if L[-1] == 0:\n            del L[-1]\n        s = \", \".join(map(str, L))\n        s = \"%s(%s)\" % ('datetime.' + self.__class__.__name__, s)\n        if self._tzinfo is not None:\n            assert s[-1:] == \")\"\n            s = s[:-1] + \", tzinfo=%r\" % self._tzinfo + \")\"\n        return s\n\n    def __str__(self):\n        \"Convert to string, for str().\"\n        return self.isoformat(sep=' ')\n\n    @classmethod\n    def strptime(cls, date_string, format):\n        'string, format -> new datetime parsed from a string (like time.strptime()).'\n        from _strptime import _strptime\n        # _strptime._strptime returns a two-element tuple.  The first\n        # element is a time.struct_time object.  The second is the\n        # microseconds (which are not defined for time.struct_time).\n        struct, micros = _strptime(date_string, format)\n        return cls(*(struct[0:6] + (micros,)))\n\n    def utcoffset(self):\n        \"\"\"Return the timezone offset in minutes east of UTC (negative west of\n        UTC).\"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(self)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _utcoffset(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(self)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        return offset\n\n    def tzname(self):\n        \"\"\"Return the timezone name.\n\n        Note that the name is 100% informational -- there's no requirement that\n        it mean anything in particular. For example, \"GMT\", \"UTC\", \"-500\",\n        \"-5:00\", \"EDT\", \"US/Eastern\", \"America/New York\" are all valid replies.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        name = self._tzinfo.tzname(self)\n        _check_tzname(name)\n        return name\n\n    def dst(self):\n        \"\"\"Return 0 if DST is not in effect, or the DST offset (in minutes\n        eastward) if DST is in effect.\n\n        This is purely informational; the DST offset has already been added to\n        the UTC offset returned by utcoffset() if applicable, so there's no\n        need to consult dst() unless you're interested in displaying the DST\n        info.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(self)\n        offset = _check_utc_offset(\"dst\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _dst(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(self)\n        offset = _check_utc_offset(\"dst\", offset)\n        return offset\n\n    # Comparisons of datetime objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) == 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) != 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) <= 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) < 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) >= 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) > 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, datetime)\n        mytz = self._tzinfo\n        ottz = other._tzinfo\n        myoff = otoff = None\n\n        if mytz is ottz:\n            base_compare = True\n        else:\n            if mytz is not None:\n                myoff = self._utcoffset()\n            if ottz is not None:\n                otoff = other._utcoffset()\n            base_compare = myoff == otoff\n\n        if base_compare:\n            return _cmp((self._year, self._month, self._day,\n                         self._hour, self._minute, self._second,\n                         self._microsecond),\n                        (other._year, other._month, other._day,\n                         other._hour, other._minute, other._second,\n                         other._microsecond))\n        if myoff is None or otoff is None:\n            raise TypeError(\"can't compare offset-naive and offset-aware datetimes\")\n        # XXX What follows could be done more efficiently...\n        diff = self - other     # this will take offsets into account\n        if diff.days < 0:\n            return -1\n        return diff and 1 or 0\n\n    def __add__(self, other):\n        \"Add a datetime and a timedelta.\"\n        if not isinstance(other, timedelta):\n            return NotImplemented\n        t = _tmxxx(self._year,\n                  self._month,\n                  self._day + other.days,\n                  self._hour,\n                  self._minute,\n                  self._second + other.seconds,\n                  self._microsecond + other.microseconds)\n        self._checkOverflow(t.year)\n        result = datetime(t.year, t.month, t.day,\n                                t.hour, t.minute, t.second,\n                                t.microsecond, tzinfo=self._tzinfo)\n        return result\n\n    __radd__ = __add__\n\n    def __sub__(self, other):\n        \"Subtract two datetimes, or a datetime and a timedelta.\"\n        if not isinstance(other, datetime):\n            if isinstance(other, timedelta):\n                return self + -other\n            return NotImplemented\n\n        days1 = self.toordinal()\n        days2 = other.toordinal()\n        secs1 = self._second + self._minute * 60 + self._hour * 3600\n        secs2 = other._second + other._minute * 60 + other._hour * 3600\n        base = timedelta(days1 - days2,\n                         secs1 - secs2,\n                         self._microsecond - other._microsecond)\n        if self._tzinfo is other._tzinfo:\n            return base\n        myoff = self._utcoffset()\n        otoff = other._utcoffset()\n        if myoff == otoff:\n            return base\n        if myoff is None or otoff is None:\n            raise TypeError(\"can't subtract offset-naive and offset-aware datetimes\")\n        return base + timedelta(minutes = otoff-myoff)\n\n    def __hash__(self):\n        if self._hashcode == -1:\n            tzoff = self._utcoffset()\n            if tzoff is None:\n                self._hashcode = hash(self._getstate()[0])\n            else:\n                days = _ymd2ord(self.year, self.month, self.day)\n                seconds = self.hour * 3600 + (self.minute - tzoff) * 60 + self.second\n                self._hashcode = hash(timedelta(days, seconds, self.microsecond))\n        return self._hashcode\n\n    # Pickle support.\n\n    def _getstate(self):\n        yhi, ylo = divmod(self._year, 256)\n        us2, us3 = divmod(self._microsecond, 256)\n        us1, us2 = divmod(us2, 256)\n        basestate = _struct.pack('10B', yhi, ylo, self._month, self._day,\n                                        self._hour, self._minute, self._second,\n                                        us1, us2, us3)\n        if self._tzinfo is None:\n            return (basestate,)\n        else:\n            return (basestate, self._tzinfo)\n\n    def __setstate(self, string, tzinfo):\n        if tzinfo is not None and not isinstance(tzinfo, _tzinfo_class):\n            raise TypeError(\"bad tzinfo state arg\")\n        (yhi, ylo, self._month, self._day, self._hour,\n            self._minute, self._second, us1, us2, us3) = (ord(string[0]),\n                ord(string[1]), ord(string[2]), ord(string[3]),\n                ord(string[4]), ord(string[5]), ord(string[6]),\n                ord(string[7]), ord(string[8]), ord(string[9]))\n        self._year = yhi * 256 + ylo\n        self._microsecond = (((us1 << 8) | us2) << 8) | us3\n        self._tzinfo = tzinfo\n\n    def __reduce__(self):\n        return (self.__class__, self._getstate())\n\n\ndatetime.min = datetime(1, 1, 1)\ndatetime.max = datetime(9999, 12, 31, 23, 59, 59, 999999)\ndatetime.resolution = timedelta(microseconds=1)\n\n\ndef _isoweek1monday(year):\n    # Helper to calculate the day number of the Monday starting week 1\n    # XXX This could be done more efficiently\n    THURSDAY = 3\n    firstday = _ymd2ord(year, 1, 1)\n    firstweekday = (firstday + 6) % 7  # See weekday() above\n    week1monday = firstday - firstweekday\n    if firstweekday > THURSDAY:\n        week1monday += 7\n    return week1monday\n\n\"\"\"\nSome time zone algebra.  For a datetime x, let\n    x.n = x stripped of its timezone -- its naive time.\n    x.o = x.utcoffset(), and assuming that doesn't raise an exception or\n          return None\n    x.d = x.dst(), and assuming that doesn't raise an exception or\n          return None\n    x.s = x's standard offset, x.o - x.d\n\nNow some derived rules, where k is a duration (timedelta).\n\n1. x.o = x.s + x.d\n   This follows from the definition of x.s.\n\n2. If x and y have the same tzinfo member, x.s = y.s.\n   This is actually a requirement, an assumption we need to make about\n   sane tzinfo classes.\n\n3. The naive UTC time corresponding to x is x.n - x.o.\n   This is again a requirement for a sane tzinfo class.\n\n4. (x+k).s = x.s\n   This follows from #2, and that datimetimetz+timedelta preserves tzinfo.\n\n5. (x+k).n = x.n + k\n   Again follows from how arithmetic is defined.\n\nNow we can explain tz.fromutc(x).  Let's assume it's an interesting case\n(meaning that the various tzinfo methods exist, and don't blow up or return\nNone when called).\n\nThe function wants to return a datetime y with timezone tz, equivalent to x.\nx is already in UTC.\n\nBy #3, we want\n\n    y.n - y.o = x.n                             [1]\n\nThe algorithm starts by attaching tz to x.n, and calling that y.  So\nx.n = y.n at the start.  Then it wants to add a duration k to y, so that [1]\nbecomes true; in effect, we want to solve [2] for k:\n\n   (y+k).n - (y+k).o = x.n                      [2]\n\nBy #1, this is the same as\n\n   (y+k).n - ((y+k).s + (y+k).d) = x.n          [3]\n\nBy #5, (y+k).n = y.n + k, which equals x.n + k because x.n=y.n at the start.\nSubstituting that into [3],\n\n   x.n + k - (y+k).s - (y+k).d = x.n; the x.n terms cancel, leaving\n   k - (y+k).s - (y+k).d = 0; rearranging,\n   k = (y+k).s - (y+k).d; by #4, (y+k).s == y.s, so\n   k = y.s - (y+k).d\n\nOn the RHS, (y+k).d can't be computed directly, but y.s can be, and we\napproximate k by ignoring the (y+k).d term at first.  Note that k can't be\nvery large, since all offset-returning methods return a duration of magnitude\nless than 24 hours.  For that reason, if y is firmly in std time, (y+k).d must\nbe 0, so ignoring it has no consequence then.\n\nIn any case, the new value is\n\n    z = y + y.s                                 [4]\n\nIt's helpful to step back at look at [4] from a higher level:  it's simply\nmapping from UTC to tz's standard time.\n\nAt this point, if\n\n    z.n - z.o = x.n                             [5]\n\nwe have an equivalent time, and are almost done.  The insecurity here is\nat the start of daylight time.  Picture US Eastern for concreteness.  The wall\ntime jumps from 1:59 to 3:00, and wall hours of the form 2:MM don't make good\nsense then.  The docs ask that an Eastern tzinfo class consider such a time to\nbe EDT (because it's \"after 2\"), which is a redundant spelling of 1:MM EST\non the day DST starts.  We want to return the 1:MM EST spelling because that's\nthe only spelling that makes sense on the local wall clock.\n\nIn fact, if [5] holds at this point, we do have the standard-time spelling,\nbut that takes a bit of proof.  We first prove a stronger result.  What's the\ndifference between the LHS and RHS of [5]?  Let\n\n    diff = x.n - (z.n - z.o)                    [6]\n\nNow\n    z.n =                       by [4]\n    (y + y.s).n =               by #5\n    y.n + y.s =                 since y.n = x.n\n    x.n + y.s =                 since z and y are have the same tzinfo member,\n                                    y.s = z.s by #2\n    x.n + z.s\n\nPlugging that back into [6] gives\n\n    diff =\n    x.n - ((x.n + z.s) - z.o) =     expanding\n    x.n - x.n - z.s + z.o =         cancelling\n    - z.s + z.o =                   by #2\n    z.d\n\nSo diff = z.d.\n\nIf [5] is true now, diff = 0, so z.d = 0 too, and we have the standard-time\nspelling we wanted in the endcase described above.  We're done.  Contrarily,\nif z.d = 0, then we have a UTC equivalent, and are also done.\n\nIf [5] is not true now, diff = z.d != 0, and z.d is the offset we need to\nadd to z (in effect, z is in tz's standard time, and we need to shift the\nlocal clock into tz's daylight time).\n\nLet\n\n    z' = z + z.d = z + diff                     [7]\n\nand we can again ask whether\n\n    z'.n - z'.o = x.n                           [8]\n\nIf so, we're done.  If not, the tzinfo class is insane, according to the\nassumptions we've made.  This also requires a bit of proof.  As before, let's\ncompute the difference between the LHS and RHS of [8] (and skipping some of\nthe justifications for the kinds of substitutions we've done several times\nalready):\n\n    diff' = x.n - (z'.n - z'.o) =           replacing z'.n via [7]\n            x.n  - (z.n + diff - z'.o) =    replacing diff via [6]\n            x.n - (z.n + x.n - (z.n - z.o) - z'.o) =\n            x.n - z.n - x.n + z.n - z.o + z'.o =    cancel x.n\n            - z.n + z.n - z.o + z'.o =              cancel z.n\n            - z.o + z'.o =                      #1 twice\n            -z.s - z.d + z'.s + z'.d =          z and z' have same tzinfo\n            z'.d - z.d\n\nSo z' is UTC-equivalent to x iff z'.d = z.d at this point.  If they are equal,\nwe've found the UTC-equivalent so are done.  In fact, we stop with [7] and\nreturn z', not bothering to compute z'.d.\n\nHow could z.d and z'd differ?  z' = z + z.d [7], so merely moving z' by\na dst() offset, and starting *from* a time already in DST (we know z.d != 0),\nwould have to change the result dst() returns:  we start in DST, and moving\na little further into it takes us out of DST.\n\nThere isn't a sane case where this can happen.  The closest it gets is at\nthe end of DST, where there's an hour in UTC with no spelling in a hybrid\ntzinfo class.  In US Eastern, that's 5:MM UTC = 0:MM EST = 1:MM EDT.  During\nthat hour, on an Eastern clock 1:MM is taken as being in standard time (6:MM\nUTC) because the docs insist on that, but 0:MM is taken as being in daylight\ntime (4:MM UTC).  There is no local time mapping to 5:MM UTC.  The local\nclock jumps from 1:59 back to 1:00 again, and repeats the 1:MM hour in\nstandard time.  Since that's what the local clock *does*, we want to map both\nUTC hours 5:MM and 6:MM to 1:MM Eastern.  The result is ambiguous\nin local time, but so it goes -- it's the way the local clock works.\n\nWhen x = 5:MM UTC is the input to this algorithm, x.o=0, y.o=-5 and y.d=0,\nso z=0:MM.  z.d=60 (minutes) then, so [5] doesn't hold and we keep going.\nz' = z + z.d = 1:MM then, and z'.d=0, and z'.d - z.d = -60 != 0 so [8]\n(correctly) concludes that z' is not UTC-equivalent to x.\n\nBecause we know z.d said z was in daylight time (else [5] would have held and\nwe would have stopped then), and we know z.d != z'.d (else [8] would have held\nand we have stopped then), and there are only 2 possible values dst() can\nreturn in Eastern, it follows that z'.d must be 0 (which it is in the example,\nbut the reasoning doesn't depend on the example -- it depends on there being\ntwo possible dst() outcomes, one zero and the other non-zero).  Therefore\nz' must be in standard time, and is the spelling we want in this case.\n\nNote again that z' is not UTC-equivalent as far as the hybrid tzinfo class is\nconcerned (because it takes z' as being in standard time rather than the\ndaylight time we intend here), but returning it gives the real-life \"local\nclock repeats an hour\" behavior when mapping the \"unspellable\" UTC hour into\ntz.\n\nWhen the input is 6:MM, z=1:MM and z.d=0, and we stop at once, again with\nthe 1:MM standard time spelling we want.\n\nSo how can this break?  One of the assumptions must be violated.  Two\npossibilities:\n\n1) [2] effectively says that y.s is invariant across all y belong to a given\n   time zone.  This isn't true if, for political reasons or continental drift,\n   a region decides to change its base offset from UTC.\n\n2) There may be versions of \"double daylight\" time where the tail end of\n   the analysis gives up a step too early.  I haven't thought about that\n   enough to say.\n\nIn any case, it's clear that the default fromutc() is strong enough to handle\n\"almost all\" time zones:  so long as the standard offset is invariant, it\ndoesn't matter if daylight time transition points change from year to year, or\nif daylight time is skipped in some years; it doesn't matter how large or\nsmall dst() may get within its bounds; and it doesn't even matter if some\nperverse time zone returns a negative dst()).  So a breaking case must be\npretty bizarre, and a tzinfo subclass can override fromutc() if it is.\n\"\"\"\n", 
    "decimal": "# Copyright (c) 2004 Python Software Foundation.\n# All rights reserved.\n\n# Written by Eric Price <eprice at tjhsst.edu>\n#    and Facundo Batista <facundo at taniquetil.com.ar>\n#    and Raymond Hettinger <python at rcn.com>\n#    and Aahz <aahz at pobox.com>\n#    and Tim Peters\n\n# This module is currently Py2.3 compatible and should be kept that way\n# unless a major compelling advantage arises.  IOW, 2.3 compatibility is\n# strongly preferred, but not guaranteed.\n\n# Also, this module should be kept in sync with the latest updates of\n# the IBM specification as it evolves.  Those updates will be treated\n# as bug fixes (deviation from the spec is a compatibility, usability\n# bug) and will be backported.  At this point the spec is stabilizing\n# and the updates are becoming fewer, smaller, and less significant.\n\n\"\"\"\nThis is a Py2.3 implementation of decimal floating point arithmetic based on\nthe General Decimal Arithmetic Specification:\n\n    http://speleotrove.com/decimal/decarith.html\n\nand IEEE standard 854-1987:\n\n    http://en.wikipedia.org/wiki/IEEE_854-1987\n\nDecimal floating point has finite precision with arbitrarily large bounds.\n\nThe purpose of this module is to support arithmetic using familiar\n\"schoolhouse\" rules and to avoid some of the tricky representation\nissues associated with binary floating point.  The package is especially\nuseful for financial applications or for contexts where users have\nexpectations that are at odds with binary floating point (for instance,\nin binary floating point, 1.00 % 0.1 gives 0.09999999999999995 instead\nof the expected Decimal('0.00') returned by decimal floating point).\n\nHere are some examples of using the decimal module:\n\n>>> from decimal import *\n>>> setcontext(ExtendedContext)\n>>> Decimal(0)\nDecimal('0')\n>>> Decimal('1')\nDecimal('1')\n>>> Decimal('-.0123')\nDecimal('-0.0123')\n>>> Decimal(123456)\nDecimal('123456')\n>>> Decimal('123.45e12345678901234567890')\nDecimal('1.2345E+12345678901234567892')\n>>> Decimal('1.33') + Decimal('1.27')\nDecimal('2.60')\n>>> Decimal('12.34') + Decimal('3.87') - Decimal('18.41')\nDecimal('-2.20')\n>>> dig = Decimal(1)\n>>> print dig / Decimal(3)\n0.333333333\n>>> getcontext().prec = 18\n>>> print dig / Decimal(3)\n0.333333333333333333\n>>> print dig.sqrt()\n1\n>>> print Decimal(3).sqrt()\n1.73205080756887729\n>>> print Decimal(3) ** 123\n4.85192780976896427E+58\n>>> inf = Decimal(1) / Decimal(0)\n>>> print inf\nInfinity\n>>> neginf = Decimal(-1) / Decimal(0)\n>>> print neginf\n-Infinity\n>>> print neginf + inf\nNaN\n>>> print neginf * inf\n-Infinity\n>>> print dig / 0\nInfinity\n>>> getcontext().traps[DivisionByZero] = 1\n>>> print dig / 0\nTraceback (most recent call last):\n  ...\n  ...\n  ...\nDivisionByZero: x / 0\n>>> c = Context()\n>>> c.traps[InvalidOperation] = 0\n>>> print c.flags[InvalidOperation]\n0\n>>> c.divide(Decimal(0), Decimal(0))\nDecimal('NaN')\n>>> c.traps[InvalidOperation] = 1\n>>> print c.flags[InvalidOperation]\n1\n>>> c.flags[InvalidOperation] = 0\n>>> print c.flags[InvalidOperation]\n0\n>>> print c.divide(Decimal(0), Decimal(0))\nTraceback (most recent call last):\n  ...\n  ...\n  ...\nInvalidOperation: 0 / 0\n>>> print c.flags[InvalidOperation]\n1\n>>> c.flags[InvalidOperation] = 0\n>>> c.traps[InvalidOperation] = 0\n>>> print c.divide(Decimal(0), Decimal(0))\nNaN\n>>> print c.flags[InvalidOperation]\n1\n>>>\n\"\"\"\n\n__all__ = [\n    # Two major classes\n    'Decimal', 'Context',\n\n    # Contexts\n    'DefaultContext', 'BasicContext', 'ExtendedContext',\n\n    # Exceptions\n    'DecimalException', 'Clamped', 'InvalidOperation', 'DivisionByZero',\n    'Inexact', 'Rounded', 'Subnormal', 'Overflow', 'Underflow',\n\n    # Constants for use in setting up contexts\n    'ROUND_DOWN', 'ROUND_HALF_UP', 'ROUND_HALF_EVEN', 'ROUND_CEILING',\n    'ROUND_FLOOR', 'ROUND_UP', 'ROUND_HALF_DOWN', 'ROUND_05UP',\n\n    # Functions for manipulating contexts\n    'setcontext', 'getcontext', 'localcontext'\n]\n\n__version__ = '1.70'    # Highest version of the spec this complies with\n\nimport copy as _copy\nimport math as _math\nimport numbers as _numbers\n\ntry:\n    from collections import namedtuple as _namedtuple\n    DecimalTuple = _namedtuple('DecimalTuple', 'sign digits exponent')\nexcept ImportError:\n    DecimalTuple = lambda *args: args\n\n# Rounding\nROUND_DOWN = 'ROUND_DOWN'\nROUND_HALF_UP = 'ROUND_HALF_UP'\nROUND_HALF_EVEN = 'ROUND_HALF_EVEN'\nROUND_CEILING = 'ROUND_CEILING'\nROUND_FLOOR = 'ROUND_FLOOR'\nROUND_UP = 'ROUND_UP'\nROUND_HALF_DOWN = 'ROUND_HALF_DOWN'\nROUND_05UP = 'ROUND_05UP'\n\n# Errors\n\nclass DecimalException(ArithmeticError):\n    \"\"\"Base exception class.\n\n    Used exceptions derive from this.\n    If an exception derives from another exception besides this (such as\n    Underflow (Inexact, Rounded, Subnormal) that indicates that it is only\n    called if the others are present.  This isn't actually used for\n    anything, though.\n\n    handle  -- Called when context._raise_error is called and the\n               trap_enabler is not set.  First argument is self, second is the\n               context.  More arguments can be given, those being after\n               the explanation in _raise_error (For example,\n               context._raise_error(NewError, '(-x)!', self._sign) would\n               call NewError().handle(context, self._sign).)\n\n    To define a new exception, it should be sufficient to have it derive\n    from DecimalException.\n    \"\"\"\n    def handle(self, context, *args):\n        pass\n\n\nclass Clamped(DecimalException):\n    \"\"\"Exponent of a 0 changed to fit bounds.\n\n    This occurs and signals clamped if the exponent of a result has been\n    altered in order to fit the constraints of a specific concrete\n    representation.  This may occur when the exponent of a zero result would\n    be outside the bounds of a representation, or when a large normal\n    number would have an encoded exponent that cannot be represented.  In\n    this latter case, the exponent is reduced to fit and the corresponding\n    number of zero digits are appended to the coefficient (\"fold-down\").\n    \"\"\"\n\nclass InvalidOperation(DecimalException):\n    \"\"\"An invalid operation was performed.\n\n    Various bad things cause this:\n\n    Something creates a signaling NaN\n    -INF + INF\n    0 * (+-)INF\n    (+-)INF / (+-)INF\n    x % 0\n    (+-)INF % x\n    x._rescale( non-integer )\n    sqrt(-x) , x > 0\n    0 ** 0\n    x ** (non-integer)\n    x ** (+-)INF\n    An operand is invalid\n\n    The result of the operation after these is a quiet positive NaN,\n    except when the cause is a signaling NaN, in which case the result is\n    also a quiet NaN, but with the original sign, and an optional\n    diagnostic information.\n    \"\"\"\n    def handle(self, context, *args):\n        if args:\n            ans = _dec_from_triple(args[0]._sign, args[0]._int, 'n', True)\n            return ans._fix_nan(context)\n        return _NaN\n\nclass ConversionSyntax(InvalidOperation):\n    \"\"\"Trying to convert badly formed string.\n\n    This occurs and signals invalid-operation if an string is being\n    converted to a number and it does not conform to the numeric string\n    syntax.  The result is [0,qNaN].\n    \"\"\"\n    def handle(self, context, *args):\n        return _NaN\n\nclass DivisionByZero(DecimalException, ZeroDivisionError):\n    \"\"\"Division by 0.\n\n    This occurs and signals division-by-zero if division of a finite number\n    by zero was attempted (during a divide-integer or divide operation, or a\n    power operation with negative right-hand operand), and the dividend was\n    not zero.\n\n    The result of the operation is [sign,inf], where sign is the exclusive\n    or of the signs of the operands for divide, or is 1 for an odd power of\n    -0, for power.\n    \"\"\"\n\n    def handle(self, context, sign, *args):\n        return _SignedInfinity[sign]\n\nclass DivisionImpossible(InvalidOperation):\n    \"\"\"Cannot perform the division adequately.\n\n    This occurs and signals invalid-operation if the integer result of a\n    divide-integer or remainder operation had too many digits (would be\n    longer than precision).  The result is [0,qNaN].\n    \"\"\"\n\n    def handle(self, context, *args):\n        return _NaN\n\nclass DivisionUndefined(InvalidOperation, ZeroDivisionError):\n    \"\"\"Undefined result of division.\n\n    This occurs and signals invalid-operation if division by zero was\n    attempted (during a divide-integer, divide, or remainder operation), and\n    the dividend is also zero.  The result is [0,qNaN].\n    \"\"\"\n\n    def handle(self, context, *args):\n        return _NaN\n\nclass Inexact(DecimalException):\n    \"\"\"Had to round, losing information.\n\n    This occurs and signals inexact whenever the result of an operation is\n    not exact (that is, it needed to be rounded and any discarded digits\n    were non-zero), or if an overflow or underflow condition occurs.  The\n    result in all cases is unchanged.\n\n    The inexact signal may be tested (or trapped) to determine if a given\n    operation (or sequence of operations) was inexact.\n    \"\"\"\n\nclass InvalidContext(InvalidOperation):\n    \"\"\"Invalid context.  Unknown rounding, for example.\n\n    This occurs and signals invalid-operation if an invalid context was\n    detected during an operation.  This can occur if contexts are not checked\n    on creation and either the precision exceeds the capability of the\n    underlying concrete representation or an unknown or unsupported rounding\n    was specified.  These aspects of the context need only be checked when\n    the values are required to be used.  The result is [0,qNaN].\n    \"\"\"\n\n    def handle(self, context, *args):\n        return _NaN\n\nclass Rounded(DecimalException):\n    \"\"\"Number got rounded (not  necessarily changed during rounding).\n\n    This occurs and signals rounded whenever the result of an operation is\n    rounded (that is, some zero or non-zero digits were discarded from the\n    coefficient), or if an overflow or underflow condition occurs.  The\n    result in all cases is unchanged.\n\n    The rounded signal may be tested (or trapped) to determine if a given\n    operation (or sequence of operations) caused a loss of precision.\n    \"\"\"\n\nclass Subnormal(DecimalException):\n    \"\"\"Exponent < Emin before rounding.\n\n    This occurs and signals subnormal whenever the result of a conversion or\n    operation is subnormal (that is, its adjusted exponent is less than\n    Emin, before any rounding).  The result in all cases is unchanged.\n\n    The subnormal signal may be tested (or trapped) to determine if a given\n    or operation (or sequence of operations) yielded a subnormal result.\n    \"\"\"\n\nclass Overflow(Inexact, Rounded):\n    \"\"\"Numerical overflow.\n\n    This occurs and signals overflow if the adjusted exponent of a result\n    (from a conversion or from an operation that is not an attempt to divide\n    by zero), after rounding, would be greater than the largest value that\n    can be handled by the implementation (the value Emax).\n\n    The result depends on the rounding mode:\n\n    For round-half-up and round-half-even (and for round-half-down and\n    round-up, if implemented), the result of the operation is [sign,inf],\n    where sign is the sign of the intermediate result.  For round-down, the\n    result is the largest finite number that can be represented in the\n    current precision, with the sign of the intermediate result.  For\n    round-ceiling, the result is the same as for round-down if the sign of\n    the intermediate result is 1, or is [0,inf] otherwise.  For round-floor,\n    the result is the same as for round-down if the sign of the intermediate\n    result is 0, or is [1,inf] otherwise.  In all cases, Inexact and Rounded\n    will also be raised.\n    \"\"\"\n\n    def handle(self, context, sign, *args):\n        if context.rounding in (ROUND_HALF_UP, ROUND_HALF_EVEN,\n                                ROUND_HALF_DOWN, ROUND_UP):\n            return _SignedInfinity[sign]\n        if sign == 0:\n            if context.rounding == ROUND_CEILING:\n                return _SignedInfinity[sign]\n            return _dec_from_triple(sign, '9'*context.prec,\n                            context.Emax-context.prec+1)\n        if sign == 1:\n            if context.rounding == ROUND_FLOOR:\n                return _SignedInfinity[sign]\n            return _dec_from_triple(sign, '9'*context.prec,\n                             context.Emax-context.prec+1)\n\n\nclass Underflow(Inexact, Rounded, Subnormal):\n    \"\"\"Numerical underflow with result rounded to 0.\n\n    This occurs and signals underflow if a result is inexact and the\n    adjusted exponent of the result would be smaller (more negative) than\n    the smallest value that can be handled by the implementation (the value\n    Emin).  That is, the result is both inexact and subnormal.\n\n    The result after an underflow will be a subnormal number rounded, if\n    necessary, so that its exponent is not less than Etiny.  This may result\n    in 0 with the sign of the intermediate result and an exponent of Etiny.\n\n    In all cases, Inexact, Rounded, and Subnormal will also be raised.\n    \"\"\"\n\n# List of public traps and flags\n_signals = [Clamped, DivisionByZero, Inexact, Overflow, Rounded,\n           Underflow, InvalidOperation, Subnormal]\n\n# Map conditions (per the spec) to signals\n_condition_map = {ConversionSyntax:InvalidOperation,\n                  DivisionImpossible:InvalidOperation,\n                  DivisionUndefined:InvalidOperation,\n                  InvalidContext:InvalidOperation}\n\n##### Context Functions ##################################################\n\n# The getcontext() and setcontext() function manage access to a thread-local\n# current context.  Py2.4 offers direct support for thread locals.  If that\n# is not available, use threading.currentThread() which is slower but will\n# work for older Pythons.  If threads are not part of the build, create a\n# mock threading object with threading.local() returning the module namespace.\n\ntry:\n    import threading\nexcept ImportError:\n    # Python was compiled without threads; create a mock object instead\n    import sys\n    class MockThreading(object):\n        def local(self, sys=sys):\n            return sys.modules[__name__]\n    threading = MockThreading()\n    del sys, MockThreading\n\ntry:\n    threading.local\n\nexcept AttributeError:\n\n    # To fix reloading, force it to create a new context\n    # Old contexts have different exceptions in their dicts, making problems.\n    if hasattr(threading.currentThread(), '__decimal_context__'):\n        del threading.currentThread().__decimal_context__\n\n    def setcontext(context):\n        \"\"\"Set this thread's context to context.\"\"\"\n        if context in (DefaultContext, BasicContext, ExtendedContext):\n            context = context.copy()\n            context.clear_flags()\n        threading.currentThread().__decimal_context__ = context\n\n    def getcontext():\n        \"\"\"Returns this thread's context.\n\n        If this thread does not yet have a context, returns\n        a new context and sets this thread's context.\n        New contexts are copies of DefaultContext.\n        \"\"\"\n        try:\n            return threading.currentThread().__decimal_context__\n        except AttributeError:\n            context = Context()\n            threading.currentThread().__decimal_context__ = context\n            return context\n\nelse:\n\n    local = threading.local()\n    if hasattr(local, '__decimal_context__'):\n        del local.__decimal_context__\n\n    def getcontext(_local=local):\n        \"\"\"Returns this thread's context.\n\n        If this thread does not yet have a context, returns\n        a new context and sets this thread's context.\n        New contexts are copies of DefaultContext.\n        \"\"\"\n        try:\n            return _local.__decimal_context__\n        except AttributeError:\n            context = Context()\n            _local.__decimal_context__ = context\n            return context\n\n    def setcontext(context, _local=local):\n        \"\"\"Set this thread's context to context.\"\"\"\n        if context in (DefaultContext, BasicContext, ExtendedContext):\n            context = context.copy()\n            context.clear_flags()\n        _local.__decimal_context__ = context\n\n    del threading, local        # Don't contaminate the namespace\n\ndef localcontext(ctx=None):\n    \"\"\"Return a context manager for a copy of the supplied context\n\n    Uses a copy of the current context if no context is specified\n    The returned context manager creates a local decimal context\n    in a with statement:\n        def sin(x):\n             with localcontext() as ctx:\n                 ctx.prec += 2\n                 # Rest of sin calculation algorithm\n                 # uses a precision 2 greater than normal\n             return +s  # Convert result to normal precision\n\n         def sin(x):\n             with localcontext(ExtendedContext):\n                 # Rest of sin calculation algorithm\n                 # uses the Extended Context from the\n                 # General Decimal Arithmetic Specification\n             return +s  # Convert result to normal context\n\n    >>> setcontext(DefaultContext)\n    >>> print getcontext().prec\n    28\n    >>> with localcontext():\n    ...     ctx = getcontext()\n    ...     ctx.prec += 2\n    ...     print ctx.prec\n    ...\n    30\n    >>> with localcontext(ExtendedContext):\n    ...     print getcontext().prec\n    ...\n    9\n    >>> print getcontext().prec\n    28\n    \"\"\"\n    if ctx is None: ctx = getcontext()\n    return _ContextManager(ctx)\n\n\n##### Decimal class #######################################################\n\nclass Decimal(object):\n    \"\"\"Floating point class for decimal arithmetic.\"\"\"\n\n    __slots__ = ('_exp','_int','_sign', '_is_special')\n    # Generally, the value of the Decimal instance is given by\n    #  (-1)**_sign * _int * 10**_exp\n    # Special values are signified by _is_special == True\n\n    # We're immutable, so use __new__ not __init__\n    def __new__(cls, value=\"0\", context=None):\n        \"\"\"Create a decimal point instance.\n\n        >>> Decimal('3.14')              # string input\n        Decimal('3.14')\n        >>> Decimal((0, (3, 1, 4), -2))  # tuple (sign, digit_tuple, exponent)\n        Decimal('3.14')\n        >>> Decimal(314)                 # int or long\n        Decimal('314')\n        >>> Decimal(Decimal(314))        # another decimal instance\n        Decimal('314')\n        >>> Decimal('  3.14  \\\\n')        # leading and trailing whitespace okay\n        Decimal('3.14')\n        \"\"\"\n\n        # Note that the coefficient, self._int, is actually stored as\n        # a string rather than as a tuple of digits.  This speeds up\n        # the \"digits to integer\" and \"integer to digits\" conversions\n        # that are used in almost every arithmetic operation on\n        # Decimals.  This is an internal detail: the as_tuple function\n        # and the Decimal constructor still deal with tuples of\n        # digits.\n\n        self = object.__new__(cls)\n\n        # From a string\n        # REs insist on real strings, so we can too.\n        if isinstance(value, basestring):\n            m = _parser(value.strip())\n            if m is None:\n                if context is None:\n                    context = getcontext()\n                return context._raise_error(ConversionSyntax,\n                                \"Invalid literal for Decimal: %r\" % value)\n\n            if m.group('sign') == \"-\":\n                self._sign = 1\n            else:\n                self._sign = 0\n            intpart = m.group('int')\n            if intpart is not None:\n                # finite number\n                fracpart = m.group('frac') or ''\n                exp = int(m.group('exp') or '0')\n                self._int = str(int(intpart+fracpart))\n                self._exp = exp - len(fracpart)\n                self._is_special = False\n            else:\n                diag = m.group('diag')\n                if diag is not None:\n                    # NaN\n                    self._int = str(int(diag or '0')).lstrip('0')\n                    if m.group('signal'):\n                        self._exp = 'N'\n                    else:\n                        self._exp = 'n'\n                else:\n                    # infinity\n                    self._int = '0'\n                    self._exp = 'F'\n                self._is_special = True\n            return self\n\n        # From an integer\n        if isinstance(value, (int,long)):\n            if value >= 0:\n                self._sign = 0\n            else:\n                self._sign = 1\n            self._exp = 0\n            self._int = str(abs(value))\n            self._is_special = False\n            return self\n\n        # From another decimal\n        if isinstance(value, Decimal):\n            self._exp  = value._exp\n            self._sign = value._sign\n            self._int  = value._int\n            self._is_special  = value._is_special\n            return self\n\n        # From an internal working value\n        if isinstance(value, _WorkRep):\n            self._sign = value.sign\n            self._int = str(value.int)\n            self._exp = int(value.exp)\n            self._is_special = False\n            return self\n\n        # tuple/list conversion (possibly from as_tuple())\n        if isinstance(value, (list,tuple)):\n            if len(value) != 3:\n                raise ValueError('Invalid tuple size in creation of Decimal '\n                                 'from list or tuple.  The list or tuple '\n                                 'should have exactly three elements.')\n            # process sign.  The isinstance test rejects floats\n            if not (isinstance(value[0], (int, long)) and value[0] in (0,1)):\n                raise ValueError(\"Invalid sign.  The first value in the tuple \"\n                                 \"should be an integer; either 0 for a \"\n                                 \"positive number or 1 for a negative number.\")\n            self._sign = value[0]\n            if value[2] == 'F':\n                # infinity: value[1] is ignored\n                self._int = '0'\n                self._exp = value[2]\n                self._is_special = True\n            else:\n                # process and validate the digits in value[1]\n                digits = []\n                for digit in value[1]:\n                    if isinstance(digit, (int, long)) and 0 <= digit <= 9:\n                        # skip leading zeros\n                        if digits or digit != 0:\n                            digits.append(digit)\n                    else:\n                        raise ValueError(\"The second value in the tuple must \"\n                                         \"be composed of integers in the range \"\n                                         \"0 through 9.\")\n                if value[2] in ('n', 'N'):\n                    # NaN: digits form the diagnostic\n                    self._int = ''.join(map(str, digits))\n                    self._exp = value[2]\n                    self._is_special = True\n                elif isinstance(value[2], (int, long)):\n                    # finite number: digits give the coefficient\n                    self._int = ''.join(map(str, digits or [0]))\n                    self._exp = value[2]\n                    self._is_special = False\n                else:\n                    raise ValueError(\"The third value in the tuple must \"\n                                     \"be an integer, or one of the \"\n                                     \"strings 'F', 'n', 'N'.\")\n            return self\n\n        if isinstance(value, float):\n            value = Decimal.from_float(value)\n            self._exp  = value._exp\n            self._sign = value._sign\n            self._int  = value._int\n            self._is_special  = value._is_special\n            return self\n\n        raise TypeError(\"Cannot convert %r to Decimal\" % value)\n\n    # @classmethod, but @decorator is not valid Python 2.3 syntax, so\n    # don't use it (see notes on Py2.3 compatibility at top of file)\n    def from_float(cls, f):\n        \"\"\"Converts a float to a decimal number, exactly.\n\n        Note that Decimal.from_float(0.1) is not the same as Decimal('0.1').\n        Since 0.1 is not exactly representable in binary floating point, the\n        value is stored as the nearest representable value which is\n        0x1.999999999999ap-4.  The exact equivalent of the value in decimal\n        is 0.1000000000000000055511151231257827021181583404541015625.\n\n        >>> Decimal.from_float(0.1)\n        Decimal('0.1000000000000000055511151231257827021181583404541015625')\n        >>> Decimal.from_float(float('nan'))\n        Decimal('NaN')\n        >>> Decimal.from_float(float('inf'))\n        Decimal('Infinity')\n        >>> Decimal.from_float(-float('inf'))\n        Decimal('-Infinity')\n        >>> Decimal.from_float(-0.0)\n        Decimal('-0')\n\n        \"\"\"\n        if isinstance(f, (int, long)):        # handle integer inputs\n            return cls(f)\n        if _math.isinf(f) or _math.isnan(f):  # raises TypeError if not a float\n            return cls(repr(f))\n        if _math.copysign(1.0, f) == 1.0:\n            sign = 0\n        else:\n            sign = 1\n        n, d = abs(f).as_integer_ratio()\n        k = d.bit_length() - 1\n        result = _dec_from_triple(sign, str(n*5**k), -k)\n        if cls is Decimal:\n            return result\n        else:\n            return cls(result)\n    from_float = classmethod(from_float)\n\n    def _isnan(self):\n        \"\"\"Returns whether the number is not actually one.\n\n        0 if a number\n        1 if NaN\n        2 if sNaN\n        \"\"\"\n        if self._is_special:\n            exp = self._exp\n            if exp == 'n':\n                return 1\n            elif exp == 'N':\n                return 2\n        return 0\n\n    def _isinfinity(self):\n        \"\"\"Returns whether the number is infinite\n\n        0 if finite or not a number\n        1 if +INF\n        -1 if -INF\n        \"\"\"\n        if self._exp == 'F':\n            if self._sign:\n                return -1\n            return 1\n        return 0\n\n    def _check_nans(self, other=None, context=None):\n        \"\"\"Returns whether the number is not actually one.\n\n        if self, other are sNaN, signal\n        if self, other are NaN return nan\n        return 0\n\n        Done before operations.\n        \"\"\"\n\n        self_is_nan = self._isnan()\n        if other is None:\n            other_is_nan = False\n        else:\n            other_is_nan = other._isnan()\n\n        if self_is_nan or other_is_nan:\n            if context is None:\n                context = getcontext()\n\n            if self_is_nan == 2:\n                return context._raise_error(InvalidOperation, 'sNaN',\n                                        self)\n            if other_is_nan == 2:\n                return context._raise_error(InvalidOperation, 'sNaN',\n                                        other)\n            if self_is_nan:\n                return self._fix_nan(context)\n\n            return other._fix_nan(context)\n        return 0\n\n    def _compare_check_nans(self, other, context):\n        \"\"\"Version of _check_nans used for the signaling comparisons\n        compare_signal, __le__, __lt__, __ge__, __gt__.\n\n        Signal InvalidOperation if either self or other is a (quiet\n        or signaling) NaN.  Signaling NaNs take precedence over quiet\n        NaNs.\n\n        Return 0 if neither operand is a NaN.\n\n        \"\"\"\n        if context is None:\n            context = getcontext()\n\n        if self._is_special or other._is_special:\n            if self.is_snan():\n                return context._raise_error(InvalidOperation,\n                                            'comparison involving sNaN',\n                                            self)\n            elif other.is_snan():\n                return context._raise_error(InvalidOperation,\n                                            'comparison involving sNaN',\n                                            other)\n            elif self.is_qnan():\n                return context._raise_error(InvalidOperation,\n                                            'comparison involving NaN',\n                                            self)\n            elif other.is_qnan():\n                return context._raise_error(InvalidOperation,\n                                            'comparison involving NaN',\n                                            other)\n        return 0\n\n    def __nonzero__(self):\n        \"\"\"Return True if self is nonzero; otherwise return False.\n\n        NaNs and infinities are considered nonzero.\n        \"\"\"\n        return self._is_special or self._int != '0'\n\n    def _cmp(self, other):\n        \"\"\"Compare the two non-NaN decimal instances self and other.\n\n        Returns -1 if self < other, 0 if self == other and 1\n        if self > other.  This routine is for internal use only.\"\"\"\n\n        if self._is_special or other._is_special:\n            self_inf = self._isinfinity()\n            other_inf = other._isinfinity()\n            if self_inf == other_inf:\n                return 0\n            elif self_inf < other_inf:\n                return -1\n            else:\n                return 1\n\n        # check for zeros;  Decimal('0') == Decimal('-0')\n        if not self:\n            if not other:\n                return 0\n            else:\n                return -((-1)**other._sign)\n        if not other:\n            return (-1)**self._sign\n\n        # If different signs, neg one is less\n        if other._sign < self._sign:\n            return -1\n        if self._sign < other._sign:\n            return 1\n\n        self_adjusted = self.adjusted()\n        other_adjusted = other.adjusted()\n        if self_adjusted == other_adjusted:\n            self_padded = self._int + '0'*(self._exp - other._exp)\n            other_padded = other._int + '0'*(other._exp - self._exp)\n            if self_padded == other_padded:\n                return 0\n            elif self_padded < other_padded:\n                return -(-1)**self._sign\n            else:\n                return (-1)**self._sign\n        elif self_adjusted > other_adjusted:\n            return (-1)**self._sign\n        else: # self_adjusted < other_adjusted\n            return -((-1)**self._sign)\n\n    # Note: The Decimal standard doesn't cover rich comparisons for\n    # Decimals.  In particular, the specification is silent on the\n    # subject of what should happen for a comparison involving a NaN.\n    # We take the following approach:\n    #\n    #   == comparisons involving a quiet NaN always return False\n    #   != comparisons involving a quiet NaN always return True\n    #   == or != comparisons involving a signaling NaN signal\n    #      InvalidOperation, and return False or True as above if the\n    #      InvalidOperation is not trapped.\n    #   <, >, <= and >= comparisons involving a (quiet or signaling)\n    #      NaN signal InvalidOperation, and return False if the\n    #      InvalidOperation is not trapped.\n    #\n    # This behavior is designed to conform as closely as possible to\n    # that specified by IEEE 754.\n\n    def __eq__(self, other, context=None):\n        other = _convert_other(other, allow_float=True)\n        if other is NotImplemented:\n            return other\n        if self._check_nans(other, context):\n            return False\n        return self._cmp(other) == 0\n\n    def __ne__(self, other, context=None):\n        other = _convert_other(other, allow_float=True)\n        if other is NotImplemented:\n            return other\n        if self._check_nans(other, context):\n            return True\n        return self._cmp(other) != 0\n\n    def __lt__(self, other, context=None):\n        other = _convert_other(other, allow_float=True)\n        if other is NotImplemented:\n            return other\n        ans = self._compare_check_nans(other, context)\n        if ans:\n            return False\n        return self._cmp(other) < 0\n\n    def __le__(self, other, context=None):\n        other = _convert_other(other, allow_float=True)\n        if other is NotImplemented:\n            return other\n        ans = self._compare_check_nans(other, context)\n        if ans:\n            return False\n        return self._cmp(other) <= 0\n\n    def __gt__(self, other, context=None):\n        other = _convert_other(other, allow_float=True)\n        if other is NotImplemented:\n            return other\n        ans = self._compare_check_nans(other, context)\n        if ans:\n            return False\n        return self._cmp(other) > 0\n\n    def __ge__(self, other, context=None):\n        other = _convert_other(other, allow_float=True)\n        if other is NotImplemented:\n            return other\n        ans = self._compare_check_nans(other, context)\n        if ans:\n            return False\n        return self._cmp(other) >= 0\n\n    def compare(self, other, context=None):\n        \"\"\"Compares one to another.\n\n        -1 => a < b\n        0  => a = b\n        1  => a > b\n        NaN => one is NaN\n        Like __cmp__, but returns Decimal instances.\n        \"\"\"\n        other = _convert_other(other, raiseit=True)\n\n        # Compare(NaN, NaN) = NaN\n        if (self._is_special or other and other._is_special):\n            ans = self._check_nans(other, context)\n            if ans:\n                return ans\n\n        return Decimal(self._cmp(other))\n\n    def __hash__(self):\n        \"\"\"x.__hash__() <==> hash(x)\"\"\"\n        # Decimal integers must hash the same as the ints\n        #\n        # The hash of a nonspecial noninteger Decimal must depend only\n        # on the value of that Decimal, and not on its representation.\n        # For example: hash(Decimal('100E-1')) == hash(Decimal('10')).\n\n        # Equality comparisons involving signaling nans can raise an\n        # exception; since equality checks are implicitly and\n        # unpredictably used when checking set and dict membership, we\n        # prevent signaling nans from being used as set elements or\n        # dict keys by making __hash__ raise an exception.\n        if self._is_special:\n            if self.is_snan():\n                raise TypeError('Cannot hash a signaling NaN value.')\n            elif self.is_nan():\n                # 0 to match hash(float('nan'))\n                return 0\n            else:\n                # values chosen to match hash(float('inf')) and\n                # hash(float('-inf')).\n                if self._sign:\n                    return -271828\n                else:\n                    return 314159\n\n        # In Python 2.7, we're allowing comparisons (but not\n        # arithmetic operations) between floats and Decimals;  so if\n        # a Decimal instance is exactly representable as a float then\n        # its hash should match that of the float.\n        self_as_float = float(self)\n        if Decimal.from_float(self_as_float) == self:\n            return hash(self_as_float)\n\n        if self._isinteger():\n            op = _WorkRep(self.to_integral_value())\n            # to make computation feasible for Decimals with large\n            # exponent, we use the fact that hash(n) == hash(m) for\n            # any two nonzero integers n and m such that (i) n and m\n            # have the same sign, and (ii) n is congruent to m modulo\n            # 2**64-1.  So we can replace hash((-1)**s*c*10**e) with\n            # hash((-1)**s*c*pow(10, e, 2**64-1).\n            return hash((-1)**op.sign*op.int*pow(10, op.exp, 2**64-1))\n        # The value of a nonzero nonspecial Decimal instance is\n        # faithfully represented by the triple consisting of its sign,\n        # its adjusted exponent, and its coefficient with trailing\n        # zeros removed.\n        return hash((self._sign,\n                     self._exp+len(self._int),\n                     self._int.rstrip('0')))\n\n    def as_tuple(self):\n        \"\"\"Represents the number as a triple tuple.\n\n        To show the internals exactly as they are.\n        \"\"\"\n        return DecimalTuple(self._sign, tuple(map(int, self._int)), self._exp)\n\n    def __repr__(self):\n        \"\"\"Represents the number as an instance of Decimal.\"\"\"\n        # Invariant:  eval(repr(d)) == d\n        return \"Decimal('%s')\" % str(self)\n\n    def __str__(self, eng=False, context=None):\n        \"\"\"Return string representation of the number in scientific notation.\n\n        Captures all of the information in the underlying representation.\n        \"\"\"\n\n        sign = ['', '-'][self._sign]\n        if self._is_special:\n            if self._exp == 'F':\n                return sign + 'Infinity'\n            elif self._exp == 'n':\n                return sign + 'NaN' + self._int\n            else: # self._exp == 'N'\n                return sign + 'sNaN' + self._int\n\n        # number of digits of self._int to left of decimal point\n        leftdigits = self._exp + len(self._int)\n\n        # dotplace is number of digits of self._int to the left of the\n        # decimal point in the mantissa of the output string (that is,\n        # after adjusting the exponent)\n        if self._exp <= 0 and leftdigits > -6:\n            # no exponent required\n            dotplace = leftdigits\n        elif not eng:\n            # usual scientific notation: 1 digit on left of the point\n            dotplace = 1\n        elif self._int == '0':\n            # engineering notation, zero\n            dotplace = (leftdigits + 1) % 3 - 1\n        else:\n            # engineering notation, nonzero\n            dotplace = (leftdigits - 1) % 3 + 1\n\n        if dotplace <= 0:\n            intpart = '0'\n            fracpart = '.' + '0'*(-dotplace) + self._int\n        elif dotplace >= len(self._int):\n            intpart = self._int+'0'*(dotplace-len(self._int))\n            fracpart = ''\n        else:\n            intpart = self._int[:dotplace]\n            fracpart = '.' + self._int[dotplace:]\n        if leftdigits == dotplace:\n            exp = ''\n        else:\n            if context is None:\n                context = getcontext()\n            exp = ['e', 'E'][context.capitals] + \"%+d\" % (leftdigits-dotplace)\n\n        return sign + intpart + fracpart + exp\n\n    def to_eng_string(self, context=None):\n        \"\"\"Convert to engineering-type string.\n\n        Engineering notation has an exponent which is a multiple of 3, so there\n        are up to 3 digits left of the decimal place.\n\n        Same rules for when in exponential and when as a value as in __str__.\n        \"\"\"\n        return self.__str__(eng=True, context=context)\n\n    def __neg__(self, context=None):\n        \"\"\"Returns a copy with the sign switched.\n\n        Rounds, if it has reason.\n        \"\"\"\n        if self._is_special:\n            ans = self._check_nans(context=context)\n            if ans:\n                return ans\n\n        if context is None:\n            context = getcontext()\n\n        if not self and context.rounding != ROUND_FLOOR:\n            # -Decimal('0') is Decimal('0'), not Decimal('-0'), except\n            # in ROUND_FLOOR rounding mode.\n            ans = self.copy_abs()\n        else:\n            ans = self.copy_negate()\n\n        return ans._fix(context)\n\n    def __pos__(self, context=None):\n        \"\"\"Returns a copy, unless it is a sNaN.\n\n        Rounds the number (if more then precision digits)\n        \"\"\"\n        if self._is_special:\n            ans = self._check_nans(context=context)\n            if ans:\n                return ans\n\n        if context is None:\n            context = getcontext()\n\n        if not self and context.rounding != ROUND_FLOOR:\n            # + (-0) = 0, except in ROUND_FLOOR rounding mode.\n            ans = self.copy_abs()\n        else:\n            ans = Decimal(self)\n\n        return ans._fix(context)\n\n    def __abs__(self, round=True, context=None):\n        \"\"\"Returns the absolute value of self.\n\n        If the keyword argument 'round' is false, do not round.  The\n        expression self.__abs__(round=False) is equivalent to\n        self.copy_abs().\n        \"\"\"\n        if not round:\n            return self.copy_abs()\n\n        if self._is_special:\n            ans = self._check_nans(context=context)\n            if ans:\n                return ans\n\n        if self._sign:\n            ans = self.__neg__(context=context)\n        else:\n            ans = self.__pos__(context=context)\n\n        return ans\n\n    def __add__(self, other, context=None):\n        \"\"\"Returns self + other.\n\n        -INF + INF (or the reverse) cause InvalidOperation errors.\n        \"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n\n        if context is None:\n            context = getcontext()\n\n        if self._is_special or other._is_special:\n            ans = self._check_nans(other, context)\n            if ans:\n                return ans\n\n            if self._isinfinity():\n                # If both INF, same sign => same as both, opposite => error.\n                if self._sign != other._sign and other._isinfinity():\n                    return context._raise_error(InvalidOperation, '-INF + INF')\n                return Decimal(self)\n            if other._isinfinity():\n                return Decimal(other)  # Can't both be infinity here\n\n        exp = min(self._exp, other._exp)\n        negativezero = 0\n        if context.rounding == ROUND_FLOOR and self._sign != other._sign:\n            # If the answer is 0, the sign should be negative, in this case.\n            negativezero = 1\n\n        if not self and not other:\n            sign = min(self._sign, other._sign)\n            if negativezero:\n                sign = 1\n            ans = _dec_from_triple(sign, '0', exp)\n            ans = ans._fix(context)\n            return ans\n        if not self:\n            exp = max(exp, other._exp - context.prec-1)\n            ans = other._rescale(exp, context.rounding)\n            ans = ans._fix(context)\n            return ans\n        if not other:\n            exp = max(exp, self._exp - context.prec-1)\n            ans = self._rescale(exp, context.rounding)\n            ans = ans._fix(context)\n            return ans\n\n        op1 = _WorkRep(self)\n        op2 = _WorkRep(other)\n        op1, op2 = _normalize(op1, op2, context.prec)\n\n        result = _WorkRep()\n        if op1.sign != op2.sign:\n            # Equal and opposite\n            if op1.int == op2.int:\n                ans = _dec_from_triple(negativezero, '0', exp)\n                ans = ans._fix(context)\n                return ans\n            if op1.int < op2.int:\n                op1, op2 = op2, op1\n                # OK, now abs(op1) > abs(op2)\n            if op1.sign == 1:\n                result.sign = 1\n                op1.sign, op2.sign = op2.sign, op1.sign\n            else:\n                result.sign = 0\n                # So we know the sign, and op1 > 0.\n        elif op1.sign == 1:\n            result.sign = 1\n            op1.sign, op2.sign = (0, 0)\n        else:\n            result.sign = 0\n        # Now, op1 > abs(op2) > 0\n\n        if op2.sign == 0:\n            result.int = op1.int + op2.int\n        else:\n            result.int = op1.int - op2.int\n\n        result.exp = op1.exp\n        ans = Decimal(result)\n        ans = ans._fix(context)\n        return ans\n\n    __radd__ = __add__\n\n    def __sub__(self, other, context=None):\n        \"\"\"Return self - other\"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n\n        if self._is_special or other._is_special:\n            ans = self._check_nans(other, context=context)\n            if ans:\n                return ans\n\n        # self - other is computed as self + other.copy_negate()\n        return self.__add__(other.copy_negate(), context=context)\n\n    def __rsub__(self, other, context=None):\n        \"\"\"Return other - self\"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n\n        return other.__sub__(self, context=context)\n\n    def __mul__(self, other, context=None):\n        \"\"\"Return self * other.\n\n        (+-) INF * 0 (or its reverse) raise InvalidOperation.\n        \"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n\n        if context is None:\n            context = getcontext()\n\n        resultsign = self._sign ^ other._sign\n\n        if self._is_special or other._is_special:\n            ans = self._check_nans(other, context)\n            if ans:\n                return ans\n\n            if self._isinfinity():\n                if not other:\n                    return context._raise_error(InvalidOperation, '(+-)INF * 0')\n                return _SignedInfinity[resultsign]\n\n            if other._isinfinity():\n                if not self:\n                    return context._raise_error(InvalidOperation, '0 * (+-)INF')\n                return _SignedInfinity[resultsign]\n\n        resultexp = self._exp + other._exp\n\n        # Special case for multiplying by zero\n        if not self or not other:\n            ans = _dec_from_triple(resultsign, '0', resultexp)\n            # Fixing in case the exponent is out of bounds\n            ans = ans._fix(context)\n            return ans\n\n        # Special case for multiplying by power of 10\n        if self._int == '1':\n            ans = _dec_from_triple(resultsign, other._int, resultexp)\n            ans = ans._fix(context)\n            return ans\n        if other._int == '1':\n            ans = _dec_from_triple(resultsign, self._int, resultexp)\n            ans = ans._fix(context)\n            return ans\n\n        op1 = _WorkRep(self)\n        op2 = _WorkRep(other)\n\n        ans = _dec_from_triple(resultsign, str(op1.int * op2.int), resultexp)\n        ans = ans._fix(context)\n\n        return ans\n    __rmul__ = __mul__\n\n    def __truediv__(self, other, context=None):\n        \"\"\"Return self / other.\"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return NotImplemented\n\n        if context is None:\n            context = getcontext()\n\n        sign = self._sign ^ other._sign\n\n        if self._is_special or other._is_special:\n            ans = self._check_nans(other, context)\n            if ans:\n                return ans\n\n            if self._isinfinity() and other._isinfinity():\n                return context._raise_error(InvalidOperation, '(+-)INF/(+-)INF')\n\n            if self._isinfinity():\n                return _SignedInfinity[sign]\n\n            if other._isinfinity():\n                context._raise_error(Clamped, 'Division by infinity')\n                return _dec_from_triple(sign, '0', context.Etiny())\n\n        # Special cases for zeroes\n        if not other:\n            if not self:\n                return context._raise_error(DivisionUndefined, '0 / 0')\n            return context._raise_error(DivisionByZero, 'x / 0', sign)\n\n        if not self:\n            exp = self._exp - other._exp\n            coeff = 0\n        else:\n            # OK, so neither = 0, INF or NaN\n            shift = len(other._int) - len(self._int) + context.prec + 1\n            exp = self._exp - other._exp - shift\n            op1 = _WorkRep(self)\n            op2 = _WorkRep(other)\n            if shift >= 0:\n                coeff, remainder = divmod(op1.int * 10**shift, op2.int)\n            else:\n                coeff, remainder = divmod(op1.int, op2.int * 10**-shift)\n            if remainder:\n                # result is not exact; adjust to ensure correct rounding\n                if coeff % 5 == 0:\n                    coeff += 1\n            else:\n                # result is exact; get as close to ideal exponent as possible\n                ideal_exp = self._exp - other._exp\n                while exp < ideal_exp and coeff % 10 == 0:\n                    coeff //= 10\n                    exp += 1\n\n        ans = _dec_from_triple(sign, str(coeff), exp)\n        return ans._fix(context)\n\n    def _divide(self, other, context):\n        \"\"\"Return (self // other, self % other), to context.prec precision.\n\n        Assumes that neither self nor other is a NaN, that self is not\n        infinite and that other is nonzero.\n        \"\"\"\n        sign = self._sign ^ other._sign\n        if other._isinfinity():\n            ideal_exp = self._exp\n        else:\n            ideal_exp = min(self._exp, other._exp)\n\n        expdiff = self.adjusted() - other.adjusted()\n        if not self or other._isinfinity() or expdiff <= -2:\n            return (_dec_from_triple(sign, '0', 0),\n                    self._rescale(ideal_exp, context.rounding))\n        if expdiff <= context.prec:\n            op1 = _WorkRep(self)\n            op2 = _WorkRep(other)\n            if op1.exp >= op2.exp:\n                op1.int *= 10**(op1.exp - op2.exp)\n            else:\n                op2.int *= 10**(op2.exp - op1.exp)\n            q, r = divmod(op1.int, op2.int)\n            if q < 10**context.prec:\n                return (_dec_from_triple(sign, str(q), 0),\n                        _dec_from_triple(self._sign, str(r), ideal_exp))\n\n        # Here the quotient is too large to be representable\n        ans = context._raise_error(DivisionImpossible,\n                                   'quotient too large in //, % or divmod')\n        return ans, ans\n\n    def __rtruediv__(self, other, context=None):\n        \"\"\"Swaps self/other and returns __truediv__.\"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n        return other.__truediv__(self, context=context)\n\n    __div__ = __truediv__\n    __rdiv__ = __rtruediv__\n\n    def __divmod__(self, other, context=None):\n        \"\"\"\n        Return (self // other, self % other)\n        \"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n\n        if context is None:\n            context = getcontext()\n\n        ans = self._check_nans(other, context)\n        if ans:\n            return (ans, ans)\n\n        sign = self._sign ^ other._sign\n        if self._isinfinity():\n            if other._isinfinity():\n                ans = context._raise_error(InvalidOperation, 'divmod(INF, INF)')\n                return ans, ans\n            else:\n                return (_SignedInfinity[sign],\n                        context._raise_error(InvalidOperation, 'INF % x'))\n\n        if not other:\n            if not self:\n                ans = context._raise_error(DivisionUndefined, 'divmod(0, 0)')\n                return ans, ans\n            else:\n                return (context._raise_error(DivisionByZero, 'x // 0', sign),\n                        context._raise_error(InvalidOperation, 'x % 0'))\n\n        quotient, remainder = self._divide(other, context)\n        remainder = remainder._fix(context)\n        return quotient, remainder\n\n    def __rdivmod__(self, other, context=None):\n        \"\"\"Swaps self/other and returns __divmod__.\"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n        return other.__divmod__(self, context=context)\n\n    def __mod__(self, other, context=None):\n        \"\"\"\n        self % other\n        \"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n\n        if context is None:\n            context = getcontext()\n\n        ans = self._check_nans(other, context)\n        if ans:\n            return ans\n\n        if self._isinfinity():\n            return context._raise_error(InvalidOperation, 'INF % x')\n        elif not other:\n            if self:\n                return context._raise_error(InvalidOperation, 'x % 0')\n            else:\n                return context._raise_error(DivisionUndefined, '0 % 0')\n\n        remainder = self._divide(other, context)[1]\n        remainder = remainder._fix(context)\n        return remainder\n\n    def __rmod__(self, other, context=None):\n        \"\"\"Swaps self/other and returns __mod__.\"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n        return other.__mod__(self, context=context)\n\n    def remainder_near(self, other, context=None):\n        \"\"\"\n        Remainder nearest to 0-  abs(remainder-near) <= other/2\n        \"\"\"\n        if context is None:\n            context = getcontext()\n\n        other = _convert_other(other, raiseit=True)\n\n        ans = self._check_nans(other, context)\n        if ans:\n            return ans\n\n        # self == +/-infinity -> InvalidOperation\n        if self._isinfinity():\n            return context._raise_error(InvalidOperation,\n                                        'remainder_near(infinity, x)')\n\n        # other == 0 -> either InvalidOperation or DivisionUndefined\n        if not other:\n            if self:\n                return context._raise_error(InvalidOperation,\n                                            'remainder_near(x, 0)')\n            else:\n                return context._raise_error(DivisionUndefined,\n                                            'remainder_near(0, 0)')\n\n        # other = +/-infinity -> remainder = self\n        if other._isinfinity():\n            ans = Decimal(self)\n            return ans._fix(context)\n\n        # self = 0 -> remainder = self, with ideal exponent\n        ideal_exponent = min(self._exp, other._exp)\n        if not self:\n            ans = _dec_from_triple(self._sign, '0', ideal_exponent)\n            return ans._fix(context)\n\n        # catch most cases of large or small quotient\n        expdiff = self.adjusted() - other.adjusted()\n        if expdiff >= context.prec + 1:\n            # expdiff >= prec+1 => abs(self/other) > 10**prec\n            return context._raise_error(DivisionImpossible)\n        if expdiff <= -2:\n            # expdiff <= -2 => abs(self/other) < 0.1\n            ans = self._rescale(ideal_exponent, context.rounding)\n            return ans._fix(context)\n\n        # adjust both arguments to have the same exponent, then divide\n        op1 = _WorkRep(self)\n        op2 = _WorkRep(other)\n        if op1.exp >= op2.exp:\n            op1.int *= 10**(op1.exp - op2.exp)\n        else:\n            op2.int *= 10**(op2.exp - op1.exp)\n        q, r = divmod(op1.int, op2.int)\n        # remainder is r*10**ideal_exponent; other is +/-op2.int *\n        # 10**ideal_exponent.   Apply correction to ensure that\n        # abs(remainder) <= abs(other)/2\n        if 2*r + (q&1) > op2.int:\n            r -= op2.int\n            q += 1\n\n        if q >= 10**context.prec:\n            return context._raise_error(DivisionImpossible)\n\n        # result has same sign as self unless r is negative\n        sign = self._sign\n        if r < 0:\n            sign = 1-sign\n            r = -r\n\n        ans = _dec_from_triple(sign, str(r), ideal_exponent)\n        return ans._fix(context)\n\n    def __floordiv__(self, other, context=None):\n        \"\"\"self // other\"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n\n        if context is None:\n            context = getcontext()\n\n        ans = self._check_nans(other, context)\n        if ans:\n            return ans\n\n        if self._isinfinity():\n            if other._isinfinity():\n                return context._raise_error(InvalidOperation, 'INF // INF')\n            else:\n                return _SignedInfinity[self._sign ^ other._sign]\n\n        if not other:\n            if self:\n                return context._raise_error(DivisionByZero, 'x // 0',\n                                            self._sign ^ other._sign)\n            else:\n                return context._raise_error(DivisionUndefined, '0 // 0')\n\n        return self._divide(other, context)[0]\n\n    def __rfloordiv__(self, other, context=None):\n        \"\"\"Swaps self/other and returns __floordiv__.\"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n        return other.__floordiv__(self, context=context)\n\n    def __float__(self):\n        \"\"\"Float representation.\"\"\"\n        if self._isnan():\n            if self.is_snan():\n                raise ValueError(\"Cannot convert signaling NaN to float\")\n            s = \"-nan\" if self._sign else \"nan\"\n        else:\n            s = str(self)\n        return float(s)\n\n    def __int__(self):\n        \"\"\"Converts self to an int, truncating if necessary.\"\"\"\n        if self._is_special:\n            if self._isnan():\n                raise ValueError(\"Cannot convert NaN to integer\")\n            elif self._isinfinity():\n                raise OverflowError(\"Cannot convert infinity to integer\")\n        s = (-1)**self._sign\n        if self._exp >= 0:\n            return s*int(self._int)*10**self._exp\n        else:\n            return s*int(self._int[:self._exp] or '0')\n\n    __trunc__ = __int__\n\n    def real(self):\n        return self\n    real = property(real)\n\n    def imag(self):\n        return Decimal(0)\n    imag = property(imag)\n\n    def conjugate(self):\n        return self\n\n    def __complex__(self):\n        return complex(float(self))\n\n    def __long__(self):\n        \"\"\"Converts to a long.\n\n        Equivalent to long(int(self))\n        \"\"\"\n        return long(self.__int__())\n\n    def _fix_nan(self, context):\n        \"\"\"Decapitate the payload of a NaN to fit the context\"\"\"\n        payload = self._int\n\n        # maximum length of payload is precision if _clamp=0,\n        # precision-1 if _clamp=1.\n        max_payload_len = context.prec - context._clamp\n        if len(payload) > max_payload_len:\n            payload = payload[len(payload)-max_payload_len:].lstrip('0')\n            return _dec_from_triple(self._sign, payload, self._exp, True)\n        return Decimal(self)\n\n    def _fix(self, context):\n        \"\"\"Round if it is necessary to keep self within prec precision.\n\n        Rounds and fixes the exponent.  Does not raise on a sNaN.\n\n        Arguments:\n        self - Decimal instance\n        context - context used.\n        \"\"\"\n\n        if self._is_special:\n            if self._isnan():\n                # decapitate payload if necessary\n                return self._fix_nan(context)\n            else:\n                # self is +/-Infinity; return unaltered\n                return Decimal(self)\n\n        # if self is zero then exponent should be between Etiny and\n        # Emax if _clamp==0, and between Etiny and Etop if _clamp==1.\n        Etiny = context.Etiny()\n        Etop = context.Etop()\n        if not self:\n            exp_max = [context.Emax, Etop][context._clamp]\n            new_exp = min(max(self._exp, Etiny), exp_max)\n            if new_exp != self._exp:\n                context._raise_error(Clamped)\n                return _dec_from_triple(self._sign, '0', new_exp)\n            else:\n                return Decimal(self)\n\n        # exp_min is the smallest allowable exponent of the result,\n        # equal to max(self.adjusted()-context.prec+1, Etiny)\n        exp_min = len(self._int) + self._exp - context.prec\n        if exp_min > Etop:\n            # overflow: exp_min > Etop iff self.adjusted() > Emax\n            ans = context._raise_error(Overflow, 'above Emax', self._sign)\n            context._raise_error(Inexact)\n            context._raise_error(Rounded)\n            return ans\n\n        self_is_subnormal = exp_min < Etiny\n        if self_is_subnormal:\n            exp_min = Etiny\n\n        # round if self has too many digits\n        if self._exp < exp_min:\n            digits = len(self._int) + self._exp - exp_min\n            if digits < 0:\n                self = _dec_from_triple(self._sign, '1', exp_min-1)\n                digits = 0\n            rounding_method = self._pick_rounding_function[context.rounding]\n            changed = rounding_method(self, digits)\n            coeff = self._int[:digits] or '0'\n            if changed > 0:\n                coeff = str(int(coeff)+1)\n                if len(coeff) > context.prec:\n                    coeff = coeff[:-1]\n                    exp_min += 1\n\n            # check whether the rounding pushed the exponent out of range\n            if exp_min > Etop:\n                ans = context._raise_error(Overflow, 'above Emax', self._sign)\n            else:\n                ans = _dec_from_triple(self._sign, coeff, exp_min)\n\n            # raise the appropriate signals, taking care to respect\n            # the precedence described in the specification\n            if changed and self_is_subnormal:\n                context._raise_error(Underflow)\n            if self_is_subnormal:\n                context._raise_error(Subnormal)\n            if changed:\n                context._raise_error(Inexact)\n            context._raise_error(Rounded)\n            if not ans:\n                # raise Clamped on underflow to 0\n                context._raise_error(Clamped)\n            return ans\n\n        if self_is_subnormal:\n            context._raise_error(Subnormal)\n\n        # fold down if _clamp == 1 and self has too few digits\n        if context._clamp == 1 and self._exp > Etop:\n            context._raise_error(Clamped)\n            self_padded = self._int + '0'*(self._exp - Etop)\n            return _dec_from_triple(self._sign, self_padded, Etop)\n\n        # here self was representable to begin with; return unchanged\n        return Decimal(self)\n\n    # for each of the rounding functions below:\n    #   self is a finite, nonzero Decimal\n    #   prec is an integer satisfying 0 <= prec < len(self._int)\n    #\n    # each function returns either -1, 0, or 1, as follows:\n    #   1 indicates that self should be rounded up (away from zero)\n    #   0 indicates that self should be truncated, and that all the\n    #     digits to be truncated are zeros (so the value is unchanged)\n    #  -1 indicates that there are nonzero digits to be truncated\n\n    def _round_down(self, prec):\n        \"\"\"Also known as round-towards-0, truncate.\"\"\"\n        if _all_zeros(self._int, prec):\n            return 0\n        else:\n            return -1\n\n    def _round_up(self, prec):\n        \"\"\"Rounds away from 0.\"\"\"\n        return -self._round_down(prec)\n\n    def _round_half_up(self, prec):\n        \"\"\"Rounds 5 up (away from 0)\"\"\"\n        if self._int[prec] in '56789':\n            return 1\n        elif _all_zeros(self._int, prec):\n            return 0\n        else:\n            return -1\n\n    def _round_half_down(self, prec):\n        \"\"\"Round 5 down\"\"\"\n        if _exact_half(self._int, prec):\n            return -1\n        else:\n            return self._round_half_up(prec)\n\n    def _round_half_even(self, prec):\n        \"\"\"Round 5 to even, rest to nearest.\"\"\"\n        if _exact_half(self._int, prec) and \\\n                (prec == 0 or self._int[prec-1] in '02468'):\n            return -1\n        else:\n            return self._round_half_up(prec)\n\n    def _round_ceiling(self, prec):\n        \"\"\"Rounds up (not away from 0 if negative.)\"\"\"\n        if self._sign:\n            return self._round_down(prec)\n        else:\n            return -self._round_down(prec)\n\n    def _round_floor(self, prec):\n        \"\"\"Rounds down (not towards 0 if negative)\"\"\"\n        if not self._sign:\n            return self._round_down(prec)\n        else:\n            return -self._round_down(prec)\n\n    def _round_05up(self, prec):\n        \"\"\"Round down unless digit prec-1 is 0 or 5.\"\"\"\n        if prec and self._int[prec-1] not in '05':\n            return self._round_down(prec)\n        else:\n            return -self._round_down(prec)\n\n    _pick_rounding_function = dict(\n        ROUND_DOWN = _round_down,\n        ROUND_UP = _round_up,\n        ROUND_HALF_UP = _round_half_up,\n        ROUND_HALF_DOWN = _round_half_down,\n        ROUND_HALF_EVEN = _round_half_even,\n        ROUND_CEILING = _round_ceiling,\n        ROUND_FLOOR = _round_floor,\n        ROUND_05UP = _round_05up,\n    )\n\n    def fma(self, other, third, context=None):\n        \"\"\"Fused multiply-add.\n\n        Returns self*other+third with no rounding of the intermediate\n        product self*other.\n\n        self and other are multiplied together, with no rounding of\n        the result.  The third operand is then added to the result,\n        and a single final rounding is performed.\n        \"\"\"\n\n        other = _convert_other(other, raiseit=True)\n\n        # compute product; raise InvalidOperation if either operand is\n        # a signaling NaN or if the product is zero times infinity.\n        if self._is_special or other._is_special:\n            if context is None:\n                context = getcontext()\n            if self._exp == 'N':\n                return context._raise_error(InvalidOperation, 'sNaN', self)\n            if other._exp == 'N':\n                return context._raise_error(InvalidOperation, 'sNaN', other)\n            if self._exp == 'n':\n                product = self\n            elif other._exp == 'n':\n                product = other\n            elif self._exp == 'F':\n                if not other:\n                    return context._raise_error(InvalidOperation,\n                                                'INF * 0 in fma')\n                product = _SignedInfinity[self._sign ^ other._sign]\n            elif other._exp == 'F':\n                if not self:\n                    return context._raise_error(InvalidOperation,\n                                                '0 * INF in fma')\n                product = _SignedInfinity[self._sign ^ other._sign]\n        else:\n            product = _dec_from_triple(self._sign ^ other._sign,\n                                       str(int(self._int) * int(other._int)),\n                                       self._exp + other._exp)\n\n        third = _convert_other(third, raiseit=True)\n        return product.__add__(third, context)\n\n    def _power_modulo(self, other, modulo, context=None):\n        \"\"\"Three argument version of __pow__\"\"\"\n\n        # if can't convert other and modulo to Decimal, raise\n        # TypeError; there's no point returning NotImplemented (no\n        # equivalent of __rpow__ for three argument pow)\n        other = _convert_other(other, raiseit=True)\n        modulo = _convert_other(modulo, raiseit=True)\n\n        if context is None:\n            context = getcontext()\n\n        # deal with NaNs: if there are any sNaNs then first one wins,\n        # (i.e. behaviour for NaNs is identical to that of fma)\n        self_is_nan = self._isnan()\n        other_is_nan = other._isnan()\n        modulo_is_nan = modulo._isnan()\n        if self_is_nan or other_is_nan or modulo_is_nan:\n            if self_is_nan == 2:\n                return context._raise_error(InvalidOperation, 'sNaN',\n                                        self)\n            if other_is_nan == 2:\n                return context._raise_error(InvalidOperation, 'sNaN',\n                                        other)\n            if modulo_is_nan == 2:\n                return context._raise_error(InvalidOperation, 'sNaN',\n                                        modulo)\n            if self_is_nan:\n                return self._fix_nan(context)\n            if other_is_nan:\n                return other._fix_nan(context)\n            return modulo._fix_nan(context)\n\n        # check inputs: we apply same restrictions as Python's pow()\n        if not (self._isinteger() and\n                other._isinteger() and\n                modulo._isinteger()):\n            return context._raise_error(InvalidOperation,\n                                        'pow() 3rd argument not allowed '\n                                        'unless all arguments are integers')\n        if other < 0:\n            return context._raise_error(InvalidOperation,\n                                        'pow() 2nd argument cannot be '\n                                        'negative when 3rd argument specified')\n        if not modulo:\n            return context._raise_error(InvalidOperation,\n                                        'pow() 3rd argument cannot be 0')\n\n        # additional restriction for decimal: the modulus must be less\n        # than 10**prec in absolute value\n        if modulo.adjusted() >= context.prec:\n            return context._raise_error(InvalidOperation,\n                                        'insufficient precision: pow() 3rd '\n                                        'argument must not have more than '\n                                        'precision digits')\n\n        # define 0**0 == NaN, for consistency with two-argument pow\n        # (even though it hurts!)\n        if not other and not self:\n            return context._raise_error(InvalidOperation,\n                                        'at least one of pow() 1st argument '\n                                        'and 2nd argument must be nonzero ;'\n                                        '0**0 is not defined')\n\n        # compute sign of result\n        if other._iseven():\n            sign = 0\n        else:\n            sign = self._sign\n\n        # convert modulo to a Python integer, and self and other to\n        # Decimal integers (i.e. force their exponents to be >= 0)\n        modulo = abs(int(modulo))\n        base = _WorkRep(self.to_integral_value())\n        exponent = _WorkRep(other.to_integral_value())\n\n        # compute result using integer pow()\n        base = (base.int % modulo * pow(10, base.exp, modulo)) % modulo\n        for i in xrange(exponent.exp):\n            base = pow(base, 10, modulo)\n        base = pow(base, exponent.int, modulo)\n\n        return _dec_from_triple(sign, str(base), 0)\n\n    def _power_exact(self, other, p):\n        \"\"\"Attempt to compute self**other exactly.\n\n        Given Decimals self and other and an integer p, attempt to\n        compute an exact result for the power self**other, with p\n        digits of precision.  Return None if self**other is not\n        exactly representable in p digits.\n\n        Assumes that elimination of special cases has already been\n        performed: self and other must both be nonspecial; self must\n        be positive and not numerically equal to 1; other must be\n        nonzero.  For efficiency, other._exp should not be too large,\n        so that 10**abs(other._exp) is a feasible calculation.\"\"\"\n\n        # In the comments below, we write x for the value of self and y for the\n        # value of other.  Write x = xc*10**xe and abs(y) = yc*10**ye, with xc\n        # and yc positive integers not divisible by 10.\n\n        # The main purpose of this method is to identify the *failure*\n        # of x**y to be exactly representable with as little effort as\n        # possible.  So we look for cheap and easy tests that\n        # eliminate the possibility of x**y being exact.  Only if all\n        # these tests are passed do we go on to actually compute x**y.\n\n        # Here's the main idea.  Express y as a rational number m/n, with m and\n        # n relatively prime and n>0.  Then for x**y to be exactly\n        # representable (at *any* precision), xc must be the nth power of a\n        # positive integer and xe must be divisible by n.  If y is negative\n        # then additionally xc must be a power of either 2 or 5, hence a power\n        # of 2**n or 5**n.\n        #\n        # There's a limit to how small |y| can be: if y=m/n as above\n        # then:\n        #\n        #  (1) if xc != 1 then for the result to be representable we\n        #      need xc**(1/n) >= 2, and hence also xc**|y| >= 2.  So\n        #      if |y| <= 1/nbits(xc) then xc < 2**nbits(xc) <=\n        #      2**(1/|y|), hence xc**|y| < 2 and the result is not\n        #      representable.\n        #\n        #  (2) if xe != 0, |xe|*(1/n) >= 1, so |xe|*|y| >= 1.  Hence if\n        #      |y| < 1/|xe| then the result is not representable.\n        #\n        # Note that since x is not equal to 1, at least one of (1) and\n        # (2) must apply.  Now |y| < 1/nbits(xc) iff |yc|*nbits(xc) <\n        # 10**-ye iff len(str(|yc|*nbits(xc)) <= -ye.\n        #\n        # There's also a limit to how large y can be, at least if it's\n        # positive: the normalized result will have coefficient xc**y,\n        # so if it's representable then xc**y < 10**p, and y <\n        # p/log10(xc).  Hence if y*log10(xc) >= p then the result is\n        # not exactly representable.\n\n        # if len(str(abs(yc*xe)) <= -ye then abs(yc*xe) < 10**-ye,\n        # so |y| < 1/xe and the result is not representable.\n        # Similarly, len(str(abs(yc)*xc_bits)) <= -ye implies |y|\n        # < 1/nbits(xc).\n\n        x = _WorkRep(self)\n        xc, xe = x.int, x.exp\n        while xc % 10 == 0:\n            xc //= 10\n            xe += 1\n\n        y = _WorkRep(other)\n        yc, ye = y.int, y.exp\n        while yc % 10 == 0:\n            yc //= 10\n            ye += 1\n\n        # case where xc == 1: result is 10**(xe*y), with xe*y\n        # required to be an integer\n        if xc == 1:\n            xe *= yc\n            # result is now 10**(xe * 10**ye);  xe * 10**ye must be integral\n            while xe % 10 == 0:\n                xe //= 10\n                ye += 1\n            if ye < 0:\n                return None\n            exponent = xe * 10**ye\n            if y.sign == 1:\n                exponent = -exponent\n            # if other is a nonnegative integer, use ideal exponent\n            if other._isinteger() and other._sign == 0:\n                ideal_exponent = self._exp*int(other)\n                zeros = min(exponent-ideal_exponent, p-1)\n            else:\n                zeros = 0\n            return _dec_from_triple(0, '1' + '0'*zeros, exponent-zeros)\n\n        # case where y is negative: xc must be either a power\n        # of 2 or a power of 5.\n        if y.sign == 1:\n            last_digit = xc % 10\n            if last_digit in (2,4,6,8):\n                # quick test for power of 2\n                if xc & -xc != xc:\n                    return None\n                # now xc is a power of 2; e is its exponent\n                e = _nbits(xc)-1\n\n                # We now have:\n                #\n                #   x = 2**e * 10**xe, e > 0, and y < 0.\n                #\n                # The exact result is:\n                #\n                #   x**y = 5**(-e*y) * 10**(e*y + xe*y)\n                #\n                # provided that both e*y and xe*y are integers.  Note that if\n                # 5**(-e*y) >= 10**p, then the result can't be expressed\n                # exactly with p digits of precision.\n                #\n                # Using the above, we can guard against large values of ye.\n                # 93/65 is an upper bound for log(10)/log(5), so if\n                #\n                #   ye >= len(str(93*p//65))\n                #\n                # then\n                #\n                #   -e*y >= -y >= 10**ye > 93*p/65 > p*log(10)/log(5),\n                #\n                # so 5**(-e*y) >= 10**p, and the coefficient of the result\n                # can't be expressed in p digits.\n\n                # emax >= largest e such that 5**e < 10**p.\n                emax = p*93//65\n                if ye >= len(str(emax)):\n                    return None\n\n                # Find -e*y and -xe*y; both must be integers\n                e = _decimal_lshift_exact(e * yc, ye)\n                xe = _decimal_lshift_exact(xe * yc, ye)\n                if e is None or xe is None:\n                    return None\n\n                if e > emax:\n                    return None\n                xc = 5**e\n\n            elif last_digit == 5:\n                # e >= log_5(xc) if xc is a power of 5; we have\n                # equality all the way up to xc=5**2658\n                e = _nbits(xc)*28//65\n                xc, remainder = divmod(5**e, xc)\n                if remainder:\n                    return None\n                while xc % 5 == 0:\n                    xc //= 5\n                    e -= 1\n\n                # Guard against large values of ye, using the same logic as in\n                # the 'xc is a power of 2' branch.  10/3 is an upper bound for\n                # log(10)/log(2).\n                emax = p*10//3\n                if ye >= len(str(emax)):\n                    return None\n\n                e = _decimal_lshift_exact(e * yc, ye)\n                xe = _decimal_lshift_exact(xe * yc, ye)\n                if e is None or xe is None:\n                    return None\n\n                if e > emax:\n                    return None\n                xc = 2**e\n            else:\n                return None\n\n            if xc >= 10**p:\n                return None\n            xe = -e-xe\n            return _dec_from_triple(0, str(xc), xe)\n\n        # now y is positive; find m and n such that y = m/n\n        if ye >= 0:\n            m, n = yc*10**ye, 1\n        else:\n            if xe != 0 and len(str(abs(yc*xe))) <= -ye:\n                return None\n            xc_bits = _nbits(xc)\n            if xc != 1 and len(str(abs(yc)*xc_bits)) <= -ye:\n                return None\n            m, n = yc, 10**(-ye)\n            while m % 2 == n % 2 == 0:\n                m //= 2\n                n //= 2\n            while m % 5 == n % 5 == 0:\n                m //= 5\n                n //= 5\n\n        # compute nth root of xc*10**xe\n        if n > 1:\n            # if 1 < xc < 2**n then xc isn't an nth power\n            if xc != 1 and xc_bits <= n:\n                return None\n\n            xe, rem = divmod(xe, n)\n            if rem != 0:\n                return None\n\n            # compute nth root of xc using Newton's method\n            a = 1L << -(-_nbits(xc)//n) # initial estimate\n            while True:\n                q, r = divmod(xc, a**(n-1))\n                if a <= q:\n                    break\n                else:\n                    a = (a*(n-1) + q)//n\n            if not (a == q and r == 0):\n                return None\n            xc = a\n\n        # now xc*10**xe is the nth root of the original xc*10**xe\n        # compute mth power of xc*10**xe\n\n        # if m > p*100//_log10_lb(xc) then m > p/log10(xc), hence xc**m >\n        # 10**p and the result is not representable.\n        if xc > 1 and m > p*100//_log10_lb(xc):\n            return None\n        xc = xc**m\n        xe *= m\n        if xc > 10**p:\n            return None\n\n        # by this point the result *is* exactly representable\n        # adjust the exponent to get as close as possible to the ideal\n        # exponent, if necessary\n        str_xc = str(xc)\n        if other._isinteger() and other._sign == 0:\n            ideal_exponent = self._exp*int(other)\n            zeros = min(xe-ideal_exponent, p-len(str_xc))\n        else:\n            zeros = 0\n        return _dec_from_triple(0, str_xc+'0'*zeros, xe-zeros)\n\n    def __pow__(self, other, modulo=None, context=None):\n        \"\"\"Return self ** other [ % modulo].\n\n        With two arguments, compute self**other.\n\n        With three arguments, compute (self**other) % modulo.  For the\n        three argument form, the following restrictions on the\n        arguments hold:\n\n         - all three arguments must be integral\n         - other must be nonnegative\n         - either self or other (or both) must be nonzero\n         - modulo must be nonzero and must have at most p digits,\n           where p is the context precision.\n\n        If any of these restrictions is violated the InvalidOperation\n        flag is raised.\n\n        The result of pow(self, other, modulo) is identical to the\n        result that would be obtained by computing (self**other) %\n        modulo with unbounded precision, but is computed more\n        efficiently.  It is always exact.\n        \"\"\"\n\n        if modulo is not None:\n            return self._power_modulo(other, modulo, context)\n\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n\n        if context is None:\n            context = getcontext()\n\n        # either argument is a NaN => result is NaN\n        ans = self._check_nans(other, context)\n        if ans:\n            return ans\n\n        # 0**0 = NaN (!), x**0 = 1 for nonzero x (including +/-Infinity)\n        if not other:\n            if not self:\n                return context._raise_error(InvalidOperation, '0 ** 0')\n            else:\n                return _One\n\n        # result has sign 1 iff self._sign is 1 and other is an odd integer\n        result_sign = 0\n        if self._sign == 1:\n            if other._isinteger():\n                if not other._iseven():\n                    result_sign = 1\n            else:\n                # -ve**noninteger = NaN\n                # (-0)**noninteger = 0**noninteger\n                if self:\n                    return context._raise_error(InvalidOperation,\n                        'x ** y with x negative and y not an integer')\n            # negate self, without doing any unwanted rounding\n            self = self.copy_negate()\n\n        # 0**(+ve or Inf)= 0; 0**(-ve or -Inf) = Infinity\n        if not self:\n            if other._sign == 0:\n                return _dec_from_triple(result_sign, '0', 0)\n            else:\n                return _SignedInfinity[result_sign]\n\n        # Inf**(+ve or Inf) = Inf; Inf**(-ve or -Inf) = 0\n        if self._isinfinity():\n            if other._sign == 0:\n                return _SignedInfinity[result_sign]\n            else:\n                return _dec_from_triple(result_sign, '0', 0)\n\n        # 1**other = 1, but the choice of exponent and the flags\n        # depend on the exponent of self, and on whether other is a\n        # positive integer, a negative integer, or neither\n        if self == _One:\n            if other._isinteger():\n                # exp = max(self._exp*max(int(other), 0),\n                # 1-context.prec) but evaluating int(other) directly\n                # is dangerous until we know other is small (other\n                # could be 1e999999999)\n                if other._sign == 1:\n                    multiplier = 0\n                elif other > context.prec:\n                    multiplier = context.prec\n                else:\n                    multiplier = int(other)\n\n                exp = self._exp * multiplier\n                if exp < 1-context.prec:\n                    exp = 1-context.prec\n                    context._raise_error(Rounded)\n            else:\n                context._raise_error(Inexact)\n                context._raise_error(Rounded)\n                exp = 1-context.prec\n\n            return _dec_from_triple(result_sign, '1'+'0'*-exp, exp)\n\n        # compute adjusted exponent of self\n        self_adj = self.adjusted()\n\n        # self ** infinity is infinity if self > 1, 0 if self < 1\n        # self ** -infinity is infinity if self < 1, 0 if self > 1\n        if other._isinfinity():\n            if (other._sign == 0) == (self_adj < 0):\n                return _dec_from_triple(result_sign, '0', 0)\n            else:\n                return _SignedInfinity[result_sign]\n\n        # from here on, the result always goes through the call\n        # to _fix at the end of this function.\n        ans = None\n        exact = False\n\n        # crude test to catch cases of extreme overflow/underflow.  If\n        # log10(self)*other >= 10**bound and bound >= len(str(Emax))\n        # then 10**bound >= 10**len(str(Emax)) >= Emax+1 and hence\n        # self**other >= 10**(Emax+1), so overflow occurs.  The test\n        # for underflow is similar.\n        bound = self._log10_exp_bound() + other.adjusted()\n        if (self_adj >= 0) == (other._sign == 0):\n            # self > 1 and other +ve, or self < 1 and other -ve\n            # possibility of overflow\n            if bound >= len(str(context.Emax)):\n                ans = _dec_from_triple(result_sign, '1', context.Emax+1)\n        else:\n            # self > 1 and other -ve, or self < 1 and other +ve\n            # possibility of underflow to 0\n            Etiny = context.Etiny()\n            if bound >= len(str(-Etiny)):\n                ans = _dec_from_triple(result_sign, '1', Etiny-1)\n\n        # try for an exact result with precision +1\n        if ans is None:\n            ans = self._power_exact(other, context.prec + 1)\n            if ans is not None:\n                if result_sign == 1:\n                    ans = _dec_from_triple(1, ans._int, ans._exp)\n                exact = True\n\n        # usual case: inexact result, x**y computed directly as exp(y*log(x))\n        if ans is None:\n            p = context.prec\n            x = _WorkRep(self)\n            xc, xe = x.int, x.exp\n            y = _WorkRep(other)\n            yc, ye = y.int, y.exp\n            if y.sign == 1:\n                yc = -yc\n\n            # compute correctly rounded result:  start with precision +3,\n            # then increase precision until result is unambiguously roundable\n            extra = 3\n            while True:\n                coeff, exp = _dpower(xc, xe, yc, ye, p+extra)\n                if coeff % (5*10**(len(str(coeff))-p-1)):\n                    break\n                extra += 3\n\n            ans = _dec_from_triple(result_sign, str(coeff), exp)\n\n        # unlike exp, ln and log10, the power function respects the\n        # rounding mode; no need to switch to ROUND_HALF_EVEN here\n\n        # There's a difficulty here when 'other' is not an integer and\n        # the result is exact.  In this case, the specification\n        # requires that the Inexact flag be raised (in spite of\n        # exactness), but since the result is exact _fix won't do this\n        # for us.  (Correspondingly, the Underflow signal should also\n        # be raised for subnormal results.)  We can't directly raise\n        # these signals either before or after calling _fix, since\n        # that would violate the precedence for signals.  So we wrap\n        # the ._fix call in a temporary context, and reraise\n        # afterwards.\n        if exact and not other._isinteger():\n            # pad with zeros up to length context.prec+1 if necessary; this\n            # ensures that the Rounded signal will be raised.\n            if len(ans._int) <= context.prec:\n                expdiff = context.prec + 1 - len(ans._int)\n                ans = _dec_from_triple(ans._sign, ans._int+'0'*expdiff,\n                                       ans._exp-expdiff)\n\n            # create a copy of the current context, with cleared flags/traps\n            newcontext = context.copy()\n            newcontext.clear_flags()\n            for exception in _signals:\n                newcontext.traps[exception] = 0\n\n            # round in the new context\n            ans = ans._fix(newcontext)\n\n            # raise Inexact, and if necessary, Underflow\n            newcontext._raise_error(Inexact)\n            if newcontext.flags[Subnormal]:\n                newcontext._raise_error(Underflow)\n\n            # propagate signals to the original context; _fix could\n            # have raised any of Overflow, Underflow, Subnormal,\n            # Inexact, Rounded, Clamped.  Overflow needs the correct\n            # arguments.  Note that the order of the exceptions is\n            # important here.\n            if newcontext.flags[Overflow]:\n                context._raise_error(Overflow, 'above Emax', ans._sign)\n            for exception in Underflow, Subnormal, Inexact, Rounded, Clamped:\n                if newcontext.flags[exception]:\n                    context._raise_error(exception)\n\n        else:\n            ans = ans._fix(context)\n\n        return ans\n\n    def __rpow__(self, other, context=None):\n        \"\"\"Swaps self/other and returns __pow__.\"\"\"\n        other = _convert_other(other)\n        if other is NotImplemented:\n            return other\n        return other.__pow__(self, context=context)\n\n    def normalize(self, context=None):\n        \"\"\"Normalize- strip trailing 0s, change anything equal to 0 to 0e0\"\"\"\n\n        if context is None:\n            context = getcontext()\n\n        if self._is_special:\n            ans = self._check_nans(context=context)\n            if ans:\n                return ans\n\n        dup = self._fix(context)\n        if dup._isinfinity():\n            return dup\n\n        if not dup:\n            return _dec_from_triple(dup._sign, '0', 0)\n        exp_max = [context.Emax, context.Etop()][context._clamp]\n        end = len(dup._int)\n        exp = dup._exp\n        while dup._int[end-1] == '0' and exp < exp_max:\n            exp += 1\n            end -= 1\n        return _dec_from_triple(dup._sign, dup._int[:end], exp)\n\n    def quantize(self, exp, rounding=None, context=None, watchexp=True):\n        \"\"\"Quantize self so its exponent is the same as that of exp.\n\n        Similar to self._rescale(exp._exp) but with error checking.\n        \"\"\"\n        exp = _convert_other(exp, raiseit=True)\n\n        if context is None:\n            context = getcontext()\n        if rounding is None:\n            rounding = context.rounding\n\n        if self._is_special or exp._is_special:\n            ans = self._check_nans(exp, context)\n            if ans:\n                return ans\n\n            if exp._isinfinity() or self._isinfinity():\n                if exp._isinfinity() and self._isinfinity():\n                    return Decimal(self)  # if both are inf, it is OK\n                return context._raise_error(InvalidOperation,\n                                        'quantize with one INF')\n\n        # if we're not watching exponents, do a simple rescale\n        if not watchexp:\n            ans = self._rescale(exp._exp, rounding)\n            # raise Inexact and Rounded where appropriate\n            if ans._exp > self._exp:\n                context._raise_error(Rounded)\n                if ans != self:\n                    context._raise_error(Inexact)\n            return ans\n\n        # exp._exp should be between Etiny and Emax\n        if not (context.Etiny() <= exp._exp <= context.Emax):\n            return context._raise_error(InvalidOperation,\n                   'target exponent out of bounds in quantize')\n\n        if not self:\n            ans = _dec_from_triple(self._sign, '0', exp._exp)\n            return ans._fix(context)\n\n        self_adjusted = self.adjusted()\n        if self_adjusted > context.Emax:\n            return context._raise_error(InvalidOperation,\n                                        'exponent of quantize result too large for current context')\n        if self_adjusted - exp._exp + 1 > context.prec:\n            return context._raise_error(InvalidOperation,\n                                        'quantize result has too many digits for current context')\n\n        ans = self._rescale(exp._exp, rounding)\n        if ans.adjusted() > context.Emax:\n            return context._raise_error(InvalidOperation,\n                                        'exponent of quantize result too large for current context')\n        if len(ans._int) > context.prec:\n            return context._raise_error(InvalidOperation,\n                                        'quantize result has too many digits for current context')\n\n        # raise appropriate flags\n        if ans and ans.adjusted() < context.Emin:\n            context._raise_error(Subnormal)\n        if ans._exp > self._exp:\n            if ans != self:\n                context._raise_error(Inexact)\n            context._raise_error(Rounded)\n\n        # call to fix takes care of any necessary folddown, and\n        # signals Clamped if necessary\n        ans = ans._fix(context)\n        return ans\n\n    def same_quantum(self, other):\n        \"\"\"Return True if self and other have the same exponent; otherwise\n        return False.\n\n        If either operand is a special value, the following rules are used:\n           * return True if both operands are infinities\n           * return True if both operands are NaNs\n           * otherwise, return False.\n        \"\"\"\n        other = _convert_other(other, raiseit=True)\n        if self._is_special or other._is_special:\n            return (self.is_nan() and other.is_nan() or\n                    self.is_infinite() and other.is_infinite())\n        return self._exp == other._exp\n\n    def _rescale(self, exp, rounding):\n        \"\"\"Rescale self so that the exponent is exp, either by padding with zeros\n        or by truncating digits, using the given rounding mode.\n\n        Specials are returned without change.  This operation is\n        quiet: it raises no flags, and uses no information from the\n        context.\n\n        exp = exp to scale to (an integer)\n        rounding = rounding mode\n        \"\"\"\n        if self._is_special:\n            return Decimal(self)\n        if not self:\n            return _dec_from_triple(self._sign, '0', exp)\n\n        if self._exp >= exp:\n            # pad answer with zeros if necessary\n            return _dec_from_triple(self._sign,\n                                        self._int + '0'*(self._exp - exp), exp)\n\n        # too many digits; round and lose data.  If self.adjusted() <\n        # exp-1, replace self by 10**(exp-1) before rounding\n        digits = len(self._int) + self._exp - exp\n        if digits < 0:\n            self = _dec_from_triple(self._sign, '1', exp-1)\n            digits = 0\n        this_function = self._pick_rounding_function[rounding]\n        changed = this_function(self, digits)\n        coeff = self._int[:digits] or '0'\n        if changed == 1:\n            coeff = str(int(coeff)+1)\n        return _dec_from_triple(self._sign, coeff, exp)\n\n    def _round(self, places, rounding):\n        \"\"\"Round a nonzero, nonspecial Decimal to a fixed number of\n        significant figures, using the given rounding mode.\n\n        Infinities, NaNs and zeros are returned unaltered.\n\n        This operation is quiet: it raises no flags, and uses no\n        information from the context.\n\n        \"\"\"\n        if places <= 0:\n            raise ValueError(\"argument should be at least 1 in _round\")\n        if self._is_special or not self:\n            return Decimal(self)\n        ans = self._rescale(self.adjusted()+1-places, rounding)\n        # it can happen that the rescale alters the adjusted exponent;\n        # for example when rounding 99.97 to 3 significant figures.\n        # When this happens we end up with an extra 0 at the end of\n        # the number; a second rescale fixes this.\n        if ans.adjusted() != self.adjusted():\n            ans = ans._rescale(ans.adjusted()+1-places, rounding)\n        return ans\n\n    def to_integral_exact(self, rounding=None, context=None):\n        \"\"\"Rounds to a nearby integer.\n\n        If no rounding mode is specified, take the rounding mode from\n        the context.  This method raises the Rounded and Inexact flags\n        when appropriate.\n\n        See also: to_integral_value, which does exactly the same as\n        this method except that it doesn't raise Inexact or Rounded.\n        \"\"\"\n        if self._is_special:\n            ans = self._check_nans(context=context)\n            if ans:\n                return ans\n            return Decimal(self)\n        if self._exp >= 0:\n            return Decimal(self)\n        if not self:\n            return _dec_from_triple(self._sign, '0', 0)\n        if context is None:\n            context = getcontext()\n        if rounding is None:\n            rounding = context.rounding\n        ans = self._rescale(0, rounding)\n        if ans != self:\n            context._raise_error(Inexact)\n        context._raise_error(Rounded)\n        return ans\n\n    def to_integral_value(self, rounding=None, context=None):\n        \"\"\"Rounds to the nearest integer, without raising inexact, rounded.\"\"\"\n        if context is None:\n            context = getcontext()\n        if rounding is None:\n            rounding = context.rounding\n        if self._is_special:\n            ans = self._check_nans(context=context)\n            if ans:\n                return ans\n            return Decimal(self)\n        if self._exp >= 0:\n            return Decimal(self)\n        else:\n            return self._rescale(0, rounding)\n\n    # the method name changed, but we provide also the old one, for compatibility\n    to_integral = to_integral_value\n\n    def sqrt(self, context=None):\n        \"\"\"Return the square root of self.\"\"\"\n        if context is None:\n            context = getcontext()\n\n        if self._is_special:\n            ans = self._check_nans(context=context)\n            if ans:\n                return ans\n\n            if self._isinfinity() and self._sign == 0:\n                return Decimal(self)\n\n        if not self:\n            # exponent = self._exp // 2.  sqrt(-0) = -0\n            ans = _dec_from_triple(self._sign, '0', self._exp // 2)\n            return ans._fix(context)\n\n        if self._sign == 1:\n            return context._raise_error(InvalidOperation, 'sqrt(-x), x > 0')\n\n        # At this point self represents a positive number.  Let p be\n        # the desired precision and express self in the form c*100**e\n        # with c a positive real number and e an integer, c and e\n        # being chosen so that 100**(p-1) <= c < 100**p.  Then the\n        # (exact) square root of self is sqrt(c)*10**e, and 10**(p-1)\n        # <= sqrt(c) < 10**p, so the closest representable Decimal at\n        # precision p is n*10**e where n = round_half_even(sqrt(c)),\n        # the closest integer to sqrt(c) with the even integer chosen\n        # in the case of a tie.\n        #\n        # To ensure correct rounding in all cases, we use the\n        # following trick: we compute the square root to an extra\n        # place (precision p+1 instead of precision p), rounding down.\n        # Then, if the result is inexact and its last digit is 0 or 5,\n        # we increase the last digit to 1 or 6 respectively; if it's\n        # exact we leave the last digit alone.  Now the final round to\n        # p places (or fewer in the case of underflow) will round\n        # correctly and raise the appropriate flags.\n\n        # use an extra digit of precision\n        prec = context.prec+1\n\n        # write argument in the form c*100**e where e = self._exp//2\n        # is the 'ideal' exponent, to be used if the square root is\n        # exactly representable.  l is the number of 'digits' of c in\n        # base 100, so that 100**(l-1) <= c < 100**l.\n        op = _WorkRep(self)\n        e = op.exp >> 1\n        if op.exp & 1:\n            c = op.int * 10\n            l = (len(self._int) >> 1) + 1\n        else:\n            c = op.int\n            l = len(self._int)+1 >> 1\n\n        # rescale so that c has exactly prec base 100 'digits'\n        shift = prec-l\n        if shift >= 0:\n            c *= 100**shift\n            exact = True\n        else:\n            c, remainder = divmod(c, 100**-shift)\n            exact = not remainder\n        e -= shift\n\n        # find n = floor(sqrt(c)) using Newton's method\n        n = 10**prec\n        while True:\n            q = c//n\n            if n <= q:\n                break\n            else:\n                n = n + q >> 1\n        exact = exact and n*n == c\n\n        if exact:\n            # result is exact; rescale to use ideal exponent e\n            if shift >= 0:\n                # assert n % 10**shift == 0\n                n //= 10**shift\n            else:\n                n *= 10**-shift\n            e += shift\n        else:\n            # result is not exact; fix last digit as described above\n            if n % 5 == 0:\n                n += 1\n\n        ans = _dec_from_triple(0, str(n), e)\n\n        # round, and fit to current context\n        context = context._shallow_copy()\n        rounding = context._set_rounding(ROUND_HALF_EVEN)\n        ans = ans._fix(context)\n        context.rounding = rounding\n\n        return ans\n\n    def max(self, other, context=None):\n        \"\"\"Returns the larger value.\n\n        Like max(self, other) except if one is not a number, returns\n        NaN (and signals if one is sNaN).  Also rounds.\n        \"\"\"\n        other = _convert_other(other, raiseit=True)\n\n        if context is None:\n            context = getcontext()\n\n        if self._is_special or other._is_special:\n            # If one operand is a quiet NaN and the other is number, then the\n            # number is always returned\n            sn = self._isnan()\n            on = other._isnan()\n            if sn or on:\n                if on == 1 and sn == 0:\n                    return self._fix(context)\n                if sn == 1 and on == 0:\n                    return other._fix(context)\n                return self._check_nans(other, context)\n\n        c = self._cmp(other)\n        if c == 0:\n            # If both operands are finite and equal in numerical value\n            # then an ordering is applied:\n            #\n            # If the signs differ then max returns the operand with the\n            # positive sign and min returns the operand with the negative sign\n            #\n            # If the signs are the same then the exponent is used to select\n            # the result.  This is exactly the ordering used in compare_total.\n            c = self.compare_total(other)\n\n        if c == -1:\n            ans = other\n        else:\n            ans = self\n\n        return ans._fix(context)\n\n    def min(self, other, context=None):\n        \"\"\"Returns the smaller value.\n\n        Like min(self, other) except if one is not a number, returns\n        NaN (and signals if one is sNaN).  Also rounds.\n        \"\"\"\n        other = _convert_other(other, raiseit=True)\n\n        if context is None:\n            context = getcontext()\n\n        if self._is_special or other._is_special:\n            # If one operand is a quiet NaN and the other is number, then the\n            # number is always returned\n            sn = self._isnan()\n            on = other._isnan()\n            if sn or on:\n                if on == 1 and sn == 0:\n                    return self._fix(context)\n                if sn == 1 and on == 0:\n                    return other._fix(context)\n                return self._check_nans(other, context)\n\n        c = self._cmp(other)\n        if c == 0:\n            c = self.compare_total(other)\n\n        if c == -1:\n            ans = self\n        else:\n            ans = other\n\n        return ans._fix(context)\n\n    def _isinteger(self):\n        \"\"\"Returns whether self is an integer\"\"\"\n        if self._is_special:\n            return False\n        if self._exp >= 0:\n            return True\n        rest = self._int[self._exp:]\n        return rest == '0'*len(rest)\n\n    def _iseven(self):\n        \"\"\"Returns True if self is even.  Assumes self is an integer.\"\"\"\n        if not self or self._exp > 0:\n            return True\n        return self._int[-1+self._exp] in '02468'\n\n    def adjusted(self):\n        \"\"\"Return the adjusted exponent of self\"\"\"\n        try:\n            return self._exp + len(self._int) - 1\n        # If NaN or Infinity, self._exp is string\n        except TypeError:\n            return 0\n\n    def canonical(self, context=None):\n        \"\"\"Returns the same Decimal object.\n\n        As we do not have different encodings for the same number, the\n        received object already is in its canonical form.\n        \"\"\"\n        return self\n\n    def compare_signal(self, other, context=None):\n        \"\"\"Compares self to the other operand numerically.\n\n        It's pretty much like compare(), but all NaNs signal, with signaling\n        NaNs taking precedence over quiet NaNs.\n        \"\"\"\n        other = _convert_other(other, raiseit = True)\n        ans = self._compare_check_nans(other, context)\n        if ans:\n            return ans\n        return self.compare(other, context=context)\n\n    def compare_total(self, other):\n        \"\"\"Compares self to other using the abstract representations.\n\n        This is not like the standard compare, which use their numerical\n        value. Note that a total ordering is defined for all possible abstract\n        representations.\n        \"\"\"\n        other = _convert_other(other, raiseit=True)\n\n        # if one is negative and the other is positive, it's easy\n        if self._sign and not other._sign:\n            return _NegativeOne\n        if not self._sign and other._sign:\n            return _One\n        sign = self._sign\n\n        # let's handle both NaN types\n        self_nan = self._isnan()\n        other_nan = other._isnan()\n        if self_nan or other_nan:\n            if self_nan == other_nan:\n                # compare payloads as though they're integers\n                self_key = len(self._int), self._int\n                other_key = len(other._int), other._int\n                if self_key < other_key:\n                    if sign:\n                        return _One\n                    else:\n                        return _NegativeOne\n                if self_key > other_key:\n                    if sign:\n                        return _NegativeOne\n                    else:\n                        return _One\n                return _Zero\n\n            if sign:\n                if self_nan == 1:\n                    return _NegativeOne\n                if other_nan == 1:\n                    return _One\n                if self_nan == 2:\n                    return _NegativeOne\n                if other_nan == 2:\n                    return _One\n            else:\n                if self_nan == 1:\n                    return _One\n                if other_nan == 1:\n                    return _NegativeOne\n                if self_nan == 2:\n                    return _One\n                if other_nan == 2:\n                    return _NegativeOne\n\n        if self < other:\n            return _NegativeOne\n        if self > other:\n            return _One\n\n        if self._exp < other._exp:\n            if sign:\n                return _One\n            else:\n                return _NegativeOne\n        if self._exp > other._exp:\n            if sign:\n                return _NegativeOne\n            else:\n                return _One\n        return _Zero\n\n\n    def compare_total_mag(self, other):\n        \"\"\"Compares self to other using abstract repr., ignoring sign.\n\n        Like compare_total, but with operand's sign ignored and assumed to be 0.\n        \"\"\"\n        other = _convert_other(other, raiseit=True)\n\n        s = self.copy_abs()\n        o = other.copy_abs()\n        return s.compare_total(o)\n\n    def copy_abs(self):\n        \"\"\"Returns a copy with the sign set to 0. \"\"\"\n        return _dec_from_triple(0, self._int, self._exp, self._is_special)\n\n    def copy_negate(self):\n        \"\"\"Returns a copy with the sign inverted.\"\"\"\n        if self._sign:\n            return _dec_from_triple(0, self._int, self._exp, self._is_special)\n        else:\n            return _dec_from_triple(1, self._int, self._exp, self._is_special)\n\n    def copy_sign(self, other):\n        \"\"\"Returns self with the sign of other.\"\"\"\n        other = _convert_other(other, raiseit=True)\n        return _dec_from_triple(other._sign, self._int,\n                                self._exp, self._is_special)\n\n    def exp(self, context=None):\n        \"\"\"Returns e ** self.\"\"\"\n\n        if context is None:\n            context = getcontext()\n\n        # exp(NaN) = NaN\n        ans = self._check_nans(context=context)\n        if ans:\n            return ans\n\n        # exp(-Infinity) = 0\n        if self._isinfinity() == -1:\n            return _Zero\n\n        # exp(0) = 1\n        if not self:\n            return _One\n\n        # exp(Infinity) = Infinity\n        if self._isinfinity() == 1:\n            return Decimal(self)\n\n        # the result is now guaranteed to be inexact (the true\n        # mathematical result is transcendental). There's no need to\n        # raise Rounded and Inexact here---they'll always be raised as\n        # a result of the call to _fix.\n        p = context.prec\n        adj = self.adjusted()\n\n        # we only need to do any computation for quite a small range\n        # of adjusted exponents---for example, -29 <= adj <= 10 for\n        # the default context.  For smaller exponent the result is\n        # indistinguishable from 1 at the given precision, while for\n        # larger exponent the result either overflows or underflows.\n        if self._sign == 0 and adj > len(str((context.Emax+1)*3)):\n            # overflow\n            ans = _dec_from_triple(0, '1', context.Emax+1)\n        elif self._sign == 1 and adj > len(str((-context.Etiny()+1)*3)):\n            # underflow to 0\n            ans = _dec_from_triple(0, '1', context.Etiny()-1)\n        elif self._sign == 0 and adj < -p:\n            # p+1 digits; final round will raise correct flags\n            ans = _dec_from_triple(0, '1' + '0'*(p-1) + '1', -p)\n        elif self._sign == 1 and adj < -p-1:\n            # p+1 digits; final round will raise correct flags\n            ans = _dec_from_triple(0, '9'*(p+1), -p-1)\n        # general case\n        else:\n            op = _WorkRep(self)\n            c, e = op.int, op.exp\n            if op.sign == 1:\n                c = -c\n\n            # compute correctly rounded result: increase precision by\n            # 3 digits at a time until we get an unambiguously\n            # roundable result\n            extra = 3\n            while True:\n                coeff, exp = _dexp(c, e, p+extra)\n                if coeff % (5*10**(len(str(coeff))-p-1)):\n                    break\n                extra += 3\n\n            ans = _dec_from_triple(0, str(coeff), exp)\n\n        # at this stage, ans should round correctly with *any*\n        # rounding mode, not just with ROUND_HALF_EVEN\n        context = context._shallow_copy()\n        rounding = context._set_rounding(ROUND_HALF_EVEN)\n        ans = ans._fix(context)\n        context.rounding = rounding\n\n        return ans\n\n    def is_canonical(self):\n        \"\"\"Return True if self is canonical; otherwise return False.\n\n        Currently, the encoding of a Decimal instance is always\n        canonical, so this method returns True for any Decimal.\n        \"\"\"\n        return True\n\n    def is_finite(self):\n        \"\"\"Return True if self is finite; otherwise return False.\n\n        A Decimal instance is considered finite if it is neither\n        infinite nor a NaN.\n        \"\"\"\n        return not self._is_special\n\n    def is_infinite(self):\n        \"\"\"Return True if self is infinite; otherwise return False.\"\"\"\n        return self._exp == 'F'\n\n    def is_nan(self):\n        \"\"\"Return True if self is a qNaN or sNaN; otherwise return False.\"\"\"\n        return self._exp in ('n', 'N')\n\n    def is_normal(self, context=None):\n        \"\"\"Return True if self is a normal number; otherwise return False.\"\"\"\n        if self._is_special or not self:\n            return False\n        if context is None:\n            context = getcontext()\n        return context.Emin <= self.adjusted()\n\n    def is_qnan(self):\n        \"\"\"Return True if self is a quiet NaN; otherwise return False.\"\"\"\n        return self._exp == 'n'\n\n    def is_signed(self):\n        \"\"\"Return True if self is negative; otherwise return False.\"\"\"\n        return self._sign == 1\n\n    def is_snan(self):\n        \"\"\"Return True if self is a signaling NaN; otherwise return False.\"\"\"\n        return self._exp == 'N'\n\n    def is_subnormal(self, context=None):\n        \"\"\"Return True if self is subnormal; otherwise return False.\"\"\"\n        if self._is_special or not self:\n            return False\n        if context is None:\n            context = getcontext()\n        return self.adjusted() < context.Emin\n\n    def is_zero(self):\n        \"\"\"Return True if self is a zero; otherwise return False.\"\"\"\n        return not self._is_special and self._int == '0'\n\n    def _ln_exp_bound(self):\n        \"\"\"Compute a lower bound for the adjusted exponent of self.ln().\n        In other words, compute r such that self.ln() >= 10**r.  Assumes\n        that self is finite and positive and that self != 1.\n        \"\"\"\n\n        # for 0.1 <= x <= 10 we use the inequalities 1-1/x <= ln(x) <= x-1\n        adj = self._exp + len(self._int) - 1\n        if adj >= 1:\n            # argument >= 10; we use 23/10 = 2.3 as a lower bound for ln(10)\n            return len(str(adj*23//10)) - 1\n        if adj <= -2:\n            # argument <= 0.1\n            return len(str((-1-adj)*23//10)) - 1\n        op = _WorkRep(self)\n        c, e = op.int, op.exp\n        if adj == 0:\n            # 1 < self < 10\n            num = str(c-10**-e)\n            den = str(c)\n            return len(num) - len(den) - (num < den)\n        # adj == -1, 0.1 <= self < 1\n        return e + len(str(10**-e - c)) - 1\n\n\n    def ln(self, context=None):\n        \"\"\"Returns the natural (base e) logarithm of self.\"\"\"\n\n        if context is None:\n            context = getcontext()\n\n        # ln(NaN) = NaN\n        ans = self._check_nans(context=context)\n        if ans:\n            return ans\n\n        # ln(0.0) == -Infinity\n        if not self:\n            return _NegativeInfinity\n\n        # ln(Infinity) = Infinity\n        if self._isinfinity() == 1:\n            return _Infinity\n\n        # ln(1.0) == 0.0\n        if self == _One:\n            return _Zero\n\n        # ln(negative) raises InvalidOperation\n        if self._sign == 1:\n            return context._raise_error(InvalidOperation,\n                                        'ln of a negative value')\n\n        # result is irrational, so necessarily inexact\n        op = _WorkRep(self)\n        c, e = op.int, op.exp\n        p = context.prec\n\n        # correctly rounded result: repeatedly increase precision by 3\n        # until we get an unambiguously roundable result\n        places = p - self._ln_exp_bound() + 2 # at least p+3 places\n        while True:\n            coeff = _dlog(c, e, places)\n            # assert len(str(abs(coeff)))-p >= 1\n            if coeff % (5*10**(len(str(abs(coeff)))-p-1)):\n                break\n            places += 3\n        ans = _dec_from_triple(int(coeff<0), str(abs(coeff)), -places)\n\n        context = context._shallow_copy()\n        rounding = context._set_rounding(ROUND_HALF_EVEN)\n        ans = ans._fix(context)\n        context.rounding = rounding\n        return ans\n\n    def _log10_exp_bound(self):\n        \"\"\"Compute a lower bound for the adjusted exponent of self.log10().\n        In other words, find r such that self.log10() >= 10**r.\n        Assumes that self is finite and positive and that self != 1.\n        \"\"\"\n\n        # For x >= 10 or x < 0.1 we only need a bound on the integer\n        # part of log10(self), and this comes directly from the\n        # exponent of x.  For 0.1 <= x <= 10 we use the inequalities\n        # 1-1/x <= log(x) <= x-1. If x > 1 we have |log10(x)| >\n        # (1-1/x)/2.31 > 0.  If x < 1 then |log10(x)| > (1-x)/2.31 > 0\n\n        adj = self._exp + len(self._int) - 1\n        if adj >= 1:\n            # self >= 10\n            return len(str(adj))-1\n        if adj <= -2:\n            # self < 0.1\n            return len(str(-1-adj))-1\n        op = _WorkRep(self)\n        c, e = op.int, op.exp\n        if adj == 0:\n            # 1 < self < 10\n            num = str(c-10**-e)\n            den = str(231*c)\n            return len(num) - len(den) - (num < den) + 2\n        # adj == -1, 0.1 <= self < 1\n        num = str(10**-e-c)\n        return len(num) + e - (num < \"231\") - 1\n\n    def log10(self, context=None):\n        \"\"\"Returns the base 10 logarithm of self.\"\"\"\n\n        if context is None:\n            context = getcontext()\n\n        # log10(NaN) = NaN\n        ans = self._check_nans(context=context)\n        if ans:\n            return ans\n\n        # log10(0.0) == -Infinity\n        if not self:\n            return _NegativeInfinity\n\n        # log10(Infinity) = Infinity\n        if self._isinfinity() == 1:\n            return _Infinity\n\n        # log10(negative or -Infinity) raises InvalidOperation\n        if self._sign == 1:\n            return context._raise_error(InvalidOperation,\n                                        'log10 of a negative value')\n\n        # log10(10**n) = n\n        if self._int[0] == '1' and self._int[1:] == '0'*(len(self._int) - 1):\n            # answer may need rounding\n            ans = Decimal(self._exp + len(self._int) - 1)\n        else:\n            # result is irrational, so necessarily inexact\n            op = _WorkRep(self)\n            c, e = op.int, op.exp\n            p = context.prec\n\n            # correctly rounded result: repeatedly increase precision\n            # until result is unambiguously roundable\n            places = p-self._log10_exp_bound()+2\n            while True:\n                coeff = _dlog10(c, e, places)\n                # assert len(str(abs(coeff)))-p >= 1\n                if coeff % (5*10**(len(str(abs(coeff)))-p-1)):\n                    break\n                places += 3\n            ans = _dec_from_triple(int(coeff<0), str(abs(coeff)), -places)\n\n        context = context._shallow_copy()\n        rounding = context._set_rounding(ROUND_HALF_EVEN)\n        ans = ans._fix(context)\n        context.rounding = rounding\n        return ans\n\n    def logb(self, context=None):\n        \"\"\" Returns the exponent of the magnitude of self's MSD.\n\n        The result is the integer which is the exponent of the magnitude\n        of the most significant digit of self (as though it were truncated\n        to a single digit while maintaining the value of that digit and\n        without limiting the resulting exponent).\n        \"\"\"\n        # logb(NaN) = NaN\n        ans = self._check_nans(context=context)\n        if ans:\n            return ans\n\n        if context is None:\n            context = getcontext()\n\n        # logb(+/-Inf) = +Inf\n        if self._isinfinity():\n            return _Infinity\n\n        # logb(0) = -Inf, DivisionByZero\n        if not self:\n            return context._raise_error(DivisionByZero, 'logb(0)', 1)\n\n        # otherwise, simply return the adjusted exponent of self, as a\n        # Decimal.  Note that no attempt is made to fit the result\n        # into the current context.\n        ans = Decimal(self.adjusted())\n        return ans._fix(context)\n\n    def _islogical(self):\n        \"\"\"Return True if self is a logical operand.\n\n        For being logical, it must be a finite number with a sign of 0,\n        an exponent of 0, and a coefficient whose digits must all be\n        either 0 or 1.\n        \"\"\"\n        if self._sign != 0 or self._exp != 0:\n            return False\n        for dig in self._int:\n            if dig not in '01':\n                return False\n        return True\n\n    def _fill_logical(self, context, opa, opb):\n        dif = context.prec - len(opa)\n        if dif > 0:\n            opa = '0'*dif + opa\n        elif dif < 0:\n            opa = opa[-context.prec:]\n        dif = context.prec - len(opb)\n        if dif > 0:\n            opb = '0'*dif + opb\n        elif dif < 0:\n            opb = opb[-context.prec:]\n        return opa, opb\n\n    def logical_and(self, other, context=None):\n        \"\"\"Applies an 'and' operation between self and other's digits.\"\"\"\n        if context is None:\n            context = getcontext()\n\n        other = _convert_other(other, raiseit=True)\n\n        if not self._islogical() or not other._islogical():\n            return context._raise_error(InvalidOperation)\n\n        # fill to context.prec\n        (opa, opb) = self._fill_logical(context, self._int, other._int)\n\n        # make the operation, and clean starting zeroes\n        result = \"\".join([str(int(a)&int(b)) for a,b in zip(opa,opb)])\n        return _dec_from_triple(0, result.lstrip('0') or '0', 0)\n\n    def logical_invert(self, context=None):\n        \"\"\"Invert all its digits.\"\"\"\n        if context is None:\n            context = getcontext()\n        return self.logical_xor(_dec_from_triple(0,'1'*context.prec,0),\n                                context)\n\n    def logical_or(self, other, context=None):\n        \"\"\"Applies an 'or' operation between self and other's digits.\"\"\"\n        if context is None:\n            context = getcontext()\n\n        other = _convert_other(other, raiseit=True)\n\n        if not self._islogical() or not other._islogical():\n            return context._raise_error(InvalidOperation)\n\n        # fill to context.prec\n        (opa, opb) = self._fill_logical(context, self._int, other._int)\n\n        # make the operation, and clean starting zeroes\n        result = \"\".join([str(int(a)|int(b)) for a,b in zip(opa,opb)])\n        return _dec_from_triple(0, result.lstrip('0') or '0', 0)\n\n    def logical_xor(self, other, context=None):\n        \"\"\"Applies an 'xor' operation between self and other's digits.\"\"\"\n        if context is None:\n            context = getcontext()\n\n        other = _convert_other(other, raiseit=True)\n\n        if not self._islogical() or not other._islogical():\n            return context._raise_error(InvalidOperation)\n\n        # fill to context.prec\n        (opa, opb) = self._fill_logical(context, self._int, other._int)\n\n        # make the operation, and clean starting zeroes\n        result = \"\".join([str(int(a)^int(b)) for a,b in zip(opa,opb)])\n        return _dec_from_triple(0, result.lstrip('0') or '0', 0)\n\n    def max_mag(self, other, context=None):\n        \"\"\"Compares the values numerically with their sign ignored.\"\"\"\n        other = _convert_other(other, raiseit=True)\n\n        if context is None:\n            context = getcontext()\n\n        if self._is_special or other._is_special:\n            # If one operand is a quiet NaN and the other is number, then the\n            # number is always returned\n            sn = self._isnan()\n            on = other._isnan()\n            if sn or on:\n                if on == 1 and sn == 0:\n                    return self._fix(context)\n                if sn == 1 and on == 0:\n                    return other._fix(context)\n                return self._check_nans(other, context)\n\n        c = self.copy_abs()._cmp(other.copy_abs())\n        if c == 0:\n            c = self.compare_total(other)\n\n        if c == -1:\n            ans = other\n        else:\n            ans = self\n\n        return ans._fix(context)\n\n    def min_mag(self, other, context=None):\n        \"\"\"Compares the values numerically with their sign ignored.\"\"\"\n        other = _convert_other(other, raiseit=True)\n\n        if context is None:\n            context = getcontext()\n\n        if self._is_special or other._is_special:\n            # If one operand is a quiet NaN and the other is number, then the\n            # number is always returned\n            sn = self._isnan()\n            on = other._isnan()\n            if sn or on:\n                if on == 1 and sn == 0:\n                    return self._fix(context)\n                if sn == 1 and on == 0:\n                    return other._fix(context)\n                return self._check_nans(other, context)\n\n        c = self.copy_abs()._cmp(other.copy_abs())\n        if c == 0:\n            c = self.compare_total(other)\n\n        if c == -1:\n            ans = self\n        else:\n            ans = other\n\n        return ans._fix(context)\n\n    def next_minus(self, context=None):\n        \"\"\"Returns the largest representable number smaller than itself.\"\"\"\n        if context is None:\n            context = getcontext()\n\n        ans = self._check_nans(context=context)\n        if ans:\n            return ans\n\n        if self._isinfinity() == -1:\n            return _NegativeInfinity\n        if self._isinfinity() == 1:\n            return _dec_from_triple(0, '9'*context.prec, context.Etop())\n\n        context = context.copy()\n        context._set_rounding(ROUND_FLOOR)\n        context._ignore_all_flags()\n        new_self = self._fix(context)\n        if new_self != self:\n            return new_self\n        return self.__sub__(_dec_from_triple(0, '1', context.Etiny()-1),\n                            context)\n\n    def next_plus(self, context=None):\n        \"\"\"Returns the smallest representable number larger than itself.\"\"\"\n        if context is None:\n            context = getcontext()\n\n        ans = self._check_nans(context=context)\n        if ans:\n            return ans\n\n        if self._isinfinity() == 1:\n            return _Infinity\n        if self._isinfinity() == -1:\n            return _dec_from_triple(1, '9'*context.prec, context.Etop())\n\n        context = context.copy()\n        context._set_rounding(ROUND_CEILING)\n        context._ignore_all_flags()\n        new_self = self._fix(context)\n        if new_self != self:\n            return new_self\n        return self.__add__(_dec_from_triple(0, '1', context.Etiny()-1),\n                            context)\n\n    def next_toward(self, other, context=None):\n        \"\"\"Returns the number closest to self, in the direction towards other.\n\n        The result is the closest representable number to self\n        (excluding self) that is in the direction towards other,\n        unless both have the same value.  If the two operands are\n        numerically equal, then the result is a copy of self with the\n        sign set to be the same as the sign of other.\n        \"\"\"\n        other = _convert_other(other, raiseit=True)\n\n        if context is None:\n            context = getcontext()\n\n        ans = self._check_nans(other, context)\n        if ans:\n            return ans\n\n        comparison = self._cmp(other)\n        if comparison == 0:\n            return self.copy_sign(other)\n\n        if comparison == -1:\n            ans = self.next_plus(context)\n        else: # comparison == 1\n            ans = self.next_minus(context)\n\n        # decide which flags to raise using value of ans\n        if ans._isinfinity():\n            context._raise_error(Overflow,\n                                 'Infinite result from next_toward',\n                                 ans._sign)\n            context._raise_error(Inexact)\n            context._raise_error(Rounded)\n        elif ans.adjusted() < context.Emin:\n            context._raise_error(Underflow)\n            context._raise_error(Subnormal)\n            context._raise_error(Inexact)\n            context._raise_error(Rounded)\n            # if precision == 1 then we don't raise Clamped for a\n            # result 0E-Etiny.\n            if not ans:\n                context._raise_error(Clamped)\n\n        return ans\n\n    def number_class(self, context=None):\n        \"\"\"Returns an indication of the class of self.\n\n        The class is one of the following strings:\n          sNaN\n          NaN\n          -Infinity\n          -Normal\n          -Subnormal\n          -Zero\n          +Zero\n          +Subnormal\n          +Normal\n          +Infinity\n        \"\"\"\n        if self.is_snan():\n            return \"sNaN\"\n        if self.is_qnan():\n            return \"NaN\"\n        inf = self._isinfinity()\n        if inf == 1:\n            return \"+Infinity\"\n        if inf == -1:\n            return \"-Infinity\"\n        if self.is_zero():\n            if self._sign:\n                return \"-Zero\"\n            else:\n                return \"+Zero\"\n        if context is None:\n            context = getcontext()\n        if self.is_subnormal(context=context):\n            if self._sign:\n                return \"-Subnormal\"\n            else:\n                return \"+Subnormal\"\n        # just a normal, regular, boring number, :)\n        if self._sign:\n            return \"-Normal\"\n        else:\n            return \"+Normal\"\n\n    def radix(self):\n        \"\"\"Just returns 10, as this is Decimal, :)\"\"\"\n        return Decimal(10)\n\n    def rotate(self, other, context=None):\n        \"\"\"Returns a rotated copy of self, value-of-other times.\"\"\"\n        if context is None:\n            context = getcontext()\n\n        other = _convert_other(other, raiseit=True)\n\n        ans = self._check_nans(other, context)\n        if ans:\n            return ans\n\n        if other._exp != 0:\n            return context._raise_error(InvalidOperation)\n        if not (-context.prec <= int(other) <= context.prec):\n            return context._raise_error(InvalidOperation)\n\n        if self._isinfinity():\n            return Decimal(self)\n\n        # get values, pad if necessary\n        torot = int(other)\n        rotdig = self._int\n        topad = context.prec - len(rotdig)\n        if topad > 0:\n            rotdig = '0'*topad + rotdig\n        elif topad < 0:\n            rotdig = rotdig[-topad:]\n\n        # let's rotate!\n        rotated = rotdig[torot:] + rotdig[:torot]\n        return _dec_from_triple(self._sign,\n                                rotated.lstrip('0') or '0', self._exp)\n\n    def scaleb(self, other, context=None):\n        \"\"\"Returns self operand after adding the second value to its exp.\"\"\"\n        if context is None:\n            context = getcontext()\n\n        other = _convert_other(other, raiseit=True)\n\n        ans = self._check_nans(other, context)\n        if ans:\n            return ans\n\n        if other._exp != 0:\n            return context._raise_error(InvalidOperation)\n        liminf = -2 * (context.Emax + context.prec)\n        limsup =  2 * (context.Emax + context.prec)\n        if not (liminf <= int(other) <= limsup):\n            return context._raise_error(InvalidOperation)\n\n        if self._isinfinity():\n            return Decimal(self)\n\n        d = _dec_from_triple(self._sign, self._int, self._exp + int(other))\n        d = d._fix(context)\n        return d\n\n    def shift(self, other, context=None):\n        \"\"\"Returns a shifted copy of self, value-of-other times.\"\"\"\n        if context is None:\n            context = getcontext()\n\n        other = _convert_other(other, raiseit=True)\n\n        ans = self._check_nans(other, context)\n        if ans:\n            return ans\n\n        if other._exp != 0:\n            return context._raise_error(InvalidOperation)\n        if not (-context.prec <= int(other) <= context.prec):\n            return context._raise_error(InvalidOperation)\n\n        if self._isinfinity():\n            return Decimal(self)\n\n        # get values, pad if necessary\n        torot = int(other)\n        rotdig = self._int\n        topad = context.prec - len(rotdig)\n        if topad > 0:\n            rotdig = '0'*topad + rotdig\n        elif topad < 0:\n            rotdig = rotdig[-topad:]\n\n        # let's shift!\n        if torot < 0:\n            shifted = rotdig[:torot]\n        else:\n            shifted = rotdig + '0'*torot\n            shifted = shifted[-context.prec:]\n\n        return _dec_from_triple(self._sign,\n                                    shifted.lstrip('0') or '0', self._exp)\n\n    # Support for pickling, copy, and deepcopy\n    def __reduce__(self):\n        return (self.__class__, (str(self),))\n\n    def __copy__(self):\n        if type(self) is Decimal:\n            return self     # I'm immutable; therefore I am my own clone\n        return self.__class__(str(self))\n\n    def __deepcopy__(self, memo):\n        if type(self) is Decimal:\n            return self     # My components are also immutable\n        return self.__class__(str(self))\n\n    # PEP 3101 support.  the _localeconv keyword argument should be\n    # considered private: it's provided for ease of testing only.\n    def __format__(self, specifier, context=None, _localeconv=None):\n        \"\"\"Format a Decimal instance according to the given specifier.\n\n        The specifier should be a standard format specifier, with the\n        form described in PEP 3101.  Formatting types 'e', 'E', 'f',\n        'F', 'g', 'G', 'n' and '%' are supported.  If the formatting\n        type is omitted it defaults to 'g' or 'G', depending on the\n        value of context.capitals.\n        \"\"\"\n\n        # Note: PEP 3101 says that if the type is not present then\n        # there should be at least one digit after the decimal point.\n        # We take the liberty of ignoring this requirement for\n        # Decimal---it's presumably there to make sure that\n        # format(float, '') behaves similarly to str(float).\n        if context is None:\n            context = getcontext()\n\n        spec = _parse_format_specifier(specifier, _localeconv=_localeconv)\n\n        # special values don't care about the type or precision\n        if self._is_special:\n            sign = _format_sign(self._sign, spec)\n            body = str(self.copy_abs())\n            return _format_align(sign, body, spec)\n\n        # a type of None defaults to 'g' or 'G', depending on context\n        if spec['type'] is None:\n            spec['type'] = ['g', 'G'][context.capitals]\n\n        # if type is '%', adjust exponent of self accordingly\n        if spec['type'] == '%':\n            self = _dec_from_triple(self._sign, self._int, self._exp+2)\n\n        # round if necessary, taking rounding mode from the context\n        rounding = context.rounding\n        precision = spec['precision']\n        if precision is not None:\n            if spec['type'] in 'eE':\n                self = self._round(precision+1, rounding)\n            elif spec['type'] in 'fF%':\n                self = self._rescale(-precision, rounding)\n            elif spec['type'] in 'gG' and len(self._int) > precision:\n                self = self._round(precision, rounding)\n        # special case: zeros with a positive exponent can't be\n        # represented in fixed point; rescale them to 0e0.\n        if not self and self._exp > 0 and spec['type'] in 'fF%':\n            self = self._rescale(0, rounding)\n\n        # figure out placement of the decimal point\n        leftdigits = self._exp + len(self._int)\n        if spec['type'] in 'eE':\n            if not self and precision is not None:\n                dotplace = 1 - precision\n            else:\n                dotplace = 1\n        elif spec['type'] in 'fF%':\n            dotplace = leftdigits\n        elif spec['type'] in 'gG':\n            if self._exp <= 0 and leftdigits > -6:\n                dotplace = leftdigits\n            else:\n                dotplace = 1\n\n        # find digits before and after decimal point, and get exponent\n        if dotplace < 0:\n            intpart = '0'\n            fracpart = '0'*(-dotplace) + self._int\n        elif dotplace > len(self._int):\n            intpart = self._int + '0'*(dotplace-len(self._int))\n            fracpart = ''\n        else:\n            intpart = self._int[:dotplace] or '0'\n            fracpart = self._int[dotplace:]\n        exp = leftdigits-dotplace\n\n        # done with the decimal-specific stuff;  hand over the rest\n        # of the formatting to the _format_number function\n        return _format_number(self._sign, intpart, fracpart, exp, spec)\n\ndef _dec_from_triple(sign, coefficient, exponent, special=False):\n    \"\"\"Create a decimal instance directly, without any validation,\n    normalization (e.g. removal of leading zeros) or argument\n    conversion.\n\n    This function is for *internal use only*.\n    \"\"\"\n\n    self = object.__new__(Decimal)\n    self._sign = sign\n    self._int = coefficient\n    self._exp = exponent\n    self._is_special = special\n\n    return self\n\n# Register Decimal as a kind of Number (an abstract base class).\n# However, do not register it as Real (because Decimals are not\n# interoperable with floats).\n_numbers.Number.register(Decimal)\n\n\n##### Context class #######################################################\n\nclass _ContextManager(object):\n    \"\"\"Context manager class to support localcontext().\n\n      Sets a copy of the supplied context in __enter__() and restores\n      the previous decimal context in __exit__()\n    \"\"\"\n    def __init__(self, new_context):\n        self.new_context = new_context.copy()\n    def __enter__(self):\n        self.saved_context = getcontext()\n        setcontext(self.new_context)\n        return self.new_context\n    def __exit__(self, t, v, tb):\n        setcontext(self.saved_context)\n\nclass Context(object):\n    \"\"\"Contains the context for a Decimal instance.\n\n    Contains:\n    prec - precision (for use in rounding, division, square roots..)\n    rounding - rounding type (how you round)\n    traps - If traps[exception] = 1, then the exception is\n                    raised when it is caused.  Otherwise, a value is\n                    substituted in.\n    flags  - When an exception is caused, flags[exception] is set.\n             (Whether or not the trap_enabler is set)\n             Should be reset by user of Decimal instance.\n    Emin -   Minimum exponent\n    Emax -   Maximum exponent\n    capitals -      If 1, 1*10^1 is printed as 1E+1.\n                    If 0, printed as 1e1\n    _clamp - If 1, change exponents if too high (Default 0)\n    \"\"\"\n\n    def __init__(self, prec=None, rounding=None,\n                 traps=None, flags=None,\n                 Emin=None, Emax=None,\n                 capitals=None, _clamp=0,\n                 _ignored_flags=None):\n        # Set defaults; for everything except flags and _ignored_flags,\n        # inherit from DefaultContext.\n        try:\n            dc = DefaultContext\n        except NameError:\n            pass\n\n        self.prec = prec if prec is not None else dc.prec\n        self.rounding = rounding if rounding is not None else dc.rounding\n        self.Emin = Emin if Emin is not None else dc.Emin\n        self.Emax = Emax if Emax is not None else dc.Emax\n        self.capitals = capitals if capitals is not None else dc.capitals\n        self._clamp = _clamp if _clamp is not None else dc._clamp\n\n        if _ignored_flags is None:\n            self._ignored_flags = []\n        else:\n            self._ignored_flags = _ignored_flags\n\n        if traps is None:\n            self.traps = dc.traps.copy()\n        elif not isinstance(traps, dict):\n            self.traps = dict((s, int(s in traps)) for s in _signals)\n        else:\n            self.traps = traps\n\n        if flags is None:\n            self.flags = dict.fromkeys(_signals, 0)\n        elif not isinstance(flags, dict):\n            self.flags = dict((s, int(s in flags)) for s in _signals)\n        else:\n            self.flags = flags\n\n    def __repr__(self):\n        \"\"\"Show the current context.\"\"\"\n        s = []\n        s.append('Context(prec=%(prec)d, rounding=%(rounding)s, '\n                 'Emin=%(Emin)d, Emax=%(Emax)d, capitals=%(capitals)d'\n                 % vars(self))\n        names = [f.__name__ for f, v in self.flags.items() if v]\n        s.append('flags=[' + ', '.join(names) + ']')\n        names = [t.__name__ for t, v in self.traps.items() if v]\n        s.append('traps=[' + ', '.join(names) + ']')\n        return ', '.join(s) + ')'\n\n    def clear_flags(self):\n        \"\"\"Reset all flags to zero\"\"\"\n        for flag in self.flags:\n            self.flags[flag] = 0\n\n    def _shallow_copy(self):\n        \"\"\"Returns a shallow copy from self.\"\"\"\n        nc = Context(self.prec, self.rounding, self.traps,\n                     self.flags, self.Emin, self.Emax,\n                     self.capitals, self._clamp, self._ignored_flags)\n        return nc\n\n    def copy(self):\n        \"\"\"Returns a deep copy from self.\"\"\"\n        nc = Context(self.prec, self.rounding, self.traps.copy(),\n                     self.flags.copy(), self.Emin, self.Emax,\n                     self.capitals, self._clamp, self._ignored_flags)\n        return nc\n    __copy__ = copy\n\n    def _raise_error(self, condition, explanation = None, *args):\n        \"\"\"Handles an error\n\n        If the flag is in _ignored_flags, returns the default response.\n        Otherwise, it sets the flag, then, if the corresponding\n        trap_enabler is set, it reraises the exception.  Otherwise, it returns\n        the default value after setting the flag.\n        \"\"\"\n        error = _condition_map.get(condition, condition)\n        if error in self._ignored_flags:\n            # Don't touch the flag\n            return error().handle(self, *args)\n\n        self.flags[error] = 1\n        if not self.traps[error]:\n            # The errors define how to handle themselves.\n            return condition().handle(self, *args)\n\n        # Errors should only be risked on copies of the context\n        # self._ignored_flags = []\n        raise error(explanation)\n\n    def _ignore_all_flags(self):\n        \"\"\"Ignore all flags, if they are raised\"\"\"\n        return self._ignore_flags(*_signals)\n\n    def _ignore_flags(self, *flags):\n        \"\"\"Ignore the flags, if they are raised\"\"\"\n        # Do not mutate-- This way, copies of a context leave the original\n        # alone.\n        self._ignored_flags = (self._ignored_flags + list(flags))\n        return list(flags)\n\n    def _regard_flags(self, *flags):\n        \"\"\"Stop ignoring the flags, if they are raised\"\"\"\n        if flags and isinstance(flags[0], (tuple,list)):\n            flags = flags[0]\n        for flag in flags:\n            self._ignored_flags.remove(flag)\n\n    # We inherit object.__hash__, so we must deny this explicitly\n    __hash__ = None\n\n    def Etiny(self):\n        \"\"\"Returns Etiny (= Emin - prec + 1)\"\"\"\n        return int(self.Emin - self.prec + 1)\n\n    def Etop(self):\n        \"\"\"Returns maximum exponent (= Emax - prec + 1)\"\"\"\n        return int(self.Emax - self.prec + 1)\n\n    def _set_rounding(self, type):\n        \"\"\"Sets the rounding type.\n\n        Sets the rounding type, and returns the current (previous)\n        rounding type.  Often used like:\n\n        context = context.copy()\n        # so you don't change the calling context\n        # if an error occurs in the middle.\n        rounding = context._set_rounding(ROUND_UP)\n        val = self.__sub__(other, context=context)\n        context._set_rounding(rounding)\n\n        This will make it round up for that operation.\n        \"\"\"\n        rounding = self.rounding\n        self.rounding= type\n        return rounding\n\n    def create_decimal(self, num='0'):\n        \"\"\"Creates a new Decimal instance but using self as context.\n\n        This method implements the to-number operation of the\n        IBM Decimal specification.\"\"\"\n\n        if isinstance(num, basestring) and num != num.strip():\n            return self._raise_error(ConversionSyntax,\n                                     \"no trailing or leading whitespace is \"\n                                     \"permitted.\")\n\n        d = Decimal(num, context=self)\n        if d._isnan() and len(d._int) > self.prec - self._clamp:\n            return self._raise_error(ConversionSyntax,\n                                     \"diagnostic info too long in NaN\")\n        return d._fix(self)\n\n    def create_decimal_from_float(self, f):\n        \"\"\"Creates a new Decimal instance from a float but rounding using self\n        as the context.\n\n        >>> context = Context(prec=5, rounding=ROUND_DOWN)\n        >>> context.create_decimal_from_float(3.1415926535897932)\n        Decimal('3.1415')\n        >>> context = Context(prec=5, traps=[Inexact])\n        >>> context.create_decimal_from_float(3.1415926535897932)\n        Traceback (most recent call last):\n            ...\n        Inexact: None\n\n        \"\"\"\n        d = Decimal.from_float(f)       # An exact conversion\n        return d._fix(self)             # Apply the context rounding\n\n    # Methods\n    def abs(self, a):\n        \"\"\"Returns the absolute value of the operand.\n\n        If the operand is negative, the result is the same as using the minus\n        operation on the operand.  Otherwise, the result is the same as using\n        the plus operation on the operand.\n\n        >>> ExtendedContext.abs(Decimal('2.1'))\n        Decimal('2.1')\n        >>> ExtendedContext.abs(Decimal('-100'))\n        Decimal('100')\n        >>> ExtendedContext.abs(Decimal('101.5'))\n        Decimal('101.5')\n        >>> ExtendedContext.abs(Decimal('-101.5'))\n        Decimal('101.5')\n        >>> ExtendedContext.abs(-1)\n        Decimal('1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.__abs__(context=self)\n\n    def add(self, a, b):\n        \"\"\"Return the sum of the two operands.\n\n        >>> ExtendedContext.add(Decimal('12'), Decimal('7.00'))\n        Decimal('19.00')\n        >>> ExtendedContext.add(Decimal('1E+2'), Decimal('1.01E+4'))\n        Decimal('1.02E+4')\n        >>> ExtendedContext.add(1, Decimal(2))\n        Decimal('3')\n        >>> ExtendedContext.add(Decimal(8), 5)\n        Decimal('13')\n        >>> ExtendedContext.add(5, 5)\n        Decimal('10')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        r = a.__add__(b, context=self)\n        if r is NotImplemented:\n            raise TypeError(\"Unable to convert %s to Decimal\" % b)\n        else:\n            return r\n\n    def _apply(self, a):\n        return str(a._fix(self))\n\n    def canonical(self, a):\n        \"\"\"Returns the same Decimal object.\n\n        As we do not have different encodings for the same number, the\n        received object already is in its canonical form.\n\n        >>> ExtendedContext.canonical(Decimal('2.50'))\n        Decimal('2.50')\n        \"\"\"\n        return a.canonical(context=self)\n\n    def compare(self, a, b):\n        \"\"\"Compares values numerically.\n\n        If the signs of the operands differ, a value representing each operand\n        ('-1' if the operand is less than zero, '0' if the operand is zero or\n        negative zero, or '1' if the operand is greater than zero) is used in\n        place of that operand for the comparison instead of the actual\n        operand.\n\n        The comparison is then effected by subtracting the second operand from\n        the first and then returning a value according to the result of the\n        subtraction: '-1' if the result is less than zero, '0' if the result is\n        zero or negative zero, or '1' if the result is greater than zero.\n\n        >>> ExtendedContext.compare(Decimal('2.1'), Decimal('3'))\n        Decimal('-1')\n        >>> ExtendedContext.compare(Decimal('2.1'), Decimal('2.1'))\n        Decimal('0')\n        >>> ExtendedContext.compare(Decimal('2.1'), Decimal('2.10'))\n        Decimal('0')\n        >>> ExtendedContext.compare(Decimal('3'), Decimal('2.1'))\n        Decimal('1')\n        >>> ExtendedContext.compare(Decimal('2.1'), Decimal('-3'))\n        Decimal('1')\n        >>> ExtendedContext.compare(Decimal('-3'), Decimal('2.1'))\n        Decimal('-1')\n        >>> ExtendedContext.compare(1, 2)\n        Decimal('-1')\n        >>> ExtendedContext.compare(Decimal(1), 2)\n        Decimal('-1')\n        >>> ExtendedContext.compare(1, Decimal(2))\n        Decimal('-1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.compare(b, context=self)\n\n    def compare_signal(self, a, b):\n        \"\"\"Compares the values of the two operands numerically.\n\n        It's pretty much like compare(), but all NaNs signal, with signaling\n        NaNs taking precedence over quiet NaNs.\n\n        >>> c = ExtendedContext\n        >>> c.compare_signal(Decimal('2.1'), Decimal('3'))\n        Decimal('-1')\n        >>> c.compare_signal(Decimal('2.1'), Decimal('2.1'))\n        Decimal('0')\n        >>> c.flags[InvalidOperation] = 0\n        >>> print c.flags[InvalidOperation]\n        0\n        >>> c.compare_signal(Decimal('NaN'), Decimal('2.1'))\n        Decimal('NaN')\n        >>> print c.flags[InvalidOperation]\n        1\n        >>> c.flags[InvalidOperation] = 0\n        >>> print c.flags[InvalidOperation]\n        0\n        >>> c.compare_signal(Decimal('sNaN'), Decimal('2.1'))\n        Decimal('NaN')\n        >>> print c.flags[InvalidOperation]\n        1\n        >>> c.compare_signal(-1, 2)\n        Decimal('-1')\n        >>> c.compare_signal(Decimal(-1), 2)\n        Decimal('-1')\n        >>> c.compare_signal(-1, Decimal(2))\n        Decimal('-1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.compare_signal(b, context=self)\n\n    def compare_total(self, a, b):\n        \"\"\"Compares two operands using their abstract representation.\n\n        This is not like the standard compare, which use their numerical\n        value. Note that a total ordering is defined for all possible abstract\n        representations.\n\n        >>> ExtendedContext.compare_total(Decimal('12.73'), Decimal('127.9'))\n        Decimal('-1')\n        >>> ExtendedContext.compare_total(Decimal('-127'),  Decimal('12'))\n        Decimal('-1')\n        >>> ExtendedContext.compare_total(Decimal('12.30'), Decimal('12.3'))\n        Decimal('-1')\n        >>> ExtendedContext.compare_total(Decimal('12.30'), Decimal('12.30'))\n        Decimal('0')\n        >>> ExtendedContext.compare_total(Decimal('12.3'),  Decimal('12.300'))\n        Decimal('1')\n        >>> ExtendedContext.compare_total(Decimal('12.3'),  Decimal('NaN'))\n        Decimal('-1')\n        >>> ExtendedContext.compare_total(1, 2)\n        Decimal('-1')\n        >>> ExtendedContext.compare_total(Decimal(1), 2)\n        Decimal('-1')\n        >>> ExtendedContext.compare_total(1, Decimal(2))\n        Decimal('-1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.compare_total(b)\n\n    def compare_total_mag(self, a, b):\n        \"\"\"Compares two operands using their abstract representation ignoring sign.\n\n        Like compare_total, but with operand's sign ignored and assumed to be 0.\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.compare_total_mag(b)\n\n    def copy_abs(self, a):\n        \"\"\"Returns a copy of the operand with the sign set to 0.\n\n        >>> ExtendedContext.copy_abs(Decimal('2.1'))\n        Decimal('2.1')\n        >>> ExtendedContext.copy_abs(Decimal('-100'))\n        Decimal('100')\n        >>> ExtendedContext.copy_abs(-1)\n        Decimal('1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.copy_abs()\n\n    def copy_decimal(self, a):\n        \"\"\"Returns a copy of the decimal object.\n\n        >>> ExtendedContext.copy_decimal(Decimal('2.1'))\n        Decimal('2.1')\n        >>> ExtendedContext.copy_decimal(Decimal('-1.00'))\n        Decimal('-1.00')\n        >>> ExtendedContext.copy_decimal(1)\n        Decimal('1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return Decimal(a)\n\n    def copy_negate(self, a):\n        \"\"\"Returns a copy of the operand with the sign inverted.\n\n        >>> ExtendedContext.copy_negate(Decimal('101.5'))\n        Decimal('-101.5')\n        >>> ExtendedContext.copy_negate(Decimal('-101.5'))\n        Decimal('101.5')\n        >>> ExtendedContext.copy_negate(1)\n        Decimal('-1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.copy_negate()\n\n    def copy_sign(self, a, b):\n        \"\"\"Copies the second operand's sign to the first one.\n\n        In detail, it returns a copy of the first operand with the sign\n        equal to the sign of the second operand.\n\n        >>> ExtendedContext.copy_sign(Decimal( '1.50'), Decimal('7.33'))\n        Decimal('1.50')\n        >>> ExtendedContext.copy_sign(Decimal('-1.50'), Decimal('7.33'))\n        Decimal('1.50')\n        >>> ExtendedContext.copy_sign(Decimal( '1.50'), Decimal('-7.33'))\n        Decimal('-1.50')\n        >>> ExtendedContext.copy_sign(Decimal('-1.50'), Decimal('-7.33'))\n        Decimal('-1.50')\n        >>> ExtendedContext.copy_sign(1, -2)\n        Decimal('-1')\n        >>> ExtendedContext.copy_sign(Decimal(1), -2)\n        Decimal('-1')\n        >>> ExtendedContext.copy_sign(1, Decimal(-2))\n        Decimal('-1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.copy_sign(b)\n\n    def divide(self, a, b):\n        \"\"\"Decimal division in a specified context.\n\n        >>> ExtendedContext.divide(Decimal('1'), Decimal('3'))\n        Decimal('0.333333333')\n        >>> ExtendedContext.divide(Decimal('2'), Decimal('3'))\n        Decimal('0.666666667')\n        >>> ExtendedContext.divide(Decimal('5'), Decimal('2'))\n        Decimal('2.5')\n        >>> ExtendedContext.divide(Decimal('1'), Decimal('10'))\n        Decimal('0.1')\n        >>> ExtendedContext.divide(Decimal('12'), Decimal('12'))\n        Decimal('1')\n        >>> ExtendedContext.divide(Decimal('8.00'), Decimal('2'))\n        Decimal('4.00')\n        >>> ExtendedContext.divide(Decimal('2.400'), Decimal('2.0'))\n        Decimal('1.20')\n        >>> ExtendedContext.divide(Decimal('1000'), Decimal('100'))\n        Decimal('10')\n        >>> ExtendedContext.divide(Decimal('1000'), Decimal('1'))\n        Decimal('1000')\n        >>> ExtendedContext.divide(Decimal('2.40E+6'), Decimal('2'))\n        Decimal('1.20E+6')\n        >>> ExtendedContext.divide(5, 5)\n        Decimal('1')\n        >>> ExtendedContext.divide(Decimal(5), 5)\n        Decimal('1')\n        >>> ExtendedContext.divide(5, Decimal(5))\n        Decimal('1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        r = a.__div__(b, context=self)\n        if r is NotImplemented:\n            raise TypeError(\"Unable to convert %s to Decimal\" % b)\n        else:\n            return r\n\n    def divide_int(self, a, b):\n        \"\"\"Divides two numbers and returns the integer part of the result.\n\n        >>> ExtendedContext.divide_int(Decimal('2'), Decimal('3'))\n        Decimal('0')\n        >>> ExtendedContext.divide_int(Decimal('10'), Decimal('3'))\n        Decimal('3')\n        >>> ExtendedContext.divide_int(Decimal('1'), Decimal('0.3'))\n        Decimal('3')\n        >>> ExtendedContext.divide_int(10, 3)\n        Decimal('3')\n        >>> ExtendedContext.divide_int(Decimal(10), 3)\n        Decimal('3')\n        >>> ExtendedContext.divide_int(10, Decimal(3))\n        Decimal('3')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        r = a.__floordiv__(b, context=self)\n        if r is NotImplemented:\n            raise TypeError(\"Unable to convert %s to Decimal\" % b)\n        else:\n            return r\n\n    def divmod(self, a, b):\n        \"\"\"Return (a // b, a % b).\n\n        >>> ExtendedContext.divmod(Decimal(8), Decimal(3))\n        (Decimal('2'), Decimal('2'))\n        >>> ExtendedContext.divmod(Decimal(8), Decimal(4))\n        (Decimal('2'), Decimal('0'))\n        >>> ExtendedContext.divmod(8, 4)\n        (Decimal('2'), Decimal('0'))\n        >>> ExtendedContext.divmod(Decimal(8), 4)\n        (Decimal('2'), Decimal('0'))\n        >>> ExtendedContext.divmod(8, Decimal(4))\n        (Decimal('2'), Decimal('0'))\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        r = a.__divmod__(b, context=self)\n        if r is NotImplemented:\n            raise TypeError(\"Unable to convert %s to Decimal\" % b)\n        else:\n            return r\n\n    def exp(self, a):\n        \"\"\"Returns e ** a.\n\n        >>> c = ExtendedContext.copy()\n        >>> c.Emin = -999\n        >>> c.Emax = 999\n        >>> c.exp(Decimal('-Infinity'))\n        Decimal('0')\n        >>> c.exp(Decimal('-1'))\n        Decimal('0.367879441')\n        >>> c.exp(Decimal('0'))\n        Decimal('1')\n        >>> c.exp(Decimal('1'))\n        Decimal('2.71828183')\n        >>> c.exp(Decimal('0.693147181'))\n        Decimal('2.00000000')\n        >>> c.exp(Decimal('+Infinity'))\n        Decimal('Infinity')\n        >>> c.exp(10)\n        Decimal('22026.4658')\n        \"\"\"\n        a =_convert_other(a, raiseit=True)\n        return a.exp(context=self)\n\n    def fma(self, a, b, c):\n        \"\"\"Returns a multiplied by b, plus c.\n\n        The first two operands are multiplied together, using multiply,\n        the third operand is then added to the result of that\n        multiplication, using add, all with only one final rounding.\n\n        >>> ExtendedContext.fma(Decimal('3'), Decimal('5'), Decimal('7'))\n        Decimal('22')\n        >>> ExtendedContext.fma(Decimal('3'), Decimal('-5'), Decimal('7'))\n        Decimal('-8')\n        >>> ExtendedContext.fma(Decimal('888565290'), Decimal('1557.96930'), Decimal('-86087.7578'))\n        Decimal('1.38435736E+12')\n        >>> ExtendedContext.fma(1, 3, 4)\n        Decimal('7')\n        >>> ExtendedContext.fma(1, Decimal(3), 4)\n        Decimal('7')\n        >>> ExtendedContext.fma(1, 3, Decimal(4))\n        Decimal('7')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.fma(b, c, context=self)\n\n    def is_canonical(self, a):\n        \"\"\"Return True if the operand is canonical; otherwise return False.\n\n        Currently, the encoding of a Decimal instance is always\n        canonical, so this method returns True for any Decimal.\n\n        >>> ExtendedContext.is_canonical(Decimal('2.50'))\n        True\n        \"\"\"\n        return a.is_canonical()\n\n    def is_finite(self, a):\n        \"\"\"Return True if the operand is finite; otherwise return False.\n\n        A Decimal instance is considered finite if it is neither\n        infinite nor a NaN.\n\n        >>> ExtendedContext.is_finite(Decimal('2.50'))\n        True\n        >>> ExtendedContext.is_finite(Decimal('-0.3'))\n        True\n        >>> ExtendedContext.is_finite(Decimal('0'))\n        True\n        >>> ExtendedContext.is_finite(Decimal('Inf'))\n        False\n        >>> ExtendedContext.is_finite(Decimal('NaN'))\n        False\n        >>> ExtendedContext.is_finite(1)\n        True\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.is_finite()\n\n    def is_infinite(self, a):\n        \"\"\"Return True if the operand is infinite; otherwise return False.\n\n        >>> ExtendedContext.is_infinite(Decimal('2.50'))\n        False\n        >>> ExtendedContext.is_infinite(Decimal('-Inf'))\n        True\n        >>> ExtendedContext.is_infinite(Decimal('NaN'))\n        False\n        >>> ExtendedContext.is_infinite(1)\n        False\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.is_infinite()\n\n    def is_nan(self, a):\n        \"\"\"Return True if the operand is a qNaN or sNaN;\n        otherwise return False.\n\n        >>> ExtendedContext.is_nan(Decimal('2.50'))\n        False\n        >>> ExtendedContext.is_nan(Decimal('NaN'))\n        True\n        >>> ExtendedContext.is_nan(Decimal('-sNaN'))\n        True\n        >>> ExtendedContext.is_nan(1)\n        False\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.is_nan()\n\n    def is_normal(self, a):\n        \"\"\"Return True if the operand is a normal number;\n        otherwise return False.\n\n        >>> c = ExtendedContext.copy()\n        >>> c.Emin = -999\n        >>> c.Emax = 999\n        >>> c.is_normal(Decimal('2.50'))\n        True\n        >>> c.is_normal(Decimal('0.1E-999'))\n        False\n        >>> c.is_normal(Decimal('0.00'))\n        False\n        >>> c.is_normal(Decimal('-Inf'))\n        False\n        >>> c.is_normal(Decimal('NaN'))\n        False\n        >>> c.is_normal(1)\n        True\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.is_normal(context=self)\n\n    def is_qnan(self, a):\n        \"\"\"Return True if the operand is a quiet NaN; otherwise return False.\n\n        >>> ExtendedContext.is_qnan(Decimal('2.50'))\n        False\n        >>> ExtendedContext.is_qnan(Decimal('NaN'))\n        True\n        >>> ExtendedContext.is_qnan(Decimal('sNaN'))\n        False\n        >>> ExtendedContext.is_qnan(1)\n        False\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.is_qnan()\n\n    def is_signed(self, a):\n        \"\"\"Return True if the operand is negative; otherwise return False.\n\n        >>> ExtendedContext.is_signed(Decimal('2.50'))\n        False\n        >>> ExtendedContext.is_signed(Decimal('-12'))\n        True\n        >>> ExtendedContext.is_signed(Decimal('-0'))\n        True\n        >>> ExtendedContext.is_signed(8)\n        False\n        >>> ExtendedContext.is_signed(-8)\n        True\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.is_signed()\n\n    def is_snan(self, a):\n        \"\"\"Return True if the operand is a signaling NaN;\n        otherwise return False.\n\n        >>> ExtendedContext.is_snan(Decimal('2.50'))\n        False\n        >>> ExtendedContext.is_snan(Decimal('NaN'))\n        False\n        >>> ExtendedContext.is_snan(Decimal('sNaN'))\n        True\n        >>> ExtendedContext.is_snan(1)\n        False\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.is_snan()\n\n    def is_subnormal(self, a):\n        \"\"\"Return True if the operand is subnormal; otherwise return False.\n\n        >>> c = ExtendedContext.copy()\n        >>> c.Emin = -999\n        >>> c.Emax = 999\n        >>> c.is_subnormal(Decimal('2.50'))\n        False\n        >>> c.is_subnormal(Decimal('0.1E-999'))\n        True\n        >>> c.is_subnormal(Decimal('0.00'))\n        False\n        >>> c.is_subnormal(Decimal('-Inf'))\n        False\n        >>> c.is_subnormal(Decimal('NaN'))\n        False\n        >>> c.is_subnormal(1)\n        False\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.is_subnormal(context=self)\n\n    def is_zero(self, a):\n        \"\"\"Return True if the operand is a zero; otherwise return False.\n\n        >>> ExtendedContext.is_zero(Decimal('0'))\n        True\n        >>> ExtendedContext.is_zero(Decimal('2.50'))\n        False\n        >>> ExtendedContext.is_zero(Decimal('-0E+2'))\n        True\n        >>> ExtendedContext.is_zero(1)\n        False\n        >>> ExtendedContext.is_zero(0)\n        True\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.is_zero()\n\n    def ln(self, a):\n        \"\"\"Returns the natural (base e) logarithm of the operand.\n\n        >>> c = ExtendedContext.copy()\n        >>> c.Emin = -999\n        >>> c.Emax = 999\n        >>> c.ln(Decimal('0'))\n        Decimal('-Infinity')\n        >>> c.ln(Decimal('1.000'))\n        Decimal('0')\n        >>> c.ln(Decimal('2.71828183'))\n        Decimal('1.00000000')\n        >>> c.ln(Decimal('10'))\n        Decimal('2.30258509')\n        >>> c.ln(Decimal('+Infinity'))\n        Decimal('Infinity')\n        >>> c.ln(1)\n        Decimal('0')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.ln(context=self)\n\n    def log10(self, a):\n        \"\"\"Returns the base 10 logarithm of the operand.\n\n        >>> c = ExtendedContext.copy()\n        >>> c.Emin = -999\n        >>> c.Emax = 999\n        >>> c.log10(Decimal('0'))\n        Decimal('-Infinity')\n        >>> c.log10(Decimal('0.001'))\n        Decimal('-3')\n        >>> c.log10(Decimal('1.000'))\n        Decimal('0')\n        >>> c.log10(Decimal('2'))\n        Decimal('0.301029996')\n        >>> c.log10(Decimal('10'))\n        Decimal('1')\n        >>> c.log10(Decimal('70'))\n        Decimal('1.84509804')\n        >>> c.log10(Decimal('+Infinity'))\n        Decimal('Infinity')\n        >>> c.log10(0)\n        Decimal('-Infinity')\n        >>> c.log10(1)\n        Decimal('0')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.log10(context=self)\n\n    def logb(self, a):\n        \"\"\" Returns the exponent of the magnitude of the operand's MSD.\n\n        The result is the integer which is the exponent of the magnitude\n        of the most significant digit of the operand (as though the\n        operand were truncated to a single digit while maintaining the\n        value of that digit and without limiting the resulting exponent).\n\n        >>> ExtendedContext.logb(Decimal('250'))\n        Decimal('2')\n        >>> ExtendedContext.logb(Decimal('2.50'))\n        Decimal('0')\n        >>> ExtendedContext.logb(Decimal('0.03'))\n        Decimal('-2')\n        >>> ExtendedContext.logb(Decimal('0'))\n        Decimal('-Infinity')\n        >>> ExtendedContext.logb(1)\n        Decimal('0')\n        >>> ExtendedContext.logb(10)\n        Decimal('1')\n        >>> ExtendedContext.logb(100)\n        Decimal('2')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.logb(context=self)\n\n    def logical_and(self, a, b):\n        \"\"\"Applies the logical operation 'and' between each operand's digits.\n\n        The operands must be both logical numbers.\n\n        >>> ExtendedContext.logical_and(Decimal('0'), Decimal('0'))\n        Decimal('0')\n        >>> ExtendedContext.logical_and(Decimal('0'), Decimal('1'))\n        Decimal('0')\n        >>> ExtendedContext.logical_and(Decimal('1'), Decimal('0'))\n        Decimal('0')\n        >>> ExtendedContext.logical_and(Decimal('1'), Decimal('1'))\n        Decimal('1')\n        >>> ExtendedContext.logical_and(Decimal('1100'), Decimal('1010'))\n        Decimal('1000')\n        >>> ExtendedContext.logical_and(Decimal('1111'), Decimal('10'))\n        Decimal('10')\n        >>> ExtendedContext.logical_and(110, 1101)\n        Decimal('100')\n        >>> ExtendedContext.logical_and(Decimal(110), 1101)\n        Decimal('100')\n        >>> ExtendedContext.logical_and(110, Decimal(1101))\n        Decimal('100')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.logical_and(b, context=self)\n\n    def logical_invert(self, a):\n        \"\"\"Invert all the digits in the operand.\n\n        The operand must be a logical number.\n\n        >>> ExtendedContext.logical_invert(Decimal('0'))\n        Decimal('111111111')\n        >>> ExtendedContext.logical_invert(Decimal('1'))\n        Decimal('111111110')\n        >>> ExtendedContext.logical_invert(Decimal('111111111'))\n        Decimal('0')\n        >>> ExtendedContext.logical_invert(Decimal('101010101'))\n        Decimal('10101010')\n        >>> ExtendedContext.logical_invert(1101)\n        Decimal('111110010')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.logical_invert(context=self)\n\n    def logical_or(self, a, b):\n        \"\"\"Applies the logical operation 'or' between each operand's digits.\n\n        The operands must be both logical numbers.\n\n        >>> ExtendedContext.logical_or(Decimal('0'), Decimal('0'))\n        Decimal('0')\n        >>> ExtendedContext.logical_or(Decimal('0'), Decimal('1'))\n        Decimal('1')\n        >>> ExtendedContext.logical_or(Decimal('1'), Decimal('0'))\n        Decimal('1')\n        >>> ExtendedContext.logical_or(Decimal('1'), Decimal('1'))\n        Decimal('1')\n        >>> ExtendedContext.logical_or(Decimal('1100'), Decimal('1010'))\n        Decimal('1110')\n        >>> ExtendedContext.logical_or(Decimal('1110'), Decimal('10'))\n        Decimal('1110')\n        >>> ExtendedContext.logical_or(110, 1101)\n        Decimal('1111')\n        >>> ExtendedContext.logical_or(Decimal(110), 1101)\n        Decimal('1111')\n        >>> ExtendedContext.logical_or(110, Decimal(1101))\n        Decimal('1111')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.logical_or(b, context=self)\n\n    def logical_xor(self, a, b):\n        \"\"\"Applies the logical operation 'xor' between each operand's digits.\n\n        The operands must be both logical numbers.\n\n        >>> ExtendedContext.logical_xor(Decimal('0'), Decimal('0'))\n        Decimal('0')\n        >>> ExtendedContext.logical_xor(Decimal('0'), Decimal('1'))\n        Decimal('1')\n        >>> ExtendedContext.logical_xor(Decimal('1'), Decimal('0'))\n        Decimal('1')\n        >>> ExtendedContext.logical_xor(Decimal('1'), Decimal('1'))\n        Decimal('0')\n        >>> ExtendedContext.logical_xor(Decimal('1100'), Decimal('1010'))\n        Decimal('110')\n        >>> ExtendedContext.logical_xor(Decimal('1111'), Decimal('10'))\n        Decimal('1101')\n        >>> ExtendedContext.logical_xor(110, 1101)\n        Decimal('1011')\n        >>> ExtendedContext.logical_xor(Decimal(110), 1101)\n        Decimal('1011')\n        >>> ExtendedContext.logical_xor(110, Decimal(1101))\n        Decimal('1011')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.logical_xor(b, context=self)\n\n    def max(self, a, b):\n        \"\"\"max compares two values numerically and returns the maximum.\n\n        If either operand is a NaN then the general rules apply.\n        Otherwise, the operands are compared as though by the compare\n        operation.  If they are numerically equal then the left-hand operand\n        is chosen as the result.  Otherwise the maximum (closer to positive\n        infinity) of the two operands is chosen as the result.\n\n        >>> ExtendedContext.max(Decimal('3'), Decimal('2'))\n        Decimal('3')\n        >>> ExtendedContext.max(Decimal('-10'), Decimal('3'))\n        Decimal('3')\n        >>> ExtendedContext.max(Decimal('1.0'), Decimal('1'))\n        Decimal('1')\n        >>> ExtendedContext.max(Decimal('7'), Decimal('NaN'))\n        Decimal('7')\n        >>> ExtendedContext.max(1, 2)\n        Decimal('2')\n        >>> ExtendedContext.max(Decimal(1), 2)\n        Decimal('2')\n        >>> ExtendedContext.max(1, Decimal(2))\n        Decimal('2')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.max(b, context=self)\n\n    def max_mag(self, a, b):\n        \"\"\"Compares the values numerically with their sign ignored.\n\n        >>> ExtendedContext.max_mag(Decimal('7'), Decimal('NaN'))\n        Decimal('7')\n        >>> ExtendedContext.max_mag(Decimal('7'), Decimal('-10'))\n        Decimal('-10')\n        >>> ExtendedContext.max_mag(1, -2)\n        Decimal('-2')\n        >>> ExtendedContext.max_mag(Decimal(1), -2)\n        Decimal('-2')\n        >>> ExtendedContext.max_mag(1, Decimal(-2))\n        Decimal('-2')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.max_mag(b, context=self)\n\n    def min(self, a, b):\n        \"\"\"min compares two values numerically and returns the minimum.\n\n        If either operand is a NaN then the general rules apply.\n        Otherwise, the operands are compared as though by the compare\n        operation.  If they are numerically equal then the left-hand operand\n        is chosen as the result.  Otherwise the minimum (closer to negative\n        infinity) of the two operands is chosen as the result.\n\n        >>> ExtendedContext.min(Decimal('3'), Decimal('2'))\n        Decimal('2')\n        >>> ExtendedContext.min(Decimal('-10'), Decimal('3'))\n        Decimal('-10')\n        >>> ExtendedContext.min(Decimal('1.0'), Decimal('1'))\n        Decimal('1.0')\n        >>> ExtendedContext.min(Decimal('7'), Decimal('NaN'))\n        Decimal('7')\n        >>> ExtendedContext.min(1, 2)\n        Decimal('1')\n        >>> ExtendedContext.min(Decimal(1), 2)\n        Decimal('1')\n        >>> ExtendedContext.min(1, Decimal(29))\n        Decimal('1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.min(b, context=self)\n\n    def min_mag(self, a, b):\n        \"\"\"Compares the values numerically with their sign ignored.\n\n        >>> ExtendedContext.min_mag(Decimal('3'), Decimal('-2'))\n        Decimal('-2')\n        >>> ExtendedContext.min_mag(Decimal('-3'), Decimal('NaN'))\n        Decimal('-3')\n        >>> ExtendedContext.min_mag(1, -2)\n        Decimal('1')\n        >>> ExtendedContext.min_mag(Decimal(1), -2)\n        Decimal('1')\n        >>> ExtendedContext.min_mag(1, Decimal(-2))\n        Decimal('1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.min_mag(b, context=self)\n\n    def minus(self, a):\n        \"\"\"Minus corresponds to unary prefix minus in Python.\n\n        The operation is evaluated using the same rules as subtract; the\n        operation minus(a) is calculated as subtract('0', a) where the '0'\n        has the same exponent as the operand.\n\n        >>> ExtendedContext.minus(Decimal('1.3'))\n        Decimal('-1.3')\n        >>> ExtendedContext.minus(Decimal('-1.3'))\n        Decimal('1.3')\n        >>> ExtendedContext.minus(1)\n        Decimal('-1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.__neg__(context=self)\n\n    def multiply(self, a, b):\n        \"\"\"multiply multiplies two operands.\n\n        If either operand is a special value then the general rules apply.\n        Otherwise, the operands are multiplied together\n        ('long multiplication'), resulting in a number which may be as long as\n        the sum of the lengths of the two operands.\n\n        >>> ExtendedContext.multiply(Decimal('1.20'), Decimal('3'))\n        Decimal('3.60')\n        >>> ExtendedContext.multiply(Decimal('7'), Decimal('3'))\n        Decimal('21')\n        >>> ExtendedContext.multiply(Decimal('0.9'), Decimal('0.8'))\n        Decimal('0.72')\n        >>> ExtendedContext.multiply(Decimal('0.9'), Decimal('-0'))\n        Decimal('-0.0')\n        >>> ExtendedContext.multiply(Decimal('654321'), Decimal('654321'))\n        Decimal('4.28135971E+11')\n        >>> ExtendedContext.multiply(7, 7)\n        Decimal('49')\n        >>> ExtendedContext.multiply(Decimal(7), 7)\n        Decimal('49')\n        >>> ExtendedContext.multiply(7, Decimal(7))\n        Decimal('49')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        r = a.__mul__(b, context=self)\n        if r is NotImplemented:\n            raise TypeError(\"Unable to convert %s to Decimal\" % b)\n        else:\n            return r\n\n    def next_minus(self, a):\n        \"\"\"Returns the largest representable number smaller than a.\n\n        >>> c = ExtendedContext.copy()\n        >>> c.Emin = -999\n        >>> c.Emax = 999\n        >>> ExtendedContext.next_minus(Decimal('1'))\n        Decimal('0.999999999')\n        >>> c.next_minus(Decimal('1E-1007'))\n        Decimal('0E-1007')\n        >>> ExtendedContext.next_minus(Decimal('-1.00000003'))\n        Decimal('-1.00000004')\n        >>> c.next_minus(Decimal('Infinity'))\n        Decimal('9.99999999E+999')\n        >>> c.next_minus(1)\n        Decimal('0.999999999')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.next_minus(context=self)\n\n    def next_plus(self, a):\n        \"\"\"Returns the smallest representable number larger than a.\n\n        >>> c = ExtendedContext.copy()\n        >>> c.Emin = -999\n        >>> c.Emax = 999\n        >>> ExtendedContext.next_plus(Decimal('1'))\n        Decimal('1.00000001')\n        >>> c.next_plus(Decimal('-1E-1007'))\n        Decimal('-0E-1007')\n        >>> ExtendedContext.next_plus(Decimal('-1.00000003'))\n        Decimal('-1.00000002')\n        >>> c.next_plus(Decimal('-Infinity'))\n        Decimal('-9.99999999E+999')\n        >>> c.next_plus(1)\n        Decimal('1.00000001')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.next_plus(context=self)\n\n    def next_toward(self, a, b):\n        \"\"\"Returns the number closest to a, in direction towards b.\n\n        The result is the closest representable number from the first\n        operand (but not the first operand) that is in the direction\n        towards the second operand, unless the operands have the same\n        value.\n\n        >>> c = ExtendedContext.copy()\n        >>> c.Emin = -999\n        >>> c.Emax = 999\n        >>> c.next_toward(Decimal('1'), Decimal('2'))\n        Decimal('1.00000001')\n        >>> c.next_toward(Decimal('-1E-1007'), Decimal('1'))\n        Decimal('-0E-1007')\n        >>> c.next_toward(Decimal('-1.00000003'), Decimal('0'))\n        Decimal('-1.00000002')\n        >>> c.next_toward(Decimal('1'), Decimal('0'))\n        Decimal('0.999999999')\n        >>> c.next_toward(Decimal('1E-1007'), Decimal('-100'))\n        Decimal('0E-1007')\n        >>> c.next_toward(Decimal('-1.00000003'), Decimal('-10'))\n        Decimal('-1.00000004')\n        >>> c.next_toward(Decimal('0.00'), Decimal('-0.0000'))\n        Decimal('-0.00')\n        >>> c.next_toward(0, 1)\n        Decimal('1E-1007')\n        >>> c.next_toward(Decimal(0), 1)\n        Decimal('1E-1007')\n        >>> c.next_toward(0, Decimal(1))\n        Decimal('1E-1007')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.next_toward(b, context=self)\n\n    def normalize(self, a):\n        \"\"\"normalize reduces an operand to its simplest form.\n\n        Essentially a plus operation with all trailing zeros removed from the\n        result.\n\n        >>> ExtendedContext.normalize(Decimal('2.1'))\n        Decimal('2.1')\n        >>> ExtendedContext.normalize(Decimal('-2.0'))\n        Decimal('-2')\n        >>> ExtendedContext.normalize(Decimal('1.200'))\n        Decimal('1.2')\n        >>> ExtendedContext.normalize(Decimal('-120'))\n        Decimal('-1.2E+2')\n        >>> ExtendedContext.normalize(Decimal('120.00'))\n        Decimal('1.2E+2')\n        >>> ExtendedContext.normalize(Decimal('0.00'))\n        Decimal('0')\n        >>> ExtendedContext.normalize(6)\n        Decimal('6')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.normalize(context=self)\n\n    def number_class(self, a):\n        \"\"\"Returns an indication of the class of the operand.\n\n        The class is one of the following strings:\n          -sNaN\n          -NaN\n          -Infinity\n          -Normal\n          -Subnormal\n          -Zero\n          +Zero\n          +Subnormal\n          +Normal\n          +Infinity\n\n        >>> c = Context(ExtendedContext)\n        >>> c.Emin = -999\n        >>> c.Emax = 999\n        >>> c.number_class(Decimal('Infinity'))\n        '+Infinity'\n        >>> c.number_class(Decimal('1E-10'))\n        '+Normal'\n        >>> c.number_class(Decimal('2.50'))\n        '+Normal'\n        >>> c.number_class(Decimal('0.1E-999'))\n        '+Subnormal'\n        >>> c.number_class(Decimal('0'))\n        '+Zero'\n        >>> c.number_class(Decimal('-0'))\n        '-Zero'\n        >>> c.number_class(Decimal('-0.1E-999'))\n        '-Subnormal'\n        >>> c.number_class(Decimal('-1E-10'))\n        '-Normal'\n        >>> c.number_class(Decimal('-2.50'))\n        '-Normal'\n        >>> c.number_class(Decimal('-Infinity'))\n        '-Infinity'\n        >>> c.number_class(Decimal('NaN'))\n        'NaN'\n        >>> c.number_class(Decimal('-NaN'))\n        'NaN'\n        >>> c.number_class(Decimal('sNaN'))\n        'sNaN'\n        >>> c.number_class(123)\n        '+Normal'\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.number_class(context=self)\n\n    def plus(self, a):\n        \"\"\"Plus corresponds to unary prefix plus in Python.\n\n        The operation is evaluated using the same rules as add; the\n        operation plus(a) is calculated as add('0', a) where the '0'\n        has the same exponent as the operand.\n\n        >>> ExtendedContext.plus(Decimal('1.3'))\n        Decimal('1.3')\n        >>> ExtendedContext.plus(Decimal('-1.3'))\n        Decimal('-1.3')\n        >>> ExtendedContext.plus(-1)\n        Decimal('-1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.__pos__(context=self)\n\n    def power(self, a, b, modulo=None):\n        \"\"\"Raises a to the power of b, to modulo if given.\n\n        With two arguments, compute a**b.  If a is negative then b\n        must be integral.  The result will be inexact unless b is\n        integral and the result is finite and can be expressed exactly\n        in 'precision' digits.\n\n        With three arguments, compute (a**b) % modulo.  For the\n        three argument form, the following restrictions on the\n        arguments hold:\n\n         - all three arguments must be integral\n         - b must be nonnegative\n         - at least one of a or b must be nonzero\n         - modulo must be nonzero and have at most 'precision' digits\n\n        The result of pow(a, b, modulo) is identical to the result\n        that would be obtained by computing (a**b) % modulo with\n        unbounded precision, but is computed more efficiently.  It is\n        always exact.\n\n        >>> c = ExtendedContext.copy()\n        >>> c.Emin = -999\n        >>> c.Emax = 999\n        >>> c.power(Decimal('2'), Decimal('3'))\n        Decimal('8')\n        >>> c.power(Decimal('-2'), Decimal('3'))\n        Decimal('-8')\n        >>> c.power(Decimal('2'), Decimal('-3'))\n        Decimal('0.125')\n        >>> c.power(Decimal('1.7'), Decimal('8'))\n        Decimal('69.7575744')\n        >>> c.power(Decimal('10'), Decimal('0.301029996'))\n        Decimal('2.00000000')\n        >>> c.power(Decimal('Infinity'), Decimal('-1'))\n        Decimal('0')\n        >>> c.power(Decimal('Infinity'), Decimal('0'))\n        Decimal('1')\n        >>> c.power(Decimal('Infinity'), Decimal('1'))\n        Decimal('Infinity')\n        >>> c.power(Decimal('-Infinity'), Decimal('-1'))\n        Decimal('-0')\n        >>> c.power(Decimal('-Infinity'), Decimal('0'))\n        Decimal('1')\n        >>> c.power(Decimal('-Infinity'), Decimal('1'))\n        Decimal('-Infinity')\n        >>> c.power(Decimal('-Infinity'), Decimal('2'))\n        Decimal('Infinity')\n        >>> c.power(Decimal('0'), Decimal('0'))\n        Decimal('NaN')\n\n        >>> c.power(Decimal('3'), Decimal('7'), Decimal('16'))\n        Decimal('11')\n        >>> c.power(Decimal('-3'), Decimal('7'), Decimal('16'))\n        Decimal('-11')\n        >>> c.power(Decimal('-3'), Decimal('8'), Decimal('16'))\n        Decimal('1')\n        >>> c.power(Decimal('3'), Decimal('7'), Decimal('-16'))\n        Decimal('11')\n        >>> c.power(Decimal('23E12345'), Decimal('67E189'), Decimal('123456789'))\n        Decimal('11729830')\n        >>> c.power(Decimal('-0'), Decimal('17'), Decimal('1729'))\n        Decimal('-0')\n        >>> c.power(Decimal('-23'), Decimal('0'), Decimal('65537'))\n        Decimal('1')\n        >>> ExtendedContext.power(7, 7)\n        Decimal('823543')\n        >>> ExtendedContext.power(Decimal(7), 7)\n        Decimal('823543')\n        >>> ExtendedContext.power(7, Decimal(7), 2)\n        Decimal('1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        r = a.__pow__(b, modulo, context=self)\n        if r is NotImplemented:\n            raise TypeError(\"Unable to convert %s to Decimal\" % b)\n        else:\n            return r\n\n    def quantize(self, a, b):\n        \"\"\"Returns a value equal to 'a' (rounded), having the exponent of 'b'.\n\n        The coefficient of the result is derived from that of the left-hand\n        operand.  It may be rounded using the current rounding setting (if the\n        exponent is being increased), multiplied by a positive power of ten (if\n        the exponent is being decreased), or is unchanged (if the exponent is\n        already equal to that of the right-hand operand).\n\n        Unlike other operations, if the length of the coefficient after the\n        quantize operation would be greater than precision then an Invalid\n        operation condition is raised.  This guarantees that, unless there is\n        an error condition, the exponent of the result of a quantize is always\n        equal to that of the right-hand operand.\n\n        Also unlike other operations, quantize will never raise Underflow, even\n        if the result is subnormal and inexact.\n\n        >>> ExtendedContext.quantize(Decimal('2.17'), Decimal('0.001'))\n        Decimal('2.170')\n        >>> ExtendedContext.quantize(Decimal('2.17'), Decimal('0.01'))\n        Decimal('2.17')\n        >>> ExtendedContext.quantize(Decimal('2.17'), Decimal('0.1'))\n        Decimal('2.2')\n        >>> ExtendedContext.quantize(Decimal('2.17'), Decimal('1e+0'))\n        Decimal('2')\n        >>> ExtendedContext.quantize(Decimal('2.17'), Decimal('1e+1'))\n        Decimal('0E+1')\n        >>> ExtendedContext.quantize(Decimal('-Inf'), Decimal('Infinity'))\n        Decimal('-Infinity')\n        >>> ExtendedContext.quantize(Decimal('2'), Decimal('Infinity'))\n        Decimal('NaN')\n        >>> ExtendedContext.quantize(Decimal('-0.1'), Decimal('1'))\n        Decimal('-0')\n        >>> ExtendedContext.quantize(Decimal('-0'), Decimal('1e+5'))\n        Decimal('-0E+5')\n        >>> ExtendedContext.quantize(Decimal('+35236450.6'), Decimal('1e-2'))\n        Decimal('NaN')\n        >>> ExtendedContext.quantize(Decimal('-35236450.6'), Decimal('1e-2'))\n        Decimal('NaN')\n        >>> ExtendedContext.quantize(Decimal('217'), Decimal('1e-1'))\n        Decimal('217.0')\n        >>> ExtendedContext.quantize(Decimal('217'), Decimal('1e-0'))\n        Decimal('217')\n        >>> ExtendedContext.quantize(Decimal('217'), Decimal('1e+1'))\n        Decimal('2.2E+2')\n        >>> ExtendedContext.quantize(Decimal('217'), Decimal('1e+2'))\n        Decimal('2E+2')\n        >>> ExtendedContext.quantize(1, 2)\n        Decimal('1')\n        >>> ExtendedContext.quantize(Decimal(1), 2)\n        Decimal('1')\n        >>> ExtendedContext.quantize(1, Decimal(2))\n        Decimal('1')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.quantize(b, context=self)\n\n    def radix(self):\n        \"\"\"Just returns 10, as this is Decimal, :)\n\n        >>> ExtendedContext.radix()\n        Decimal('10')\n        \"\"\"\n        return Decimal(10)\n\n    def remainder(self, a, b):\n        \"\"\"Returns the remainder from integer division.\n\n        The result is the residue of the dividend after the operation of\n        calculating integer division as described for divide-integer, rounded\n        to precision digits if necessary.  The sign of the result, if\n        non-zero, is the same as that of the original dividend.\n\n        This operation will fail under the same conditions as integer division\n        (that is, if integer division on the same two operands would fail, the\n        remainder cannot be calculated).\n\n        >>> ExtendedContext.remainder(Decimal('2.1'), Decimal('3'))\n        Decimal('2.1')\n        >>> ExtendedContext.remainder(Decimal('10'), Decimal('3'))\n        Decimal('1')\n        >>> ExtendedContext.remainder(Decimal('-10'), Decimal('3'))\n        Decimal('-1')\n        >>> ExtendedContext.remainder(Decimal('10.2'), Decimal('1'))\n        Decimal('0.2')\n        >>> ExtendedContext.remainder(Decimal('10'), Decimal('0.3'))\n        Decimal('0.1')\n        >>> ExtendedContext.remainder(Decimal('3.6'), Decimal('1.3'))\n        Decimal('1.0')\n        >>> ExtendedContext.remainder(22, 6)\n        Decimal('4')\n        >>> ExtendedContext.remainder(Decimal(22), 6)\n        Decimal('4')\n        >>> ExtendedContext.remainder(22, Decimal(6))\n        Decimal('4')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        r = a.__mod__(b, context=self)\n        if r is NotImplemented:\n            raise TypeError(\"Unable to convert %s to Decimal\" % b)\n        else:\n            return r\n\n    def remainder_near(self, a, b):\n        \"\"\"Returns to be \"a - b * n\", where n is the integer nearest the exact\n        value of \"x / b\" (if two integers are equally near then the even one\n        is chosen).  If the result is equal to 0 then its sign will be the\n        sign of a.\n\n        This operation will fail under the same conditions as integer division\n        (that is, if integer division on the same two operands would fail, the\n        remainder cannot be calculated).\n\n        >>> ExtendedContext.remainder_near(Decimal('2.1'), Decimal('3'))\n        Decimal('-0.9')\n        >>> ExtendedContext.remainder_near(Decimal('10'), Decimal('6'))\n        Decimal('-2')\n        >>> ExtendedContext.remainder_near(Decimal('10'), Decimal('3'))\n        Decimal('1')\n        >>> ExtendedContext.remainder_near(Decimal('-10'), Decimal('3'))\n        Decimal('-1')\n        >>> ExtendedContext.remainder_near(Decimal('10.2'), Decimal('1'))\n        Decimal('0.2')\n        >>> ExtendedContext.remainder_near(Decimal('10'), Decimal('0.3'))\n        Decimal('0.1')\n        >>> ExtendedContext.remainder_near(Decimal('3.6'), Decimal('1.3'))\n        Decimal('-0.3')\n        >>> ExtendedContext.remainder_near(3, 11)\n        Decimal('3')\n        >>> ExtendedContext.remainder_near(Decimal(3), 11)\n        Decimal('3')\n        >>> ExtendedContext.remainder_near(3, Decimal(11))\n        Decimal('3')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.remainder_near(b, context=self)\n\n    def rotate(self, a, b):\n        \"\"\"Returns a rotated copy of a, b times.\n\n        The coefficient of the result is a rotated copy of the digits in\n        the coefficient of the first operand.  The number of places of\n        rotation is taken from the absolute value of the second operand,\n        with the rotation being to the left if the second operand is\n        positive or to the right otherwise.\n\n        >>> ExtendedContext.rotate(Decimal('34'), Decimal('8'))\n        Decimal('400000003')\n        >>> ExtendedContext.rotate(Decimal('12'), Decimal('9'))\n        Decimal('12')\n        >>> ExtendedContext.rotate(Decimal('123456789'), Decimal('-2'))\n        Decimal('891234567')\n        >>> ExtendedContext.rotate(Decimal('123456789'), Decimal('0'))\n        Decimal('123456789')\n        >>> ExtendedContext.rotate(Decimal('123456789'), Decimal('+2'))\n        Decimal('345678912')\n        >>> ExtendedContext.rotate(1333333, 1)\n        Decimal('13333330')\n        >>> ExtendedContext.rotate(Decimal(1333333), 1)\n        Decimal('13333330')\n        >>> ExtendedContext.rotate(1333333, Decimal(1))\n        Decimal('13333330')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.rotate(b, context=self)\n\n    def same_quantum(self, a, b):\n        \"\"\"Returns True if the two operands have the same exponent.\n\n        The result is never affected by either the sign or the coefficient of\n        either operand.\n\n        >>> ExtendedContext.same_quantum(Decimal('2.17'), Decimal('0.001'))\n        False\n        >>> ExtendedContext.same_quantum(Decimal('2.17'), Decimal('0.01'))\n        True\n        >>> ExtendedContext.same_quantum(Decimal('2.17'), Decimal('1'))\n        False\n        >>> ExtendedContext.same_quantum(Decimal('Inf'), Decimal('-Inf'))\n        True\n        >>> ExtendedContext.same_quantum(10000, -1)\n        True\n        >>> ExtendedContext.same_quantum(Decimal(10000), -1)\n        True\n        >>> ExtendedContext.same_quantum(10000, Decimal(-1))\n        True\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.same_quantum(b)\n\n    def scaleb (self, a, b):\n        \"\"\"Returns the first operand after adding the second value its exp.\n\n        >>> ExtendedContext.scaleb(Decimal('7.50'), Decimal('-2'))\n        Decimal('0.0750')\n        >>> ExtendedContext.scaleb(Decimal('7.50'), Decimal('0'))\n        Decimal('7.50')\n        >>> ExtendedContext.scaleb(Decimal('7.50'), Decimal('3'))\n        Decimal('7.50E+3')\n        >>> ExtendedContext.scaleb(1, 4)\n        Decimal('1E+4')\n        >>> ExtendedContext.scaleb(Decimal(1), 4)\n        Decimal('1E+4')\n        >>> ExtendedContext.scaleb(1, Decimal(4))\n        Decimal('1E+4')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.scaleb(b, context=self)\n\n    def shift(self, a, b):\n        \"\"\"Returns a shifted copy of a, b times.\n\n        The coefficient of the result is a shifted copy of the digits\n        in the coefficient of the first operand.  The number of places\n        to shift is taken from the absolute value of the second operand,\n        with the shift being to the left if the second operand is\n        positive or to the right otherwise.  Digits shifted into the\n        coefficient are zeros.\n\n        >>> ExtendedContext.shift(Decimal('34'), Decimal('8'))\n        Decimal('400000000')\n        >>> ExtendedContext.shift(Decimal('12'), Decimal('9'))\n        Decimal('0')\n        >>> ExtendedContext.shift(Decimal('123456789'), Decimal('-2'))\n        Decimal('1234567')\n        >>> ExtendedContext.shift(Decimal('123456789'), Decimal('0'))\n        Decimal('123456789')\n        >>> ExtendedContext.shift(Decimal('123456789'), Decimal('+2'))\n        Decimal('345678900')\n        >>> ExtendedContext.shift(88888888, 2)\n        Decimal('888888800')\n        >>> ExtendedContext.shift(Decimal(88888888), 2)\n        Decimal('888888800')\n        >>> ExtendedContext.shift(88888888, Decimal(2))\n        Decimal('888888800')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.shift(b, context=self)\n\n    def sqrt(self, a):\n        \"\"\"Square root of a non-negative number to context precision.\n\n        If the result must be inexact, it is rounded using the round-half-even\n        algorithm.\n\n        >>> ExtendedContext.sqrt(Decimal('0'))\n        Decimal('0')\n        >>> ExtendedContext.sqrt(Decimal('-0'))\n        Decimal('-0')\n        >>> ExtendedContext.sqrt(Decimal('0.39'))\n        Decimal('0.624499800')\n        >>> ExtendedContext.sqrt(Decimal('100'))\n        Decimal('10')\n        >>> ExtendedContext.sqrt(Decimal('1'))\n        Decimal('1')\n        >>> ExtendedContext.sqrt(Decimal('1.0'))\n        Decimal('1.0')\n        >>> ExtendedContext.sqrt(Decimal('1.00'))\n        Decimal('1.0')\n        >>> ExtendedContext.sqrt(Decimal('7'))\n        Decimal('2.64575131')\n        >>> ExtendedContext.sqrt(Decimal('10'))\n        Decimal('3.16227766')\n        >>> ExtendedContext.sqrt(2)\n        Decimal('1.41421356')\n        >>> ExtendedContext.prec\n        9\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.sqrt(context=self)\n\n    def subtract(self, a, b):\n        \"\"\"Return the difference between the two operands.\n\n        >>> ExtendedContext.subtract(Decimal('1.3'), Decimal('1.07'))\n        Decimal('0.23')\n        >>> ExtendedContext.subtract(Decimal('1.3'), Decimal('1.30'))\n        Decimal('0.00')\n        >>> ExtendedContext.subtract(Decimal('1.3'), Decimal('2.07'))\n        Decimal('-0.77')\n        >>> ExtendedContext.subtract(8, 5)\n        Decimal('3')\n        >>> ExtendedContext.subtract(Decimal(8), 5)\n        Decimal('3')\n        >>> ExtendedContext.subtract(8, Decimal(5))\n        Decimal('3')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        r = a.__sub__(b, context=self)\n        if r is NotImplemented:\n            raise TypeError(\"Unable to convert %s to Decimal\" % b)\n        else:\n            return r\n\n    def to_eng_string(self, a):\n        \"\"\"Converts a number to a string, using scientific notation.\n\n        The operation is not affected by the context.\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.to_eng_string(context=self)\n\n    def to_sci_string(self, a):\n        \"\"\"Converts a number to a string, using scientific notation.\n\n        The operation is not affected by the context.\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.__str__(context=self)\n\n    def to_integral_exact(self, a):\n        \"\"\"Rounds to an integer.\n\n        When the operand has a negative exponent, the result is the same\n        as using the quantize() operation using the given operand as the\n        left-hand-operand, 1E+0 as the right-hand-operand, and the precision\n        of the operand as the precision setting; Inexact and Rounded flags\n        are allowed in this operation.  The rounding mode is taken from the\n        context.\n\n        >>> ExtendedContext.to_integral_exact(Decimal('2.1'))\n        Decimal('2')\n        >>> ExtendedContext.to_integral_exact(Decimal('100'))\n        Decimal('100')\n        >>> ExtendedContext.to_integral_exact(Decimal('100.0'))\n        Decimal('100')\n        >>> ExtendedContext.to_integral_exact(Decimal('101.5'))\n        Decimal('102')\n        >>> ExtendedContext.to_integral_exact(Decimal('-101.5'))\n        Decimal('-102')\n        >>> ExtendedContext.to_integral_exact(Decimal('10E+5'))\n        Decimal('1.0E+6')\n        >>> ExtendedContext.to_integral_exact(Decimal('7.89E+77'))\n        Decimal('7.89E+77')\n        >>> ExtendedContext.to_integral_exact(Decimal('-Inf'))\n        Decimal('-Infinity')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.to_integral_exact(context=self)\n\n    def to_integral_value(self, a):\n        \"\"\"Rounds to an integer.\n\n        When the operand has a negative exponent, the result is the same\n        as using the quantize() operation using the given operand as the\n        left-hand-operand, 1E+0 as the right-hand-operand, and the precision\n        of the operand as the precision setting, except that no flags will\n        be set.  The rounding mode is taken from the context.\n\n        >>> ExtendedContext.to_integral_value(Decimal('2.1'))\n        Decimal('2')\n        >>> ExtendedContext.to_integral_value(Decimal('100'))\n        Decimal('100')\n        >>> ExtendedContext.to_integral_value(Decimal('100.0'))\n        Decimal('100')\n        >>> ExtendedContext.to_integral_value(Decimal('101.5'))\n        Decimal('102')\n        >>> ExtendedContext.to_integral_value(Decimal('-101.5'))\n        Decimal('-102')\n        >>> ExtendedContext.to_integral_value(Decimal('10E+5'))\n        Decimal('1.0E+6')\n        >>> ExtendedContext.to_integral_value(Decimal('7.89E+77'))\n        Decimal('7.89E+77')\n        >>> ExtendedContext.to_integral_value(Decimal('-Inf'))\n        Decimal('-Infinity')\n        \"\"\"\n        a = _convert_other(a, raiseit=True)\n        return a.to_integral_value(context=self)\n\n    # the method name changed, but we provide also the old one, for compatibility\n    to_integral = to_integral_value\n\nclass _WorkRep(object):\n    __slots__ = ('sign','int','exp')\n    # sign: 0 or 1\n    # int:  int or long\n    # exp:  None, int, or string\n\n    def __init__(self, value=None):\n        if value is None:\n            self.sign = None\n            self.int = 0\n            self.exp = None\n        elif isinstance(value, Decimal):\n            self.sign = value._sign\n            self.int = int(value._int)\n            self.exp = value._exp\n        else:\n            # assert isinstance(value, tuple)\n            self.sign = value[0]\n            self.int = value[1]\n            self.exp = value[2]\n\n    def __repr__(self):\n        return \"(%r, %r, %r)\" % (self.sign, self.int, self.exp)\n\n    __str__ = __repr__\n\n\n\ndef _normalize(op1, op2, prec = 0):\n    \"\"\"Normalizes op1, op2 to have the same exp and length of coefficient.\n\n    Done during addition.\n    \"\"\"\n    if op1.exp < op2.exp:\n        tmp = op2\n        other = op1\n    else:\n        tmp = op1\n        other = op2\n\n    # Let exp = min(tmp.exp - 1, tmp.adjusted() - precision - 1).\n    # Then adding 10**exp to tmp has the same effect (after rounding)\n    # as adding any positive quantity smaller than 10**exp; similarly\n    # for subtraction.  So if other is smaller than 10**exp we replace\n    # it with 10**exp.  This avoids tmp.exp - other.exp getting too large.\n    tmp_len = len(str(tmp.int))\n    other_len = len(str(other.int))\n    exp = tmp.exp + min(-1, tmp_len - prec - 2)\n    if other_len + other.exp - 1 < exp:\n        other.int = 1\n        other.exp = exp\n\n    tmp.int *= 10 ** (tmp.exp - other.exp)\n    tmp.exp = other.exp\n    return op1, op2\n\n##### Integer arithmetic functions used by ln, log10, exp and __pow__ #####\n\n# This function from Tim Peters was taken from here:\n# http://mail.python.org/pipermail/python-list/1999-July/007758.html\n# The correction being in the function definition is for speed, and\n# the whole function is not resolved with math.log because of avoiding\n# the use of floats.\ndef _nbits(n, correction = {\n        '0': 4, '1': 3, '2': 2, '3': 2,\n        '4': 1, '5': 1, '6': 1, '7': 1,\n        '8': 0, '9': 0, 'a': 0, 'b': 0,\n        'c': 0, 'd': 0, 'e': 0, 'f': 0}):\n    \"\"\"Number of bits in binary representation of the positive integer n,\n    or 0 if n == 0.\n    \"\"\"\n    if n < 0:\n        raise ValueError(\"The argument to _nbits should be nonnegative.\")\n    hex_n = \"%x\" % n\n    return 4*len(hex_n) - correction[hex_n[0]]\n\ndef _decimal_lshift_exact(n, e):\n    \"\"\" Given integers n and e, return n * 10**e if it's an integer, else None.\n\n    The computation is designed to avoid computing large powers of 10\n    unnecessarily.\n\n    >>> _decimal_lshift_exact(3, 4)\n    30000\n    >>> _decimal_lshift_exact(300, -999999999)  # returns None\n\n    \"\"\"\n    if n == 0:\n        return 0\n    elif e >= 0:\n        return n * 10**e\n    else:\n        # val_n = largest power of 10 dividing n.\n        str_n = str(abs(n))\n        val_n = len(str_n) - len(str_n.rstrip('0'))\n        return None if val_n < -e else n // 10**-e\n\ndef _sqrt_nearest(n, a):\n    \"\"\"Closest integer to the square root of the positive integer n.  a is\n    an initial approximation to the square root.  Any positive integer\n    will do for a, but the closer a is to the square root of n the\n    faster convergence will be.\n\n    \"\"\"\n    if n <= 0 or a <= 0:\n        raise ValueError(\"Both arguments to _sqrt_nearest should be positive.\")\n\n    b=0\n    while a != b:\n        b, a = a, a--n//a>>1\n    return a\n\ndef _rshift_nearest(x, shift):\n    \"\"\"Given an integer x and a nonnegative integer shift, return closest\n    integer to x / 2**shift; use round-to-even in case of a tie.\n\n    \"\"\"\n    b, q = 1L << shift, x >> shift\n    return q + (2*(x & (b-1)) + (q&1) > b)\n\ndef _div_nearest(a, b):\n    \"\"\"Closest integer to a/b, a and b positive integers; rounds to even\n    in the case of a tie.\n\n    \"\"\"\n    q, r = divmod(a, b)\n    return q + (2*r + (q&1) > b)\n\ndef _ilog(x, M, L = 8):\n    \"\"\"Integer approximation to M*log(x/M), with absolute error boundable\n    in terms only of x/M.\n\n    Given positive integers x and M, return an integer approximation to\n    M * log(x/M).  For L = 8 and 0.1 <= x/M <= 10 the difference\n    between the approximation and the exact result is at most 22.  For\n    L = 8 and 1.0 <= x/M <= 10.0 the difference is at most 15.  In\n    both cases these are upper bounds on the error; it will usually be\n    much smaller.\"\"\"\n\n    # The basic algorithm is the following: let log1p be the function\n    # log1p(x) = log(1+x).  Then log(x/M) = log1p((x-M)/M).  We use\n    # the reduction\n    #\n    #    log1p(y) = 2*log1p(y/(1+sqrt(1+y)))\n    #\n    # repeatedly until the argument to log1p is small (< 2**-L in\n    # absolute value).  For small y we can use the Taylor series\n    # expansion\n    #\n    #    log1p(y) ~ y - y**2/2 + y**3/3 - ... - (-y)**T/T\n    #\n    # truncating at T such that y**T is small enough.  The whole\n    # computation is carried out in a form of fixed-point arithmetic,\n    # with a real number z being represented by an integer\n    # approximation to z*M.  To avoid loss of precision, the y below\n    # is actually an integer approximation to 2**R*y*M, where R is the\n    # number of reductions performed so far.\n\n    y = x-M\n    # argument reduction; R = number of reductions performed\n    R = 0\n    while (R <= L and long(abs(y)) << L-R >= M or\n           R > L and abs(y) >> R-L >= M):\n        y = _div_nearest(long(M*y) << 1,\n                         M + _sqrt_nearest(M*(M+_rshift_nearest(y, R)), M))\n        R += 1\n\n    # Taylor series with T terms\n    T = -int(-10*len(str(M))//(3*L))\n    yshift = _rshift_nearest(y, R)\n    w = _div_nearest(M, T)\n    for k in xrange(T-1, 0, -1):\n        w = _div_nearest(M, k) - _div_nearest(yshift*w, M)\n\n    return _div_nearest(w*y, M)\n\ndef _dlog10(c, e, p):\n    \"\"\"Given integers c, e and p with c > 0, p >= 0, compute an integer\n    approximation to 10**p * log10(c*10**e), with an absolute error of\n    at most 1.  Assumes that c*10**e is not exactly 1.\"\"\"\n\n    # increase precision by 2; compensate for this by dividing\n    # final result by 100\n    p += 2\n\n    # write c*10**e as d*10**f with either:\n    #   f >= 0 and 1 <= d <= 10, or\n    #   f <= 0 and 0.1 <= d <= 1.\n    # Thus for c*10**e close to 1, f = 0\n    l = len(str(c))\n    f = e+l - (e+l >= 1)\n\n    if p > 0:\n        M = 10**p\n        k = e+p-f\n        if k >= 0:\n            c *= 10**k\n        else:\n            c = _div_nearest(c, 10**-k)\n\n        log_d = _ilog(c, M) # error < 5 + 22 = 27\n        log_10 = _log10_digits(p) # error < 1\n        log_d = _div_nearest(log_d*M, log_10)\n        log_tenpower = f*M # exact\n    else:\n        log_d = 0  # error < 2.31\n        log_tenpower = _div_nearest(f, 10**-p) # error < 0.5\n\n    return _div_nearest(log_tenpower+log_d, 100)\n\ndef _dlog(c, e, p):\n    \"\"\"Given integers c, e and p with c > 0, compute an integer\n    approximation to 10**p * log(c*10**e), with an absolute error of\n    at most 1.  Assumes that c*10**e is not exactly 1.\"\"\"\n\n    # Increase precision by 2. The precision increase is compensated\n    # for at the end with a division by 100.\n    p += 2\n\n    # rewrite c*10**e as d*10**f with either f >= 0 and 1 <= d <= 10,\n    # or f <= 0 and 0.1 <= d <= 1.  Then we can compute 10**p * log(c*10**e)\n    # as 10**p * log(d) + 10**p*f * log(10).\n    l = len(str(c))\n    f = e+l - (e+l >= 1)\n\n    # compute approximation to 10**p*log(d), with error < 27\n    if p > 0:\n        k = e+p-f\n        if k >= 0:\n            c *= 10**k\n        else:\n            c = _div_nearest(c, 10**-k)  # error of <= 0.5 in c\n\n        # _ilog magnifies existing error in c by a factor of at most 10\n        log_d = _ilog(c, 10**p) # error < 5 + 22 = 27\n    else:\n        # p <= 0: just approximate the whole thing by 0; error < 2.31\n        log_d = 0\n\n    # compute approximation to f*10**p*log(10), with error < 11.\n    if f:\n        extra = len(str(abs(f)))-1\n        if p + extra >= 0:\n            # error in f * _log10_digits(p+extra) < |f| * 1 = |f|\n            # after division, error < |f|/10**extra + 0.5 < 10 + 0.5 < 11\n            f_log_ten = _div_nearest(f*_log10_digits(p+extra), 10**extra)\n        else:\n            f_log_ten = 0\n    else:\n        f_log_ten = 0\n\n    # error in sum < 11+27 = 38; error after division < 0.38 + 0.5 < 1\n    return _div_nearest(f_log_ten + log_d, 100)\n\nclass _Log10Memoize(object):\n    \"\"\"Class to compute, store, and allow retrieval of, digits of the\n    constant log(10) = 2.302585....  This constant is needed by\n    Decimal.ln, Decimal.log10, Decimal.exp and Decimal.__pow__.\"\"\"\n    def __init__(self):\n        self.digits = \"23025850929940456840179914546843642076011014886\"\n\n    def getdigits(self, p):\n        \"\"\"Given an integer p >= 0, return floor(10**p)*log(10).\n\n        For example, self.getdigits(3) returns 2302.\n        \"\"\"\n        # digits are stored as a string, for quick conversion to\n        # integer in the case that we've already computed enough\n        # digits; the stored digits should always be correct\n        # (truncated, not rounded to nearest).\n        if p < 0:\n            raise ValueError(\"p should be nonnegative\")\n\n        if p >= len(self.digits):\n            # compute p+3, p+6, p+9, ... digits; continue until at\n            # least one of the extra digits is nonzero\n            extra = 3\n            while True:\n                # compute p+extra digits, correct to within 1ulp\n                M = 10**(p+extra+2)\n                digits = str(_div_nearest(_ilog(10*M, M), 100))\n                if digits[-extra:] != '0'*extra:\n                    break\n                extra += 3\n            # keep all reliable digits so far; remove trailing zeros\n            # and next nonzero digit\n            self.digits = digits.rstrip('0')[:-1]\n        return int(self.digits[:p+1])\n\n_log10_digits = _Log10Memoize().getdigits\n\ndef _iexp(x, M, L=8):\n    \"\"\"Given integers x and M, M > 0, such that x/M is small in absolute\n    value, compute an integer approximation to M*exp(x/M).  For 0 <=\n    x/M <= 2.4, the absolute error in the result is bounded by 60 (and\n    is usually much smaller).\"\"\"\n\n    # Algorithm: to compute exp(z) for a real number z, first divide z\n    # by a suitable power R of 2 so that |z/2**R| < 2**-L.  Then\n    # compute expm1(z/2**R) = exp(z/2**R) - 1 using the usual Taylor\n    # series\n    #\n    #     expm1(x) = x + x**2/2! + x**3/3! + ...\n    #\n    # Now use the identity\n    #\n    #     expm1(2x) = expm1(x)*(expm1(x)+2)\n    #\n    # R times to compute the sequence expm1(z/2**R),\n    # expm1(z/2**(R-1)), ... , exp(z/2), exp(z).\n\n    # Find R such that x/2**R/M <= 2**-L\n    R = _nbits((long(x)<<L)//M)\n\n    # Taylor series.  (2**L)**T > M\n    T = -int(-10*len(str(M))//(3*L))\n    y = _div_nearest(x, T)\n    Mshift = long(M)<<R\n    for i in xrange(T-1, 0, -1):\n        y = _div_nearest(x*(Mshift + y), Mshift * i)\n\n    # Expansion\n    for k in xrange(R-1, -1, -1):\n        Mshift = long(M)<<(k+2)\n        y = _div_nearest(y*(y+Mshift), Mshift)\n\n    return M+y\n\ndef _dexp(c, e, p):\n    \"\"\"Compute an approximation to exp(c*10**e), with p decimal places of\n    precision.\n\n    Returns integers d, f such that:\n\n      10**(p-1) <= d <= 10**p, and\n      (d-1)*10**f < exp(c*10**e) < (d+1)*10**f\n\n    In other words, d*10**f is an approximation to exp(c*10**e) with p\n    digits of precision, and with an error in d of at most 1.  This is\n    almost, but not quite, the same as the error being < 1ulp: when d\n    = 10**(p-1) the error could be up to 10 ulp.\"\"\"\n\n    # we'll call iexp with M = 10**(p+2), giving p+3 digits of precision\n    p += 2\n\n    # compute log(10) with extra precision = adjusted exponent of c*10**e\n    extra = max(0, e + len(str(c)) - 1)\n    q = p + extra\n\n    # compute quotient c*10**e/(log(10)) = c*10**(e+q)/(log(10)*10**q),\n    # rounding down\n    shift = e+q\n    if shift >= 0:\n        cshift = c*10**shift\n    else:\n        cshift = c//10**-shift\n    quot, rem = divmod(cshift, _log10_digits(q))\n\n    # reduce remainder back to original precision\n    rem = _div_nearest(rem, 10**extra)\n\n    # error in result of _iexp < 120;  error after division < 0.62\n    return _div_nearest(_iexp(rem, 10**p), 1000), quot - p + 3\n\ndef _dpower(xc, xe, yc, ye, p):\n    \"\"\"Given integers xc, xe, yc and ye representing Decimals x = xc*10**xe and\n    y = yc*10**ye, compute x**y.  Returns a pair of integers (c, e) such that:\n\n      10**(p-1) <= c <= 10**p, and\n      (c-1)*10**e < x**y < (c+1)*10**e\n\n    in other words, c*10**e is an approximation to x**y with p digits\n    of precision, and with an error in c of at most 1.  (This is\n    almost, but not quite, the same as the error being < 1ulp: when c\n    == 10**(p-1) we can only guarantee error < 10ulp.)\n\n    We assume that: x is positive and not equal to 1, and y is nonzero.\n    \"\"\"\n\n    # Find b such that 10**(b-1) <= |y| <= 10**b\n    b = len(str(abs(yc))) + ye\n\n    # log(x) = lxc*10**(-p-b-1), to p+b+1 places after the decimal point\n    lxc = _dlog(xc, xe, p+b+1)\n\n    # compute product y*log(x) = yc*lxc*10**(-p-b-1+ye) = pc*10**(-p-1)\n    shift = ye-b\n    if shift >= 0:\n        pc = lxc*yc*10**shift\n    else:\n        pc = _div_nearest(lxc*yc, 10**-shift)\n\n    if pc == 0:\n        # we prefer a result that isn't exactly 1; this makes it\n        # easier to compute a correctly rounded result in __pow__\n        if ((len(str(xc)) + xe >= 1) == (yc > 0)): # if x**y > 1:\n            coeff, exp = 10**(p-1)+1, 1-p\n        else:\n            coeff, exp = 10**p-1, -p\n    else:\n        coeff, exp = _dexp(pc, -(p+1), p+1)\n        coeff = _div_nearest(coeff, 10)\n        exp += 1\n\n    return coeff, exp\n\ndef _log10_lb(c, correction = {\n        '1': 100, '2': 70, '3': 53, '4': 40, '5': 31,\n        '6': 23, '7': 16, '8': 10, '9': 5}):\n    \"\"\"Compute a lower bound for 100*log10(c) for a positive integer c.\"\"\"\n    if c <= 0:\n        raise ValueError(\"The argument to _log10_lb should be nonnegative.\")\n    str_c = str(c)\n    return 100*len(str_c) - correction[str_c[0]]\n\n##### Helper Functions ####################################################\n\ndef _convert_other(other, raiseit=False, allow_float=False):\n    \"\"\"Convert other to Decimal.\n\n    Verifies that it's ok to use in an implicit construction.\n    If allow_float is true, allow conversion from float;  this\n    is used in the comparison methods (__eq__ and friends).\n\n    \"\"\"\n    if isinstance(other, Decimal):\n        return other\n    if isinstance(other, (int, long)):\n        return Decimal(other)\n    if allow_float and isinstance(other, float):\n        return Decimal.from_float(other)\n\n    if raiseit:\n        raise TypeError(\"Unable to convert %s to Decimal\" % other)\n    return NotImplemented\n\n##### Setup Specific Contexts ############################################\n\n# The default context prototype used by Context()\n# Is mutable, so that new contexts can have different default values\n\nDefaultContext = Context(\n        prec=28, rounding=ROUND_HALF_EVEN,\n        traps=[DivisionByZero, Overflow, InvalidOperation],\n        flags=[],\n        Emax=999999999,\n        Emin=-999999999,\n        capitals=1\n)\n\n# Pre-made alternate contexts offered by the specification\n# Don't change these; the user should be able to select these\n# contexts and be able to reproduce results from other implementations\n# of the spec.\n\nBasicContext = Context(\n        prec=9, rounding=ROUND_HALF_UP,\n        traps=[DivisionByZero, Overflow, InvalidOperation, Clamped, Underflow],\n        flags=[],\n)\n\nExtendedContext = Context(\n        prec=9, rounding=ROUND_HALF_EVEN,\n        traps=[],\n        flags=[],\n)\n\n\n##### crud for parsing strings #############################################\n#\n# Regular expression used for parsing numeric strings.  Additional\n# comments:\n#\n# 1. Uncomment the two '\\s*' lines to allow leading and/or trailing\n# whitespace.  But note that the specification disallows whitespace in\n# a numeric string.\n#\n# 2. For finite numbers (not infinities and NaNs) the body of the\n# number between the optional sign and the optional exponent must have\n# at least one decimal digit, possibly after the decimal point.  The\n# lookahead expression '(?=\\d|\\.\\d)' checks this.\n\nimport re\n_parser = re.compile(r\"\"\"        # A numeric string consists of:\n#    \\s*\n    (?P<sign>[-+])?              # an optional sign, followed by either...\n    (\n        (?=\\d|\\.\\d)              # ...a number (with at least one digit)\n        (?P<int>\\d*)             # having a (possibly empty) integer part\n        (\\.(?P<frac>\\d*))?       # followed by an optional fractional part\n        (E(?P<exp>[-+]?\\d+))?    # followed by an optional exponent, or...\n    |\n        Inf(inity)?              # ...an infinity, or...\n    |\n        (?P<signal>s)?           # ...an (optionally signaling)\n        NaN                      # NaN\n        (?P<diag>\\d*)            # with (possibly empty) diagnostic info.\n    )\n#    \\s*\n    \\Z\n\"\"\", re.VERBOSE | re.IGNORECASE | re.UNICODE).match\n\n_all_zeros = re.compile('0*$').match\n_exact_half = re.compile('50*$').match\n\n##### PEP3101 support functions ##############################################\n# The functions in this section have little to do with the Decimal\n# class, and could potentially be reused or adapted for other pure\n# Python numeric classes that want to implement __format__\n#\n# A format specifier for Decimal looks like:\n#\n#   [[fill]align][sign][0][minimumwidth][,][.precision][type]\n\n_parse_format_specifier_regex = re.compile(r\"\"\"\\A\n(?:\n   (?P<fill>.)?\n   (?P<align>[<>=^])\n)?\n(?P<sign>[-+ ])?\n(?P<zeropad>0)?\n(?P<minimumwidth>(?!0)\\d+)?\n(?P<thousands_sep>,)?\n(?:\\.(?P<precision>0|(?!0)\\d+))?\n(?P<type>[eEfFgGn%])?\n\\Z\n\"\"\", re.VERBOSE)\n\ndel re\n\n# The locale module is only needed for the 'n' format specifier.  The\n# rest of the PEP 3101 code functions quite happily without it, so we\n# don't care too much if locale isn't present.\ntry:\n    import locale as _locale\nexcept ImportError:\n    pass\n\ndef _parse_format_specifier(format_spec, _localeconv=None):\n    \"\"\"Parse and validate a format specifier.\n\n    Turns a standard numeric format specifier into a dict, with the\n    following entries:\n\n      fill: fill character to pad field to minimum width\n      align: alignment type, either '<', '>', '=' or '^'\n      sign: either '+', '-' or ' '\n      minimumwidth: nonnegative integer giving minimum width\n      zeropad: boolean, indicating whether to pad with zeros\n      thousands_sep: string to use as thousands separator, or ''\n      grouping: grouping for thousands separators, in format\n        used by localeconv\n      decimal_point: string to use for decimal point\n      precision: nonnegative integer giving precision, or None\n      type: one of the characters 'eEfFgG%', or None\n      unicode: boolean (always True for Python 3.x)\n\n    \"\"\"\n    m = _parse_format_specifier_regex.match(format_spec)\n    if m is None:\n        raise ValueError(\"Invalid format specifier: \" + format_spec)\n\n    # get the dictionary\n    format_dict = m.groupdict()\n\n    # zeropad; defaults for fill and alignment.  If zero padding\n    # is requested, the fill and align fields should be absent.\n    fill = format_dict['fill']\n    align = format_dict['align']\n    format_dict['zeropad'] = (format_dict['zeropad'] is not None)\n    if format_dict['zeropad']:\n        if fill is not None:\n            raise ValueError(\"Fill character conflicts with '0'\"\n                             \" in format specifier: \" + format_spec)\n        if align is not None:\n            raise ValueError(\"Alignment conflicts with '0' in \"\n                             \"format specifier: \" + format_spec)\n    format_dict['fill'] = fill or ' '\n    # PEP 3101 originally specified that the default alignment should\n    # be left;  it was later agreed that right-aligned makes more sense\n    # for numeric types.  See http://bugs.python.org/issue6857.\n    format_dict['align'] = align or '>'\n\n    # default sign handling: '-' for negative, '' for positive\n    if format_dict['sign'] is None:\n        format_dict['sign'] = '-'\n\n    # minimumwidth defaults to 0; precision remains None if not given\n    format_dict['minimumwidth'] = int(format_dict['minimumwidth'] or '0')\n    if format_dict['precision'] is not None:\n        format_dict['precision'] = int(format_dict['precision'])\n\n    # if format type is 'g' or 'G' then a precision of 0 makes little\n    # sense; convert it to 1.  Same if format type is unspecified.\n    if format_dict['precision'] == 0:\n        if format_dict['type'] is None or format_dict['type'] in 'gG':\n            format_dict['precision'] = 1\n\n    # determine thousands separator, grouping, and decimal separator, and\n    # add appropriate entries to format_dict\n    if format_dict['type'] == 'n':\n        # apart from separators, 'n' behaves just like 'g'\n        format_dict['type'] = 'g'\n        if _localeconv is None:\n            _localeconv = _locale.localeconv()\n        if format_dict['thousands_sep'] is not None:\n            raise ValueError(\"Explicit thousands separator conflicts with \"\n                             \"'n' type in format specifier: \" + format_spec)\n        format_dict['thousands_sep'] = _localeconv['thousands_sep']\n        format_dict['grouping'] = _localeconv['grouping']\n        format_dict['decimal_point'] = _localeconv['decimal_point']\n    else:\n        if format_dict['thousands_sep'] is None:\n            format_dict['thousands_sep'] = ''\n        format_dict['grouping'] = [3, 0]\n        format_dict['decimal_point'] = '.'\n\n    # record whether return type should be str or unicode\n    format_dict['unicode'] = isinstance(format_spec, unicode)\n\n    return format_dict\n\ndef _format_align(sign, body, spec):\n    \"\"\"Given an unpadded, non-aligned numeric string 'body' and sign\n    string 'sign', add padding and alignment conforming to the given\n    format specifier dictionary 'spec' (as produced by\n    parse_format_specifier).\n\n    Also converts result to unicode if necessary.\n\n    \"\"\"\n    # how much extra space do we have to play with?\n    minimumwidth = spec['minimumwidth']\n    fill = spec['fill']\n    padding = fill*(minimumwidth - len(sign) - len(body))\n\n    align = spec['align']\n    if align == '<':\n        result = sign + body + padding\n    elif align == '>':\n        result = padding + sign + body\n    elif align == '=':\n        result = sign + padding + body\n    elif align == '^':\n        half = len(padding)//2\n        result = padding[:half] + sign + body + padding[half:]\n    else:\n        raise ValueError('Unrecognised alignment field')\n\n    # make sure that result is unicode if necessary\n    if spec['unicode']:\n        result = unicode(result)\n\n    return result\n\ndef _group_lengths(grouping):\n    \"\"\"Convert a localeconv-style grouping into a (possibly infinite)\n    iterable of integers representing group lengths.\n\n    \"\"\"\n    # The result from localeconv()['grouping'], and the input to this\n    # function, should be a list of integers in one of the\n    # following three forms:\n    #\n    #   (1) an empty list, or\n    #   (2) nonempty list of positive integers + [0]\n    #   (3) list of positive integers + [locale.CHAR_MAX], or\n\n    from itertools import chain, repeat\n    if not grouping:\n        return []\n    elif grouping[-1] == 0 and len(grouping) >= 2:\n        return chain(grouping[:-1], repeat(grouping[-2]))\n    elif grouping[-1] == _locale.CHAR_MAX:\n        return grouping[:-1]\n    else:\n        raise ValueError('unrecognised format for grouping')\n\ndef _insert_thousands_sep(digits, spec, min_width=1):\n    \"\"\"Insert thousands separators into a digit string.\n\n    spec is a dictionary whose keys should include 'thousands_sep' and\n    'grouping'; typically it's the result of parsing the format\n    specifier using _parse_format_specifier.\n\n    The min_width keyword argument gives the minimum length of the\n    result, which will be padded on the left with zeros if necessary.\n\n    If necessary, the zero padding adds an extra '0' on the left to\n    avoid a leading thousands separator.  For example, inserting\n    commas every three digits in '123456', with min_width=8, gives\n    '0,123,456', even though that has length 9.\n\n    \"\"\"\n\n    sep = spec['thousands_sep']\n    grouping = spec['grouping']\n\n    groups = []\n    for l in _group_lengths(grouping):\n        if l <= 0:\n            raise ValueError(\"group length should be positive\")\n        # max(..., 1) forces at least 1 digit to the left of a separator\n        l = min(max(len(digits), min_width, 1), l)\n        groups.append('0'*(l - len(digits)) + digits[-l:])\n        digits = digits[:-l]\n        min_width -= l\n        if not digits and min_width <= 0:\n            break\n        min_width -= len(sep)\n    else:\n        l = max(len(digits), min_width, 1)\n        groups.append('0'*(l - len(digits)) + digits[-l:])\n    return sep.join(reversed(groups))\n\ndef _format_sign(is_negative, spec):\n    \"\"\"Determine sign character.\"\"\"\n\n    if is_negative:\n        return '-'\n    elif spec['sign'] in ' +':\n        return spec['sign']\n    else:\n        return ''\n\ndef _format_number(is_negative, intpart, fracpart, exp, spec):\n    \"\"\"Format a number, given the following data:\n\n    is_negative: true if the number is negative, else false\n    intpart: string of digits that must appear before the decimal point\n    fracpart: string of digits that must come after the point\n    exp: exponent, as an integer\n    spec: dictionary resulting from parsing the format specifier\n\n    This function uses the information in spec to:\n      insert separators (decimal separator and thousands separators)\n      format the sign\n      format the exponent\n      add trailing '%' for the '%' type\n      zero-pad if necessary\n      fill and align if necessary\n    \"\"\"\n\n    sign = _format_sign(is_negative, spec)\n\n    if fracpart:\n        fracpart = spec['decimal_point'] + fracpart\n\n    if exp != 0 or spec['type'] in 'eE':\n        echar = {'E': 'E', 'e': 'e', 'G': 'E', 'g': 'e'}[spec['type']]\n        fracpart += \"{0}{1:+}\".format(echar, exp)\n    if spec['type'] == '%':\n        fracpart += '%'\n\n    if spec['zeropad']:\n        min_width = spec['minimumwidth'] - len(fracpart) - len(sign)\n    else:\n        min_width = 0\n    intpart = _insert_thousands_sep(intpart, spec, min_width)\n\n    return _format_align(sign, intpart+fracpart, spec)\n\n\n##### Useful Constants (internal use only) ################################\n\n# Reusable defaults\n_Infinity = Decimal('Inf')\n_NegativeInfinity = Decimal('-Inf')\n_NaN = Decimal('NaN')\n_Zero = Decimal(0)\n_One = Decimal(1)\n_NegativeOne = Decimal(-1)\n\n# _SignedInfinity[sign] is infinity w/ that sign\n_SignedInfinity = (_Infinity, _NegativeInfinity)\n\n\n\nif __name__ == '__main__':\n    import doctest, sys\n    doctest.testmod(sys.modules[__name__])\n", 
    "difflib": "\"\"\"\nModule difflib -- helpers for computing deltas between objects.\n\nFunction get_close_matches(word, possibilities, n=3, cutoff=0.6):\n    Use SequenceMatcher to return list of the best \"good enough\" matches.\n\nFunction context_diff(a, b):\n    For two lists of strings, return a delta in context diff format.\n\nFunction ndiff(a, b):\n    Return a delta: the difference between `a` and `b` (lists of strings).\n\nFunction restore(delta, which):\n    Return one of the two sequences that generated an ndiff delta.\n\nFunction unified_diff(a, b):\n    For two lists of strings, return a delta in unified diff format.\n\nClass SequenceMatcher:\n    A flexible class for comparing pairs of sequences of any type.\n\nClass Differ:\n    For producing human-readable deltas from sequences of lines of text.\n\nClass HtmlDiff:\n    For producing HTML side by side comparison with change highlights.\n\"\"\"\n\n__all__ = ['get_close_matches', 'ndiff', 'restore', 'SequenceMatcher',\n           'Differ','IS_CHARACTER_JUNK', 'IS_LINE_JUNK', 'context_diff',\n           'unified_diff', 'HtmlDiff', 'Match']\n\nimport heapq\nfrom collections import namedtuple as _namedtuple\nfrom functools import reduce\n\nMatch = _namedtuple('Match', 'a b size')\n\ndef _calculate_ratio(matches, length):\n    if length:\n        return 2.0 * matches / length\n    return 1.0\n\nclass SequenceMatcher:\n\n    \"\"\"\n    SequenceMatcher is a flexible class for comparing pairs of sequences of\n    any type, so long as the sequence elements are hashable.  The basic\n    algorithm predates, and is a little fancier than, an algorithm\n    published in the late 1980's by Ratcliff and Obershelp under the\n    hyperbolic name \"gestalt pattern matching\".  The basic idea is to find\n    the longest contiguous matching subsequence that contains no \"junk\"\n    elements (R-O doesn't address junk).  The same idea is then applied\n    recursively to the pieces of the sequences to the left and to the right\n    of the matching subsequence.  This does not yield minimal edit\n    sequences, but does tend to yield matches that \"look right\" to people.\n\n    SequenceMatcher tries to compute a \"human-friendly diff\" between two\n    sequences.  Unlike e.g. UNIX(tm) diff, the fundamental notion is the\n    longest *contiguous* & junk-free matching subsequence.  That's what\n    catches peoples' eyes.  The Windows(tm) windiff has another interesting\n    notion, pairing up elements that appear uniquely in each sequence.\n    That, and the method here, appear to yield more intuitive difference\n    reports than does diff.  This method appears to be the least vulnerable\n    to synching up on blocks of \"junk lines\", though (like blank lines in\n    ordinary text files, or maybe \"<P>\" lines in HTML files).  That may be\n    because this is the only method of the 3 that has a *concept* of\n    \"junk\" <wink>.\n\n    Example, comparing two strings, and considering blanks to be \"junk\":\n\n    >>> s = SequenceMatcher(lambda x: x == \" \",\n    ...                     \"private Thread currentThread;\",\n    ...                     \"private volatile Thread currentThread;\")\n    >>>\n\n    .ratio() returns a float in [0, 1], measuring the \"similarity\" of the\n    sequences.  As a rule of thumb, a .ratio() value over 0.6 means the\n    sequences are close matches:\n\n    >>> print round(s.ratio(), 3)\n    0.866\n    >>>\n\n    If you're only interested in where the sequences match,\n    .get_matching_blocks() is handy:\n\n    >>> for block in s.get_matching_blocks():\n    ...     print \"a[%d] and b[%d] match for %d elements\" % block\n    a[0] and b[0] match for 8 elements\n    a[8] and b[17] match for 21 elements\n    a[29] and b[38] match for 0 elements\n\n    Note that the last tuple returned by .get_matching_blocks() is always a\n    dummy, (len(a), len(b), 0), and this is the only case in which the last\n    tuple element (number of elements matched) is 0.\n\n    If you want to know how to change the first sequence into the second,\n    use .get_opcodes():\n\n    >>> for opcode in s.get_opcodes():\n    ...     print \"%6s a[%d:%d] b[%d:%d]\" % opcode\n     equal a[0:8] b[0:8]\n    insert a[8:8] b[8:17]\n     equal a[8:29] b[17:38]\n\n    See the Differ class for a fancy human-friendly file differencer, which\n    uses SequenceMatcher both to compare sequences of lines, and to compare\n    sequences of characters within similar (near-matching) lines.\n\n    See also function get_close_matches() in this module, which shows how\n    simple code building on SequenceMatcher can be used to do useful work.\n\n    Timing:  Basic R-O is cubic time worst case and quadratic time expected\n    case.  SequenceMatcher is quadratic time for the worst case and has\n    expected-case behavior dependent in a complicated way on how many\n    elements the sequences have in common; best case time is linear.\n\n    Methods:\n\n    __init__(isjunk=None, a='', b='')\n        Construct a SequenceMatcher.\n\n    set_seqs(a, b)\n        Set the two sequences to be compared.\n\n    set_seq1(a)\n        Set the first sequence to be compared.\n\n    set_seq2(b)\n        Set the second sequence to be compared.\n\n    find_longest_match(alo, ahi, blo, bhi)\n        Find longest matching block in a[alo:ahi] and b[blo:bhi].\n\n    get_matching_blocks()\n        Return list of triples describing matching subsequences.\n\n    get_opcodes()\n        Return list of 5-tuples describing how to turn a into b.\n\n    ratio()\n        Return a measure of the sequences' similarity (float in [0,1]).\n\n    quick_ratio()\n        Return an upper bound on .ratio() relatively quickly.\n\n    real_quick_ratio()\n        Return an upper bound on ratio() very quickly.\n    \"\"\"\n\n    def __init__(self, isjunk=None, a='', b='', autojunk=True):\n        \"\"\"Construct a SequenceMatcher.\n\n        Optional arg isjunk is None (the default), or a one-argument\n        function that takes a sequence element and returns true iff the\n        element is junk.  None is equivalent to passing \"lambda x: 0\", i.e.\n        no elements are considered to be junk.  For example, pass\n            lambda x: x in \" \\\\t\"\n        if you're comparing lines as sequences of characters, and don't\n        want to synch up on blanks or hard tabs.\n\n        Optional arg a is the first of two sequences to be compared.  By\n        default, an empty string.  The elements of a must be hashable.  See\n        also .set_seqs() and .set_seq1().\n\n        Optional arg b is the second of two sequences to be compared.  By\n        default, an empty string.  The elements of b must be hashable. See\n        also .set_seqs() and .set_seq2().\n\n        Optional arg autojunk should be set to False to disable the\n        \"automatic junk heuristic\" that treats popular elements as junk\n        (see module documentation for more information).\n        \"\"\"\n\n        # Members:\n        # a\n        #      first sequence\n        # b\n        #      second sequence; differences are computed as \"what do\n        #      we need to do to 'a' to change it into 'b'?\"\n        # b2j\n        #      for x in b, b2j[x] is a list of the indices (into b)\n        #      at which x appears; junk elements do not appear\n        # fullbcount\n        #      for x in b, fullbcount[x] == the number of times x\n        #      appears in b; only materialized if really needed (used\n        #      only for computing quick_ratio())\n        # matching_blocks\n        #      a list of (i, j, k) triples, where a[i:i+k] == b[j:j+k];\n        #      ascending & non-overlapping in i and in j; terminated by\n        #      a dummy (len(a), len(b), 0) sentinel\n        # opcodes\n        #      a list of (tag, i1, i2, j1, j2) tuples, where tag is\n        #      one of\n        #          'replace'   a[i1:i2] should be replaced by b[j1:j2]\n        #          'delete'    a[i1:i2] should be deleted\n        #          'insert'    b[j1:j2] should be inserted\n        #          'equal'     a[i1:i2] == b[j1:j2]\n        # isjunk\n        #      a user-supplied function taking a sequence element and\n        #      returning true iff the element is \"junk\" -- this has\n        #      subtle but helpful effects on the algorithm, which I'll\n        #      get around to writing up someday <0.9 wink>.\n        #      DON'T USE!  Only __chain_b uses this.  Use isbjunk.\n        # isbjunk\n        #      for x in b, isbjunk(x) == isjunk(x) but much faster;\n        #      it's really the __contains__ method of a hidden dict.\n        #      DOES NOT WORK for x in a!\n        # isbpopular\n        #      for x in b, isbpopular(x) is true iff b is reasonably long\n        #      (at least 200 elements) and x accounts for more than 1 + 1% of\n        #      its elements (when autojunk is enabled).\n        #      DOES NOT WORK for x in a!\n\n        self.isjunk = isjunk\n        self.a = self.b = None\n        self.autojunk = autojunk\n        self.set_seqs(a, b)\n\n    def set_seqs(self, a, b):\n        \"\"\"Set the two sequences to be compared.\n\n        >>> s = SequenceMatcher()\n        >>> s.set_seqs(\"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        \"\"\"\n\n        self.set_seq1(a)\n        self.set_seq2(b)\n\n    def set_seq1(self, a):\n        \"\"\"Set the first sequence to be compared.\n\n        The second sequence to be compared is not changed.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.set_seq1(\"bcde\")\n        >>> s.ratio()\n        1.0\n        >>>\n\n        SequenceMatcher computes and caches detailed information about the\n        second sequence, so if you want to compare one sequence S against\n        many sequences, use .set_seq2(S) once and call .set_seq1(x)\n        repeatedly for each of the other sequences.\n\n        See also set_seqs() and set_seq2().\n        \"\"\"\n\n        if a is self.a:\n            return\n        self.a = a\n        self.matching_blocks = self.opcodes = None\n\n    def set_seq2(self, b):\n        \"\"\"Set the second sequence to be compared.\n\n        The first sequence to be compared is not changed.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.set_seq2(\"abcd\")\n        >>> s.ratio()\n        1.0\n        >>>\n\n        SequenceMatcher computes and caches detailed information about the\n        second sequence, so if you want to compare one sequence S against\n        many sequences, use .set_seq2(S) once and call .set_seq1(x)\n        repeatedly for each of the other sequences.\n\n        See also set_seqs() and set_seq1().\n        \"\"\"\n\n        if b is self.b:\n            return\n        self.b = b\n        self.matching_blocks = self.opcodes = None\n        self.fullbcount = None\n        self.__chain_b()\n\n    # For each element x in b, set b2j[x] to a list of the indices in\n    # b where x appears; the indices are in increasing order; note that\n    # the number of times x appears in b is len(b2j[x]) ...\n    # when self.isjunk is defined, junk elements don't show up in this\n    # map at all, which stops the central find_longest_match method\n    # from starting any matching block at a junk element ...\n    # also creates the fast isbjunk function ...\n    # b2j also does not contain entries for \"popular\" elements, meaning\n    # elements that account for more than 1 + 1% of the total elements, and\n    # when the sequence is reasonably large (>= 200 elements); this can\n    # be viewed as an adaptive notion of semi-junk, and yields an enormous\n    # speedup when, e.g., comparing program files with hundreds of\n    # instances of \"return NULL;\" ...\n    # note that this is only called when b changes; so for cross-product\n    # kinds of matches, it's best to call set_seq2 once, then set_seq1\n    # repeatedly\n\n    def __chain_b(self):\n        # Because isjunk is a user-defined (not C) function, and we test\n        # for junk a LOT, it's important to minimize the number of calls.\n        # Before the tricks described here, __chain_b was by far the most\n        # time-consuming routine in the whole module!  If anyone sees\n        # Jim Roskind, thank him again for profile.py -- I never would\n        # have guessed that.\n        # The first trick is to build b2j ignoring the possibility\n        # of junk.  I.e., we don't call isjunk at all yet.  Throwing\n        # out the junk later is much cheaper than building b2j \"right\"\n        # from the start.\n        b = self.b\n        self.b2j = b2j = {}\n\n        for i, elt in enumerate(b):\n            indices = b2j.setdefault(elt, [])\n            indices.append(i)\n\n        # Purge junk elements\n        junk = set()\n        isjunk = self.isjunk\n        if isjunk:\n            for elt in list(b2j.keys()):  # using list() since b2j is modified\n                if isjunk(elt):\n                    junk.add(elt)\n                    del b2j[elt]\n\n        # Purge popular elements that are not junk\n        popular = set()\n        n = len(b)\n        if self.autojunk and n >= 200:\n            ntest = n // 100 + 1\n            for elt, idxs in list(b2j.items()):\n                if len(idxs) > ntest:\n                    popular.add(elt)\n                    del b2j[elt]\n\n        # Now for x in b, isjunk(x) == x in junk, but the latter is much faster.\n        # Sicne the number of *unique* junk elements is probably small, the\n        # memory burden of keeping this set alive is likely trivial compared to\n        # the size of b2j.\n        self.isbjunk = junk.__contains__\n        self.isbpopular = popular.__contains__\n\n    def find_longest_match(self, alo, ahi, blo, bhi):\n        \"\"\"Find longest matching block in a[alo:ahi] and b[blo:bhi].\n\n        If isjunk is not defined:\n\n        Return (i,j,k) such that a[i:i+k] is equal to b[j:j+k], where\n            alo <= i <= i+k <= ahi\n            blo <= j <= j+k <= bhi\n        and for all (i',j',k') meeting those conditions,\n            k >= k'\n            i <= i'\n            and if i == i', j <= j'\n\n        In other words, of all maximal matching blocks, return one that\n        starts earliest in a, and of all those maximal matching blocks that\n        start earliest in a, return the one that starts earliest in b.\n\n        >>> s = SequenceMatcher(None, \" abcd\", \"abcd abcd\")\n        >>> s.find_longest_match(0, 5, 0, 9)\n        Match(a=0, b=4, size=5)\n\n        If isjunk is defined, first the longest matching block is\n        determined as above, but with the additional restriction that no\n        junk element appears in the block.  Then that block is extended as\n        far as possible by matching (only) junk elements on both sides.  So\n        the resulting block never matches on junk except as identical junk\n        happens to be adjacent to an \"interesting\" match.\n\n        Here's the same example as before, but considering blanks to be\n        junk.  That prevents \" abcd\" from matching the \" abcd\" at the tail\n        end of the second sequence directly.  Instead only the \"abcd\" can\n        match, and matches the leftmost \"abcd\" in the second sequence:\n\n        >>> s = SequenceMatcher(lambda x: x==\" \", \" abcd\", \"abcd abcd\")\n        >>> s.find_longest_match(0, 5, 0, 9)\n        Match(a=1, b=0, size=4)\n\n        If no blocks match, return (alo, blo, 0).\n\n        >>> s = SequenceMatcher(None, \"ab\", \"c\")\n        >>> s.find_longest_match(0, 2, 0, 1)\n        Match(a=0, b=0, size=0)\n        \"\"\"\n\n        # CAUTION:  stripping common prefix or suffix would be incorrect.\n        # E.g.,\n        #    ab\n        #    acab\n        # Longest matching block is \"ab\", but if common prefix is\n        # stripped, it's \"a\" (tied with \"b\").  UNIX(tm) diff does so\n        # strip, so ends up claiming that ab is changed to acab by\n        # inserting \"ca\" in the middle.  That's minimal but unintuitive:\n        # \"it's obvious\" that someone inserted \"ac\" at the front.\n        # Windiff ends up at the same place as diff, but by pairing up\n        # the unique 'b's and then matching the first two 'a's.\n\n        a, b, b2j, isbjunk = self.a, self.b, self.b2j, self.isbjunk\n        besti, bestj, bestsize = alo, blo, 0\n        # find longest junk-free match\n        # during an iteration of the loop, j2len[j] = length of longest\n        # junk-free match ending with a[i-1] and b[j]\n        j2len = {}\n        nothing = []\n        for i in xrange(alo, ahi):\n            # look at all instances of a[i] in b; note that because\n            # b2j has no junk keys, the loop is skipped if a[i] is junk\n            j2lenget = j2len.get\n            newj2len = {}\n            for j in b2j.get(a[i], nothing):\n                # a[i] matches b[j]\n                if j < blo:\n                    continue\n                if j >= bhi:\n                    break\n                k = newj2len[j] = j2lenget(j-1, 0) + 1\n                if k > bestsize:\n                    besti, bestj, bestsize = i-k+1, j-k+1, k\n            j2len = newj2len\n\n        # Extend the best by non-junk elements on each end.  In particular,\n        # \"popular\" non-junk elements aren't in b2j, which greatly speeds\n        # the inner loop above, but also means \"the best\" match so far\n        # doesn't contain any junk *or* popular non-junk elements.\n        while besti > alo and bestj > blo and \\\n              not isbjunk(b[bestj-1]) and \\\n              a[besti-1] == b[bestj-1]:\n            besti, bestj, bestsize = besti-1, bestj-1, bestsize+1\n        while besti+bestsize < ahi and bestj+bestsize < bhi and \\\n              not isbjunk(b[bestj+bestsize]) and \\\n              a[besti+bestsize] == b[bestj+bestsize]:\n            bestsize += 1\n\n        # Now that we have a wholly interesting match (albeit possibly\n        # empty!), we may as well suck up the matching junk on each\n        # side of it too.  Can't think of a good reason not to, and it\n        # saves post-processing the (possibly considerable) expense of\n        # figuring out what to do with it.  In the case of an empty\n        # interesting match, this is clearly the right thing to do,\n        # because no other kind of match is possible in the regions.\n        while besti > alo and bestj > blo and \\\n              isbjunk(b[bestj-1]) and \\\n              a[besti-1] == b[bestj-1]:\n            besti, bestj, bestsize = besti-1, bestj-1, bestsize+1\n        while besti+bestsize < ahi and bestj+bestsize < bhi and \\\n              isbjunk(b[bestj+bestsize]) and \\\n              a[besti+bestsize] == b[bestj+bestsize]:\n            bestsize = bestsize + 1\n\n        return Match(besti, bestj, bestsize)\n\n    def get_matching_blocks(self):\n        \"\"\"Return list of triples describing matching subsequences.\n\n        Each triple is of the form (i, j, n), and means that\n        a[i:i+n] == b[j:j+n].  The triples are monotonically increasing in\n        i and in j.  New in Python 2.5, it's also guaranteed that if\n        (i, j, n) and (i', j', n') are adjacent triples in the list, and\n        the second is not the last triple in the list, then i+n != i' or\n        j+n != j'.  IOW, adjacent triples never describe adjacent equal\n        blocks.\n\n        The last triple is a dummy, (len(a), len(b), 0), and is the only\n        triple with n==0.\n\n        >>> s = SequenceMatcher(None, \"abxcd\", \"abcd\")\n        >>> s.get_matching_blocks()\n        [Match(a=0, b=0, size=2), Match(a=3, b=2, size=2), Match(a=5, b=4, size=0)]\n        \"\"\"\n\n        if self.matching_blocks is not None:\n            return self.matching_blocks\n        la, lb = len(self.a), len(self.b)\n\n        # This is most naturally expressed as a recursive algorithm, but\n        # at least one user bumped into extreme use cases that exceeded\n        # the recursion limit on their box.  So, now we maintain a list\n        # ('queue`) of blocks we still need to look at, and append partial\n        # results to `matching_blocks` in a loop; the matches are sorted\n        # at the end.\n        queue = [(0, la, 0, lb)]\n        matching_blocks = []\n        while queue:\n            alo, ahi, blo, bhi = queue.pop()\n            i, j, k = x = self.find_longest_match(alo, ahi, blo, bhi)\n            # a[alo:i] vs b[blo:j] unknown\n            # a[i:i+k] same as b[j:j+k]\n            # a[i+k:ahi] vs b[j+k:bhi] unknown\n            if k:   # if k is 0, there was no matching block\n                matching_blocks.append(x)\n                if alo < i and blo < j:\n                    queue.append((alo, i, blo, j))\n                if i+k < ahi and j+k < bhi:\n                    queue.append((i+k, ahi, j+k, bhi))\n        matching_blocks.sort()\n\n        # It's possible that we have adjacent equal blocks in the\n        # matching_blocks list now.  Starting with 2.5, this code was added\n        # to collapse them.\n        i1 = j1 = k1 = 0\n        non_adjacent = []\n        for i2, j2, k2 in matching_blocks:\n            # Is this block adjacent to i1, j1, k1?\n            if i1 + k1 == i2 and j1 + k1 == j2:\n                # Yes, so collapse them -- this just increases the length of\n                # the first block by the length of the second, and the first\n                # block so lengthened remains the block to compare against.\n                k1 += k2\n            else:\n                # Not adjacent.  Remember the first block (k1==0 means it's\n                # the dummy we started with), and make the second block the\n                # new block to compare against.\n                if k1:\n                    non_adjacent.append((i1, j1, k1))\n                i1, j1, k1 = i2, j2, k2\n        if k1:\n            non_adjacent.append((i1, j1, k1))\n\n        non_adjacent.append( (la, lb, 0) )\n        self.matching_blocks = map(Match._make, non_adjacent)\n        return self.matching_blocks\n\n    def get_opcodes(self):\n        \"\"\"Return list of 5-tuples describing how to turn a into b.\n\n        Each tuple is of the form (tag, i1, i2, j1, j2).  The first tuple\n        has i1 == j1 == 0, and remaining tuples have i1 == the i2 from the\n        tuple preceding it, and likewise for j1 == the previous j2.\n\n        The tags are strings, with these meanings:\n\n        'replace':  a[i1:i2] should be replaced by b[j1:j2]\n        'delete':   a[i1:i2] should be deleted.\n                    Note that j1==j2 in this case.\n        'insert':   b[j1:j2] should be inserted at a[i1:i1].\n                    Note that i1==i2 in this case.\n        'equal':    a[i1:i2] == b[j1:j2]\n\n        >>> a = \"qabxcd\"\n        >>> b = \"abycdf\"\n        >>> s = SequenceMatcher(None, a, b)\n        >>> for tag, i1, i2, j1, j2 in s.get_opcodes():\n        ...    print (\"%7s a[%d:%d] (%s) b[%d:%d] (%s)\" %\n        ...           (tag, i1, i2, a[i1:i2], j1, j2, b[j1:j2]))\n         delete a[0:1] (q) b[0:0] ()\n          equal a[1:3] (ab) b[0:2] (ab)\n        replace a[3:4] (x) b[2:3] (y)\n          equal a[4:6] (cd) b[3:5] (cd)\n         insert a[6:6] () b[5:6] (f)\n        \"\"\"\n\n        if self.opcodes is not None:\n            return self.opcodes\n        i = j = 0\n        self.opcodes = answer = []\n        for ai, bj, size in self.get_matching_blocks():\n            # invariant:  we've pumped out correct diffs to change\n            # a[:i] into b[:j], and the next matching block is\n            # a[ai:ai+size] == b[bj:bj+size].  So we need to pump\n            # out a diff to change a[i:ai] into b[j:bj], pump out\n            # the matching block, and move (i,j) beyond the match\n            tag = ''\n            if i < ai and j < bj:\n                tag = 'replace'\n            elif i < ai:\n                tag = 'delete'\n            elif j < bj:\n                tag = 'insert'\n            if tag:\n                answer.append( (tag, i, ai, j, bj) )\n            i, j = ai+size, bj+size\n            # the list of matching blocks is terminated by a\n            # sentinel with size 0\n            if size:\n                answer.append( ('equal', ai, i, bj, j) )\n        return answer\n\n    def get_grouped_opcodes(self, n=3):\n        \"\"\" Isolate change clusters by eliminating ranges with no changes.\n\n        Return a generator of groups with up to n lines of context.\n        Each group is in the same format as returned by get_opcodes().\n\n        >>> from pprint import pprint\n        >>> a = map(str, range(1,40))\n        >>> b = a[:]\n        >>> b[8:8] = ['i']     # Make an insertion\n        >>> b[20] += 'x'       # Make a replacement\n        >>> b[23:28] = []      # Make a deletion\n        >>> b[30] += 'y'       # Make another replacement\n        >>> pprint(list(SequenceMatcher(None,a,b).get_grouped_opcodes()))\n        [[('equal', 5, 8, 5, 8), ('insert', 8, 8, 8, 9), ('equal', 8, 11, 9, 12)],\n         [('equal', 16, 19, 17, 20),\n          ('replace', 19, 20, 20, 21),\n          ('equal', 20, 22, 21, 23),\n          ('delete', 22, 27, 23, 23),\n          ('equal', 27, 30, 23, 26)],\n         [('equal', 31, 34, 27, 30),\n          ('replace', 34, 35, 30, 31),\n          ('equal', 35, 38, 31, 34)]]\n        \"\"\"\n\n        codes = self.get_opcodes()\n        if not codes:\n            codes = [(\"equal\", 0, 1, 0, 1)]\n        # Fixup leading and trailing groups if they show no changes.\n        if codes[0][0] == 'equal':\n            tag, i1, i2, j1, j2 = codes[0]\n            codes[0] = tag, max(i1, i2-n), i2, max(j1, j2-n), j2\n        if codes[-1][0] == 'equal':\n            tag, i1, i2, j1, j2 = codes[-1]\n            codes[-1] = tag, i1, min(i2, i1+n), j1, min(j2, j1+n)\n\n        nn = n + n\n        group = []\n        for tag, i1, i2, j1, j2 in codes:\n            # End the current group and start a new one whenever\n            # there is a large range with no changes.\n            if tag == 'equal' and i2-i1 > nn:\n                group.append((tag, i1, min(i2, i1+n), j1, min(j2, j1+n)))\n                yield group\n                group = []\n                i1, j1 = max(i1, i2-n), max(j1, j2-n)\n            group.append((tag, i1, i2, j1 ,j2))\n        if group and not (len(group)==1 and group[0][0] == 'equal'):\n            yield group\n\n    def ratio(self):\n        \"\"\"Return a measure of the sequences' similarity (float in [0,1]).\n\n        Where T is the total number of elements in both sequences, and\n        M is the number of matches, this is 2.0*M / T.\n        Note that this is 1 if the sequences are identical, and 0 if\n        they have nothing in common.\n\n        .ratio() is expensive to compute if you haven't already computed\n        .get_matching_blocks() or .get_opcodes(), in which case you may\n        want to try .quick_ratio() or .real_quick_ratio() first to get an\n        upper bound.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.quick_ratio()\n        0.75\n        >>> s.real_quick_ratio()\n        1.0\n        \"\"\"\n\n        matches = reduce(lambda sum, triple: sum + triple[-1],\n                         self.get_matching_blocks(), 0)\n        return _calculate_ratio(matches, len(self.a) + len(self.b))\n\n    def quick_ratio(self):\n        \"\"\"Return an upper bound on ratio() relatively quickly.\n\n        This isn't defined beyond that it is an upper bound on .ratio(), and\n        is faster to compute.\n        \"\"\"\n\n        # viewing a and b as multisets, set matches to the cardinality\n        # of their intersection; this counts the number of matches\n        # without regard to order, so is clearly an upper bound\n        if self.fullbcount is None:\n            self.fullbcount = fullbcount = {}\n            for elt in self.b:\n                fullbcount[elt] = fullbcount.get(elt, 0) + 1\n        fullbcount = self.fullbcount\n        # avail[x] is the number of times x appears in 'b' less the\n        # number of times we've seen it in 'a' so far ... kinda\n        avail = {}\n        availhas, matches = avail.__contains__, 0\n        for elt in self.a:\n            if availhas(elt):\n                numb = avail[elt]\n            else:\n                numb = fullbcount.get(elt, 0)\n            avail[elt] = numb - 1\n            if numb > 0:\n                matches = matches + 1\n        return _calculate_ratio(matches, len(self.a) + len(self.b))\n\n    def real_quick_ratio(self):\n        \"\"\"Return an upper bound on ratio() very quickly.\n\n        This isn't defined beyond that it is an upper bound on .ratio(), and\n        is faster to compute than either .ratio() or .quick_ratio().\n        \"\"\"\n\n        la, lb = len(self.a), len(self.b)\n        # can't have more matches than the number of elements in the\n        # shorter sequence\n        return _calculate_ratio(min(la, lb), la + lb)\n\ndef get_close_matches(word, possibilities, n=3, cutoff=0.6):\n    \"\"\"Use SequenceMatcher to return list of the best \"good enough\" matches.\n\n    word is a sequence for which close matches are desired (typically a\n    string).\n\n    possibilities is a list of sequences against which to match word\n    (typically a list of strings).\n\n    Optional arg n (default 3) is the maximum number of close matches to\n    return.  n must be > 0.\n\n    Optional arg cutoff (default 0.6) is a float in [0, 1].  Possibilities\n    that don't score at least that similar to word are ignored.\n\n    The best (no more than n) matches among the possibilities are returned\n    in a list, sorted by similarity score, most similar first.\n\n    >>> get_close_matches(\"appel\", [\"ape\", \"apple\", \"peach\", \"puppy\"])\n    ['apple', 'ape']\n    >>> import keyword as _keyword\n    >>> get_close_matches(\"wheel\", _keyword.kwlist)\n    ['while']\n    >>> get_close_matches(\"apple\", _keyword.kwlist)\n    []\n    >>> get_close_matches(\"accept\", _keyword.kwlist)\n    ['except']\n    \"\"\"\n\n    if not n >  0:\n        raise ValueError(\"n must be > 0: %r\" % (n,))\n    if not 0.0 <= cutoff <= 1.0:\n        raise ValueError(\"cutoff must be in [0.0, 1.0]: %r\" % (cutoff,))\n    result = []\n    s = SequenceMatcher()\n    s.set_seq2(word)\n    for x in possibilities:\n        s.set_seq1(x)\n        if s.real_quick_ratio() >= cutoff and \\\n           s.quick_ratio() >= cutoff and \\\n           s.ratio() >= cutoff:\n            result.append((s.ratio(), x))\n\n    # Move the best scorers to head of list\n    result = heapq.nlargest(n, result)\n    # Strip scores for the best n matches\n    return [x for score, x in result]\n\ndef _count_leading(line, ch):\n    \"\"\"\n    Return number of `ch` characters at the start of `line`.\n\n    Example:\n\n    >>> _count_leading('   abc', ' ')\n    3\n    \"\"\"\n\n    i, n = 0, len(line)\n    while i < n and line[i] == ch:\n        i += 1\n    return i\n\nclass Differ:\n    r\"\"\"\n    Differ is a class for comparing sequences of lines of text, and\n    producing human-readable differences or deltas.  Differ uses\n    SequenceMatcher both to compare sequences of lines, and to compare\n    sequences of characters within similar (near-matching) lines.\n\n    Each line of a Differ delta begins with a two-letter code:\n\n        '- '    line unique to sequence 1\n        '+ '    line unique to sequence 2\n        '  '    line common to both sequences\n        '? '    line not present in either input sequence\n\n    Lines beginning with '? ' attempt to guide the eye to intraline\n    differences, and were not present in either input sequence.  These lines\n    can be confusing if the sequences contain tab characters.\n\n    Note that Differ makes no claim to produce a *minimal* diff.  To the\n    contrary, minimal diffs are often counter-intuitive, because they synch\n    up anywhere possible, sometimes accidental matches 100 pages apart.\n    Restricting synch points to contiguous matches preserves some notion of\n    locality, at the occasional cost of producing a longer diff.\n\n    Example: Comparing two texts.\n\n    First we set up the texts, sequences of individual single-line strings\n    ending with newlines (such sequences can also be obtained from the\n    `readlines()` method of file-like objects):\n\n    >>> text1 = '''  1. Beautiful is better than ugly.\n    ...   2. Explicit is better than implicit.\n    ...   3. Simple is better than complex.\n    ...   4. Complex is better than complicated.\n    ... '''.splitlines(1)\n    >>> len(text1)\n    4\n    >>> text1[0][-1]\n    '\\n'\n    >>> text2 = '''  1. Beautiful is better than ugly.\n    ...   3.   Simple is better than complex.\n    ...   4. Complicated is better than complex.\n    ...   5. Flat is better than nested.\n    ... '''.splitlines(1)\n\n    Next we instantiate a Differ object:\n\n    >>> d = Differ()\n\n    Note that when instantiating a Differ object we may pass functions to\n    filter out line and character 'junk'.  See Differ.__init__ for details.\n\n    Finally, we compare the two:\n\n    >>> result = list(d.compare(text1, text2))\n\n    'result' is a list of strings, so let's pretty-print it:\n\n    >>> from pprint import pprint as _pprint\n    >>> _pprint(result)\n    ['    1. Beautiful is better than ugly.\\n',\n     '-   2. Explicit is better than implicit.\\n',\n     '-   3. Simple is better than complex.\\n',\n     '+   3.   Simple is better than complex.\\n',\n     '?     ++\\n',\n     '-   4. Complex is better than complicated.\\n',\n     '?            ^                     ---- ^\\n',\n     '+   4. Complicated is better than complex.\\n',\n     '?           ++++ ^                      ^\\n',\n     '+   5. Flat is better than nested.\\n']\n\n    As a single multi-line string it looks like this:\n\n    >>> print ''.join(result),\n        1. Beautiful is better than ugly.\n    -   2. Explicit is better than implicit.\n    -   3. Simple is better than complex.\n    +   3.   Simple is better than complex.\n    ?     ++\n    -   4. Complex is better than complicated.\n    ?            ^                     ---- ^\n    +   4. Complicated is better than complex.\n    ?           ++++ ^                      ^\n    +   5. Flat is better than nested.\n\n    Methods:\n\n    __init__(linejunk=None, charjunk=None)\n        Construct a text differencer, with optional filters.\n\n    compare(a, b)\n        Compare two sequences of lines; generate the resulting delta.\n    \"\"\"\n\n    def __init__(self, linejunk=None, charjunk=None):\n        \"\"\"\n        Construct a text differencer, with optional filters.\n\n        The two optional keyword parameters are for filter functions:\n\n        - `linejunk`: A function that should accept a single string argument,\n          and return true iff the string is junk. The module-level function\n          `IS_LINE_JUNK` may be used to filter out lines without visible\n          characters, except for at most one splat ('#').  It is recommended\n          to leave linejunk None; as of Python 2.3, the underlying\n          SequenceMatcher class has grown an adaptive notion of \"noise\" lines\n          that's better than any static definition the author has ever been\n          able to craft.\n\n        - `charjunk`: A function that should accept a string of length 1. The\n          module-level function `IS_CHARACTER_JUNK` may be used to filter out\n          whitespace characters (a blank or tab; **note**: bad idea to include\n          newline in this!).  Use of IS_CHARACTER_JUNK is recommended.\n        \"\"\"\n\n        self.linejunk = linejunk\n        self.charjunk = charjunk\n\n    def compare(self, a, b):\n        r\"\"\"\n        Compare two sequences of lines; generate the resulting delta.\n\n        Each sequence must contain individual single-line strings ending with\n        newlines. Such sequences can be obtained from the `readlines()` method\n        of file-like objects.  The delta generated also consists of newline-\n        terminated strings, ready to be printed as-is via the writeline()\n        method of a file-like object.\n\n        Example:\n\n        >>> print ''.join(Differ().compare('one\\ntwo\\nthree\\n'.splitlines(1),\n        ...                                'ore\\ntree\\nemu\\n'.splitlines(1))),\n        - one\n        ?  ^\n        + ore\n        ?  ^\n        - two\n        - three\n        ?  -\n        + tree\n        + emu\n        \"\"\"\n\n        cruncher = SequenceMatcher(self.linejunk, a, b)\n        for tag, alo, ahi, blo, bhi in cruncher.get_opcodes():\n            if tag == 'replace':\n                g = self._fancy_replace(a, alo, ahi, b, blo, bhi)\n            elif tag == 'delete':\n                g = self._dump('-', a, alo, ahi)\n            elif tag == 'insert':\n                g = self._dump('+', b, blo, bhi)\n            elif tag == 'equal':\n                g = self._dump(' ', a, alo, ahi)\n            else:\n                raise ValueError, 'unknown tag %r' % (tag,)\n\n            for line in g:\n                yield line\n\n    def _dump(self, tag, x, lo, hi):\n        \"\"\"Generate comparison results for a same-tagged range.\"\"\"\n        for i in xrange(lo, hi):\n            yield '%s %s' % (tag, x[i])\n\n    def _plain_replace(self, a, alo, ahi, b, blo, bhi):\n        assert alo < ahi and blo < bhi\n        # dump the shorter block first -- reduces the burden on short-term\n        # memory if the blocks are of very different sizes\n        if bhi - blo < ahi - alo:\n            first  = self._dump('+', b, blo, bhi)\n            second = self._dump('-', a, alo, ahi)\n        else:\n            first  = self._dump('-', a, alo, ahi)\n            second = self._dump('+', b, blo, bhi)\n\n        for g in first, second:\n            for line in g:\n                yield line\n\n    def _fancy_replace(self, a, alo, ahi, b, blo, bhi):\n        r\"\"\"\n        When replacing one block of lines with another, search the blocks\n        for *similar* lines; the best-matching pair (if any) is used as a\n        synch point, and intraline difference marking is done on the\n        similar pair. Lots of work, but often worth it.\n\n        Example:\n\n        >>> d = Differ()\n        >>> results = d._fancy_replace(['abcDefghiJkl\\n'], 0, 1,\n        ...                            ['abcdefGhijkl\\n'], 0, 1)\n        >>> print ''.join(results),\n        - abcDefghiJkl\n        ?    ^  ^  ^\n        + abcdefGhijkl\n        ?    ^  ^  ^\n        \"\"\"\n\n        # don't synch up unless the lines have a similarity score of at\n        # least cutoff; best_ratio tracks the best score seen so far\n        best_ratio, cutoff = 0.74, 0.75\n        cruncher = SequenceMatcher(self.charjunk)\n        eqi, eqj = None, None   # 1st indices of equal lines (if any)\n\n        # search for the pair that matches best without being identical\n        # (identical lines must be junk lines, & we don't want to synch up\n        # on junk -- unless we have to)\n        for j in xrange(blo, bhi):\n            bj = b[j]\n            cruncher.set_seq2(bj)\n            for i in xrange(alo, ahi):\n                ai = a[i]\n                if ai == bj:\n                    if eqi is None:\n                        eqi, eqj = i, j\n                    continue\n                cruncher.set_seq1(ai)\n                # computing similarity is expensive, so use the quick\n                # upper bounds first -- have seen this speed up messy\n                # compares by a factor of 3.\n                # note that ratio() is only expensive to compute the first\n                # time it's called on a sequence pair; the expensive part\n                # of the computation is cached by cruncher\n                if cruncher.real_quick_ratio() > best_ratio and \\\n                      cruncher.quick_ratio() > best_ratio and \\\n                      cruncher.ratio() > best_ratio:\n                    best_ratio, best_i, best_j = cruncher.ratio(), i, j\n        if best_ratio < cutoff:\n            # no non-identical \"pretty close\" pair\n            if eqi is None:\n                # no identical pair either -- treat it as a straight replace\n                for line in self._plain_replace(a, alo, ahi, b, blo, bhi):\n                    yield line\n                return\n            # no close pair, but an identical pair -- synch up on that\n            best_i, best_j, best_ratio = eqi, eqj, 1.0\n        else:\n            # there's a close pair, so forget the identical pair (if any)\n            eqi = None\n\n        # a[best_i] very similar to b[best_j]; eqi is None iff they're not\n        # identical\n\n        # pump out diffs from before the synch point\n        for line in self._fancy_helper(a, alo, best_i, b, blo, best_j):\n            yield line\n\n        # do intraline marking on the synch pair\n        aelt, belt = a[best_i], b[best_j]\n        if eqi is None:\n            # pump out a '-', '?', '+', '?' quad for the synched lines\n            atags = btags = \"\"\n            cruncher.set_seqs(aelt, belt)\n            for tag, ai1, ai2, bj1, bj2 in cruncher.get_opcodes():\n                la, lb = ai2 - ai1, bj2 - bj1\n                if tag == 'replace':\n                    atags += '^' * la\n                    btags += '^' * lb\n                elif tag == 'delete':\n                    atags += '-' * la\n                elif tag == 'insert':\n                    btags += '+' * lb\n                elif tag == 'equal':\n                    atags += ' ' * la\n                    btags += ' ' * lb\n                else:\n                    raise ValueError, 'unknown tag %r' % (tag,)\n            for line in self._qformat(aelt, belt, atags, btags):\n                yield line\n        else:\n            # the synch pair is identical\n            yield '  ' + aelt\n\n        # pump out diffs from after the synch point\n        for line in self._fancy_helper(a, best_i+1, ahi, b, best_j+1, bhi):\n            yield line\n\n    def _fancy_helper(self, a, alo, ahi, b, blo, bhi):\n        g = []\n        if alo < ahi:\n            if blo < bhi:\n                g = self._fancy_replace(a, alo, ahi, b, blo, bhi)\n            else:\n                g = self._dump('-', a, alo, ahi)\n        elif blo < bhi:\n            g = self._dump('+', b, blo, bhi)\n\n        for line in g:\n            yield line\n\n    def _qformat(self, aline, bline, atags, btags):\n        r\"\"\"\n        Format \"?\" output and deal with leading tabs.\n\n        Example:\n\n        >>> d = Differ()\n        >>> results = d._qformat('\\tabcDefghiJkl\\n', '\\tabcdefGhijkl\\n',\n        ...                      '  ^ ^  ^      ', '  ^ ^  ^      ')\n        >>> for line in results: print repr(line)\n        ...\n        '- \\tabcDefghiJkl\\n'\n        '? \\t ^ ^  ^\\n'\n        '+ \\tabcdefGhijkl\\n'\n        '? \\t ^ ^  ^\\n'\n        \"\"\"\n\n        # Can hurt, but will probably help most of the time.\n        common = min(_count_leading(aline, \"\\t\"),\n                     _count_leading(bline, \"\\t\"))\n        common = min(common, _count_leading(atags[:common], \" \"))\n        common = min(common, _count_leading(btags[:common], \" \"))\n        atags = atags[common:].rstrip()\n        btags = btags[common:].rstrip()\n\n        yield \"- \" + aline\n        if atags:\n            yield \"? %s%s\\n\" % (\"\\t\" * common, atags)\n\n        yield \"+ \" + bline\n        if btags:\n            yield \"? %s%s\\n\" % (\"\\t\" * common, btags)\n\n# With respect to junk, an earlier version of ndiff simply refused to\n# *start* a match with a junk element.  The result was cases like this:\n#     before: private Thread currentThread;\n#     after:  private volatile Thread currentThread;\n# If you consider whitespace to be junk, the longest contiguous match\n# not starting with junk is \"e Thread currentThread\".  So ndiff reported\n# that \"e volatil\" was inserted between the 't' and the 'e' in \"private\".\n# While an accurate view, to people that's absurd.  The current version\n# looks for matching blocks that are entirely junk-free, then extends the\n# longest one of those as far as possible but only with matching junk.\n# So now \"currentThread\" is matched, then extended to suck up the\n# preceding blank; then \"private\" is matched, and extended to suck up the\n# following blank; then \"Thread\" is matched; and finally ndiff reports\n# that \"volatile \" was inserted before \"Thread\".  The only quibble\n# remaining is that perhaps it was really the case that \" volatile\"\n# was inserted after \"private\".  I can live with that <wink>.\n\nimport re\n\ndef IS_LINE_JUNK(line, pat=re.compile(r\"\\s*#?\\s*$\").match):\n    r\"\"\"\n    Return 1 for ignorable line: iff `line` is blank or contains a single '#'.\n\n    Examples:\n\n    >>> IS_LINE_JUNK('\\n')\n    True\n    >>> IS_LINE_JUNK('  #   \\n')\n    True\n    >>> IS_LINE_JUNK('hello\\n')\n    False\n    \"\"\"\n\n    return pat(line) is not None\n\ndef IS_CHARACTER_JUNK(ch, ws=\" \\t\"):\n    r\"\"\"\n    Return 1 for ignorable character: iff `ch` is a space or tab.\n\n    Examples:\n\n    >>> IS_CHARACTER_JUNK(' ')\n    True\n    >>> IS_CHARACTER_JUNK('\\t')\n    True\n    >>> IS_CHARACTER_JUNK('\\n')\n    False\n    >>> IS_CHARACTER_JUNK('x')\n    False\n    \"\"\"\n\n    return ch in ws\n\n\n########################################################################\n###  Unified Diff\n########################################################################\n\ndef _format_range_unified(start, stop):\n    'Convert range to the \"ed\" format'\n    # Per the diff spec at http://www.unix.org/single_unix_specification/\n    beginning = start + 1     # lines start numbering with one\n    length = stop - start\n    if length == 1:\n        return '{}'.format(beginning)\n    if not length:\n        beginning -= 1        # empty ranges begin at line just before the range\n    return '{},{}'.format(beginning, length)\n\ndef unified_diff(a, b, fromfile='', tofile='', fromfiledate='',\n                 tofiledate='', n=3, lineterm='\\n'):\n    r\"\"\"\n    Compare two sequences of lines; generate the delta as a unified diff.\n\n    Unified diffs are a compact way of showing line changes and a few\n    lines of context.  The number of context lines is set by 'n' which\n    defaults to three.\n\n    By default, the diff control lines (those with ---, +++, or @@) are\n    created with a trailing newline.  This is helpful so that inputs\n    created from file.readlines() result in diffs that are suitable for\n    file.writelines() since both the inputs and outputs have trailing\n    newlines.\n\n    For inputs that do not have trailing newlines, set the lineterm\n    argument to \"\" so that the output will be uniformly newline free.\n\n    The unidiff format normally has a header for filenames and modification\n    times.  Any or all of these may be specified using strings for\n    'fromfile', 'tofile', 'fromfiledate', and 'tofiledate'.\n    The modification times are normally expressed in the ISO 8601 format.\n\n    Example:\n\n    >>> for line in unified_diff('one two three four'.split(),\n    ...             'zero one tree four'.split(), 'Original', 'Current',\n    ...             '2005-01-26 23:30:50', '2010-04-02 10:20:52',\n    ...             lineterm=''):\n    ...     print line                  # doctest: +NORMALIZE_WHITESPACE\n    --- Original        2005-01-26 23:30:50\n    +++ Current         2010-04-02 10:20:52\n    @@ -1,4 +1,4 @@\n    +zero\n     one\n    -two\n    -three\n    +tree\n     four\n    \"\"\"\n\n    started = False\n    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):\n        if not started:\n            started = True\n            fromdate = '\\t{}'.format(fromfiledate) if fromfiledate else ''\n            todate = '\\t{}'.format(tofiledate) if tofiledate else ''\n            yield '--- {}{}{}'.format(fromfile, fromdate, lineterm)\n            yield '+++ {}{}{}'.format(tofile, todate, lineterm)\n\n        first, last = group[0], group[-1]\n        file1_range = _format_range_unified(first[1], last[2])\n        file2_range = _format_range_unified(first[3], last[4])\n        yield '@@ -{} +{} @@{}'.format(file1_range, file2_range, lineterm)\n\n        for tag, i1, i2, j1, j2 in group:\n            if tag == 'equal':\n                for line in a[i1:i2]:\n                    yield ' ' + line\n                continue\n            if tag in ('replace', 'delete'):\n                for line in a[i1:i2]:\n                    yield '-' + line\n            if tag in ('replace', 'insert'):\n                for line in b[j1:j2]:\n                    yield '+' + line\n\n\n########################################################################\n###  Context Diff\n########################################################################\n\ndef _format_range_context(start, stop):\n    'Convert range to the \"ed\" format'\n    # Per the diff spec at http://www.unix.org/single_unix_specification/\n    beginning = start + 1     # lines start numbering with one\n    length = stop - start\n    if not length:\n        beginning -= 1        # empty ranges begin at line just before the range\n    if length <= 1:\n        return '{}'.format(beginning)\n    return '{},{}'.format(beginning, beginning + length - 1)\n\n# See http://www.unix.org/single_unix_specification/\ndef context_diff(a, b, fromfile='', tofile='',\n                 fromfiledate='', tofiledate='', n=3, lineterm='\\n'):\n    r\"\"\"\n    Compare two sequences of lines; generate the delta as a context diff.\n\n    Context diffs are a compact way of showing line changes and a few\n    lines of context.  The number of context lines is set by 'n' which\n    defaults to three.\n\n    By default, the diff control lines (those with *** or ---) are\n    created with a trailing newline.  This is helpful so that inputs\n    created from file.readlines() result in diffs that are suitable for\n    file.writelines() since both the inputs and outputs have trailing\n    newlines.\n\n    For inputs that do not have trailing newlines, set the lineterm\n    argument to \"\" so that the output will be uniformly newline free.\n\n    The context diff format normally has a header for filenames and\n    modification times.  Any or all of these may be specified using\n    strings for 'fromfile', 'tofile', 'fromfiledate', and 'tofiledate'.\n    The modification times are normally expressed in the ISO 8601 format.\n    If not specified, the strings default to blanks.\n\n    Example:\n\n    >>> print ''.join(context_diff('one\\ntwo\\nthree\\nfour\\n'.splitlines(1),\n    ...       'zero\\none\\ntree\\nfour\\n'.splitlines(1), 'Original', 'Current')),\n    *** Original\n    --- Current\n    ***************\n    *** 1,4 ****\n      one\n    ! two\n    ! three\n      four\n    --- 1,4 ----\n    + zero\n      one\n    ! tree\n      four\n    \"\"\"\n\n    prefix = dict(insert='+ ', delete='- ', replace='! ', equal='  ')\n    started = False\n    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):\n        if not started:\n            started = True\n            fromdate = '\\t{}'.format(fromfiledate) if fromfiledate else ''\n            todate = '\\t{}'.format(tofiledate) if tofiledate else ''\n            yield '*** {}{}{}'.format(fromfile, fromdate, lineterm)\n            yield '--- {}{}{}'.format(tofile, todate, lineterm)\n\n        first, last = group[0], group[-1]\n        yield '***************' + lineterm\n\n        file1_range = _format_range_context(first[1], last[2])\n        yield '*** {} ****{}'.format(file1_range, lineterm)\n\n        if any(tag in ('replace', 'delete') for tag, _, _, _, _ in group):\n            for tag, i1, i2, _, _ in group:\n                if tag != 'insert':\n                    for line in a[i1:i2]:\n                        yield prefix[tag] + line\n\n        file2_range = _format_range_context(first[3], last[4])\n        yield '--- {} ----{}'.format(file2_range, lineterm)\n\n        if any(tag in ('replace', 'insert') for tag, _, _, _, _ in group):\n            for tag, _, _, j1, j2 in group:\n                if tag != 'delete':\n                    for line in b[j1:j2]:\n                        yield prefix[tag] + line\n\ndef ndiff(a, b, linejunk=None, charjunk=IS_CHARACTER_JUNK):\n    r\"\"\"\n    Compare `a` and `b` (lists of strings); return a `Differ`-style delta.\n\n    Optional keyword parameters `linejunk` and `charjunk` are for filter\n    functions (or None):\n\n    - linejunk: A function that should accept a single string argument, and\n      return true iff the string is junk.  The default is None, and is\n      recommended; as of Python 2.3, an adaptive notion of \"noise\" lines is\n      used that does a good job on its own.\n\n    - charjunk: A function that should accept a string of length 1. The\n      default is module-level function IS_CHARACTER_JUNK, which filters out\n      whitespace characters (a blank or tab; note: bad idea to include newline\n      in this!).\n\n    Tools/scripts/ndiff.py is a command-line front-end to this function.\n\n    Example:\n\n    >>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(1),\n    ...              'ore\\ntree\\nemu\\n'.splitlines(1))\n    >>> print ''.join(diff),\n    - one\n    ?  ^\n    + ore\n    ?  ^\n    - two\n    - three\n    ?  -\n    + tree\n    + emu\n    \"\"\"\n    return Differ(linejunk, charjunk).compare(a, b)\n\ndef _mdiff(fromlines, tolines, context=None, linejunk=None,\n           charjunk=IS_CHARACTER_JUNK):\n    r\"\"\"Returns generator yielding marked up from/to side by side differences.\n\n    Arguments:\n    fromlines -- list of text lines to compared to tolines\n    tolines -- list of text lines to be compared to fromlines\n    context -- number of context lines to display on each side of difference,\n               if None, all from/to text lines will be generated.\n    linejunk -- passed on to ndiff (see ndiff documentation)\n    charjunk -- passed on to ndiff (see ndiff documentation)\n\n    This function returns an iterator which returns a tuple:\n    (from line tuple, to line tuple, boolean flag)\n\n    from/to line tuple -- (line num, line text)\n        line num -- integer or None (to indicate a context separation)\n        line text -- original line text with following markers inserted:\n            '\\0+' -- marks start of added text\n            '\\0-' -- marks start of deleted text\n            '\\0^' -- marks start of changed text\n            '\\1' -- marks end of added/deleted/changed text\n\n    boolean flag -- None indicates context separation, True indicates\n        either \"from\" or \"to\" line contains a change, otherwise False.\n\n    This function/iterator was originally developed to generate side by side\n    file difference for making HTML pages (see HtmlDiff class for example\n    usage).\n\n    Note, this function utilizes the ndiff function to generate the side by\n    side difference markup.  Optional ndiff arguments may be passed to this\n    function and they in turn will be passed to ndiff.\n    \"\"\"\n    import re\n\n    # regular expression for finding intraline change indices\n    change_re = re.compile('(\\++|\\-+|\\^+)')\n\n    # create the difference iterator to generate the differences\n    diff_lines_iterator = ndiff(fromlines,tolines,linejunk,charjunk)\n\n    def _make_line(lines, format_key, side, num_lines=[0,0]):\n        \"\"\"Returns line of text with user's change markup and line formatting.\n\n        lines -- list of lines from the ndiff generator to produce a line of\n                 text from.  When producing the line of text to return, the\n                 lines used are removed from this list.\n        format_key -- '+' return first line in list with \"add\" markup around\n                          the entire line.\n                      '-' return first line in list with \"delete\" markup around\n                          the entire line.\n                      '?' return first line in list with add/delete/change\n                          intraline markup (indices obtained from second line)\n                      None return first line in list with no markup\n        side -- indice into the num_lines list (0=from,1=to)\n        num_lines -- from/to current line number.  This is NOT intended to be a\n                     passed parameter.  It is present as a keyword argument to\n                     maintain memory of the current line numbers between calls\n                     of this function.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        num_lines[side] += 1\n        # Handle case where no user markup is to be added, just return line of\n        # text with user's line format to allow for usage of the line number.\n        if format_key is None:\n            return (num_lines[side],lines.pop(0)[2:])\n        # Handle case of intraline changes\n        if format_key == '?':\n            text, markers = lines.pop(0), lines.pop(0)\n            # find intraline changes (store change type and indices in tuples)\n            sub_info = []\n            def record_sub_info(match_object,sub_info=sub_info):\n                sub_info.append([match_object.group(1)[0],match_object.span()])\n                return match_object.group(1)\n            change_re.sub(record_sub_info,markers)\n            # process each tuple inserting our special marks that won't be\n            # noticed by an xml/html escaper.\n            for key,(begin,end) in sub_info[::-1]:\n                text = text[0:begin]+'\\0'+key+text[begin:end]+'\\1'+text[end:]\n            text = text[2:]\n        # Handle case of add/delete entire line\n        else:\n            text = lines.pop(0)[2:]\n            # if line of text is just a newline, insert a space so there is\n            # something for the user to highlight and see.\n            if not text:\n                text = ' '\n            # insert marks that won't be noticed by an xml/html escaper.\n            text = '\\0' + format_key + text + '\\1'\n        # Return line of text, first allow user's line formatter to do its\n        # thing (such as adding the line number) then replace the special\n        # marks with what the user's change markup.\n        return (num_lines[side],text)\n\n    def _line_iterator():\n        \"\"\"Yields from/to lines of text with a change indication.\n\n        This function is an iterator.  It itself pulls lines from a\n        differencing iterator, processes them and yields them.  When it can\n        it yields both a \"from\" and a \"to\" line, otherwise it will yield one\n        or the other.  In addition to yielding the lines of from/to text, a\n        boolean flag is yielded to indicate if the text line(s) have\n        differences in them.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        lines = []\n        num_blanks_pending, num_blanks_to_yield = 0, 0\n        while True:\n            # Load up next 4 lines so we can look ahead, create strings which\n            # are a concatenation of the first character of each of the 4 lines\n            # so we can do some very readable comparisons.\n            while len(lines) < 4:\n                try:\n                    lines.append(diff_lines_iterator.next())\n                except StopIteration:\n                    lines.append('X')\n            s = ''.join([line[0] for line in lines])\n            if s.startswith('X'):\n                # When no more lines, pump out any remaining blank lines so the\n                # corresponding add/delete lines get a matching blank line so\n                # all line pairs get yielded at the next level.\n                num_blanks_to_yield = num_blanks_pending\n            elif s.startswith('-?+?'):\n                # simple intraline change\n                yield _make_line(lines,'?',0), _make_line(lines,'?',1), True\n                continue\n            elif s.startswith('--++'):\n                # in delete block, add block coming: we do NOT want to get\n                # caught up on blank lines yet, just process the delete line\n                num_blanks_pending -= 1\n                yield _make_line(lines,'-',0), None, True\n                continue\n            elif s.startswith(('--?+', '--+', '- ')):\n                # in delete block and see a intraline change or unchanged line\n                # coming: yield the delete line and then blanks\n                from_line,to_line = _make_line(lines,'-',0), None\n                num_blanks_to_yield,num_blanks_pending = num_blanks_pending-1,0\n            elif s.startswith('-+?'):\n                # intraline change\n                yield _make_line(lines,None,0), _make_line(lines,'?',1), True\n                continue\n            elif s.startswith('-?+'):\n                # intraline change\n                yield _make_line(lines,'?',0), _make_line(lines,None,1), True\n                continue\n            elif s.startswith('-'):\n                # delete FROM line\n                num_blanks_pending -= 1\n                yield _make_line(lines,'-',0), None, True\n                continue\n            elif s.startswith('+--'):\n                # in add block, delete block coming: we do NOT want to get\n                # caught up on blank lines yet, just process the add line\n                num_blanks_pending += 1\n                yield None, _make_line(lines,'+',1), True\n                continue\n            elif s.startswith(('+ ', '+-')):\n                # will be leaving an add block: yield blanks then add line\n                from_line, to_line = None, _make_line(lines,'+',1)\n                num_blanks_to_yield,num_blanks_pending = num_blanks_pending+1,0\n            elif s.startswith('+'):\n                # inside an add block, yield the add line\n                num_blanks_pending += 1\n                yield None, _make_line(lines,'+',1), True\n                continue\n            elif s.startswith(' '):\n                # unchanged text, yield it to both sides\n                yield _make_line(lines[:],None,0),_make_line(lines,None,1),False\n                continue\n            # Catch up on the blank lines so when we yield the next from/to\n            # pair, they are lined up.\n            while(num_blanks_to_yield < 0):\n                num_blanks_to_yield += 1\n                yield None,('','\\n'),True\n            while(num_blanks_to_yield > 0):\n                num_blanks_to_yield -= 1\n                yield ('','\\n'),None,True\n            if s.startswith('X'):\n                raise StopIteration\n            else:\n                yield from_line,to_line,True\n\n    def _line_pair_iterator():\n        \"\"\"Yields from/to lines of text with a change indication.\n\n        This function is an iterator.  It itself pulls lines from the line\n        iterator.  Its difference from that iterator is that this function\n        always yields a pair of from/to text lines (with the change\n        indication).  If necessary it will collect single from/to lines\n        until it has a matching pair from/to pair to yield.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        line_iterator = _line_iterator()\n        fromlines,tolines=[],[]\n        while True:\n            # Collecting lines of text until we have a from/to pair\n            while (len(fromlines)==0 or len(tolines)==0):\n                from_line, to_line, found_diff =line_iterator.next()\n                if from_line is not None:\n                    fromlines.append((from_line,found_diff))\n                if to_line is not None:\n                    tolines.append((to_line,found_diff))\n            # Once we have a pair, remove them from the collection and yield it\n            from_line, fromDiff = fromlines.pop(0)\n            to_line, to_diff = tolines.pop(0)\n            yield (from_line,to_line,fromDiff or to_diff)\n\n    # Handle case where user does not want context differencing, just yield\n    # them up without doing anything else with them.\n    line_pair_iterator = _line_pair_iterator()\n    if context is None:\n        while True:\n            yield line_pair_iterator.next()\n    # Handle case where user wants context differencing.  We must do some\n    # storage of lines until we know for sure that they are to be yielded.\n    else:\n        context += 1\n        lines_to_write = 0\n        while True:\n            # Store lines up until we find a difference, note use of a\n            # circular queue because we only need to keep around what\n            # we need for context.\n            index, contextLines = 0, [None]*(context)\n            found_diff = False\n            while(found_diff is False):\n                from_line, to_line, found_diff = line_pair_iterator.next()\n                i = index % context\n                contextLines[i] = (from_line, to_line, found_diff)\n                index += 1\n            # Yield lines that we have collected so far, but first yield\n            # the user's separator.\n            if index > context:\n                yield None, None, None\n                lines_to_write = context\n            else:\n                lines_to_write = index\n                index = 0\n            while(lines_to_write):\n                i = index % context\n                index += 1\n                yield contextLines[i]\n                lines_to_write -= 1\n            # Now yield the context lines after the change\n            lines_to_write = context-1\n            while(lines_to_write):\n                from_line, to_line, found_diff = line_pair_iterator.next()\n                # If another change within the context, extend the context\n                if found_diff:\n                    lines_to_write = context-1\n                else:\n                    lines_to_write -= 1\n                yield from_line, to_line, found_diff\n\n\n_file_template = \"\"\"\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n          \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n\n<html>\n\n<head>\n    <meta http-equiv=\"Content-Type\"\n          content=\"text/html; charset=ISO-8859-1\" />\n    <title></title>\n    <style type=\"text/css\">%(styles)s\n    </style>\n</head>\n\n<body>\n    %(table)s%(legend)s\n</body>\n\n</html>\"\"\"\n\n_styles = \"\"\"\n        table.diff {font-family:Courier; border:medium;}\n        .diff_header {background-color:#e0e0e0}\n        td.diff_header {text-align:right}\n        .diff_next {background-color:#c0c0c0}\n        .diff_add {background-color:#aaffaa}\n        .diff_chg {background-color:#ffff77}\n        .diff_sub {background-color:#ffaaaa}\"\"\"\n\n_table_template = \"\"\"\n    <table class=\"diff\" id=\"difflib_chg_%(prefix)s_top\"\n           cellspacing=\"0\" cellpadding=\"0\" rules=\"groups\" >\n        <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>\n        <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>\n        %(header_row)s\n        <tbody>\n%(data_rows)s        </tbody>\n    </table>\"\"\"\n\n_legend = \"\"\"\n    <table class=\"diff\" summary=\"Legends\">\n        <tr> <th colspan=\"2\"> Legends </th> </tr>\n        <tr> <td> <table border=\"\" summary=\"Colors\">\n                      <tr><th> Colors </th> </tr>\n                      <tr><td class=\"diff_add\">&nbsp;Added&nbsp;</td></tr>\n                      <tr><td class=\"diff_chg\">Changed</td> </tr>\n                      <tr><td class=\"diff_sub\">Deleted</td> </tr>\n                  </table></td>\n             <td> <table border=\"\" summary=\"Links\">\n                      <tr><th colspan=\"2\"> Links </th> </tr>\n                      <tr><td>(f)irst change</td> </tr>\n                      <tr><td>(n)ext change</td> </tr>\n                      <tr><td>(t)op</td> </tr>\n                  </table></td> </tr>\n    </table>\"\"\"\n\nclass HtmlDiff(object):\n    \"\"\"For producing HTML side by side comparison with change highlights.\n\n    This class can be used to create an HTML table (or a complete HTML file\n    containing the table) showing a side by side, line by line comparison\n    of text with inter-line and intra-line change highlights.  The table can\n    be generated in either full or contextual difference mode.\n\n    The following methods are provided for HTML generation:\n\n    make_table -- generates HTML for a single side by side table\n    make_file -- generates complete HTML file with a single side by side table\n\n    See tools/scripts/diff.py for an example usage of this class.\n    \"\"\"\n\n    _file_template = _file_template\n    _styles = _styles\n    _table_template = _table_template\n    _legend = _legend\n    _default_prefix = 0\n\n    def __init__(self,tabsize=8,wrapcolumn=None,linejunk=None,\n                 charjunk=IS_CHARACTER_JUNK):\n        \"\"\"HtmlDiff instance initializer\n\n        Arguments:\n        tabsize -- tab stop spacing, defaults to 8.\n        wrapcolumn -- column number where lines are broken and wrapped,\n            defaults to None where lines are not wrapped.\n        linejunk,charjunk -- keyword arguments passed into ndiff() (used to by\n            HtmlDiff() to generate the side by side HTML differences).  See\n            ndiff() documentation for argument default values and descriptions.\n        \"\"\"\n        self._tabsize = tabsize\n        self._wrapcolumn = wrapcolumn\n        self._linejunk = linejunk\n        self._charjunk = charjunk\n\n    def make_file(self,fromlines,tolines,fromdesc='',todesc='',context=False,\n                  numlines=5):\n        \"\"\"Returns HTML file of side by side comparison with change highlights\n\n        Arguments:\n        fromlines -- list of \"from\" lines\n        tolines -- list of \"to\" lines\n        fromdesc -- \"from\" file column header string\n        todesc -- \"to\" file column header string\n        context -- set to True for contextual differences (defaults to False\n            which shows full differences).\n        numlines -- number of context lines.  When context is set True,\n            controls number of lines displayed before and after the change.\n            When context is False, controls the number of lines to place\n            the \"next\" link anchors before the next change (so click of\n            \"next\" link jumps to just before the change).\n        \"\"\"\n\n        return self._file_template % dict(\n            styles = self._styles,\n            legend = self._legend,\n            table = self.make_table(fromlines,tolines,fromdesc,todesc,\n                                    context=context,numlines=numlines))\n\n    def _tab_newline_replace(self,fromlines,tolines):\n        \"\"\"Returns from/to line lists with tabs expanded and newlines removed.\n\n        Instead of tab characters being replaced by the number of spaces\n        needed to fill in to the next tab stop, this function will fill\n        the space with tab characters.  This is done so that the difference\n        algorithms can identify changes in a file when tabs are replaced by\n        spaces and vice versa.  At the end of the HTML generation, the tab\n        characters will be replaced with a nonbreakable space.\n        \"\"\"\n        def expand_tabs(line):\n            # hide real spaces\n            line = line.replace(' ','\\0')\n            # expand tabs into spaces\n            line = line.expandtabs(self._tabsize)\n            # replace spaces from expanded tabs back into tab characters\n            # (we'll replace them with markup after we do differencing)\n            line = line.replace(' ','\\t')\n            return line.replace('\\0',' ').rstrip('\\n')\n        fromlines = [expand_tabs(line) for line in fromlines]\n        tolines = [expand_tabs(line) for line in tolines]\n        return fromlines,tolines\n\n    def _split_line(self,data_list,line_num,text):\n        \"\"\"Builds list of text lines by splitting text lines at wrap point\n\n        This function will determine if the input text line needs to be\n        wrapped (split) into separate lines.  If so, the first wrap point\n        will be determined and the first line appended to the output\n        text line list.  This function is used recursively to handle\n        the second part of the split line to further split it.\n        \"\"\"\n        # if blank line or context separator, just add it to the output list\n        if not line_num:\n            data_list.append((line_num,text))\n            return\n\n        # if line text doesn't need wrapping, just add it to the output list\n        size = len(text)\n        max = self._wrapcolumn\n        if (size <= max) or ((size -(text.count('\\0')*3)) <= max):\n            data_list.append((line_num,text))\n            return\n\n        # scan text looking for the wrap point, keeping track if the wrap\n        # point is inside markers\n        i = 0\n        n = 0\n        mark = ''\n        while n < max and i < size:\n            if text[i] == '\\0':\n                i += 1\n                mark = text[i]\n                i += 1\n            elif text[i] == '\\1':\n                i += 1\n                mark = ''\n            else:\n                i += 1\n                n += 1\n\n        # wrap point is inside text, break it up into separate lines\n        line1 = text[:i]\n        line2 = text[i:]\n\n        # if wrap point is inside markers, place end marker at end of first\n        # line and start marker at beginning of second line because each\n        # line will have its own table tag markup around it.\n        if mark:\n            line1 = line1 + '\\1'\n            line2 = '\\0' + mark + line2\n\n        # tack on first line onto the output list\n        data_list.append((line_num,line1))\n\n        # use this routine again to wrap the remaining text\n        self._split_line(data_list,'>',line2)\n\n    def _line_wrapper(self,diffs):\n        \"\"\"Returns iterator that splits (wraps) mdiff text lines\"\"\"\n\n        # pull from/to data and flags from mdiff iterator\n        for fromdata,todata,flag in diffs:\n            # check for context separators and pass them through\n            if flag is None:\n                yield fromdata,todata,flag\n                continue\n            (fromline,fromtext),(toline,totext) = fromdata,todata\n            # for each from/to line split it at the wrap column to form\n            # list of text lines.\n            fromlist,tolist = [],[]\n            self._split_line(fromlist,fromline,fromtext)\n            self._split_line(tolist,toline,totext)\n            # yield from/to line in pairs inserting blank lines as\n            # necessary when one side has more wrapped lines\n            while fromlist or tolist:\n                if fromlist:\n                    fromdata = fromlist.pop(0)\n                else:\n                    fromdata = ('',' ')\n                if tolist:\n                    todata = tolist.pop(0)\n                else:\n                    todata = ('',' ')\n                yield fromdata,todata,flag\n\n    def _collect_lines(self,diffs):\n        \"\"\"Collects mdiff output into separate lists\n\n        Before storing the mdiff from/to data into a list, it is converted\n        into a single line of text with HTML markup.\n        \"\"\"\n\n        fromlist,tolist,flaglist = [],[],[]\n        # pull from/to data and flags from mdiff style iterator\n        for fromdata,todata,flag in diffs:\n            try:\n                # store HTML markup of the lines into the lists\n                fromlist.append(self._format_line(0,flag,*fromdata))\n                tolist.append(self._format_line(1,flag,*todata))\n            except TypeError:\n                # exceptions occur for lines where context separators go\n                fromlist.append(None)\n                tolist.append(None)\n            flaglist.append(flag)\n        return fromlist,tolist,flaglist\n\n    def _format_line(self,side,flag,linenum,text):\n        \"\"\"Returns HTML markup of \"from\" / \"to\" text lines\n\n        side -- 0 or 1 indicating \"from\" or \"to\" text\n        flag -- indicates if difference on line\n        linenum -- line number (used for line number column)\n        text -- line text to be marked up\n        \"\"\"\n        try:\n            linenum = '%d' % linenum\n            id = ' id=\"%s%s\"' % (self._prefix[side],linenum)\n        except TypeError:\n            # handle blank lines where linenum is '>' or ''\n            id = ''\n        # replace those things that would get confused with HTML symbols\n        text=text.replace(\"&\",\"&amp;\").replace(\">\",\"&gt;\").replace(\"<\",\"&lt;\")\n\n        # make space non-breakable so they don't get compressed or line wrapped\n        text = text.replace(' ','&nbsp;').rstrip()\n\n        return '<td class=\"diff_header\"%s>%s</td><td nowrap=\"nowrap\">%s</td>' \\\n               % (id,linenum,text)\n\n    def _make_prefix(self):\n        \"\"\"Create unique anchor prefixes\"\"\"\n\n        # Generate a unique anchor prefix so multiple tables\n        # can exist on the same HTML page without conflicts.\n        fromprefix = \"from%d_\" % HtmlDiff._default_prefix\n        toprefix = \"to%d_\" % HtmlDiff._default_prefix\n        HtmlDiff._default_prefix += 1\n        # store prefixes so line format method has access\n        self._prefix = [fromprefix,toprefix]\n\n    def _convert_flags(self,fromlist,tolist,flaglist,context,numlines):\n        \"\"\"Makes list of \"next\" links\"\"\"\n\n        # all anchor names will be generated using the unique \"to\" prefix\n        toprefix = self._prefix[1]\n\n        # process change flags, generating middle column of next anchors/links\n        next_id = ['']*len(flaglist)\n        next_href = ['']*len(flaglist)\n        num_chg, in_change = 0, False\n        last = 0\n        for i,flag in enumerate(flaglist):\n            if flag:\n                if not in_change:\n                    in_change = True\n                    last = i\n                    # at the beginning of a change, drop an anchor a few lines\n                    # (the context lines) before the change for the previous\n                    # link\n                    i = max([0,i-numlines])\n                    next_id[i] = ' id=\"difflib_chg_%s_%d\"' % (toprefix,num_chg)\n                    # at the beginning of a change, drop a link to the next\n                    # change\n                    num_chg += 1\n                    next_href[last] = '<a href=\"#difflib_chg_%s_%d\">n</a>' % (\n                         toprefix,num_chg)\n            else:\n                in_change = False\n        # check for cases where there is no content to avoid exceptions\n        if not flaglist:\n            flaglist = [False]\n            next_id = ['']\n            next_href = ['']\n            last = 0\n            if context:\n                fromlist = ['<td></td><td>&nbsp;No Differences Found&nbsp;</td>']\n                tolist = fromlist\n            else:\n                fromlist = tolist = ['<td></td><td>&nbsp;Empty File&nbsp;</td>']\n        # if not a change on first line, drop a link\n        if not flaglist[0]:\n            next_href[0] = '<a href=\"#difflib_chg_%s_0\">f</a>' % toprefix\n        # redo the last link to link to the top\n        next_href[last] = '<a href=\"#difflib_chg_%s_top\">t</a>' % (toprefix)\n\n        return fromlist,tolist,flaglist,next_href,next_id\n\n    def make_table(self,fromlines,tolines,fromdesc='',todesc='',context=False,\n                   numlines=5):\n        \"\"\"Returns HTML table of side by side comparison with change highlights\n\n        Arguments:\n        fromlines -- list of \"from\" lines\n        tolines -- list of \"to\" lines\n        fromdesc -- \"from\" file column header string\n        todesc -- \"to\" file column header string\n        context -- set to True for contextual differences (defaults to False\n            which shows full differences).\n        numlines -- number of context lines.  When context is set True,\n            controls number of lines displayed before and after the change.\n            When context is False, controls the number of lines to place\n            the \"next\" link anchors before the next change (so click of\n            \"next\" link jumps to just before the change).\n        \"\"\"\n\n        # make unique anchor prefixes so that multiple tables may exist\n        # on the same page without conflict.\n        self._make_prefix()\n\n        # change tabs to spaces before it gets more difficult after we insert\n        # markup\n        fromlines,tolines = self._tab_newline_replace(fromlines,tolines)\n\n        # create diffs iterator which generates side by side from/to data\n        if context:\n            context_lines = numlines\n        else:\n            context_lines = None\n        diffs = _mdiff(fromlines,tolines,context_lines,linejunk=self._linejunk,\n                      charjunk=self._charjunk)\n\n        # set up iterator to wrap lines that exceed desired width\n        if self._wrapcolumn:\n            diffs = self._line_wrapper(diffs)\n\n        # collect up from/to lines and flags into lists (also format the lines)\n        fromlist,tolist,flaglist = self._collect_lines(diffs)\n\n        # process change flags, generating middle column of next anchors/links\n        fromlist,tolist,flaglist,next_href,next_id = self._convert_flags(\n            fromlist,tolist,flaglist,context,numlines)\n\n        s = []\n        fmt = '            <tr><td class=\"diff_next\"%s>%s</td>%s' + \\\n              '<td class=\"diff_next\">%s</td>%s</tr>\\n'\n        for i in range(len(flaglist)):\n            if flaglist[i] is None:\n                # mdiff yields None on separator lines skip the bogus ones\n                # generated for the first line\n                if i > 0:\n                    s.append('        </tbody>        \\n        <tbody>\\n')\n            else:\n                s.append( fmt % (next_id[i],next_href[i],fromlist[i],\n                                           next_href[i],tolist[i]))\n        if fromdesc or todesc:\n            header_row = '<thead><tr>%s%s%s%s</tr></thead>' % (\n                '<th class=\"diff_next\"><br /></th>',\n                '<th colspan=\"2\" class=\"diff_header\">%s</th>' % fromdesc,\n                '<th class=\"diff_next\"><br /></th>',\n                '<th colspan=\"2\" class=\"diff_header\">%s</th>' % todesc)\n        else:\n            header_row = ''\n\n        table = self._table_template % dict(\n            data_rows=''.join(s),\n            header_row=header_row,\n            prefix=self._prefix[1])\n\n        return table.replace('\\0+','<span class=\"diff_add\">'). \\\n                     replace('\\0-','<span class=\"diff_sub\">'). \\\n                     replace('\\0^','<span class=\"diff_chg\">'). \\\n                     replace('\\1','</span>'). \\\n                     replace('\\t','&nbsp;')\n\ndel re\n\ndef restore(delta, which):\n    r\"\"\"\n    Generate one of the two sequences that generated a delta.\n\n    Given a `delta` produced by `Differ.compare()` or `ndiff()`, extract\n    lines originating from file 1 or 2 (parameter `which`), stripping off line\n    prefixes.\n\n    Examples:\n\n    >>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(1),\n    ...              'ore\\ntree\\nemu\\n'.splitlines(1))\n    >>> diff = list(diff)\n    >>> print ''.join(restore(diff, 1)),\n    one\n    two\n    three\n    >>> print ''.join(restore(diff, 2)),\n    ore\n    tree\n    emu\n    \"\"\"\n    try:\n        tag = {1: \"- \", 2: \"+ \"}[int(which)]\n    except KeyError:\n        raise ValueError, ('unknown delta choice (must be 1 or 2): %r'\n                           % which)\n    prefixes = (\"  \", tag)\n    for line in delta:\n        if line[:2] in prefixes:\n            yield line[2:]\n\ndef _test():\n    import doctest, difflib\n    return doctest.testmod(difflib)\n\nif __name__ == \"__main__\":\n    _test()\n", 
    "dis": "\"\"\"Disassembler of Python byte code into mnemonics.\"\"\"\n\nimport sys\nimport types\n\nfrom opcode import *\nfrom opcode import __all__ as _opcodes_all\n\n__all__ = [\"dis\", \"disassemble\", \"distb\", \"disco\",\n           \"findlinestarts\", \"findlabels\"] + _opcodes_all\ndel _opcodes_all\n\n_have_code = (types.MethodType, types.FunctionType, types.CodeType,\n              types.ClassType, type)\n\ndef dis(x=None):\n    \"\"\"Disassemble classes, methods, functions, or code.\n\n    With no argument, disassemble the last traceback.\n\n    \"\"\"\n    if x is None:\n        distb()\n        return\n    if isinstance(x, types.InstanceType):\n        x = x.__class__\n    if hasattr(x, 'im_func'):\n        x = x.im_func\n    if hasattr(x, 'func_code'):\n        x = x.func_code\n    if hasattr(x, '__dict__'):\n        items = x.__dict__.items()\n        items.sort()\n        for name, x1 in items:\n            if isinstance(x1, _have_code):\n                print \"Disassembly of %s:\" % name\n                try:\n                    dis(x1)\n                except TypeError, msg:\n                    print \"Sorry:\", msg\n                print\n    elif hasattr(x, 'co_code'):\n        disassemble(x)\n    elif isinstance(x, str):\n        disassemble_string(x)\n    else:\n        raise TypeError, \\\n              \"don't know how to disassemble %s objects\" % \\\n              type(x).__name__\n\ndef distb(tb=None):\n    \"\"\"Disassemble a traceback (default: last traceback).\"\"\"\n    if tb is None:\n        try:\n            tb = sys.last_traceback\n        except AttributeError:\n            raise RuntimeError, \"no last traceback to disassemble\"\n        while tb.tb_next: tb = tb.tb_next\n    disassemble(tb.tb_frame.f_code, tb.tb_lasti)\n\ndef disassemble(co, lasti=-1):\n    \"\"\"Disassemble a code object.\"\"\"\n    code = co.co_code\n    labels = findlabels(code)\n    linestarts = dict(findlinestarts(co))\n    n = len(code)\n    i = 0\n    extended_arg = 0\n    free = None\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        if i in linestarts:\n            if i > 0:\n                print\n            print \"%3d\" % linestarts[i],\n        else:\n            print '   ',\n\n        if i == lasti: print '-->',\n        else: print '   ',\n        if i in labels: print '>>',\n        else: print '  ',\n        print repr(i).rjust(4),\n        print opname[op].ljust(20),\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg\n            extended_arg = 0\n            i = i+2\n            if op == EXTENDED_ARG:\n                extended_arg = oparg*65536L\n            print repr(oparg).rjust(5),\n            if op in hasconst:\n                print '(' + repr(co.co_consts[oparg]) + ')',\n            elif op in hasname:\n                print '(' + co.co_names[oparg] + ')',\n            elif op in hasjrel:\n                print '(to ' + repr(i + oparg) + ')',\n            elif op in haslocal:\n                print '(' + co.co_varnames[oparg] + ')',\n            elif op in hascompare:\n                print '(' + cmp_op[oparg] + ')',\n            elif op in hasfree:\n                if free is None:\n                    free = co.co_cellvars + co.co_freevars\n                print '(' + free[oparg] + ')',\n        print\n\ndef disassemble_string(code, lasti=-1, varnames=None, names=None,\n                       constants=None):\n    labels = findlabels(code)\n    n = len(code)\n    i = 0\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        if i == lasti: print '-->',\n        else: print '   ',\n        if i in labels: print '>>',\n        else: print '  ',\n        print repr(i).rjust(4),\n        print opname[op].ljust(15),\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256\n            i = i+2\n            print repr(oparg).rjust(5),\n            if op in hasconst:\n                if constants:\n                    print '(' + repr(constants[oparg]) + ')',\n                else:\n                    print '(%d)'%oparg,\n            elif op in hasname:\n                if names is not None:\n                    print '(' + names[oparg] + ')',\n                else:\n                    print '(%d)'%oparg,\n            elif op in hasjrel:\n                print '(to ' + repr(i + oparg) + ')',\n            elif op in haslocal:\n                if varnames:\n                    print '(' + varnames[oparg] + ')',\n                else:\n                    print '(%d)' % oparg,\n            elif op in hascompare:\n                print '(' + cmp_op[oparg] + ')',\n        print\n\ndisco = disassemble                     # XXX For backwards compatibility\n\ndef findlabels(code):\n    \"\"\"Detect all offsets in a byte code which are jump targets.\n\n    Return the list of offsets.\n\n    \"\"\"\n    labels = []\n    n = len(code)\n    i = 0\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256\n            i = i+2\n            label = -1\n            if op in hasjrel:\n                label = i+oparg\n            elif op in hasjabs:\n                label = oparg\n            if label >= 0:\n                if label not in labels:\n                    labels.append(label)\n    return labels\n\ndef findlinestarts(code):\n    \"\"\"Find the offsets in a byte code which are start of lines in the source.\n\n    Generate pairs (offset, lineno) as described in Python/compile.c.\n\n    \"\"\"\n    byte_increments = [ord(c) for c in code.co_lnotab[0::2]]\n    line_increments = [ord(c) for c in code.co_lnotab[1::2]]\n\n    lastlineno = None\n    lineno = code.co_firstlineno\n    addr = 0\n    for byte_incr, line_incr in zip(byte_increments, line_increments):\n        if byte_incr:\n            if lineno != lastlineno:\n                yield (addr, lineno)\n                lastlineno = lineno\n            addr += byte_incr\n        lineno += line_incr\n    if lineno != lastlineno:\n        yield (addr, lineno)\n\ndef _test():\n    \"\"\"Simple test program to disassemble a file.\"\"\"\n    if sys.argv[1:]:\n        if sys.argv[2:]:\n            sys.stderr.write(\"usage: python dis.py [-|file]\\n\")\n            sys.exit(2)\n        fn = sys.argv[1]\n        if not fn or fn == \"-\":\n            fn = None\n    else:\n        fn = None\n    if fn is None:\n        f = sys.stdin\n    else:\n        f = open(fn)\n    source = f.read()\n    if fn is not None:\n        f.close()\n    else:\n        fn = \"<stdin>\"\n    code = compile(source, fn, \"exec\")\n    dis(code)\n\nif __name__ == \"__main__\":\n    _test()\n", 
    "doctest": "# Module doctest.\n# Released to the public domain 16-Jan-2001, by Tim Peters (tim@python.org).\n# Major enhancements and refactoring by:\n#     Jim Fulton\n#     Edward Loper\n\n# Provided as-is; use at your own risk; no warranty; no promises; enjoy!\n\nr\"\"\"Module doctest -- a framework for running examples in docstrings.\n\nIn simplest use, end each module M to be tested with:\n\ndef _test():\n    import doctest\n    doctest.testmod()\n\nif __name__ == \"__main__\":\n    _test()\n\nThen running the module as a script will cause the examples in the\ndocstrings to get executed and verified:\n\npython M.py\n\nThis won't display anything unless an example fails, in which case the\nfailing example(s) and the cause(s) of the failure(s) are printed to stdout\n(why not stderr? because stderr is a lame hack <0.2 wink>), and the final\nline of output is \"Test failed.\".\n\nRun it with the -v switch instead:\n\npython M.py -v\n\nand a detailed report of all examples tried is printed to stdout, along\nwith assorted summaries at the end.\n\nYou can force verbose mode by passing \"verbose=True\" to testmod, or prohibit\nit by passing \"verbose=False\".  In either of those cases, sys.argv is not\nexamined by testmod.\n\nThere are a variety of other ways to run doctests, including integration\nwith the unittest framework, and support for running non-Python text\nfiles containing doctests.  There are also many ways to override parts\nof doctest's default behaviors.  See the Library Reference Manual for\ndetails.\n\"\"\"\n\n__docformat__ = 'reStructuredText en'\n\n__all__ = [\n    # 0, Option Flags\n    'register_optionflag',\n    'DONT_ACCEPT_TRUE_FOR_1',\n    'DONT_ACCEPT_BLANKLINE',\n    'NORMALIZE_WHITESPACE',\n    'ELLIPSIS',\n    'SKIP',\n    'IGNORE_EXCEPTION_DETAIL',\n    'COMPARISON_FLAGS',\n    'REPORT_UDIFF',\n    'REPORT_CDIFF',\n    'REPORT_NDIFF',\n    'REPORT_ONLY_FIRST_FAILURE',\n    'REPORTING_FLAGS',\n    # 1. Utility Functions\n    # 2. Example & DocTest\n    'Example',\n    'DocTest',\n    # 3. Doctest Parser\n    'DocTestParser',\n    # 4. Doctest Finder\n    'DocTestFinder',\n    # 5. Doctest Runner\n    'DocTestRunner',\n    'OutputChecker',\n    'DocTestFailure',\n    'UnexpectedException',\n    'DebugRunner',\n    # 6. Test Functions\n    'testmod',\n    'testfile',\n    'run_docstring_examples',\n    # 7. Tester\n    'Tester',\n    # 8. Unittest Support\n    'DocTestSuite',\n    'DocFileSuite',\n    'set_unittest_reportflags',\n    # 9. Debugging Support\n    'script_from_examples',\n    'testsource',\n    'debug_src',\n    'debug',\n]\n\nimport __future__\n\nimport sys, traceback, inspect, linecache, os, re\nimport unittest, difflib, pdb, tempfile\nimport warnings\nfrom StringIO import StringIO\nfrom collections import namedtuple\n\nTestResults = namedtuple('TestResults', 'failed attempted')\n\n# There are 4 basic classes:\n#  - Example: a <source, want> pair, plus an intra-docstring line number.\n#  - DocTest: a collection of examples, parsed from a docstring, plus\n#    info about where the docstring came from (name, filename, lineno).\n#  - DocTestFinder: extracts DocTests from a given object's docstring and\n#    its contained objects' docstrings.\n#  - DocTestRunner: runs DocTest cases, and accumulates statistics.\n#\n# So the basic picture is:\n#\n#                             list of:\n# +------+                   +---------+                   +-------+\n# |object| --DocTestFinder-> | DocTest | --DocTestRunner-> |results|\n# +------+                   +---------+                   +-------+\n#                            | Example |\n#                            |   ...   |\n#                            | Example |\n#                            +---------+\n\n# Option constants.\n\nOPTIONFLAGS_BY_NAME = {}\ndef register_optionflag(name):\n    # Create a new flag unless `name` is already known.\n    return OPTIONFLAGS_BY_NAME.setdefault(name, 1 << len(OPTIONFLAGS_BY_NAME))\n\nDONT_ACCEPT_TRUE_FOR_1 = register_optionflag('DONT_ACCEPT_TRUE_FOR_1')\nDONT_ACCEPT_BLANKLINE = register_optionflag('DONT_ACCEPT_BLANKLINE')\nNORMALIZE_WHITESPACE = register_optionflag('NORMALIZE_WHITESPACE')\nELLIPSIS = register_optionflag('ELLIPSIS')\nSKIP = register_optionflag('SKIP')\nIGNORE_EXCEPTION_DETAIL = register_optionflag('IGNORE_EXCEPTION_DETAIL')\n\nCOMPARISON_FLAGS = (DONT_ACCEPT_TRUE_FOR_1 |\n                    DONT_ACCEPT_BLANKLINE |\n                    NORMALIZE_WHITESPACE |\n                    ELLIPSIS |\n                    SKIP |\n                    IGNORE_EXCEPTION_DETAIL)\n\nREPORT_UDIFF = register_optionflag('REPORT_UDIFF')\nREPORT_CDIFF = register_optionflag('REPORT_CDIFF')\nREPORT_NDIFF = register_optionflag('REPORT_NDIFF')\nREPORT_ONLY_FIRST_FAILURE = register_optionflag('REPORT_ONLY_FIRST_FAILURE')\n\nREPORTING_FLAGS = (REPORT_UDIFF |\n                   REPORT_CDIFF |\n                   REPORT_NDIFF |\n                   REPORT_ONLY_FIRST_FAILURE)\n\n# Special string markers for use in `want` strings:\nBLANKLINE_MARKER = '<BLANKLINE>'\nELLIPSIS_MARKER = '...'\n\n######################################################################\n## Table of Contents\n######################################################################\n#  1. Utility Functions\n#  2. Example & DocTest -- store test cases\n#  3. DocTest Parser -- extracts examples from strings\n#  4. DocTest Finder -- extracts test cases from objects\n#  5. DocTest Runner -- runs test cases\n#  6. Test Functions -- convenient wrappers for testing\n#  7. Tester Class -- for backwards compatibility\n#  8. Unittest Support\n#  9. Debugging Support\n# 10. Example Usage\n\n######################################################################\n## 1. Utility Functions\n######################################################################\n\ndef _extract_future_flags(globs):\n    \"\"\"\n    Return the compiler-flags associated with the future features that\n    have been imported into the given namespace (globs).\n    \"\"\"\n    flags = 0\n    for fname in __future__.all_feature_names:\n        feature = globs.get(fname, None)\n        if feature is getattr(__future__, fname):\n            flags |= feature.compiler_flag\n    return flags\n\ndef _normalize_module(module, depth=2):\n    \"\"\"\n    Return the module specified by `module`.  In particular:\n      - If `module` is a module, then return module.\n      - If `module` is a string, then import and return the\n        module with that name.\n      - If `module` is None, then return the calling module.\n        The calling module is assumed to be the module of\n        the stack frame at the given depth in the call stack.\n    \"\"\"\n    if inspect.ismodule(module):\n        return module\n    elif isinstance(module, (str, unicode)):\n        return __import__(module, globals(), locals(), [\"*\"])\n    elif module is None:\n        return sys.modules[sys._getframe(depth).f_globals['__name__']]\n    else:\n        raise TypeError(\"Expected a module, string, or None\")\n\ndef _load_testfile(filename, package, module_relative):\n    if module_relative:\n        package = _normalize_module(package, 3)\n        filename = _module_relative_path(package, filename)\n        if hasattr(package, '__loader__'):\n            if hasattr(package.__loader__, 'get_data'):\n                file_contents = package.__loader__.get_data(filename)\n                # get_data() opens files as 'rb', so one must do the equivalent\n                # conversion as universal newlines would do.\n                return file_contents.replace(os.linesep, '\\n'), filename\n    with open(filename) as f:\n        return f.read(), filename\n\n# Use sys.stdout encoding for ouput.\n_encoding = getattr(sys.__stdout__, 'encoding', None) or 'utf-8'\n\ndef _indent(s, indent=4):\n    \"\"\"\n    Add the given number of space characters to the beginning of\n    every non-blank line in `s`, and return the result.\n    If the string `s` is Unicode, it is encoded using the stdout\n    encoding and the `backslashreplace` error handler.\n    \"\"\"\n    if isinstance(s, unicode):\n        s = s.encode(_encoding, 'backslashreplace')\n    # This regexp matches the start of non-blank lines:\n    return re.sub('(?m)^(?!$)', indent*' ', s)\n\ndef _exception_traceback(exc_info):\n    \"\"\"\n    Return a string containing a traceback message for the given\n    exc_info tuple (as returned by sys.exc_info()).\n    \"\"\"\n    # Get a traceback message.\n    excout = StringIO()\n    exc_type, exc_val, exc_tb = exc_info\n    traceback.print_exception(exc_type, exc_val, exc_tb, file=excout)\n    return excout.getvalue()\n\n# Override some StringIO methods.\nclass _SpoofOut(StringIO):\n    def getvalue(self):\n        result = StringIO.getvalue(self)\n        # If anything at all was written, make sure there's a trailing\n        # newline.  There's no way for the expected output to indicate\n        # that a trailing newline is missing.\n        if result and not result.endswith(\"\\n\"):\n            result += \"\\n\"\n        # Prevent softspace from screwing up the next test case, in\n        # case they used print with a trailing comma in an example.\n        if hasattr(self, \"softspace\"):\n            del self.softspace\n        return result\n\n    def truncate(self,   size=None):\n        StringIO.truncate(self, size)\n        if hasattr(self, \"softspace\"):\n            del self.softspace\n        if not self.buf:\n            # Reset it to an empty string, to make sure it's not unicode.\n            self.buf = ''\n\n# Worst-case linear-time ellipsis matching.\ndef _ellipsis_match(want, got):\n    \"\"\"\n    Essentially the only subtle case:\n    >>> _ellipsis_match('aa...aa', 'aaa')\n    False\n    \"\"\"\n    if ELLIPSIS_MARKER not in want:\n        return want == got\n\n    # Find \"the real\" strings.\n    ws = want.split(ELLIPSIS_MARKER)\n    assert len(ws) >= 2\n\n    # Deal with exact matches possibly needed at one or both ends.\n    startpos, endpos = 0, len(got)\n    w = ws[0]\n    if w:   # starts with exact match\n        if got.startswith(w):\n            startpos = len(w)\n            del ws[0]\n        else:\n            return False\n    w = ws[-1]\n    if w:   # ends with exact match\n        if got.endswith(w):\n            endpos -= len(w)\n            del ws[-1]\n        else:\n            return False\n\n    if startpos > endpos:\n        # Exact end matches required more characters than we have, as in\n        # _ellipsis_match('aa...aa', 'aaa')\n        return False\n\n    # For the rest, we only need to find the leftmost non-overlapping\n    # match for each piece.  If there's no overall match that way alone,\n    # there's no overall match period.\n    for w in ws:\n        # w may be '' at times, if there are consecutive ellipses, or\n        # due to an ellipsis at the start or end of `want`.  That's OK.\n        # Search for an empty string succeeds, and doesn't change startpos.\n        startpos = got.find(w, startpos, endpos)\n        if startpos < 0:\n            return False\n        startpos += len(w)\n\n    return True\n\ndef _comment_line(line):\n    \"Return a commented form of the given line\"\n    line = line.rstrip()\n    if line:\n        return '# '+line\n    else:\n        return '#'\n\ndef _strip_exception_details(msg):\n    # Support for IGNORE_EXCEPTION_DETAIL.\n    # Get rid of everything except the exception name; in particular, drop\n    # the possibly dotted module path (if any) and the exception message (if\n    # any).  We assume that a colon is never part of a dotted name, or of an\n    # exception name.\n    # E.g., given\n    #    \"foo.bar.MyError: la di da\"\n    # return \"MyError\"\n    # Or for \"abc.def\" or \"abc.def:\\n\" return \"def\".\n\n    start, end = 0, len(msg)\n    # The exception name must appear on the first line.\n    i = msg.find(\"\\n\")\n    if i >= 0:\n        end = i\n    # retain up to the first colon (if any)\n    i = msg.find(':', 0, end)\n    if i >= 0:\n        end = i\n    # retain just the exception name\n    i = msg.rfind('.', 0, end)\n    if i >= 0:\n        start = i+1\n    return msg[start: end]\n\nclass _OutputRedirectingPdb(pdb.Pdb):\n    \"\"\"\n    A specialized version of the python debugger that redirects stdout\n    to a given stream when interacting with the user.  Stdout is *not*\n    redirected when traced code is executed.\n    \"\"\"\n    def __init__(self, out):\n        self.__out = out\n        self.__debugger_used = False\n        pdb.Pdb.__init__(self, stdout=out)\n        # still use input() to get user input\n        self.use_rawinput = 1\n\n    def set_trace(self, frame=None):\n        self.__debugger_used = True\n        if frame is None:\n            frame = sys._getframe().f_back\n        pdb.Pdb.set_trace(self, frame)\n\n    def set_continue(self):\n        # Calling set_continue unconditionally would break unit test\n        # coverage reporting, as Bdb.set_continue calls sys.settrace(None).\n        if self.__debugger_used:\n            pdb.Pdb.set_continue(self)\n\n    def trace_dispatch(self, *args):\n        # Redirect stdout to the given stream.\n        save_stdout = sys.stdout\n        sys.stdout = self.__out\n        # Call Pdb's trace dispatch method.\n        try:\n            return pdb.Pdb.trace_dispatch(self, *args)\n        finally:\n            sys.stdout = save_stdout\n\n# [XX] Normalize with respect to os.path.pardir?\ndef _module_relative_path(module, path):\n    if not inspect.ismodule(module):\n        raise TypeError, 'Expected a module: %r' % module\n    if path.startswith('/'):\n        raise ValueError, 'Module-relative files may not have absolute paths'\n\n    # Find the base directory for the path.\n    if hasattr(module, '__file__'):\n        # A normal module/package\n        basedir = os.path.split(module.__file__)[0]\n    elif module.__name__ == '__main__':\n        # An interactive session.\n        if len(sys.argv)>0 and sys.argv[0] != '':\n            basedir = os.path.split(sys.argv[0])[0]\n        else:\n            basedir = os.curdir\n    else:\n        # A module w/o __file__ (this includes builtins)\n        raise ValueError(\"Can't resolve paths relative to the module \" +\n                         module + \" (it has no __file__)\")\n\n    # Combine the base directory and the path.\n    return os.path.join(basedir, *(path.split('/')))\n\n######################################################################\n## 2. Example & DocTest\n######################################################################\n## - An \"example\" is a <source, want> pair, where \"source\" is a\n##   fragment of source code, and \"want\" is the expected output for\n##   \"source.\"  The Example class also includes information about\n##   where the example was extracted from.\n##\n## - A \"doctest\" is a collection of examples, typically extracted from\n##   a string (such as an object's docstring).  The DocTest class also\n##   includes information about where the string was extracted from.\n\nclass Example:\n    \"\"\"\n    A single doctest example, consisting of source code and expected\n    output.  `Example` defines the following attributes:\n\n      - source: A single Python statement, always ending with a newline.\n        The constructor adds a newline if needed.\n\n      - want: The expected output from running the source code (either\n        from stdout, or a traceback in case of exception).  `want` ends\n        with a newline unless it's empty, in which case it's an empty\n        string.  The constructor adds a newline if needed.\n\n      - exc_msg: The exception message generated by the example, if\n        the example is expected to generate an exception; or `None` if\n        it is not expected to generate an exception.  This exception\n        message is compared against the return value of\n        `traceback.format_exception_only()`.  `exc_msg` ends with a\n        newline unless it's `None`.  The constructor adds a newline\n        if needed.\n\n      - lineno: The line number within the DocTest string containing\n        this Example where the Example begins.  This line number is\n        zero-based, with respect to the beginning of the DocTest.\n\n      - indent: The example's indentation in the DocTest string.\n        I.e., the number of space characters that precede the\n        example's first prompt.\n\n      - options: A dictionary mapping from option flags to True or\n        False, which is used to override default options for this\n        example.  Any option flags not contained in this dictionary\n        are left at their default value (as specified by the\n        DocTestRunner's optionflags).  By default, no options are set.\n    \"\"\"\n    def __init__(self, source, want, exc_msg=None, lineno=0, indent=0,\n                 options=None):\n        # Normalize inputs.\n        if not source.endswith('\\n'):\n            source += '\\n'\n        if want and not want.endswith('\\n'):\n            want += '\\n'\n        if exc_msg is not None and not exc_msg.endswith('\\n'):\n            exc_msg += '\\n'\n        # Store properties.\n        self.source = source\n        self.want = want\n        self.lineno = lineno\n        self.indent = indent\n        if options is None: options = {}\n        self.options = options\n        self.exc_msg = exc_msg\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self.source == other.source and \\\n               self.want == other.want and \\\n               self.lineno == other.lineno and \\\n               self.indent == other.indent and \\\n               self.options == other.options and \\\n               self.exc_msg == other.exc_msg\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self.source, self.want, self.lineno, self.indent,\n                     self.exc_msg))\n\n\nclass DocTest:\n    \"\"\"\n    A collection of doctest examples that should be run in a single\n    namespace.  Each `DocTest` defines the following attributes:\n\n      - examples: the list of examples.\n\n      - globs: The namespace (aka globals) that the examples should\n        be run in.\n\n      - name: A name identifying the DocTest (typically, the name of\n        the object whose docstring this DocTest was extracted from).\n\n      - filename: The name of the file that this DocTest was extracted\n        from, or `None` if the filename is unknown.\n\n      - lineno: The line number within filename where this DocTest\n        begins, or `None` if the line number is unavailable.  This\n        line number is zero-based, with respect to the beginning of\n        the file.\n\n      - docstring: The string that the examples were extracted from,\n        or `None` if the string is unavailable.\n    \"\"\"\n    def __init__(self, examples, globs, name, filename, lineno, docstring):\n        \"\"\"\n        Create a new DocTest containing the given examples.  The\n        DocTest's globals are initialized with a copy of `globs`.\n        \"\"\"\n        assert not isinstance(examples, basestring), \\\n               \"DocTest no longer accepts str; use DocTestParser instead\"\n        self.examples = examples\n        self.docstring = docstring\n        self.globs = globs.copy()\n        self.name = name\n        self.filename = filename\n        self.lineno = lineno\n\n    def __repr__(self):\n        if len(self.examples) == 0:\n            examples = 'no examples'\n        elif len(self.examples) == 1:\n            examples = '1 example'\n        else:\n            examples = '%d examples' % len(self.examples)\n        return ('<DocTest %s from %s:%s (%s)>' %\n                (self.name, self.filename, self.lineno, examples))\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self.examples == other.examples and \\\n               self.docstring == other.docstring and \\\n               self.globs == other.globs and \\\n               self.name == other.name and \\\n               self.filename == other.filename and \\\n               self.lineno == other.lineno\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self.docstring, self.name, self.filename, self.lineno))\n\n    # This lets us sort tests by name:\n    def __cmp__(self, other):\n        if not isinstance(other, DocTest):\n            return -1\n        return cmp((self.name, self.filename, self.lineno, id(self)),\n                   (other.name, other.filename, other.lineno, id(other)))\n\n######################################################################\n## 3. DocTestParser\n######################################################################\n\nclass DocTestParser:\n    \"\"\"\n    A class used to parse strings containing doctest examples.\n    \"\"\"\n    # This regular expression is used to find doctest examples in a\n    # string.  It defines three groups: `source` is the source code\n    # (including leading indentation and prompts); `indent` is the\n    # indentation of the first (PS1) line of the source code; and\n    # `want` is the expected output (including leading indentation).\n    _EXAMPLE_RE = re.compile(r'''\n        # Source consists of a PS1 line followed by zero or more PS2 lines.\n        (?P<source>\n            (?:^(?P<indent> [ ]*) >>>    .*)    # PS1 line\n            (?:\\n           [ ]*  \\.\\.\\. .*)*)  # PS2 lines\n        \\n?\n        # Want consists of any non-blank lines that do not start with PS1.\n        (?P<want> (?:(?![ ]*$)    # Not a blank line\n                     (?![ ]*>>>)  # Not a line starting with PS1\n                     .+$\\n?       # But any other line\n                  )*)\n        ''', re.MULTILINE | re.VERBOSE)\n\n    # A regular expression for handling `want` strings that contain\n    # expected exceptions.  It divides `want` into three pieces:\n    #    - the traceback header line (`hdr`)\n    #    - the traceback stack (`stack`)\n    #    - the exception message (`msg`), as generated by\n    #      traceback.format_exception_only()\n    # `msg` may have multiple lines.  We assume/require that the\n    # exception message is the first non-indented line starting with a word\n    # character following the traceback header line.\n    _EXCEPTION_RE = re.compile(r\"\"\"\n        # Grab the traceback header.  Different versions of Python have\n        # said different things on the first traceback line.\n        ^(?P<hdr> Traceback\\ \\(\n            (?: most\\ recent\\ call\\ last\n            |   innermost\\ last\n            ) \\) :\n        )\n        \\s* $                # toss trailing whitespace on the header.\n        (?P<stack> .*?)      # don't blink: absorb stuff until...\n        ^ (?P<msg> \\w+ .*)   #     a line *starts* with alphanum.\n        \"\"\", re.VERBOSE | re.MULTILINE | re.DOTALL)\n\n    # A callable returning a true value iff its argument is a blank line\n    # or contains a single comment.\n    _IS_BLANK_OR_COMMENT = re.compile(r'^[ ]*(#.*)?$').match\n\n    def parse(self, string, name='<string>'):\n        \"\"\"\n        Divide the given string into examples and intervening text,\n        and return them as a list of alternating Examples and strings.\n        Line numbers for the Examples are 0-based.  The optional\n        argument `name` is a name identifying this string, and is only\n        used for error messages.\n        \"\"\"\n        string = string.expandtabs()\n        # If all lines begin with the same indentation, then strip it.\n        min_indent = self._min_indent(string)\n        if min_indent > 0:\n            string = '\\n'.join([l[min_indent:] for l in string.split('\\n')])\n\n        output = []\n        charno, lineno = 0, 0\n        # Find all doctest examples in the string:\n        for m in self._EXAMPLE_RE.finditer(string):\n            # Add the pre-example text to `output`.\n            output.append(string[charno:m.start()])\n            # Update lineno (lines before this example)\n            lineno += string.count('\\n', charno, m.start())\n            # Extract info from the regexp match.\n            (source, options, want, exc_msg) = \\\n                     self._parse_example(m, name, lineno)\n            # Create an Example, and add it to the list.\n            if not self._IS_BLANK_OR_COMMENT(source):\n                output.append( Example(source, want, exc_msg,\n                                    lineno=lineno,\n                                    indent=min_indent+len(m.group('indent')),\n                                    options=options) )\n            # Update lineno (lines inside this example)\n            lineno += string.count('\\n', m.start(), m.end())\n            # Update charno.\n            charno = m.end()\n        # Add any remaining post-example text to `output`.\n        output.append(string[charno:])\n        return output\n\n    def get_doctest(self, string, globs, name, filename, lineno):\n        \"\"\"\n        Extract all doctest examples from the given string, and\n        collect them into a `DocTest` object.\n\n        `globs`, `name`, `filename`, and `lineno` are attributes for\n        the new `DocTest` object.  See the documentation for `DocTest`\n        for more information.\n        \"\"\"\n        return DocTest(self.get_examples(string, name), globs,\n                       name, filename, lineno, string)\n\n    def get_examples(self, string, name='<string>'):\n        \"\"\"\n        Extract all doctest examples from the given string, and return\n        them as a list of `Example` objects.  Line numbers are\n        0-based, because it's most common in doctests that nothing\n        interesting appears on the same line as opening triple-quote,\n        and so the first interesting line is called \\\"line 1\\\" then.\n\n        The optional argument `name` is a name identifying this\n        string, and is only used for error messages.\n        \"\"\"\n        return [x for x in self.parse(string, name)\n                if isinstance(x, Example)]\n\n    def _parse_example(self, m, name, lineno):\n        \"\"\"\n        Given a regular expression match from `_EXAMPLE_RE` (`m`),\n        return a pair `(source, want)`, where `source` is the matched\n        example's source code (with prompts and indentation stripped);\n        and `want` is the example's expected output (with indentation\n        stripped).\n\n        `name` is the string's name, and `lineno` is the line number\n        where the example starts; both are used for error messages.\n        \"\"\"\n        # Get the example's indentation level.\n        indent = len(m.group('indent'))\n\n        # Divide source into lines; check that they're properly\n        # indented; and then strip their indentation & prompts.\n        source_lines = m.group('source').split('\\n')\n        self._check_prompt_blank(source_lines, indent, name, lineno)\n        self._check_prefix(source_lines[1:], ' '*indent + '.', name, lineno)\n        source = '\\n'.join([sl[indent+4:] for sl in source_lines])\n\n        # Divide want into lines; check that it's properly indented; and\n        # then strip the indentation.  Spaces before the last newline should\n        # be preserved, so plain rstrip() isn't good enough.\n        want = m.group('want')\n        want_lines = want.split('\\n')\n        if len(want_lines) > 1 and re.match(r' *$', want_lines[-1]):\n            del want_lines[-1]  # forget final newline & spaces after it\n        self._check_prefix(want_lines, ' '*indent, name,\n                           lineno + len(source_lines))\n        want = '\\n'.join([wl[indent:] for wl in want_lines])\n\n        # If `want` contains a traceback message, then extract it.\n        m = self._EXCEPTION_RE.match(want)\n        if m:\n            exc_msg = m.group('msg')\n        else:\n            exc_msg = None\n\n        # Extract options from the source.\n        options = self._find_options(source, name, lineno)\n\n        return source, options, want, exc_msg\n\n    # This regular expression looks for option directives in the\n    # source code of an example.  Option directives are comments\n    # starting with \"doctest:\".  Warning: this may give false\n    # positives for string-literals that contain the string\n    # \"#doctest:\".  Eliminating these false positives would require\n    # actually parsing the string; but we limit them by ignoring any\n    # line containing \"#doctest:\" that is *followed* by a quote mark.\n    _OPTION_DIRECTIVE_RE = re.compile(r'#\\s*doctest:\\s*([^\\n\\'\"]*)$',\n                                      re.MULTILINE)\n\n    def _find_options(self, source, name, lineno):\n        \"\"\"\n        Return a dictionary containing option overrides extracted from\n        option directives in the given source string.\n\n        `name` is the string's name, and `lineno` is the line number\n        where the example starts; both are used for error messages.\n        \"\"\"\n        options = {}\n        # (note: with the current regexp, this will match at most once:)\n        for m in self._OPTION_DIRECTIVE_RE.finditer(source):\n            option_strings = m.group(1).replace(',', ' ').split()\n            for option in option_strings:\n                if (option[0] not in '+-' or\n                    option[1:] not in OPTIONFLAGS_BY_NAME):\n                    raise ValueError('line %r of the doctest for %s '\n                                     'has an invalid option: %r' %\n                                     (lineno+1, name, option))\n                flag = OPTIONFLAGS_BY_NAME[option[1:]]\n                options[flag] = (option[0] == '+')\n        if options and self._IS_BLANK_OR_COMMENT(source):\n            raise ValueError('line %r of the doctest for %s has an option '\n                             'directive on a line with no example: %r' %\n                             (lineno, name, source))\n        return options\n\n    # This regular expression finds the indentation of every non-blank\n    # line in a string.\n    _INDENT_RE = re.compile('^([ ]*)(?=\\S)', re.MULTILINE)\n\n    def _min_indent(self, s):\n        \"Return the minimum indentation of any non-blank line in `s`\"\n        indents = [len(indent) for indent in self._INDENT_RE.findall(s)]\n        if len(indents) > 0:\n            return min(indents)\n        else:\n            return 0\n\n    def _check_prompt_blank(self, lines, indent, name, lineno):\n        \"\"\"\n        Given the lines of a source string (including prompts and\n        leading indentation), check to make sure that every prompt is\n        followed by a space character.  If any line is not followed by\n        a space character, then raise ValueError.\n        \"\"\"\n        for i, line in enumerate(lines):\n            if len(line) >= indent+4 and line[indent+3] != ' ':\n                raise ValueError('line %r of the docstring for %s '\n                                 'lacks blank after %s: %r' %\n                                 (lineno+i+1, name,\n                                  line[indent:indent+3], line))\n\n    def _check_prefix(self, lines, prefix, name, lineno):\n        \"\"\"\n        Check that every line in the given list starts with the given\n        prefix; if any line does not, then raise a ValueError.\n        \"\"\"\n        for i, line in enumerate(lines):\n            if line and not line.startswith(prefix):\n                raise ValueError('line %r of the docstring for %s has '\n                                 'inconsistent leading whitespace: %r' %\n                                 (lineno+i+1, name, line))\n\n\n######################################################################\n## 4. DocTest Finder\n######################################################################\n\nclass DocTestFinder:\n    \"\"\"\n    A class used to extract the DocTests that are relevant to a given\n    object, from its docstring and the docstrings of its contained\n    objects.  Doctests can currently be extracted from the following\n    object types: modules, functions, classes, methods, staticmethods,\n    classmethods, and properties.\n    \"\"\"\n\n    def __init__(self, verbose=False, parser=DocTestParser(),\n                 recurse=True, exclude_empty=True):\n        \"\"\"\n        Create a new doctest finder.\n\n        The optional argument `parser` specifies a class or\n        function that should be used to create new DocTest objects (or\n        objects that implement the same interface as DocTest).  The\n        signature for this factory function should match the signature\n        of the DocTest constructor.\n\n        If the optional argument `recurse` is false, then `find` will\n        only examine the given object, and not any contained objects.\n\n        If the optional argument `exclude_empty` is false, then `find`\n        will include tests for objects with empty docstrings.\n        \"\"\"\n        self._parser = parser\n        self._verbose = verbose\n        self._recurse = recurse\n        self._exclude_empty = exclude_empty\n\n    def find(self, obj, name=None, module=None, globs=None, extraglobs=None):\n        \"\"\"\n        Return a list of the DocTests that are defined by the given\n        object's docstring, or by any of its contained objects'\n        docstrings.\n\n        The optional parameter `module` is the module that contains\n        the given object.  If the module is not specified or is None, then\n        the test finder will attempt to automatically determine the\n        correct module.  The object's module is used:\n\n            - As a default namespace, if `globs` is not specified.\n            - To prevent the DocTestFinder from extracting DocTests\n              from objects that are imported from other modules.\n            - To find the name of the file containing the object.\n            - To help find the line number of the object within its\n              file.\n\n        Contained objects whose module does not match `module` are ignored.\n\n        If `module` is False, no attempt to find the module will be made.\n        This is obscure, of use mostly in tests:  if `module` is False, or\n        is None but cannot be found automatically, then all objects are\n        considered to belong to the (non-existent) module, so all contained\n        objects will (recursively) be searched for doctests.\n\n        The globals for each DocTest is formed by combining `globs`\n        and `extraglobs` (bindings in `extraglobs` override bindings\n        in `globs`).  A new copy of the globals dictionary is created\n        for each DocTest.  If `globs` is not specified, then it\n        defaults to the module's `__dict__`, if specified, or {}\n        otherwise.  If `extraglobs` is not specified, then it defaults\n        to {}.\n\n        \"\"\"\n        # If name was not specified, then extract it from the object.\n        if name is None:\n            name = getattr(obj, '__name__', None)\n            if name is None:\n                raise ValueError(\"DocTestFinder.find: name must be given \"\n                        \"when obj.__name__ doesn't exist: %r\" %\n                                 (type(obj),))\n\n        # Find the module that contains the given object (if obj is\n        # a module, then module=obj.).  Note: this may fail, in which\n        # case module will be None.\n        if module is False:\n            module = None\n        elif module is None:\n            module = inspect.getmodule(obj)\n\n        # Read the module's source code.  This is used by\n        # DocTestFinder._find_lineno to find the line number for a\n        # given object's docstring.\n        try:\n            file = inspect.getsourcefile(obj) or inspect.getfile(obj)\n            if module is not None:\n                # Supply the module globals in case the module was\n                # originally loaded via a PEP 302 loader and\n                # file is not a valid filesystem path\n                source_lines = linecache.getlines(file, module.__dict__)\n            else:\n                # No access to a loader, so assume it's a normal\n                # filesystem path\n                source_lines = linecache.getlines(file)\n            if not source_lines:\n                source_lines = None\n        except TypeError:\n            source_lines = None\n\n        # Initialize globals, and merge in extraglobs.\n        if globs is None:\n            if module is None:\n                globs = {}\n            else:\n                globs = module.__dict__.copy()\n        else:\n            globs = globs.copy()\n        if extraglobs is not None:\n            globs.update(extraglobs)\n        if '__name__' not in globs:\n            globs['__name__'] = '__main__'  # provide a default module name\n\n        # Recursively explore `obj`, extracting DocTests.\n        tests = []\n        self._find(tests, obj, name, module, source_lines, globs, {})\n        # Sort the tests by alpha order of names, for consistency in\n        # verbose-mode output.  This was a feature of doctest in Pythons\n        # <= 2.3 that got lost by accident in 2.4.  It was repaired in\n        # 2.4.4 and 2.5.\n        tests.sort()\n        return tests\n\n    def _from_module(self, module, object):\n        \"\"\"\n        Return true if the given object is defined in the given\n        module.\n        \"\"\"\n        if module is None:\n            return True\n        elif inspect.getmodule(object) is not None:\n            return module is inspect.getmodule(object)\n        elif inspect.isfunction(object):\n            return module.__dict__ is object.func_globals\n        elif inspect.isclass(object):\n            return module.__name__ == object.__module__\n        elif hasattr(object, '__module__'):\n            return module.__name__ == object.__module__\n        elif isinstance(object, property):\n            return True # [XX] no way not be sure.\n        else:\n            raise ValueError(\"object must be a class or function\")\n\n    def _find(self, tests, obj, name, module, source_lines, globs, seen):\n        \"\"\"\n        Find tests for the given object and any contained objects, and\n        add them to `tests`.\n        \"\"\"\n        if self._verbose:\n            print 'Finding tests in %s' % name\n\n        # If we've already processed this object, then ignore it.\n        if id(obj) in seen:\n            return\n        seen[id(obj)] = 1\n\n        # Find a test for this object, and add it to the list of tests.\n        test = self._get_test(obj, name, module, globs, source_lines)\n        if test is not None:\n            tests.append(test)\n\n        # Look for tests in a module's contained objects.\n        if inspect.ismodule(obj) and self._recurse:\n            for valname, val in obj.__dict__.items():\n                valname = '%s.%s' % (name, valname)\n                # Recurse to functions & classes.\n                if ((inspect.isfunction(val) or inspect.isclass(val)) and\n                    self._from_module(module, val)):\n                    self._find(tests, val, valname, module, source_lines,\n                               globs, seen)\n\n        # Look for tests in a module's __test__ dictionary.\n        if inspect.ismodule(obj) and self._recurse:\n            for valname, val in getattr(obj, '__test__', {}).items():\n                if not isinstance(valname, basestring):\n                    raise ValueError(\"DocTestFinder.find: __test__ keys \"\n                                     \"must be strings: %r\" %\n                                     (type(valname),))\n                if not (inspect.isfunction(val) or inspect.isclass(val) or\n                        inspect.ismethod(val) or inspect.ismodule(val) or\n                        isinstance(val, basestring)):\n                    raise ValueError(\"DocTestFinder.find: __test__ values \"\n                                     \"must be strings, functions, methods, \"\n                                     \"classes, or modules: %r\" %\n                                     (type(val),))\n                valname = '%s.__test__.%s' % (name, valname)\n                self._find(tests, val, valname, module, source_lines,\n                           globs, seen)\n\n        # Look for tests in a class's contained objects.\n        if inspect.isclass(obj) and self._recurse:\n            for valname, val in obj.__dict__.items():\n                # Special handling for staticmethod/classmethod.\n                if isinstance(val, staticmethod):\n                    val = getattr(obj, valname)\n                if isinstance(val, classmethod):\n                    val = getattr(obj, valname).im_func\n\n                # Recurse to methods, properties, and nested classes.\n                if ((inspect.isfunction(val) or inspect.isclass(val) or\n                      isinstance(val, property)) and\n                      self._from_module(module, val)):\n                    valname = '%s.%s' % (name, valname)\n                    self._find(tests, val, valname, module, source_lines,\n                               globs, seen)\n\n    def _get_test(self, obj, name, module, globs, source_lines):\n        \"\"\"\n        Return a DocTest for the given object, if it defines a docstring;\n        otherwise, return None.\n        \"\"\"\n        # Extract the object's docstring.  If it doesn't have one,\n        # then return None (no test for this object).\n        if isinstance(obj, basestring):\n            docstring = obj\n        else:\n            try:\n                if obj.__doc__ is None:\n                    docstring = ''\n                else:\n                    docstring = obj.__doc__\n                    if not isinstance(docstring, basestring):\n                        docstring = str(docstring)\n            except (TypeError, AttributeError):\n                docstring = ''\n\n        # Find the docstring's location in the file.\n        lineno = self._find_lineno(obj, source_lines)\n\n        # Don't bother if the docstring is empty.\n        if self._exclude_empty and not docstring:\n            return None\n\n        # Return a DocTest for this object.\n        if module is None:\n            filename = None\n        else:\n            filename = getattr(module, '__file__', module.__name__)\n            if filename[-4:] in (\".pyc\", \".pyo\"):\n                filename = filename[:-1]\n        return self._parser.get_doctest(docstring, globs, name,\n                                        filename, lineno)\n\n    def _find_lineno(self, obj, source_lines):\n        \"\"\"\n        Return a line number of the given object's docstring.  Note:\n        this method assumes that the object has a docstring.\n        \"\"\"\n        lineno = None\n\n        # Find the line number for modules.\n        if inspect.ismodule(obj):\n            lineno = 0\n\n        # Find the line number for classes.\n        # Note: this could be fooled if a class is defined multiple\n        # times in a single file.\n        if inspect.isclass(obj):\n            if source_lines is None:\n                return None\n            pat = re.compile(r'^\\s*class\\s*%s\\b' %\n                             getattr(obj, '__name__', '-'))\n            for i, line in enumerate(source_lines):\n                if pat.match(line):\n                    lineno = i\n                    break\n\n        # Find the line number for functions & methods.\n        if inspect.ismethod(obj): obj = obj.im_func\n        if inspect.isfunction(obj): obj = obj.func_code\n        if inspect.istraceback(obj): obj = obj.tb_frame\n        if inspect.isframe(obj): obj = obj.f_code\n        if inspect.iscode(obj):\n            lineno = getattr(obj, 'co_firstlineno', None)-1\n\n        # Find the line number where the docstring starts.  Assume\n        # that it's the first line that begins with a quote mark.\n        # Note: this could be fooled by a multiline function\n        # signature, where a continuation line begins with a quote\n        # mark.\n        if lineno is not None:\n            if source_lines is None:\n                return lineno+1\n            pat = re.compile('(^|.*:)\\s*\\w*(\"|\\')')\n            for lineno in range(lineno, len(source_lines)):\n                if pat.match(source_lines[lineno]):\n                    return lineno\n\n        # We couldn't find the line number.\n        return None\n\n######################################################################\n## 5. DocTest Runner\n######################################################################\n\nclass DocTestRunner:\n    \"\"\"\n    A class used to run DocTest test cases, and accumulate statistics.\n    The `run` method is used to process a single DocTest case.  It\n    returns a tuple `(f, t)`, where `t` is the number of test cases\n    tried, and `f` is the number of test cases that failed.\n\n        >>> tests = DocTestFinder().find(_TestClass)\n        >>> runner = DocTestRunner(verbose=False)\n        >>> tests.sort(key = lambda test: test.name)\n        >>> for test in tests:\n        ...     print test.name, '->', runner.run(test)\n        _TestClass -> TestResults(failed=0, attempted=2)\n        _TestClass.__init__ -> TestResults(failed=0, attempted=2)\n        _TestClass.get -> TestResults(failed=0, attempted=2)\n        _TestClass.square -> TestResults(failed=0, attempted=1)\n\n    The `summarize` method prints a summary of all the test cases that\n    have been run by the runner, and returns an aggregated `(f, t)`\n    tuple:\n\n        >>> runner.summarize(verbose=1)\n        4 items passed all tests:\n           2 tests in _TestClass\n           2 tests in _TestClass.__init__\n           2 tests in _TestClass.get\n           1 tests in _TestClass.square\n        7 tests in 4 items.\n        7 passed and 0 failed.\n        Test passed.\n        TestResults(failed=0, attempted=7)\n\n    The aggregated number of tried examples and failed examples is\n    also available via the `tries` and `failures` attributes:\n\n        >>> runner.tries\n        7\n        >>> runner.failures\n        0\n\n    The comparison between expected outputs and actual outputs is done\n    by an `OutputChecker`.  This comparison may be customized with a\n    number of option flags; see the documentation for `testmod` for\n    more information.  If the option flags are insufficient, then the\n    comparison may also be customized by passing a subclass of\n    `OutputChecker` to the constructor.\n\n    The test runner's display output can be controlled in two ways.\n    First, an output function (`out) can be passed to\n    `TestRunner.run`; this function will be called with strings that\n    should be displayed.  It defaults to `sys.stdout.write`.  If\n    capturing the output is not sufficient, then the display output\n    can be also customized by subclassing DocTestRunner, and\n    overriding the methods `report_start`, `report_success`,\n    `report_unexpected_exception`, and `report_failure`.\n    \"\"\"\n    # This divider string is used to separate failure messages, and to\n    # separate sections of the summary.\n    DIVIDER = \"*\" * 70\n\n    def __init__(self, checker=None, verbose=None, optionflags=0):\n        \"\"\"\n        Create a new test runner.\n\n        Optional keyword arg `checker` is the `OutputChecker` that\n        should be used to compare the expected outputs and actual\n        outputs of doctest examples.\n\n        Optional keyword arg 'verbose' prints lots of stuff if true,\n        only failures if false; by default, it's true iff '-v' is in\n        sys.argv.\n\n        Optional argument `optionflags` can be used to control how the\n        test runner compares expected output to actual output, and how\n        it displays failures.  See the documentation for `testmod` for\n        more information.\n        \"\"\"\n        self._checker = checker or OutputChecker()\n        if verbose is None:\n            verbose = '-v' in sys.argv\n        self._verbose = verbose\n        self.optionflags = optionflags\n        self.original_optionflags = optionflags\n\n        # Keep track of the examples we've run.\n        self.tries = 0\n        self.failures = 0\n        self._name2ft = {}\n\n        # Create a fake output target for capturing doctest output.\n        self._fakeout = _SpoofOut()\n\n    #/////////////////////////////////////////////////////////////////\n    # Reporting methods\n    #/////////////////////////////////////////////////////////////////\n\n    def report_start(self, out, test, example):\n        \"\"\"\n        Report that the test runner is about to process the given\n        example.  (Only displays a message if verbose=True)\n        \"\"\"\n        if self._verbose:\n            if example.want:\n                out('Trying:\\n' + _indent(example.source) +\n                    'Expecting:\\n' + _indent(example.want))\n            else:\n                out('Trying:\\n' + _indent(example.source) +\n                    'Expecting nothing\\n')\n\n    def report_success(self, out, test, example, got):\n        \"\"\"\n        Report that the given example ran successfully.  (Only\n        displays a message if verbose=True)\n        \"\"\"\n        if self._verbose:\n            out(\"ok\\n\")\n\n    def report_failure(self, out, test, example, got):\n        \"\"\"\n        Report that the given example failed.\n        \"\"\"\n        out(self._failure_header(test, example) +\n            self._checker.output_difference(example, got, self.optionflags))\n\n    def report_unexpected_exception(self, out, test, example, exc_info):\n        \"\"\"\n        Report that the given example raised an unexpected exception.\n        \"\"\"\n        out(self._failure_header(test, example) +\n            'Exception raised:\\n' + _indent(_exception_traceback(exc_info)))\n\n    def _failure_header(self, test, example):\n        out = [self.DIVIDER]\n        if test.filename:\n            if test.lineno is not None and example.lineno is not None:\n                lineno = test.lineno + example.lineno + 1\n            else:\n                lineno = '?'\n            out.append('File \"%s\", line %s, in %s' %\n                       (test.filename, lineno, test.name))\n        else:\n            out.append('Line %s, in %s' % (example.lineno+1, test.name))\n        out.append('Failed example:')\n        source = example.source\n        out.append(_indent(source))\n        return '\\n'.join(out)\n\n    #/////////////////////////////////////////////////////////////////\n    # DocTest Running\n    #/////////////////////////////////////////////////////////////////\n\n    def __run(self, test, compileflags, out):\n        \"\"\"\n        Run the examples in `test`.  Write the outcome of each example\n        with one of the `DocTestRunner.report_*` methods, using the\n        writer function `out`.  `compileflags` is the set of compiler\n        flags that should be used to execute examples.  Return a tuple\n        `(f, t)`, where `t` is the number of examples tried, and `f`\n        is the number of examples that failed.  The examples are run\n        in the namespace `test.globs`.\n        \"\"\"\n        # Keep track of the number of failures and tries.\n        failures = tries = 0\n\n        # Save the option flags (since option directives can be used\n        # to modify them).\n        original_optionflags = self.optionflags\n\n        SUCCESS, FAILURE, BOOM = range(3) # `outcome` state\n\n        check = self._checker.check_output\n\n        # Process each example.\n        for examplenum, example in enumerate(test.examples):\n\n            # If REPORT_ONLY_FIRST_FAILURE is set, then suppress\n            # reporting after the first failure.\n            quiet = (self.optionflags & REPORT_ONLY_FIRST_FAILURE and\n                     failures > 0)\n\n            # Merge in the example's options.\n            self.optionflags = original_optionflags\n            if example.options:\n                for (optionflag, val) in example.options.items():\n                    if val:\n                        self.optionflags |= optionflag\n                    else:\n                        self.optionflags &= ~optionflag\n\n            # If 'SKIP' is set, then skip this example.\n            if self.optionflags & SKIP:\n                continue\n\n            # Record that we started this example.\n            tries += 1\n            if not quiet:\n                self.report_start(out, test, example)\n\n            # Use a special filename for compile(), so we can retrieve\n            # the source code during interactive debugging (see\n            # __patched_linecache_getlines).\n            filename = '<doctest %s[%d]>' % (test.name, examplenum)\n\n            # Run the example in the given context (globs), and record\n            # any exception that gets raised.  (But don't intercept\n            # keyboard interrupts.)\n            try:\n                # Don't blink!  This is where the user's code gets run.\n                exec compile(example.source, filename, \"single\",\n                             compileflags, 1) in test.globs\n                self.debugger.set_continue() # ==== Example Finished ====\n                exception = None\n            except KeyboardInterrupt:\n                raise\n            except:\n                exception = sys.exc_info()\n                self.debugger.set_continue() # ==== Example Finished ====\n\n            got = self._fakeout.getvalue()  # the actual output\n            self._fakeout.truncate(0)\n            outcome = FAILURE   # guilty until proved innocent or insane\n\n            # If the example executed without raising any exceptions,\n            # verify its output.\n            if exception is None:\n                if check(example.want, got, self.optionflags):\n                    outcome = SUCCESS\n\n            # The example raised an exception:  check if it was expected.\n            else:\n                exc_info = sys.exc_info()\n                exc_msg = traceback.format_exception_only(*exc_info[:2])[-1]\n                if not quiet:\n                    got += _exception_traceback(exc_info)\n\n                # If `example.exc_msg` is None, then we weren't expecting\n                # an exception.\n                if example.exc_msg is None:\n                    outcome = BOOM\n\n                # We expected an exception:  see whether it matches.\n                elif check(example.exc_msg, exc_msg, self.optionflags):\n                    outcome = SUCCESS\n\n                # Another chance if they didn't care about the detail.\n                elif self.optionflags & IGNORE_EXCEPTION_DETAIL:\n                    if check(_strip_exception_details(example.exc_msg),\n                             _strip_exception_details(exc_msg),\n                             self.optionflags):\n                        outcome = SUCCESS\n\n            # Report the outcome.\n            if outcome is SUCCESS:\n                if not quiet:\n                    self.report_success(out, test, example, got)\n            elif outcome is FAILURE:\n                if not quiet:\n                    self.report_failure(out, test, example, got)\n                failures += 1\n            elif outcome is BOOM:\n                if not quiet:\n                    self.report_unexpected_exception(out, test, example,\n                                                     exc_info)\n                failures += 1\n            else:\n                assert False, (\"unknown outcome\", outcome)\n\n        # Restore the option flags (in case they were modified)\n        self.optionflags = original_optionflags\n\n        # Record and return the number of failures and tries.\n        self.__record_outcome(test, failures, tries)\n        return TestResults(failures, tries)\n\n    def __record_outcome(self, test, f, t):\n        \"\"\"\n        Record the fact that the given DocTest (`test`) generated `f`\n        failures out of `t` tried examples.\n        \"\"\"\n        f2, t2 = self._name2ft.get(test.name, (0,0))\n        self._name2ft[test.name] = (f+f2, t+t2)\n        self.failures += f\n        self.tries += t\n\n    __LINECACHE_FILENAME_RE = re.compile(r'<doctest '\n                                         r'(?P<name>.+)'\n                                         r'\\[(?P<examplenum>\\d+)\\]>$')\n    def __patched_linecache_getlines(self, filename, module_globals=None):\n        m = self.__LINECACHE_FILENAME_RE.match(filename)\n        if m and m.group('name') == self.test.name:\n            example = self.test.examples[int(m.group('examplenum'))]\n            source = example.source\n            if isinstance(source, unicode):\n                source = source.encode('ascii', 'backslashreplace')\n            return source.splitlines(True)\n        else:\n            return self.save_linecache_getlines(filename, module_globals)\n\n    def run(self, test, compileflags=None, out=None, clear_globs=True):\n        \"\"\"\n        Run the examples in `test`, and display the results using the\n        writer function `out`.\n\n        The examples are run in the namespace `test.globs`.  If\n        `clear_globs` is true (the default), then this namespace will\n        be cleared after the test runs, to help with garbage\n        collection.  If you would like to examine the namespace after\n        the test completes, then use `clear_globs=False`.\n\n        `compileflags` gives the set of flags that should be used by\n        the Python compiler when running the examples.  If not\n        specified, then it will default to the set of future-import\n        flags that apply to `globs`.\n\n        The output of each example is checked using\n        `DocTestRunner.check_output`, and the results are formatted by\n        the `DocTestRunner.report_*` methods.\n        \"\"\"\n        self.test = test\n\n        if compileflags is None:\n            compileflags = _extract_future_flags(test.globs)\n\n        save_stdout = sys.stdout\n        if out is None:\n            out = save_stdout.write\n        sys.stdout = self._fakeout\n\n        # Patch pdb.set_trace to restore sys.stdout during interactive\n        # debugging (so it's not still redirected to self._fakeout).\n        # Note that the interactive output will go to *our*\n        # save_stdout, even if that's not the real sys.stdout; this\n        # allows us to write test cases for the set_trace behavior.\n        save_set_trace = pdb.set_trace\n        self.debugger = _OutputRedirectingPdb(save_stdout)\n        self.debugger.reset()\n        pdb.set_trace = self.debugger.set_trace\n\n        # Patch linecache.getlines, so we can see the example's source\n        # when we're inside the debugger.\n        self.save_linecache_getlines = linecache.getlines\n        linecache.getlines = self.__patched_linecache_getlines\n\n        # Make sure sys.displayhook just prints the value to stdout\n        save_displayhook = sys.displayhook\n        sys.displayhook = sys.__displayhook__\n\n        try:\n            return self.__run(test, compileflags, out)\n        finally:\n            sys.stdout = save_stdout\n            pdb.set_trace = save_set_trace\n            linecache.getlines = self.save_linecache_getlines\n            sys.displayhook = save_displayhook\n            if clear_globs:\n                test.globs.clear()\n\n    #/////////////////////////////////////////////////////////////////\n    # Summarization\n    #/////////////////////////////////////////////////////////////////\n    def summarize(self, verbose=None):\n        \"\"\"\n        Print a summary of all the test cases that have been run by\n        this DocTestRunner, and return a tuple `(f, t)`, where `f` is\n        the total number of failed examples, and `t` is the total\n        number of tried examples.\n\n        The optional `verbose` argument controls how detailed the\n        summary is.  If the verbosity is not specified, then the\n        DocTestRunner's verbosity is used.\n        \"\"\"\n        if verbose is None:\n            verbose = self._verbose\n        notests = []\n        passed = []\n        failed = []\n        totalt = totalf = 0\n        for x in self._name2ft.items():\n            name, (f, t) = x\n            assert f <= t\n            totalt += t\n            totalf += f\n            if t == 0:\n                notests.append(name)\n            elif f == 0:\n                passed.append( (name, t) )\n            else:\n                failed.append(x)\n        if verbose:\n            if notests:\n                print len(notests), \"items had no tests:\"\n                notests.sort()\n                for thing in notests:\n                    print \"   \", thing\n            if passed:\n                print len(passed), \"items passed all tests:\"\n                passed.sort()\n                for thing, count in passed:\n                    print \" %3d tests in %s\" % (count, thing)\n        if failed:\n            print self.DIVIDER\n            print len(failed), \"items had failures:\"\n            failed.sort()\n            for thing, (f, t) in failed:\n                print \" %3d of %3d in %s\" % (f, t, thing)\n        if verbose:\n            print totalt, \"tests in\", len(self._name2ft), \"items.\"\n            print totalt - totalf, \"passed and\", totalf, \"failed.\"\n        if totalf:\n            print \"***Test Failed***\", totalf, \"failures.\"\n        elif verbose:\n            print \"Test passed.\"\n        return TestResults(totalf, totalt)\n\n    #/////////////////////////////////////////////////////////////////\n    # Backward compatibility cruft to maintain doctest.master.\n    #/////////////////////////////////////////////////////////////////\n    def merge(self, other):\n        d = self._name2ft\n        for name, (f, t) in other._name2ft.items():\n            if name in d:\n                # Don't print here by default, since doing\n                #     so breaks some of the buildbots\n                #print \"*** DocTestRunner.merge: '\" + name + \"' in both\" \\\n                #    \" testers; summing outcomes.\"\n                f2, t2 = d[name]\n                f = f + f2\n                t = t + t2\n            d[name] = f, t\n\nclass OutputChecker:\n    \"\"\"\n    A class used to check the whether the actual output from a doctest\n    example matches the expected output.  `OutputChecker` defines two\n    methods: `check_output`, which compares a given pair of outputs,\n    and returns true if they match; and `output_difference`, which\n    returns a string describing the differences between two outputs.\n    \"\"\"\n    def check_output(self, want, got, optionflags):\n        \"\"\"\n        Return True iff the actual output from an example (`got`)\n        matches the expected output (`want`).  These strings are\n        always considered to match if they are identical; but\n        depending on what option flags the test runner is using,\n        several non-exact match types are also possible.  See the\n        documentation for `TestRunner` for more information about\n        option flags.\n        \"\"\"\n        # Handle the common case first, for efficiency:\n        # if they're string-identical, always return true.\n        if got == want:\n            return True\n\n        # The values True and False replaced 1 and 0 as the return\n        # value for boolean comparisons in Python 2.3.\n        if not (optionflags & DONT_ACCEPT_TRUE_FOR_1):\n            if (got,want) == (\"True\\n\", \"1\\n\"):\n                return True\n            if (got,want) == (\"False\\n\", \"0\\n\"):\n                return True\n\n        # <BLANKLINE> can be used as a special sequence to signify a\n        # blank line, unless the DONT_ACCEPT_BLANKLINE flag is used.\n        if not (optionflags & DONT_ACCEPT_BLANKLINE):\n            # Replace <BLANKLINE> in want with a blank line.\n            want = re.sub('(?m)^%s\\s*?$' % re.escape(BLANKLINE_MARKER),\n                          '', want)\n            # If a line in got contains only spaces, then remove the\n            # spaces.\n            got = re.sub('(?m)^\\s*?$', '', got)\n            if got == want:\n                return True\n\n        # This flag causes doctest to ignore any differences in the\n        # contents of whitespace strings.  Note that this can be used\n        # in conjunction with the ELLIPSIS flag.\n        if optionflags & NORMALIZE_WHITESPACE:\n            got = ' '.join(got.split())\n            want = ' '.join(want.split())\n            if got == want:\n                return True\n\n        # The ELLIPSIS flag says to let the sequence \"...\" in `want`\n        # match any substring in `got`.\n        if optionflags & ELLIPSIS:\n            if _ellipsis_match(want, got):\n                return True\n\n        # We didn't find any match; return false.\n        return False\n\n    # Should we do a fancy diff?\n    def _do_a_fancy_diff(self, want, got, optionflags):\n        # Not unless they asked for a fancy diff.\n        if not optionflags & (REPORT_UDIFF |\n                              REPORT_CDIFF |\n                              REPORT_NDIFF):\n            return False\n\n        # If expected output uses ellipsis, a meaningful fancy diff is\n        # too hard ... or maybe not.  In two real-life failures Tim saw,\n        # a diff was a major help anyway, so this is commented out.\n        # [todo] _ellipsis_match() knows which pieces do and don't match,\n        # and could be the basis for a kick-ass diff in this case.\n        ##if optionflags & ELLIPSIS and ELLIPSIS_MARKER in want:\n        ##    return False\n\n        # ndiff does intraline difference marking, so can be useful even\n        # for 1-line differences.\n        if optionflags & REPORT_NDIFF:\n            return True\n\n        # The other diff types need at least a few lines to be helpful.\n        return want.count('\\n') > 2 and got.count('\\n') > 2\n\n    def output_difference(self, example, got, optionflags):\n        \"\"\"\n        Return a string describing the differences between the\n        expected output for a given example (`example`) and the actual\n        output (`got`).  `optionflags` is the set of option flags used\n        to compare `want` and `got`.\n        \"\"\"\n        want = example.want\n        # If <BLANKLINE>s are being used, then replace blank lines\n        # with <BLANKLINE> in the actual output string.\n        if not (optionflags & DONT_ACCEPT_BLANKLINE):\n            got = re.sub('(?m)^[ ]*(?=\\n)', BLANKLINE_MARKER, got)\n\n        # Check if we should use diff.\n        if self._do_a_fancy_diff(want, got, optionflags):\n            # Split want & got into lines.\n            want_lines = want.splitlines(True)  # True == keep line ends\n            got_lines = got.splitlines(True)\n            # Use difflib to find their differences.\n            if optionflags & REPORT_UDIFF:\n                diff = difflib.unified_diff(want_lines, got_lines, n=2)\n                diff = list(diff)[2:] # strip the diff header\n                kind = 'unified diff with -expected +actual'\n            elif optionflags & REPORT_CDIFF:\n                diff = difflib.context_diff(want_lines, got_lines, n=2)\n                diff = list(diff)[2:] # strip the diff header\n                kind = 'context diff with expected followed by actual'\n            elif optionflags & REPORT_NDIFF:\n                engine = difflib.Differ(charjunk=difflib.IS_CHARACTER_JUNK)\n                diff = list(engine.compare(want_lines, got_lines))\n                kind = 'ndiff with -expected +actual'\n            else:\n                assert 0, 'Bad diff option'\n            # Remove trailing whitespace on diff output.\n            diff = [line.rstrip() + '\\n' for line in diff]\n            return 'Differences (%s):\\n' % kind + _indent(''.join(diff))\n\n        # If we're not using diff, then simply list the expected\n        # output followed by the actual output.\n        if want and got:\n            return 'Expected:\\n%sGot:\\n%s' % (_indent(want), _indent(got))\n        elif want:\n            return 'Expected:\\n%sGot nothing\\n' % _indent(want)\n        elif got:\n            return 'Expected nothing\\nGot:\\n%s' % _indent(got)\n        else:\n            return 'Expected nothing\\nGot nothing\\n'\n\nclass DocTestFailure(Exception):\n    \"\"\"A DocTest example has failed in debugging mode.\n\n    The exception instance has variables:\n\n    - test: the DocTest object being run\n\n    - example: the Example object that failed\n\n    - got: the actual output\n    \"\"\"\n    def __init__(self, test, example, got):\n        self.test = test\n        self.example = example\n        self.got = got\n\n    def __str__(self):\n        return str(self.test)\n\nclass UnexpectedException(Exception):\n    \"\"\"A DocTest example has encountered an unexpected exception\n\n    The exception instance has variables:\n\n    - test: the DocTest object being run\n\n    - example: the Example object that failed\n\n    - exc_info: the exception info\n    \"\"\"\n    def __init__(self, test, example, exc_info):\n        self.test = test\n        self.example = example\n        self.exc_info = exc_info\n\n    def __str__(self):\n        return str(self.test)\n\nclass DebugRunner(DocTestRunner):\n    r\"\"\"Run doc tests but raise an exception as soon as there is a failure.\n\n       If an unexpected exception occurs, an UnexpectedException is raised.\n       It contains the test, the example, and the original exception:\n\n         >>> runner = DebugRunner(verbose=False)\n         >>> test = DocTestParser().get_doctest('>>> raise KeyError\\n42',\n         ...                                    {}, 'foo', 'foo.py', 0)\n         >>> try:\n         ...     runner.run(test)\n         ... except UnexpectedException, failure:\n         ...     pass\n\n         >>> failure.test is test\n         True\n\n         >>> failure.example.want\n         '42\\n'\n\n         >>> exc_info = failure.exc_info\n         >>> raise exc_info[0], exc_info[1], exc_info[2]\n         Traceback (most recent call last):\n         ...\n         KeyError\n\n       We wrap the original exception to give the calling application\n       access to the test and example information.\n\n       If the output doesn't match, then a DocTestFailure is raised:\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 1\n         ...      >>> x\n         ...      2\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> try:\n         ...    runner.run(test)\n         ... except DocTestFailure, failure:\n         ...    pass\n\n       DocTestFailure objects provide access to the test:\n\n         >>> failure.test is test\n         True\n\n       As well as to the example:\n\n         >>> failure.example.want\n         '2\\n'\n\n       and the actual output:\n\n         >>> failure.got\n         '1\\n'\n\n       If a failure or error occurs, the globals are left intact:\n\n         >>> del test.globs['__builtins__']\n         >>> test.globs\n         {'x': 1}\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 2\n         ...      >>> raise KeyError\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> runner.run(test)\n         Traceback (most recent call last):\n         ...\n         UnexpectedException: <DocTest foo from foo.py:0 (2 examples)>\n\n         >>> del test.globs['__builtins__']\n         >>> test.globs\n         {'x': 2}\n\n       But the globals are cleared if there is no error:\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 2\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> runner.run(test)\n         TestResults(failed=0, attempted=1)\n\n         >>> test.globs\n         {}\n\n       \"\"\"\n\n    def run(self, test, compileflags=None, out=None, clear_globs=True):\n        r = DocTestRunner.run(self, test, compileflags, out, False)\n        if clear_globs:\n            test.globs.clear()\n        return r\n\n    def report_unexpected_exception(self, out, test, example, exc_info):\n        raise UnexpectedException(test, example, exc_info)\n\n    def report_failure(self, out, test, example, got):\n        raise DocTestFailure(test, example, got)\n\n######################################################################\n## 6. Test Functions\n######################################################################\n# These should be backwards compatible.\n\n# For backward compatibility, a global instance of a DocTestRunner\n# class, updated by testmod.\nmaster = None\n\ndef testmod(m=None, name=None, globs=None, verbose=None,\n            report=True, optionflags=0, extraglobs=None,\n            raise_on_error=False, exclude_empty=False):\n    \"\"\"m=None, name=None, globs=None, verbose=None, report=True,\n       optionflags=0, extraglobs=None, raise_on_error=False,\n       exclude_empty=False\n\n    Test examples in docstrings in functions and classes reachable\n    from module m (or the current module if m is not supplied), starting\n    with m.__doc__.\n\n    Also test examples reachable from dict m.__test__ if it exists and is\n    not None.  m.__test__ maps names to functions, classes and strings;\n    function and class docstrings are tested even if the name is private;\n    strings are tested directly, as if they were docstrings.\n\n    Return (#failures, #tests).\n\n    See help(doctest) for an overview.\n\n    Optional keyword arg \"name\" gives the name of the module; by default\n    use m.__name__.\n\n    Optional keyword arg \"globs\" gives a dict to be used as the globals\n    when executing examples; by default, use m.__dict__.  A copy of this\n    dict is actually used for each docstring, so that each docstring's\n    examples start with a clean slate.\n\n    Optional keyword arg \"extraglobs\" gives a dictionary that should be\n    merged into the globals that are used to execute examples.  By\n    default, no extra globals are used.  This is new in 2.4.\n\n    Optional keyword arg \"verbose\" prints lots of stuff if true, prints\n    only failures if false; by default, it's true iff \"-v\" is in sys.argv.\n\n    Optional keyword arg \"report\" prints a summary at the end when true,\n    else prints nothing at the end.  In verbose mode, the summary is\n    detailed, else very brief (in fact, empty if all tests passed).\n\n    Optional keyword arg \"optionflags\" or's together module constants,\n    and defaults to 0.  This is new in 2.3.  Possible values (see the\n    docs for details):\n\n        DONT_ACCEPT_TRUE_FOR_1\n        DONT_ACCEPT_BLANKLINE\n        NORMALIZE_WHITESPACE\n        ELLIPSIS\n        SKIP\n        IGNORE_EXCEPTION_DETAIL\n        REPORT_UDIFF\n        REPORT_CDIFF\n        REPORT_NDIFF\n        REPORT_ONLY_FIRST_FAILURE\n\n    Optional keyword arg \"raise_on_error\" raises an exception on the\n    first unexpected exception or failure. This allows failures to be\n    post-mortem debugged.\n\n    Advanced tomfoolery:  testmod runs methods of a local instance of\n    class doctest.Tester, then merges the results into (or creates)\n    global Tester instance doctest.master.  Methods of doctest.master\n    can be called directly too, if you want to do something unusual.\n    Passing report=0 to testmod is especially useful then, to delay\n    displaying a summary.  Invoke doctest.master.summarize(verbose)\n    when you're done fiddling.\n    \"\"\"\n    global master\n\n    # If no module was given, then use __main__.\n    if m is None:\n        # DWA - m will still be None if this wasn't invoked from the command\n        # line, in which case the following TypeError is about as good an error\n        # as we should expect\n        m = sys.modules.get('__main__')\n\n    # Check that we were actually given a module.\n    if not inspect.ismodule(m):\n        raise TypeError(\"testmod: module required; %r\" % (m,))\n\n    # If no name was given, then use the module's name.\n    if name is None:\n        name = m.__name__\n\n    # Find, parse, and run all tests in the given module.\n    finder = DocTestFinder(exclude_empty=exclude_empty)\n\n    if raise_on_error:\n        runner = DebugRunner(verbose=verbose, optionflags=optionflags)\n    else:\n        runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n\n    for test in finder.find(m, name, globs=globs, extraglobs=extraglobs):\n        runner.run(test)\n\n    if report:\n        runner.summarize()\n\n    if master is None:\n        master = runner\n    else:\n        master.merge(runner)\n\n    return TestResults(runner.failures, runner.tries)\n\ndef testfile(filename, module_relative=True, name=None, package=None,\n             globs=None, verbose=None, report=True, optionflags=0,\n             extraglobs=None, raise_on_error=False, parser=DocTestParser(),\n             encoding=None):\n    \"\"\"\n    Test examples in the given file.  Return (#failures, #tests).\n\n    Optional keyword arg \"module_relative\" specifies how filenames\n    should be interpreted:\n\n      - If \"module_relative\" is True (the default), then \"filename\"\n         specifies a module-relative path.  By default, this path is\n         relative to the calling module's directory; but if the\n         \"package\" argument is specified, then it is relative to that\n         package.  To ensure os-independence, \"filename\" should use\n         \"/\" characters to separate path segments, and should not\n         be an absolute path (i.e., it may not begin with \"/\").\n\n      - If \"module_relative\" is False, then \"filename\" specifies an\n        os-specific path.  The path may be absolute or relative (to\n        the current working directory).\n\n    Optional keyword arg \"name\" gives the name of the test; by default\n    use the file's basename.\n\n    Optional keyword argument \"package\" is a Python package or the\n    name of a Python package whose directory should be used as the\n    base directory for a module relative filename.  If no package is\n    specified, then the calling module's directory is used as the base\n    directory for module relative filenames.  It is an error to\n    specify \"package\" if \"module_relative\" is False.\n\n    Optional keyword arg \"globs\" gives a dict to be used as the globals\n    when executing examples; by default, use {}.  A copy of this dict\n    is actually used for each docstring, so that each docstring's\n    examples start with a clean slate.\n\n    Optional keyword arg \"extraglobs\" gives a dictionary that should be\n    merged into the globals that are used to execute examples.  By\n    default, no extra globals are used.\n\n    Optional keyword arg \"verbose\" prints lots of stuff if true, prints\n    only failures if false; by default, it's true iff \"-v\" is in sys.argv.\n\n    Optional keyword arg \"report\" prints a summary at the end when true,\n    else prints nothing at the end.  In verbose mode, the summary is\n    detailed, else very brief (in fact, empty if all tests passed).\n\n    Optional keyword arg \"optionflags\" or's together module constants,\n    and defaults to 0.  Possible values (see the docs for details):\n\n        DONT_ACCEPT_TRUE_FOR_1\n        DONT_ACCEPT_BLANKLINE\n        NORMALIZE_WHITESPACE\n        ELLIPSIS\n        SKIP\n        IGNORE_EXCEPTION_DETAIL\n        REPORT_UDIFF\n        REPORT_CDIFF\n        REPORT_NDIFF\n        REPORT_ONLY_FIRST_FAILURE\n\n    Optional keyword arg \"raise_on_error\" raises an exception on the\n    first unexpected exception or failure. This allows failures to be\n    post-mortem debugged.\n\n    Optional keyword arg \"parser\" specifies a DocTestParser (or\n    subclass) that should be used to extract tests from the files.\n\n    Optional keyword arg \"encoding\" specifies an encoding that should\n    be used to convert the file to unicode.\n\n    Advanced tomfoolery:  testmod runs methods of a local instance of\n    class doctest.Tester, then merges the results into (or creates)\n    global Tester instance doctest.master.  Methods of doctest.master\n    can be called directly too, if you want to do something unusual.\n    Passing report=0 to testmod is especially useful then, to delay\n    displaying a summary.  Invoke doctest.master.summarize(verbose)\n    when you're done fiddling.\n    \"\"\"\n    global master\n\n    if package and not module_relative:\n        raise ValueError(\"Package may only be specified for module-\"\n                         \"relative paths.\")\n\n    # Relativize the path\n    text, filename = _load_testfile(filename, package, module_relative)\n\n    # If no name was given, then use the file's name.\n    if name is None:\n        name = os.path.basename(filename)\n\n    # Assemble the globals.\n    if globs is None:\n        globs = {}\n    else:\n        globs = globs.copy()\n    if extraglobs is not None:\n        globs.update(extraglobs)\n    if '__name__' not in globs:\n        globs['__name__'] = '__main__'\n\n    if raise_on_error:\n        runner = DebugRunner(verbose=verbose, optionflags=optionflags)\n    else:\n        runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n\n    if encoding is not None:\n        text = text.decode(encoding)\n\n    # Read the file, convert it to a test, and run it.\n    test = parser.get_doctest(text, globs, name, filename, 0)\n    runner.run(test)\n\n    if report:\n        runner.summarize()\n\n    if master is None:\n        master = runner\n    else:\n        master.merge(runner)\n\n    return TestResults(runner.failures, runner.tries)\n\ndef run_docstring_examples(f, globs, verbose=False, name=\"NoName\",\n                           compileflags=None, optionflags=0):\n    \"\"\"\n    Test examples in the given object's docstring (`f`), using `globs`\n    as globals.  Optional argument `name` is used in failure messages.\n    If the optional argument `verbose` is true, then generate output\n    even if there are no failures.\n\n    `compileflags` gives the set of flags that should be used by the\n    Python compiler when running the examples.  If not specified, then\n    it will default to the set of future-import flags that apply to\n    `globs`.\n\n    Optional keyword arg `optionflags` specifies options for the\n    testing and output.  See the documentation for `testmod` for more\n    information.\n    \"\"\"\n    # Find, parse, and run all tests in the given module.\n    finder = DocTestFinder(verbose=verbose, recurse=False)\n    runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n    for test in finder.find(f, name, globs=globs):\n        runner.run(test, compileflags=compileflags)\n\n######################################################################\n## 7. Tester\n######################################################################\n# This is provided only for backwards compatibility.  It's not\n# actually used in any way.\n\nclass Tester:\n    def __init__(self, mod=None, globs=None, verbose=None, optionflags=0):\n\n        warnings.warn(\"class Tester is deprecated; \"\n                      \"use class doctest.DocTestRunner instead\",\n                      DeprecationWarning, stacklevel=2)\n        if mod is None and globs is None:\n            raise TypeError(\"Tester.__init__: must specify mod or globs\")\n        if mod is not None and not inspect.ismodule(mod):\n            raise TypeError(\"Tester.__init__: mod must be a module; %r\" %\n                            (mod,))\n        if globs is None:\n            globs = mod.__dict__\n        self.globs = globs\n\n        self.verbose = verbose\n        self.optionflags = optionflags\n        self.testfinder = DocTestFinder()\n        self.testrunner = DocTestRunner(verbose=verbose,\n                                        optionflags=optionflags)\n\n    def runstring(self, s, name):\n        test = DocTestParser().get_doctest(s, self.globs, name, None, None)\n        if self.verbose:\n            print \"Running string\", name\n        (f,t) = self.testrunner.run(test)\n        if self.verbose:\n            print f, \"of\", t, \"examples failed in string\", name\n        return TestResults(f,t)\n\n    def rundoc(self, object, name=None, module=None):\n        f = t = 0\n        tests = self.testfinder.find(object, name, module=module,\n                                     globs=self.globs)\n        for test in tests:\n            (f2, t2) = self.testrunner.run(test)\n            (f,t) = (f+f2, t+t2)\n        return TestResults(f,t)\n\n    def rundict(self, d, name, module=None):\n        import types\n        m = types.ModuleType(name)\n        m.__dict__.update(d)\n        if module is None:\n            module = False\n        return self.rundoc(m, name, module)\n\n    def run__test__(self, d, name):\n        import types\n        m = types.ModuleType(name)\n        m.__test__ = d\n        return self.rundoc(m, name)\n\n    def summarize(self, verbose=None):\n        return self.testrunner.summarize(verbose)\n\n    def merge(self, other):\n        self.testrunner.merge(other.testrunner)\n\n######################################################################\n## 8. Unittest Support\n######################################################################\n\n_unittest_reportflags = 0\n\ndef set_unittest_reportflags(flags):\n    \"\"\"Sets the unittest option flags.\n\n    The old flag is returned so that a runner could restore the old\n    value if it wished to:\n\n      >>> import doctest\n      >>> old = doctest._unittest_reportflags\n      >>> doctest.set_unittest_reportflags(REPORT_NDIFF |\n      ...                          REPORT_ONLY_FIRST_FAILURE) == old\n      True\n\n      >>> doctest._unittest_reportflags == (REPORT_NDIFF |\n      ...                                   REPORT_ONLY_FIRST_FAILURE)\n      True\n\n    Only reporting flags can be set:\n\n      >>> doctest.set_unittest_reportflags(ELLIPSIS)\n      Traceback (most recent call last):\n      ...\n      ValueError: ('Only reporting flags allowed', 8)\n\n      >>> doctest.set_unittest_reportflags(old) == (REPORT_NDIFF |\n      ...                                   REPORT_ONLY_FIRST_FAILURE)\n      True\n    \"\"\"\n    global _unittest_reportflags\n\n    if (flags & REPORTING_FLAGS) != flags:\n        raise ValueError(\"Only reporting flags allowed\", flags)\n    old = _unittest_reportflags\n    _unittest_reportflags = flags\n    return old\n\n\nclass DocTestCase(unittest.TestCase):\n\n    def __init__(self, test, optionflags=0, setUp=None, tearDown=None,\n                 checker=None):\n\n        unittest.TestCase.__init__(self)\n        self._dt_optionflags = optionflags\n        self._dt_checker = checker\n        self._dt_test = test\n        self._dt_setUp = setUp\n        self._dt_tearDown = tearDown\n\n    def setUp(self):\n        test = self._dt_test\n\n        if self._dt_setUp is not None:\n            self._dt_setUp(test)\n\n    def tearDown(self):\n        test = self._dt_test\n\n        if self._dt_tearDown is not None:\n            self._dt_tearDown(test)\n\n        test.globs.clear()\n\n    def runTest(self):\n        test = self._dt_test\n        old = sys.stdout\n        new = StringIO()\n        optionflags = self._dt_optionflags\n\n        if not (optionflags & REPORTING_FLAGS):\n            # The option flags don't include any reporting flags,\n            # so add the default reporting flags\n            optionflags |= _unittest_reportflags\n\n        runner = DocTestRunner(optionflags=optionflags,\n                               checker=self._dt_checker, verbose=False)\n\n        try:\n            runner.DIVIDER = \"-\"*70\n            failures, tries = runner.run(\n                test, out=new.write, clear_globs=False)\n        finally:\n            sys.stdout = old\n\n        if failures:\n            raise self.failureException(self.format_failure(new.getvalue()))\n\n    def format_failure(self, err):\n        test = self._dt_test\n        if test.lineno is None:\n            lineno = 'unknown line number'\n        else:\n            lineno = '%s' % test.lineno\n        lname = '.'.join(test.name.split('.')[-1:])\n        return ('Failed doctest test for %s\\n'\n                '  File \"%s\", line %s, in %s\\n\\n%s'\n                % (test.name, test.filename, lineno, lname, err)\n                )\n\n    def debug(self):\n        r\"\"\"Run the test case without results and without catching exceptions\n\n           The unit test framework includes a debug method on test cases\n           and test suites to support post-mortem debugging.  The test code\n           is run in such a way that errors are not caught.  This way a\n           caller can catch the errors and initiate post-mortem debugging.\n\n           The DocTestCase provides a debug method that raises\n           UnexpectedException errors if there is an unexpected\n           exception:\n\n             >>> test = DocTestParser().get_doctest('>>> raise KeyError\\n42',\n             ...                {}, 'foo', 'foo.py', 0)\n             >>> case = DocTestCase(test)\n             >>> try:\n             ...     case.debug()\n             ... except UnexpectedException, failure:\n             ...     pass\n\n           The UnexpectedException contains the test, the example, and\n           the original exception:\n\n             >>> failure.test is test\n             True\n\n             >>> failure.example.want\n             '42\\n'\n\n             >>> exc_info = failure.exc_info\n             >>> raise exc_info[0], exc_info[1], exc_info[2]\n             Traceback (most recent call last):\n             ...\n             KeyError\n\n           If the output doesn't match, then a DocTestFailure is raised:\n\n             >>> test = DocTestParser().get_doctest('''\n             ...      >>> x = 1\n             ...      >>> x\n             ...      2\n             ...      ''', {}, 'foo', 'foo.py', 0)\n             >>> case = DocTestCase(test)\n\n             >>> try:\n             ...    case.debug()\n             ... except DocTestFailure, failure:\n             ...    pass\n\n           DocTestFailure objects provide access to the test:\n\n             >>> failure.test is test\n             True\n\n           As well as to the example:\n\n             >>> failure.example.want\n             '2\\n'\n\n           and the actual output:\n\n             >>> failure.got\n             '1\\n'\n\n           \"\"\"\n\n        self.setUp()\n        runner = DebugRunner(optionflags=self._dt_optionflags,\n                             checker=self._dt_checker, verbose=False)\n        runner.run(self._dt_test, clear_globs=False)\n        self.tearDown()\n\n    def id(self):\n        return self._dt_test.name\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self._dt_test == other._dt_test and \\\n               self._dt_optionflags == other._dt_optionflags and \\\n               self._dt_setUp == other._dt_setUp and \\\n               self._dt_tearDown == other._dt_tearDown and \\\n               self._dt_checker == other._dt_checker\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self._dt_optionflags, self._dt_setUp, self._dt_tearDown,\n                     self._dt_checker))\n\n    def __repr__(self):\n        name = self._dt_test.name.split('.')\n        return \"%s (%s)\" % (name[-1], '.'.join(name[:-1]))\n\n    __str__ = __repr__\n\n    def shortDescription(self):\n        return \"Doctest: \" + self._dt_test.name\n\nclass SkipDocTestCase(DocTestCase):\n    def __init__(self, module):\n        self.module = module\n        DocTestCase.__init__(self, None)\n\n    def setUp(self):\n        self.skipTest(\"DocTestSuite will not work with -O2 and above\")\n\n    def test_skip(self):\n        pass\n\n    def shortDescription(self):\n        return \"Skipping tests from %s\" % self.module.__name__\n\n    __str__ = shortDescription\n\n\ndef DocTestSuite(module=None, globs=None, extraglobs=None, test_finder=None,\n                 **options):\n    \"\"\"\n    Convert doctest tests for a module to a unittest test suite.\n\n    This converts each documentation string in a module that\n    contains doctest tests to a unittest test case.  If any of the\n    tests in a doc string fail, then the test case fails.  An exception\n    is raised showing the name of the file containing the test and a\n    (sometimes approximate) line number.\n\n    The `module` argument provides the module to be tested.  The argument\n    can be either a module or a module name.\n\n    If no argument is given, the calling module is used.\n\n    A number of options may be provided as keyword arguments:\n\n    setUp\n      A set-up function.  This is called before running the\n      tests in each file. The setUp function will be passed a DocTest\n      object.  The setUp function can access the test globals as the\n      globs attribute of the test passed.\n\n    tearDown\n      A tear-down function.  This is called after running the\n      tests in each file.  The tearDown function will be passed a DocTest\n      object.  The tearDown function can access the test globals as the\n      globs attribute of the test passed.\n\n    globs\n      A dictionary containing initial global variables for the tests.\n\n    optionflags\n       A set of doctest option flags expressed as an integer.\n    \"\"\"\n\n    if test_finder is None:\n        test_finder = DocTestFinder()\n\n    module = _normalize_module(module)\n    tests = test_finder.find(module, globs=globs, extraglobs=extraglobs)\n\n    if not tests and sys.flags.optimize >=2:\n        # Skip doctests when running with -O2\n        suite = unittest.TestSuite()\n        suite.addTest(SkipDocTestCase(module))\n        return suite\n    elif not tests:\n        # Why do we want to do this? Because it reveals a bug that might\n        # otherwise be hidden.\n        # It is probably a bug that this exception is not also raised if the\n        # number of doctest examples in tests is zero (i.e. if no doctest\n        # examples were found).  However, we should probably not be raising\n        # an exception at all here, though it is too late to make this change\n        # for a maintenance release.  See also issue #14649.\n        raise ValueError(module, \"has no docstrings\")\n\n    tests.sort()\n    suite = unittest.TestSuite()\n\n    for test in tests:\n        if len(test.examples) == 0:\n            continue\n        if not test.filename:\n            filename = module.__file__\n            if filename[-4:] in (\".pyc\", \".pyo\"):\n                filename = filename[:-1]\n            test.filename = filename\n        suite.addTest(DocTestCase(test, **options))\n\n    return suite\n\nclass DocFileCase(DocTestCase):\n\n    def id(self):\n        return '_'.join(self._dt_test.name.split('.'))\n\n    def __repr__(self):\n        return self._dt_test.filename\n    __str__ = __repr__\n\n    def format_failure(self, err):\n        return ('Failed doctest test for %s\\n  File \"%s\", line 0\\n\\n%s'\n                % (self._dt_test.name, self._dt_test.filename, err)\n                )\n\ndef DocFileTest(path, module_relative=True, package=None,\n                globs=None, parser=DocTestParser(),\n                encoding=None, **options):\n    if globs is None:\n        globs = {}\n    else:\n        globs = globs.copy()\n\n    if package and not module_relative:\n        raise ValueError(\"Package may only be specified for module-\"\n                         \"relative paths.\")\n\n    # Relativize the path.\n    doc, path = _load_testfile(path, package, module_relative)\n\n    if \"__file__\" not in globs:\n        globs[\"__file__\"] = path\n\n    # Find the file and read it.\n    name = os.path.basename(path)\n\n    # If an encoding is specified, use it to convert the file to unicode\n    if encoding is not None:\n        doc = doc.decode(encoding)\n\n    # Convert it to a test, and wrap it in a DocFileCase.\n    test = parser.get_doctest(doc, globs, name, path, 0)\n    return DocFileCase(test, **options)\n\ndef DocFileSuite(*paths, **kw):\n    \"\"\"A unittest suite for one or more doctest files.\n\n    The path to each doctest file is given as a string; the\n    interpretation of that string depends on the keyword argument\n    \"module_relative\".\n\n    A number of options may be provided as keyword arguments:\n\n    module_relative\n      If \"module_relative\" is True, then the given file paths are\n      interpreted as os-independent module-relative paths.  By\n      default, these paths are relative to the calling module's\n      directory; but if the \"package\" argument is specified, then\n      they are relative to that package.  To ensure os-independence,\n      \"filename\" should use \"/\" characters to separate path\n      segments, and may not be an absolute path (i.e., it may not\n      begin with \"/\").\n\n      If \"module_relative\" is False, then the given file paths are\n      interpreted as os-specific paths.  These paths may be absolute\n      or relative (to the current working directory).\n\n    package\n      A Python package or the name of a Python package whose directory\n      should be used as the base directory for module relative paths.\n      If \"package\" is not specified, then the calling module's\n      directory is used as the base directory for module relative\n      filenames.  It is an error to specify \"package\" if\n      \"module_relative\" is False.\n\n    setUp\n      A set-up function.  This is called before running the\n      tests in each file. The setUp function will be passed a DocTest\n      object.  The setUp function can access the test globals as the\n      globs attribute of the test passed.\n\n    tearDown\n      A tear-down function.  This is called after running the\n      tests in each file.  The tearDown function will be passed a DocTest\n      object.  The tearDown function can access the test globals as the\n      globs attribute of the test passed.\n\n    globs\n      A dictionary containing initial global variables for the tests.\n\n    optionflags\n      A set of doctest option flags expressed as an integer.\n\n    parser\n      A DocTestParser (or subclass) that should be used to extract\n      tests from the files.\n\n    encoding\n      An encoding that will be used to convert the files to unicode.\n    \"\"\"\n    suite = unittest.TestSuite()\n\n    # We do this here so that _normalize_module is called at the right\n    # level.  If it were called in DocFileTest, then this function\n    # would be the caller and we might guess the package incorrectly.\n    if kw.get('module_relative', True):\n        kw['package'] = _normalize_module(kw.get('package'))\n\n    for path in paths:\n        suite.addTest(DocFileTest(path, **kw))\n\n    return suite\n\n######################################################################\n## 9. Debugging Support\n######################################################################\n\ndef script_from_examples(s):\n    r\"\"\"Extract script from text with examples.\n\n       Converts text with examples to a Python script.  Example input is\n       converted to regular code.  Example output and all other words\n       are converted to comments:\n\n       >>> text = '''\n       ...       Here are examples of simple math.\n       ...\n       ...           Python has super accurate integer addition\n       ...\n       ...           >>> 2 + 2\n       ...           5\n       ...\n       ...           And very friendly error messages:\n       ...\n       ...           >>> 1/0\n       ...           To Infinity\n       ...           And\n       ...           Beyond\n       ...\n       ...           You can use logic if you want:\n       ...\n       ...           >>> if 0:\n       ...           ...    blah\n       ...           ...    blah\n       ...           ...\n       ...\n       ...           Ho hum\n       ...           '''\n\n       >>> print script_from_examples(text)\n       # Here are examples of simple math.\n       #\n       #     Python has super accurate integer addition\n       #\n       2 + 2\n       # Expected:\n       ## 5\n       #\n       #     And very friendly error messages:\n       #\n       1/0\n       # Expected:\n       ## To Infinity\n       ## And\n       ## Beyond\n       #\n       #     You can use logic if you want:\n       #\n       if 0:\n          blah\n          blah\n       #\n       #     Ho hum\n       <BLANKLINE>\n       \"\"\"\n    output = []\n    for piece in DocTestParser().parse(s):\n        if isinstance(piece, Example):\n            # Add the example's source code (strip trailing NL)\n            output.append(piece.source[:-1])\n            # Add the expected output:\n            want = piece.want\n            if want:\n                output.append('# Expected:')\n                output += ['## '+l for l in want.split('\\n')[:-1]]\n        else:\n            # Add non-example text.\n            output += [_comment_line(l)\n                       for l in piece.split('\\n')[:-1]]\n\n    # Trim junk on both ends.\n    while output and output[-1] == '#':\n        output.pop()\n    while output and output[0] == '#':\n        output.pop(0)\n    # Combine the output, and return it.\n    # Add a courtesy newline to prevent exec from choking (see bug #1172785)\n    return '\\n'.join(output) + '\\n'\n\ndef testsource(module, name):\n    \"\"\"Extract the test sources from a doctest docstring as a script.\n\n    Provide the module (or dotted name of the module) containing the\n    test to be debugged and the name (within the module) of the object\n    with the doc string with tests to be debugged.\n    \"\"\"\n    module = _normalize_module(module)\n    tests = DocTestFinder().find(module)\n    test = [t for t in tests if t.name == name]\n    if not test:\n        raise ValueError(name, \"not found in tests\")\n    test = test[0]\n    testsrc = script_from_examples(test.docstring)\n    return testsrc\n\ndef debug_src(src, pm=False, globs=None):\n    \"\"\"Debug a single doctest docstring, in argument `src`'\"\"\"\n    testsrc = script_from_examples(src)\n    debug_script(testsrc, pm, globs)\n\ndef debug_script(src, pm=False, globs=None):\n    \"Debug a test script.  `src` is the script, as a string.\"\n    import pdb\n\n    # Note that tempfile.NameTemporaryFile() cannot be used.  As the\n    # docs say, a file so created cannot be opened by name a second time\n    # on modern Windows boxes, and execfile() needs to open it.\n    srcfilename = tempfile.mktemp(\".py\", \"doctestdebug\")\n    f = open(srcfilename, 'w')\n    f.write(src)\n    f.close()\n\n    try:\n        if globs:\n            globs = globs.copy()\n        else:\n            globs = {}\n\n        if pm:\n            try:\n                execfile(srcfilename, globs, globs)\n            except:\n                print sys.exc_info()[1]\n                pdb.post_mortem(sys.exc_info()[2])\n        else:\n            # Note that %r is vital here.  '%s' instead can, e.g., cause\n            # backslashes to get treated as metacharacters on Windows.\n            pdb.run(\"execfile(%r)\" % srcfilename, globs, globs)\n\n    finally:\n        os.remove(srcfilename)\n\ndef debug(module, name, pm=False):\n    \"\"\"Debug a single doctest docstring.\n\n    Provide the module (or dotted name of the module) containing the\n    test to be debugged and the name (within the module) of the object\n    with the docstring with tests to be debugged.\n    \"\"\"\n    module = _normalize_module(module)\n    testsrc = testsource(module, name)\n    debug_script(testsrc, pm, module.__dict__)\n\n######################################################################\n## 10. Example Usage\n######################################################################\nclass _TestClass:\n    \"\"\"\n    A pointless class, for sanity-checking of docstring testing.\n\n    Methods:\n        square()\n        get()\n\n    >>> _TestClass(13).get() + _TestClass(-12).get()\n    1\n    >>> hex(_TestClass(13).square().get())\n    '0xa9'\n    \"\"\"\n\n    def __init__(self, val):\n        \"\"\"val -> _TestClass object with associated value val.\n\n        >>> t = _TestClass(123)\n        >>> print t.get()\n        123\n        \"\"\"\n\n        self.val = val\n\n    def square(self):\n        \"\"\"square() -> square TestClass's associated value\n\n        >>> _TestClass(13).square().get()\n        169\n        \"\"\"\n\n        self.val = self.val ** 2\n        return self\n\n    def get(self):\n        \"\"\"get() -> return TestClass's associated value.\n\n        >>> x = _TestClass(-42)\n        >>> print x.get()\n        -42\n        \"\"\"\n\n        return self.val\n\n__test__ = {\"_TestClass\": _TestClass,\n            \"string\": r\"\"\"\n                      Example of a string object, searched as-is.\n                      >>> x = 1; y = 2\n                      >>> x + y, x * y\n                      (3, 2)\n                      \"\"\",\n\n            \"bool-int equivalence\": r\"\"\"\n                                    In 2.2, boolean expressions displayed\n                                    0 or 1.  By default, we still accept\n                                    them.  This can be disabled by passing\n                                    DONT_ACCEPT_TRUE_FOR_1 to the new\n                                    optionflags argument.\n                                    >>> 4 == 4\n                                    1\n                                    >>> 4 == 4\n                                    True\n                                    >>> 4 > 4\n                                    0\n                                    >>> 4 > 4\n                                    False\n                                    \"\"\",\n\n            \"blank lines\": r\"\"\"\n                Blank lines can be marked with <BLANKLINE>:\n                    >>> print 'foo\\n\\nbar\\n'\n                    foo\n                    <BLANKLINE>\n                    bar\n                    <BLANKLINE>\n            \"\"\",\n\n            \"ellipsis\": r\"\"\"\n                If the ellipsis flag is used, then '...' can be used to\n                elide substrings in the desired output:\n                    >>> print range(1000) #doctest: +ELLIPSIS\n                    [0, 1, 2, ..., 999]\n            \"\"\",\n\n            \"whitespace normalization\": r\"\"\"\n                If the whitespace normalization flag is used, then\n                differences in whitespace are ignored.\n                    >>> print range(30) #doctest: +NORMALIZE_WHITESPACE\n                    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n                     15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n                     27, 28, 29]\n            \"\"\",\n           }\n\n\ndef _test():\n    testfiles = [arg for arg in sys.argv[1:] if arg and arg[0] != '-']\n    if not testfiles:\n        name = os.path.basename(sys.argv[0])\n        if '__loader__' in globals():          # python -m\n            name, _ = os.path.splitext(name)\n        print(\"usage: {0} [-v] file ...\".format(name))\n        return 2\n    for filename in testfiles:\n        if filename.endswith(\".py\"):\n            # It is a module -- insert its dir into sys.path and try to\n            # import it. If it is part of a package, that possibly\n            # won't work because of package imports.\n            dirname, filename = os.path.split(filename)\n            sys.path.insert(0, dirname)\n            m = __import__(filename[:-3])\n            del sys.path[0]\n            failures, _ = testmod(m)\n        else:\n            failures, _ = testfile(filename, module_relative=False)\n        if failures:\n            return 1\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(_test())\n", 
    "dummy_thread": "\"\"\"Drop-in replacement for the thread module.\n\nMeant to be used as a brain-dead substitute so that threaded code does\nnot need to be rewritten for when the thread module is not present.\n\nSuggested usage is::\n\n    try:\n        import thread\n    except ImportError:\n        import dummy_thread as thread\n\n\"\"\"\n# Exports only things specified by thread documentation;\n# skipping obsolete synonyms allocate(), start_new(), exit_thread().\n__all__ = ['error', 'start_new_thread', 'exit', 'get_ident', 'allocate_lock',\n           'interrupt_main', 'LockType']\n\nimport traceback as _traceback\n\nclass error(Exception):\n    \"\"\"Dummy implementation of thread.error.\"\"\"\n\n    def __init__(self, *args):\n        self.args = args\n\ndef start_new_thread(function, args, kwargs={}):\n    \"\"\"Dummy implementation of thread.start_new_thread().\n\n    Compatibility is maintained by making sure that ``args`` is a\n    tuple and ``kwargs`` is a dictionary.  If an exception is raised\n    and it is SystemExit (which can be done by thread.exit()) it is\n    caught and nothing is done; all other exceptions are printed out\n    by using traceback.print_exc().\n\n    If the executed function calls interrupt_main the KeyboardInterrupt will be\n    raised when the function returns.\n\n    \"\"\"\n    if type(args) != type(tuple()):\n        raise TypeError(\"2nd arg must be a tuple\")\n    if type(kwargs) != type(dict()):\n        raise TypeError(\"3rd arg must be a dict\")\n    global _main\n    _main = False\n    try:\n        function(*args, **kwargs)\n    except SystemExit:\n        pass\n    except:\n        _traceback.print_exc()\n    _main = True\n    global _interrupt\n    if _interrupt:\n        _interrupt = False\n        raise KeyboardInterrupt\n\ndef exit():\n    \"\"\"Dummy implementation of thread.exit().\"\"\"\n    raise SystemExit\n\ndef get_ident():\n    \"\"\"Dummy implementation of thread.get_ident().\n\n    Since this module should only be used when threadmodule is not\n    available, it is safe to assume that the current process is the\n    only thread.  Thus a constant can be safely returned.\n    \"\"\"\n    return -1\n\ndef allocate_lock():\n    \"\"\"Dummy implementation of thread.allocate_lock().\"\"\"\n    return LockType()\n\ndef stack_size(size=None):\n    \"\"\"Dummy implementation of thread.stack_size().\"\"\"\n    if size is not None:\n        raise error(\"setting thread stack size not supported\")\n    return 0\n\nclass LockType(object):\n    \"\"\"Class implementing dummy implementation of thread.LockType.\n\n    Compatibility is maintained by maintaining self.locked_status\n    which is a boolean that stores the state of the lock.  Pickling of\n    the lock, though, should not be done since if the thread module is\n    then used with an unpickled ``lock()`` from here problems could\n    occur from this class not having atomic methods.\n\n    \"\"\"\n\n    def __init__(self):\n        self.locked_status = False\n\n    def acquire(self, waitflag=None):\n        \"\"\"Dummy implementation of acquire().\n\n        For blocking calls, self.locked_status is automatically set to\n        True and returned appropriately based on value of\n        ``waitflag``.  If it is non-blocking, then the value is\n        actually checked and not set if it is already acquired.  This\n        is all done so that threading.Condition's assert statements\n        aren't triggered and throw a little fit.\n\n        \"\"\"\n        if waitflag is None or waitflag:\n            self.locked_status = True\n            return True\n        else:\n            if not self.locked_status:\n                self.locked_status = True\n                return True\n            else:\n                return False\n\n    __enter__ = acquire\n\n    def __exit__(self, typ, val, tb):\n        self.release()\n\n    def release(self):\n        \"\"\"Release the dummy lock.\"\"\"\n        # XXX Perhaps shouldn't actually bother to test?  Could lead\n        #     to problems for complex, threaded code.\n        if not self.locked_status:\n            raise error\n        self.locked_status = False\n        return True\n\n    def locked(self):\n        return self.locked_status\n\n# Used to signal that interrupt_main was called in a \"thread\"\n_interrupt = False\n# True when not executing in a \"thread\"\n_main = True\n\ndef interrupt_main():\n    \"\"\"Set _interrupt flag to True to have start_new_thread raise\n    KeyboardInterrupt upon exiting.\"\"\"\n    if _main:\n        raise KeyboardInterrupt\n    else:\n        global _interrupt\n        _interrupt = True\n", 
    "dummy_threading": "\"\"\"Faux ``threading`` version using ``dummy_thread`` instead of ``thread``.\n\nThe module ``_dummy_threading`` is added to ``sys.modules`` in order\nto not have ``threading`` considered imported.  Had ``threading`` been\ndirectly imported it would have made all subsequent imports succeed\nregardless of whether ``thread`` was available which is not desired.\n\n\"\"\"\nfrom sys import modules as sys_modules\n\nimport dummy_thread\n\n# Declaring now so as to not have to nest ``try``s to get proper clean-up.\nholding_thread = False\nholding_threading = False\nholding__threading_local = False\n\ntry:\n    # Could have checked if ``thread`` was not in sys.modules and gone\n    # a different route, but decided to mirror technique used with\n    # ``threading`` below.\n    if 'thread' in sys_modules:\n        held_thread = sys_modules['thread']\n        holding_thread = True\n    # Must have some module named ``thread`` that implements its API\n    # in order to initially import ``threading``.\n    sys_modules['thread'] = sys_modules['dummy_thread']\n\n    if 'threading' in sys_modules:\n        # If ``threading`` is already imported, might as well prevent\n        # trying to import it more than needed by saving it if it is\n        # already imported before deleting it.\n        held_threading = sys_modules['threading']\n        holding_threading = True\n        del sys_modules['threading']\n\n    if '_threading_local' in sys_modules:\n        # If ``_threading_local`` is already imported, might as well prevent\n        # trying to import it more than needed by saving it if it is\n        # already imported before deleting it.\n        held__threading_local = sys_modules['_threading_local']\n        holding__threading_local = True\n        del sys_modules['_threading_local']\n\n    import threading\n    # Need a copy of the code kept somewhere...\n    sys_modules['_dummy_threading'] = sys_modules['threading']\n    del sys_modules['threading']\n    sys_modules['_dummy__threading_local'] = sys_modules['_threading_local']\n    del sys_modules['_threading_local']\n    from _dummy_threading import *\n    from _dummy_threading import __all__\n\nfinally:\n    # Put back ``threading`` if we overwrote earlier\n\n    if holding_threading:\n        sys_modules['threading'] = held_threading\n        del held_threading\n    del holding_threading\n\n    # Put back ``_threading_local`` if we overwrote earlier\n\n    if holding__threading_local:\n        sys_modules['_threading_local'] = held__threading_local\n        del held__threading_local\n    del holding__threading_local\n\n    # Put back ``thread`` if we overwrote, else del the entry we made\n    if holding_thread:\n        sys_modules['thread'] = held_thread\n        del held_thread\n    else:\n        del sys_modules['thread']\n    del holding_thread\n\n    del dummy_thread\n    del sys_modules\n", 
    "email.__init__": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"A package for parsing, handling, and generating email messages.\"\"\"\n\n__version__ = '4.0.3'\n\n__all__ = [\n    # Old names\n    'base64MIME',\n    'Charset',\n    'Encoders',\n    'Errors',\n    'Generator',\n    'Header',\n    'Iterators',\n    'Message',\n    'MIMEAudio',\n    'MIMEBase',\n    'MIMEImage',\n    'MIMEMessage',\n    'MIMEMultipart',\n    'MIMENonMultipart',\n    'MIMEText',\n    'Parser',\n    'quopriMIME',\n    'Utils',\n    'message_from_string',\n    'message_from_file',\n    # new names\n    'base64mime',\n    'charset',\n    'encoders',\n    'errors',\n    'generator',\n    'header',\n    'iterators',\n    'message',\n    'mime',\n    'parser',\n    'quoprimime',\n    'utils',\n    ]\n\n\n\f\n# Some convenience routines.  Don't import Parser and Message as side-effects\n# of importing email since those cascadingly import most of the rest of the\n# email package.\ndef message_from_string(s, *args, **kws):\n    \"\"\"Parse a string into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    \"\"\"\n    from email.parser import Parser\n    return Parser(*args, **kws).parsestr(s)\n\n\ndef message_from_file(fp, *args, **kws):\n    \"\"\"Read a file and parse its contents into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    \"\"\"\n    from email.parser import Parser\n    return Parser(*args, **kws).parse(fp)\n\n\n\f\n# Lazy loading to provide name mapping from new-style names (PEP 8 compatible\n# email 4.0 module names), to old-style names (email 3.0 module names).\nimport sys\n\nclass LazyImporter(object):\n    def __init__(self, module_name):\n        self.__name__ = 'email.' + module_name\n\n    def __getattr__(self, name):\n        __import__(self.__name__)\n        mod = sys.modules[self.__name__]\n        self.__dict__.update(mod.__dict__)\n        return getattr(mod, name)\n\n\n_LOWERNAMES = [\n    # email.<old name> -> email.<new name is lowercased old name>\n    'Charset',\n    'Encoders',\n    'Errors',\n    'FeedParser',\n    'Generator',\n    'Header',\n    'Iterators',\n    'Message',\n    'Parser',\n    'Utils',\n    'base64MIME',\n    'quopriMIME',\n    ]\n\n_MIMENAMES = [\n    # email.MIME<old name> -> email.mime.<new name is lowercased old name>\n    'Audio',\n    'Base',\n    'Image',\n    'Message',\n    'Multipart',\n    'NonMultipart',\n    'Text',\n    ]\n\nfor _name in _LOWERNAMES:\n    importer = LazyImporter(_name.lower())\n    sys.modules['email.' + _name] = importer\n    setattr(sys.modules['email'], _name, importer)\n\n\nimport email.mime\nfor _name in _MIMENAMES:\n    importer = LazyImporter('mime.' + _name.lower())\n    sys.modules['email.MIME' + _name] = importer\n    setattr(sys.modules['email'], 'MIME' + _name, importer)\n    setattr(sys.modules['email.mime'], _name, importer)\n", 
    "email._parseaddr": "# Copyright (C) 2002-2007 Python Software Foundation\n# Contact: email-sig@python.org\n\n\"\"\"Email address parsing code.\n\nLifted directly from rfc822.py.  This should eventually be rewritten.\n\"\"\"\n\n__all__ = [\n    'mktime_tz',\n    'parsedate',\n    'parsedate_tz',\n    'quote',\n    ]\n\nimport time, calendar\n\nSPACE = ' '\nEMPTYSTRING = ''\nCOMMASPACE = ', '\n\n# Parse a date field\n_monthnames = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul',\n               'aug', 'sep', 'oct', 'nov', 'dec',\n               'january', 'february', 'march', 'april', 'may', 'june', 'july',\n               'august', 'september', 'october', 'november', 'december']\n\n_daynames = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n\n# The timezone table does not include the military time zones defined\n# in RFC822, other than Z.  According to RFC1123, the description in\n# RFC822 gets the signs wrong, so we can't rely on any such time\n# zones.  RFC1123 recommends that numeric timezone indicators be used\n# instead of timezone names.\n\n_timezones = {'UT':0, 'UTC':0, 'GMT':0, 'Z':0,\n              'AST': -400, 'ADT': -300,  # Atlantic (used in Canada)\n              'EST': -500, 'EDT': -400,  # Eastern\n              'CST': -600, 'CDT': -500,  # Central\n              'MST': -700, 'MDT': -600,  # Mountain\n              'PST': -800, 'PDT': -700   # Pacific\n              }\n\n\ndef parsedate_tz(data):\n    \"\"\"Convert a date string to a time tuple.\n\n    Accounts for military timezones.\n    \"\"\"\n    data = data.split()\n    # The FWS after the comma after the day-of-week is optional, so search and\n    # adjust for this.\n    if data[0].endswith(',') or data[0].lower() in _daynames:\n        # There's a dayname here. Skip it\n        del data[0]\n    else:\n        i = data[0].rfind(',')\n        if i >= 0:\n            data[0] = data[0][i+1:]\n    if len(data) == 3: # RFC 850 date, deprecated\n        stuff = data[0].split('-')\n        if len(stuff) == 3:\n            data = stuff + data[1:]\n    if len(data) == 4:\n        s = data[3]\n        i = s.find('+')\n        if i > 0:\n            data[3:] = [s[:i], s[i+1:]]\n        else:\n            data.append('') # Dummy tz\n    if len(data) < 5:\n        return None\n    data = data[:5]\n    [dd, mm, yy, tm, tz] = data\n    mm = mm.lower()\n    if mm not in _monthnames:\n        dd, mm = mm, dd.lower()\n        if mm not in _monthnames:\n            return None\n    mm = _monthnames.index(mm) + 1\n    if mm > 12:\n        mm -= 12\n    if dd[-1] == ',':\n        dd = dd[:-1]\n    i = yy.find(':')\n    if i > 0:\n        yy, tm = tm, yy\n    if yy[-1] == ',':\n        yy = yy[:-1]\n    if not yy[0].isdigit():\n        yy, tz = tz, yy\n    if tm[-1] == ',':\n        tm = tm[:-1]\n    tm = tm.split(':')\n    if len(tm) == 2:\n        [thh, tmm] = tm\n        tss = '0'\n    elif len(tm) == 3:\n        [thh, tmm, tss] = tm\n    else:\n        return None\n    try:\n        yy = int(yy)\n        dd = int(dd)\n        thh = int(thh)\n        tmm = int(tmm)\n        tss = int(tss)\n    except ValueError:\n        return None\n    # Check for a yy specified in two-digit format, then convert it to the\n    # appropriate four-digit format, according to the POSIX standard. RFC 822\n    # calls for a two-digit yy, but RFC 2822 (which obsoletes RFC 822)\n    # mandates a 4-digit yy. For more information, see the documentation for\n    # the time module.\n    if yy < 100:\n        # The year is between 1969 and 1999 (inclusive).\n        if yy > 68:\n            yy += 1900\n        # The year is between 2000 and 2068 (inclusive).\n        else:\n            yy += 2000\n    tzoffset = None\n    tz = tz.upper()\n    if tz in _timezones:\n        tzoffset = _timezones[tz]\n    else:\n        try:\n            tzoffset = int(tz)\n        except ValueError:\n            pass\n    # Convert a timezone offset into seconds ; -0500 -> -18000\n    if tzoffset:\n        if tzoffset < 0:\n            tzsign = -1\n            tzoffset = -tzoffset\n        else:\n            tzsign = 1\n        tzoffset = tzsign * ( (tzoffset//100)*3600 + (tzoffset % 100)*60)\n    # Daylight Saving Time flag is set to -1, since DST is unknown.\n    return yy, mm, dd, thh, tmm, tss, 0, 1, -1, tzoffset\n\n\ndef parsedate(data):\n    \"\"\"Convert a time string to a time tuple.\"\"\"\n    t = parsedate_tz(data)\n    if isinstance(t, tuple):\n        return t[:9]\n    else:\n        return t\n\n\ndef mktime_tz(data):\n    \"\"\"Turn a 10-tuple as returned by parsedate_tz() into a POSIX timestamp.\"\"\"\n    if data[9] is None:\n        # No zone info, so localtime is better assumption than GMT\n        return time.mktime(data[:8] + (-1,))\n    else:\n        t = calendar.timegm(data)\n        return t - data[9]\n\n\ndef quote(str):\n    \"\"\"Prepare string to be used in a quoted string.\n\n    Turns backslash and double quote characters into quoted pairs.  These\n    are the only characters that need to be quoted inside a quoted string.\n    Does not add the surrounding double quotes.\n    \"\"\"\n    return str.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n\n\nclass AddrlistClass:\n    \"\"\"Address parser class by Ben Escoto.\n\n    To understand what this class does, it helps to have a copy of RFC 2822 in\n    front of you.\n\n    Note: this class interface is deprecated and may be removed in the future.\n    Use rfc822.AddressList instead.\n    \"\"\"\n\n    def __init__(self, field):\n        \"\"\"Initialize a new instance.\n\n        `field' is an unparsed address header field, containing\n        one or more addresses.\n        \"\"\"\n        self.specials = '()<>@,:;.\\\"[]'\n        self.pos = 0\n        self.LWS = ' \\t'\n        self.CR = '\\r\\n'\n        self.FWS = self.LWS + self.CR\n        self.atomends = self.specials + self.LWS + self.CR\n        # Note that RFC 2822 now specifies `.' as obs-phrase, meaning that it\n        # is obsolete syntax.  RFC 2822 requires that we recognize obsolete\n        # syntax, so allow dots in phrases.\n        self.phraseends = self.atomends.replace('.', '')\n        self.field = field\n        self.commentlist = []\n\n    def gotonext(self):\n        \"\"\"Parse up to the start of the next address.\"\"\"\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS + '\\n\\r':\n                self.pos += 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            else:\n                break\n\n    def getaddrlist(self):\n        \"\"\"Parse all addresses.\n\n        Returns a list containing all of the addresses.\n        \"\"\"\n        result = []\n        while self.pos < len(self.field):\n            ad = self.getaddress()\n            if ad:\n                result += ad\n            else:\n                result.append(('', ''))\n        return result\n\n    def getaddress(self):\n        \"\"\"Parse the next address.\"\"\"\n        self.commentlist = []\n        self.gotonext()\n\n        oldpos = self.pos\n        oldcl = self.commentlist\n        plist = self.getphraselist()\n\n        self.gotonext()\n        returnlist = []\n\n        if self.pos >= len(self.field):\n            # Bad email address technically, no domain.\n            if plist:\n                returnlist = [(SPACE.join(self.commentlist), plist[0])]\n\n        elif self.field[self.pos] in '.@':\n            # email address is just an addrspec\n            # this isn't very efficient since we start over\n            self.pos = oldpos\n            self.commentlist = oldcl\n            addrspec = self.getaddrspec()\n            returnlist = [(SPACE.join(self.commentlist), addrspec)]\n\n        elif self.field[self.pos] == ':':\n            # address is a group\n            returnlist = []\n\n            fieldlen = len(self.field)\n            self.pos += 1\n            while self.pos < len(self.field):\n                self.gotonext()\n                if self.pos < fieldlen and self.field[self.pos] == ';':\n                    self.pos += 1\n                    break\n                returnlist = returnlist + self.getaddress()\n\n        elif self.field[self.pos] == '<':\n            # Address is a phrase then a route addr\n            routeaddr = self.getrouteaddr()\n\n            if self.commentlist:\n                returnlist = [(SPACE.join(plist) + ' (' +\n                               ' '.join(self.commentlist) + ')', routeaddr)]\n            else:\n                returnlist = [(SPACE.join(plist), routeaddr)]\n\n        else:\n            if plist:\n                returnlist = [(SPACE.join(self.commentlist), plist[0])]\n            elif self.field[self.pos] in self.specials:\n                self.pos += 1\n\n        self.gotonext()\n        if self.pos < len(self.field) and self.field[self.pos] == ',':\n            self.pos += 1\n        return returnlist\n\n    def getrouteaddr(self):\n        \"\"\"Parse a route address (Return-path value).\n\n        This method just skips all the route stuff and returns the addrspec.\n        \"\"\"\n        if self.field[self.pos] != '<':\n            return\n\n        expectroute = False\n        self.pos += 1\n        self.gotonext()\n        adlist = ''\n        while self.pos < len(self.field):\n            if expectroute:\n                self.getdomain()\n                expectroute = False\n            elif self.field[self.pos] == '>':\n                self.pos += 1\n                break\n            elif self.field[self.pos] == '@':\n                self.pos += 1\n                expectroute = True\n            elif self.field[self.pos] == ':':\n                self.pos += 1\n            else:\n                adlist = self.getaddrspec()\n                self.pos += 1\n                break\n            self.gotonext()\n\n        return adlist\n\n    def getaddrspec(self):\n        \"\"\"Parse an RFC 2822 addr-spec.\"\"\"\n        aslist = []\n\n        self.gotonext()\n        while self.pos < len(self.field):\n            if self.field[self.pos] == '.':\n                aslist.append('.')\n                self.pos += 1\n            elif self.field[self.pos] == '\"':\n                aslist.append('\"%s\"' % quote(self.getquote()))\n            elif self.field[self.pos] in self.atomends:\n                break\n            else:\n                aslist.append(self.getatom())\n            self.gotonext()\n\n        if self.pos >= len(self.field) or self.field[self.pos] != '@':\n            return EMPTYSTRING.join(aslist)\n\n        aslist.append('@')\n        self.pos += 1\n        self.gotonext()\n        return EMPTYSTRING.join(aslist) + self.getdomain()\n\n    def getdomain(self):\n        \"\"\"Get the complete domain name from an address.\"\"\"\n        sdlist = []\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS:\n                self.pos += 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            elif self.field[self.pos] == '[':\n                sdlist.append(self.getdomainliteral())\n            elif self.field[self.pos] == '.':\n                self.pos += 1\n                sdlist.append('.')\n            elif self.field[self.pos] in self.atomends:\n                break\n            else:\n                sdlist.append(self.getatom())\n        return EMPTYSTRING.join(sdlist)\n\n    def getdelimited(self, beginchar, endchars, allowcomments=True):\n        \"\"\"Parse a header fragment delimited by special characters.\n\n        `beginchar' is the start character for the fragment.\n        If self is not looking at an instance of `beginchar' then\n        getdelimited returns the empty string.\n\n        `endchars' is a sequence of allowable end-delimiting characters.\n        Parsing stops when one of these is encountered.\n\n        If `allowcomments' is non-zero, embedded RFC 2822 comments are allowed\n        within the parsed fragment.\n        \"\"\"\n        if self.field[self.pos] != beginchar:\n            return ''\n\n        slist = ['']\n        quote = False\n        self.pos += 1\n        while self.pos < len(self.field):\n            if quote:\n                slist.append(self.field[self.pos])\n                quote = False\n            elif self.field[self.pos] in endchars:\n                self.pos += 1\n                break\n            elif allowcomments and self.field[self.pos] == '(':\n                slist.append(self.getcomment())\n                continue        # have already advanced pos from getcomment\n            elif self.field[self.pos] == '\\\\':\n                quote = True\n            else:\n                slist.append(self.field[self.pos])\n            self.pos += 1\n\n        return EMPTYSTRING.join(slist)\n\n    def getquote(self):\n        \"\"\"Get a quote-delimited fragment from self's field.\"\"\"\n        return self.getdelimited('\"', '\"\\r', False)\n\n    def getcomment(self):\n        \"\"\"Get a parenthesis-delimited fragment from self's field.\"\"\"\n        return self.getdelimited('(', ')\\r', True)\n\n    def getdomainliteral(self):\n        \"\"\"Parse an RFC 2822 domain-literal.\"\"\"\n        return '[%s]' % self.getdelimited('[', ']\\r', False)\n\n    def getatom(self, atomends=None):\n        \"\"\"Parse an RFC 2822 atom.\n\n        Optional atomends specifies a different set of end token delimiters\n        (the default is to use self.atomends).  This is used e.g. in\n        getphraselist() since phrase endings must not include the `.' (which\n        is legal in phrases).\"\"\"\n        atomlist = ['']\n        if atomends is None:\n            atomends = self.atomends\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in atomends:\n                break\n            else:\n                atomlist.append(self.field[self.pos])\n            self.pos += 1\n\n        return EMPTYSTRING.join(atomlist)\n\n    def getphraselist(self):\n        \"\"\"Parse a sequence of RFC 2822 phrases.\n\n        A phrase is a sequence of words, which are in turn either RFC 2822\n        atoms or quoted-strings.  Phrases are canonicalized by squeezing all\n        runs of continuous whitespace into one space.\n        \"\"\"\n        plist = []\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.FWS:\n                self.pos += 1\n            elif self.field[self.pos] == '\"':\n                plist.append(self.getquote())\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            elif self.field[self.pos] in self.phraseends:\n                break\n            else:\n                plist.append(self.getatom(self.phraseends))\n\n        return plist\n\nclass AddressList(AddrlistClass):\n    \"\"\"An AddressList encapsulates a list of parsed RFC 2822 addresses.\"\"\"\n    def __init__(self, field):\n        AddrlistClass.__init__(self, field)\n        if field:\n            self.addresslist = self.getaddrlist()\n        else:\n            self.addresslist = []\n\n    def __len__(self):\n        return len(self.addresslist)\n\n    def __add__(self, other):\n        # Set union\n        newaddr = AddressList(None)\n        newaddr.addresslist = self.addresslist[:]\n        for x in other.addresslist:\n            if not x in self.addresslist:\n                newaddr.addresslist.append(x)\n        return newaddr\n\n    def __iadd__(self, other):\n        # Set union, in-place\n        for x in other.addresslist:\n            if not x in self.addresslist:\n                self.addresslist.append(x)\n        return self\n\n    def __sub__(self, other):\n        # Set difference\n        newaddr = AddressList(None)\n        for x in self.addresslist:\n            if not x in other.addresslist:\n                newaddr.addresslist.append(x)\n        return newaddr\n\n    def __isub__(self, other):\n        # Set difference, in-place\n        for x in other.addresslist:\n            if x in self.addresslist:\n                self.addresslist.remove(x)\n        return self\n\n    def __getitem__(self, index):\n        # Make indexing, slices, and 'in' work\n        return self.addresslist[index]\n", 
    "email.base64mime": "# Copyright (C) 2002-2006 Python Software Foundation\n# Author: Ben Gertzfield\n# Contact: email-sig@python.org\n\n\"\"\"Base64 content transfer encoding per RFCs 2045-2047.\n\nThis module handles the content transfer encoding method defined in RFC 2045\nto encode arbitrary 8-bit data using the three 8-bit bytes in four 7-bit\ncharacters encoding known as Base64.\n\nIt is used in the MIME standards for email to attach images, audio, and text\nusing some 8-bit character sets to messages.\n\nThis module provides an interface to encode and decode both headers and bodies\nwith Base64 encoding.\n\nRFC 2045 defines a method for including character set information in an\n`encoded-word' in a header.  This method is commonly used for 8-bit real names\nin To:, From:, Cc:, etc. fields, as well as Subject: lines.\n\nThis module does not do the line wrapping or end-of-line character conversion\nnecessary for proper internationalized headers; it only does dumb encoding and\ndecoding.  To deal with the various line wrapping issues, use the email.header\nmodule.\n\"\"\"\n\n__all__ = [\n    'base64_len',\n    'body_decode',\n    'body_encode',\n    'decode',\n    'decodestring',\n    'encode',\n    'encodestring',\n    'header_encode',\n    ]\n\n\nfrom binascii import b2a_base64, a2b_base64\nfrom email.utils import fix_eols\n\nCRLF = '\\r\\n'\nNL = '\\n'\nEMPTYSTRING = ''\n\n# See also Charset.py\nMISC_LEN = 7\n\n\n\f\n# Helpers\ndef base64_len(s):\n    \"\"\"Return the length of s when it is encoded with base64.\"\"\"\n    groups_of_3, leftover = divmod(len(s), 3)\n    # 4 bytes out for each 3 bytes (or nonzero fraction thereof) in.\n    # Thanks, Tim!\n    n = groups_of_3 * 4\n    if leftover:\n        n += 4\n    return n\n\n\n\f\ndef header_encode(header, charset='iso-8859-1', keep_eols=False,\n                  maxlinelen=76, eol=NL):\n    \"\"\"Encode a single header line with Base64 encoding in a given charset.\n\n    Defined in RFC 2045, this Base64 encoding is identical to normal Base64\n    encoding, except that each line must be intelligently wrapped (respecting\n    the Base64 encoding), and subsequent lines must start with a space.\n\n    charset names the character set to use to encode the header.  It defaults\n    to iso-8859-1.\n\n    End-of-line characters (\\\\r, \\\\n, \\\\r\\\\n) will be automatically converted\n    to the canonical email line separator \\\\r\\\\n unless the keep_eols\n    parameter is True (the default is False).\n\n    Each line of the header will be terminated in the value of eol, which\n    defaults to \"\\\\n\".  Set this to \"\\\\r\\\\n\" if you are using the result of\n    this function directly in email.\n\n    The resulting string will be in the form:\n\n    \"=?charset?b?WW/5ciBtYXp66XLrIHf8eiBhIGhhbXBzdGHuciBBIFlv+XIgbWF6euly?=\\\\n\n      =?charset?b?6yB3/HogYSBoYW1wc3Rh7nIgQkMgWW/5ciBtYXp66XLrIHf8eiBhIGhh?=\"\n\n    with each line wrapped at, at most, maxlinelen characters (defaults to 76\n    characters).\n    \"\"\"\n    # Return empty headers unchanged\n    if not header:\n        return header\n\n    if not keep_eols:\n        header = fix_eols(header)\n\n    # Base64 encode each line, in encoded chunks no greater than maxlinelen in\n    # length, after the RFC chrome is added in.\n    base64ed = []\n    max_encoded = maxlinelen - len(charset) - MISC_LEN\n    max_unencoded = max_encoded * 3 // 4\n\n    for i in range(0, len(header), max_unencoded):\n        base64ed.append(b2a_base64(header[i:i+max_unencoded]))\n\n    # Now add the RFC chrome to each encoded chunk\n    lines = []\n    for line in base64ed:\n        # Ignore the last character of each line if it is a newline\n        if line.endswith(NL):\n            line = line[:-1]\n        # Add the chrome\n        lines.append('=?%s?b?%s?=' % (charset, line))\n    # Glue the lines together and return it.  BAW: should we be able to\n    # specify the leading whitespace in the joiner?\n    joiner = eol + ' '\n    return joiner.join(lines)\n\n\n\f\ndef encode(s, binary=True, maxlinelen=76, eol=NL):\n    \"\"\"Encode a string with base64.\n\n    Each line will be wrapped at, at most, maxlinelen characters (defaults to\n    76 characters).\n\n    If binary is False, end-of-line characters will be converted to the\n    canonical email end-of-line sequence \\\\r\\\\n.  Otherwise they will be left\n    verbatim (this is the default).\n\n    Each line of encoded text will end with eol, which defaults to \"\\\\n\".  Set\n    this to \"\\\\r\\\\n\" if you will be using the result of this function directly\n    in an email.\n    \"\"\"\n    if not s:\n        return s\n\n    if not binary:\n        s = fix_eols(s)\n\n    encvec = []\n    max_unencoded = maxlinelen * 3 // 4\n    for i in range(0, len(s), max_unencoded):\n        # BAW: should encode() inherit b2a_base64()'s dubious behavior in\n        # adding a newline to the encoded string?\n        enc = b2a_base64(s[i:i + max_unencoded])\n        if enc.endswith(NL) and eol != NL:\n            enc = enc[:-1] + eol\n        encvec.append(enc)\n    return EMPTYSTRING.join(encvec)\n\n\n# For convenience and backwards compatibility w/ standard base64 module\nbody_encode = encode\nencodestring = encode\n\n\n\f\ndef decode(s, convert_eols=None):\n    \"\"\"Decode a raw base64 string.\n\n    If convert_eols is set to a string value, all canonical email linefeeds,\n    e.g. \"\\\\r\\\\n\", in the decoded text will be converted to the value of\n    convert_eols.  os.linesep is a good choice for convert_eols if you are\n    decoding a text attachment.\n\n    This function does not parse a full MIME header value encoded with\n    base64 (like =?iso-8895-1?b?bmloISBuaWgh?=) -- please use the high\n    level email.header class for that functionality.\n    \"\"\"\n    if not s:\n        return s\n\n    dec = a2b_base64(s)\n    if convert_eols:\n        return dec.replace(CRLF, convert_eols)\n    return dec\n\n\n# For convenience and backwards compatibility w/ standard base64 module\nbody_decode = decode\ndecodestring = decode\n", 
    "email.charset": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Ben Gertzfield, Barry Warsaw\n# Contact: email-sig@python.org\n\n__all__ = [\n    'Charset',\n    'add_alias',\n    'add_charset',\n    'add_codec',\n    ]\n\nimport codecs\nimport email.base64mime\nimport email.quoprimime\n\nfrom email import errors\nfrom email.encoders import encode_7or8bit\n\n\n\f\n# Flags for types of header encodings\nQP          = 1 # Quoted-Printable\nBASE64      = 2 # Base64\nSHORTEST    = 3 # the shorter of QP and base64, but only for headers\n\n# In \"=?charset?q?hello_world?=\", the =?, ?q?, and ?= add up to 7\nMISC_LEN = 7\n\nDEFAULT_CHARSET = 'us-ascii'\n\n\n\f\n# Defaults\nCHARSETS = {\n    # input        header enc  body enc output conv\n    'iso-8859-1':  (QP,        QP,      None),\n    'iso-8859-2':  (QP,        QP,      None),\n    'iso-8859-3':  (QP,        QP,      None),\n    'iso-8859-4':  (QP,        QP,      None),\n    # iso-8859-5 is Cyrillic, and not especially used\n    # iso-8859-6 is Arabic, also not particularly used\n    # iso-8859-7 is Greek, QP will not make it readable\n    # iso-8859-8 is Hebrew, QP will not make it readable\n    'iso-8859-9':  (QP,        QP,      None),\n    'iso-8859-10': (QP,        QP,      None),\n    # iso-8859-11 is Thai, QP will not make it readable\n    'iso-8859-13': (QP,        QP,      None),\n    'iso-8859-14': (QP,        QP,      None),\n    'iso-8859-15': (QP,        QP,      None),\n    'iso-8859-16': (QP,        QP,      None),\n    'windows-1252':(QP,        QP,      None),\n    'viscii':      (QP,        QP,      None),\n    'us-ascii':    (None,      None,    None),\n    'big5':        (BASE64,    BASE64,  None),\n    'gb2312':      (BASE64,    BASE64,  None),\n    'euc-jp':      (BASE64,    None,    'iso-2022-jp'),\n    'shift_jis':   (BASE64,    None,    'iso-2022-jp'),\n    'iso-2022-jp': (BASE64,    None,    None),\n    'koi8-r':      (BASE64,    BASE64,  None),\n    'utf-8':       (SHORTEST,  BASE64, 'utf-8'),\n    # We're making this one up to represent raw unencoded 8-bit\n    '8bit':        (None,      BASE64, 'utf-8'),\n    }\n\n# Aliases for other commonly-used names for character sets.  Map\n# them to the real ones used in email.\nALIASES = {\n    'latin_1': 'iso-8859-1',\n    'latin-1': 'iso-8859-1',\n    'latin_2': 'iso-8859-2',\n    'latin-2': 'iso-8859-2',\n    'latin_3': 'iso-8859-3',\n    'latin-3': 'iso-8859-3',\n    'latin_4': 'iso-8859-4',\n    'latin-4': 'iso-8859-4',\n    'latin_5': 'iso-8859-9',\n    'latin-5': 'iso-8859-9',\n    'latin_6': 'iso-8859-10',\n    'latin-6': 'iso-8859-10',\n    'latin_7': 'iso-8859-13',\n    'latin-7': 'iso-8859-13',\n    'latin_8': 'iso-8859-14',\n    'latin-8': 'iso-8859-14',\n    'latin_9': 'iso-8859-15',\n    'latin-9': 'iso-8859-15',\n    'latin_10':'iso-8859-16',\n    'latin-10':'iso-8859-16',\n    'cp949':   'ks_c_5601-1987',\n    'euc_jp':  'euc-jp',\n    'euc_kr':  'euc-kr',\n    'ascii':   'us-ascii',\n    }\n\n\n# Map charsets to their Unicode codec strings.\nCODEC_MAP = {\n    'gb2312':      'eucgb2312_cn',\n    'big5':        'big5_tw',\n    # Hack: We don't want *any* conversion for stuff marked us-ascii, as all\n    # sorts of garbage might be sent to us in the guise of 7-bit us-ascii.\n    # Let that stuff pass through without conversion to/from Unicode.\n    'us-ascii':    None,\n    }\n\n\n\f\n# Convenience functions for extending the above mappings\ndef add_charset(charset, header_enc=None, body_enc=None, output_charset=None):\n    \"\"\"Add character set properties to the global registry.\n\n    charset is the input character set, and must be the canonical name of a\n    character set.\n\n    Optional header_enc and body_enc is either Charset.QP for\n    quoted-printable, Charset.BASE64 for base64 encoding, Charset.SHORTEST for\n    the shortest of qp or base64 encoding, or None for no encoding.  SHORTEST\n    is only valid for header_enc.  It describes how message headers and\n    message bodies in the input charset are to be encoded.  Default is no\n    encoding.\n\n    Optional output_charset is the character set that the output should be\n    in.  Conversions will proceed from input charset, to Unicode, to the\n    output charset when the method Charset.convert() is called.  The default\n    is to output in the same character set as the input.\n\n    Both input_charset and output_charset must have Unicode codec entries in\n    the module's charset-to-codec mapping; use add_codec(charset, codecname)\n    to add codecs the module does not know about.  See the codecs module's\n    documentation for more information.\n    \"\"\"\n    if body_enc == SHORTEST:\n        raise ValueError('SHORTEST not allowed for body_enc')\n    CHARSETS[charset] = (header_enc, body_enc, output_charset)\n\n\ndef add_alias(alias, canonical):\n    \"\"\"Add a character set alias.\n\n    alias is the alias name, e.g. latin-1\n    canonical is the character set's canonical name, e.g. iso-8859-1\n    \"\"\"\n    ALIASES[alias] = canonical\n\n\ndef add_codec(charset, codecname):\n    \"\"\"Add a codec that map characters in the given charset to/from Unicode.\n\n    charset is the canonical name of a character set.  codecname is the name\n    of a Python codec, as appropriate for the second argument to the unicode()\n    built-in, or to the encode() method of a Unicode string.\n    \"\"\"\n    CODEC_MAP[charset] = codecname\n\n\n\f\nclass Charset:\n    \"\"\"Map character sets to their email properties.\n\n    This class provides information about the requirements imposed on email\n    for a specific character set.  It also provides convenience routines for\n    converting between character sets, given the availability of the\n    applicable codecs.  Given a character set, it will do its best to provide\n    information on how to use that character set in an email in an\n    RFC-compliant way.\n\n    Certain character sets must be encoded with quoted-printable or base64\n    when used in email headers or bodies.  Certain character sets must be\n    converted outright, and are not allowed in email.  Instances of this\n    module expose the following information about a character set:\n\n    input_charset: The initial character set specified.  Common aliases\n                   are converted to their `official' email names (e.g. latin_1\n                   is converted to iso-8859-1).  Defaults to 7-bit us-ascii.\n\n    header_encoding: If the character set must be encoded before it can be\n                     used in an email header, this attribute will be set to\n                     Charset.QP (for quoted-printable), Charset.BASE64 (for\n                     base64 encoding), or Charset.SHORTEST for the shortest of\n                     QP or BASE64 encoding.  Otherwise, it will be None.\n\n    body_encoding: Same as header_encoding, but describes the encoding for the\n                   mail message's body, which indeed may be different than the\n                   header encoding.  Charset.SHORTEST is not allowed for\n                   body_encoding.\n\n    output_charset: Some character sets must be converted before they can be\n                    used in email headers or bodies.  If the input_charset is\n                    one of them, this attribute will contain the name of the\n                    charset output will be converted to.  Otherwise, it will\n                    be None.\n\n    input_codec: The name of the Python codec used to convert the\n                 input_charset to Unicode.  If no conversion codec is\n                 necessary, this attribute will be None.\n\n    output_codec: The name of the Python codec used to convert Unicode\n                  to the output_charset.  If no conversion codec is necessary,\n                  this attribute will have the same value as the input_codec.\n    \"\"\"\n    def __init__(self, input_charset=DEFAULT_CHARSET):\n        # RFC 2046, $4.1.2 says charsets are not case sensitive.  We coerce to\n        # unicode because its .lower() is locale insensitive.  If the argument\n        # is already a unicode, we leave it at that, but ensure that the\n        # charset is ASCII, as the standard (RFC XXX) requires.\n        try:\n            if isinstance(input_charset, unicode):\n                input_charset.encode('ascii')\n            else:\n                input_charset = unicode(input_charset, 'ascii')\n        except UnicodeError:\n            raise errors.CharsetError(input_charset)\n        input_charset = input_charset.lower().encode('ascii')\n        # Set the input charset after filtering through the aliases and/or codecs\n        if not (input_charset in ALIASES or input_charset in CHARSETS):\n            try:\n                input_charset = codecs.lookup(input_charset).name\n            except LookupError:\n                pass\n        self.input_charset = ALIASES.get(input_charset, input_charset)\n        # We can try to guess which encoding and conversion to use by the\n        # charset_map dictionary.  Try that first, but let the user override\n        # it.\n        henc, benc, conv = CHARSETS.get(self.input_charset,\n                                        (SHORTEST, BASE64, None))\n        if not conv:\n            conv = self.input_charset\n        # Set the attributes, allowing the arguments to override the default.\n        self.header_encoding = henc\n        self.body_encoding = benc\n        self.output_charset = ALIASES.get(conv, conv)\n        # Now set the codecs.  If one isn't defined for input_charset,\n        # guess and try a Unicode codec with the same name as input_codec.\n        self.input_codec = CODEC_MAP.get(self.input_charset,\n                                         self.input_charset)\n        self.output_codec = CODEC_MAP.get(self.output_charset,\n                                          self.output_charset)\n\n    def __str__(self):\n        return self.input_charset.lower()\n\n    __repr__ = __str__\n\n    def __eq__(self, other):\n        return str(self) == str(other).lower()\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def get_body_encoding(self):\n        \"\"\"Return the content-transfer-encoding used for body encoding.\n\n        This is either the string `quoted-printable' or `base64' depending on\n        the encoding used, or it is a function in which case you should call\n        the function with a single argument, the Message object being\n        encoded.  The function should then set the Content-Transfer-Encoding\n        header itself to whatever is appropriate.\n\n        Returns \"quoted-printable\" if self.body_encoding is QP.\n        Returns \"base64\" if self.body_encoding is BASE64.\n        Returns \"7bit\" otherwise.\n        \"\"\"\n        assert self.body_encoding != SHORTEST\n        if self.body_encoding == QP:\n            return 'quoted-printable'\n        elif self.body_encoding == BASE64:\n            return 'base64'\n        else:\n            return encode_7or8bit\n\n    def convert(self, s):\n        \"\"\"Convert a string from the input_codec to the output_codec.\"\"\"\n        if self.input_codec != self.output_codec:\n            return unicode(s, self.input_codec).encode(self.output_codec)\n        else:\n            return s\n\n    def to_splittable(self, s):\n        \"\"\"Convert a possibly multibyte string to a safely splittable format.\n\n        Uses the input_codec to try and convert the string to Unicode, so it\n        can be safely split on character boundaries (even for multibyte\n        characters).\n\n        Returns the string as-is if it isn't known how to convert it to\n        Unicode with the input_charset.\n\n        Characters that could not be converted to Unicode will be replaced\n        with the Unicode replacement character U+FFFD.\n        \"\"\"\n        if isinstance(s, unicode) or self.input_codec is None:\n            return s\n        try:\n            return unicode(s, self.input_codec, 'replace')\n        except LookupError:\n            # Input codec not installed on system, so return the original\n            # string unchanged.\n            return s\n\n    def from_splittable(self, ustr, to_output=True):\n        \"\"\"Convert a splittable string back into an encoded string.\n\n        Uses the proper codec to try and convert the string from Unicode back\n        into an encoded format.  Return the string as-is if it is not Unicode,\n        or if it could not be converted from Unicode.\n\n        Characters that could not be converted from Unicode will be replaced\n        with an appropriate character (usually '?').\n\n        If to_output is True (the default), uses output_codec to convert to an\n        encoded format.  If to_output is False, uses input_codec.\n        \"\"\"\n        if to_output:\n            codec = self.output_codec\n        else:\n            codec = self.input_codec\n        if not isinstance(ustr, unicode) or codec is None:\n            return ustr\n        try:\n            return ustr.encode(codec, 'replace')\n        except LookupError:\n            # Output codec not installed\n            return ustr\n\n    def get_output_charset(self):\n        \"\"\"Return the output character set.\n\n        This is self.output_charset if that is not None, otherwise it is\n        self.input_charset.\n        \"\"\"\n        return self.output_charset or self.input_charset\n\n    def encoded_header_len(self, s):\n        \"\"\"Return the length of the encoded header string.\"\"\"\n        cset = self.get_output_charset()\n        # The len(s) of a 7bit encoding is len(s)\n        if self.header_encoding == BASE64:\n            return email.base64mime.base64_len(s) + len(cset) + MISC_LEN\n        elif self.header_encoding == QP:\n            return email.quoprimime.header_quopri_len(s) + len(cset) + MISC_LEN\n        elif self.header_encoding == SHORTEST:\n            lenb64 = email.base64mime.base64_len(s)\n            lenqp = email.quoprimime.header_quopri_len(s)\n            return min(lenb64, lenqp) + len(cset) + MISC_LEN\n        else:\n            return len(s)\n\n    def header_encode(self, s, convert=False):\n        \"\"\"Header-encode a string, optionally converting it to output_charset.\n\n        If convert is True, the string will be converted from the input\n        charset to the output charset automatically.  This is not useful for\n        multibyte character sets, which have line length issues (multibyte\n        characters must be split on a character, not a byte boundary); use the\n        high-level Header class to deal with these issues.  convert defaults\n        to False.\n\n        The type of encoding (base64 or quoted-printable) will be based on\n        self.header_encoding.\n        \"\"\"\n        cset = self.get_output_charset()\n        if convert:\n            s = self.convert(s)\n        # 7bit/8bit encodings return the string unchanged (modulo conversions)\n        if self.header_encoding == BASE64:\n            return email.base64mime.header_encode(s, cset)\n        elif self.header_encoding == QP:\n            return email.quoprimime.header_encode(s, cset, maxlinelen=None)\n        elif self.header_encoding == SHORTEST:\n            lenb64 = email.base64mime.base64_len(s)\n            lenqp = email.quoprimime.header_quopri_len(s)\n            if lenb64 < lenqp:\n                return email.base64mime.header_encode(s, cset)\n            else:\n                return email.quoprimime.header_encode(s, cset, maxlinelen=None)\n        else:\n            return s\n\n    def body_encode(self, s, convert=True):\n        \"\"\"Body-encode a string and convert it to output_charset.\n\n        If convert is True (the default), the string will be converted from\n        the input charset to output charset automatically.  Unlike\n        header_encode(), there are no issues with byte boundaries and\n        multibyte charsets in email bodies, so this is usually pretty safe.\n\n        The type of encoding (base64 or quoted-printable) will be based on\n        self.body_encoding.\n        \"\"\"\n        if convert:\n            s = self.convert(s)\n        # 7bit/8bit encodings return the string unchanged (module conversions)\n        if self.body_encoding is BASE64:\n            return email.base64mime.body_encode(s)\n        elif self.body_encoding is QP:\n            return email.quoprimime.body_encode(s)\n        else:\n            return s\n", 
    "email.encoders": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Encodings and related functions.\"\"\"\n\n__all__ = [\n    'encode_7or8bit',\n    'encode_base64',\n    'encode_noop',\n    'encode_quopri',\n    ]\n\nimport base64\n\nfrom quopri import encodestring as _encodestring\n\n\n\f\ndef _qencode(s):\n    enc = _encodestring(s, quotetabs=True)\n    # Must encode spaces, which quopri.encodestring() doesn't do\n    return enc.replace(' ', '=20')\n\n\ndef _bencode(s):\n    # We can't quite use base64.encodestring() since it tacks on a \"courtesy\n    # newline\".  Blech!\n    if not s:\n        return s\n    hasnewline = (s[-1] == '\\n')\n    value = base64.encodestring(s)\n    if not hasnewline and value[-1] == '\\n':\n        return value[:-1]\n    return value\n\n\n\f\ndef encode_base64(msg):\n    \"\"\"Encode the message's payload in Base64.\n\n    Also, add an appropriate Content-Transfer-Encoding header.\n    \"\"\"\n    orig = msg.get_payload()\n    encdata = _bencode(orig)\n    msg.set_payload(encdata)\n    msg['Content-Transfer-Encoding'] = 'base64'\n\n\n\f\ndef encode_quopri(msg):\n    \"\"\"Encode the message's payload in quoted-printable.\n\n    Also, add an appropriate Content-Transfer-Encoding header.\n    \"\"\"\n    orig = msg.get_payload()\n    encdata = _qencode(orig)\n    msg.set_payload(encdata)\n    msg['Content-Transfer-Encoding'] = 'quoted-printable'\n\n\n\f\ndef encode_7or8bit(msg):\n    \"\"\"Set the Content-Transfer-Encoding header to 7bit or 8bit.\"\"\"\n    orig = msg.get_payload()\n    if orig is None:\n        # There's no payload.  For backwards compatibility we use 7bit\n        msg['Content-Transfer-Encoding'] = '7bit'\n        return\n    # We play a trick to make this go fast.  If encoding to ASCII succeeds, we\n    # know the data must be 7bit, otherwise treat it as 8bit.\n    try:\n        orig.encode('ascii')\n    except UnicodeError:\n        msg['Content-Transfer-Encoding'] = '8bit'\n    else:\n        msg['Content-Transfer-Encoding'] = '7bit'\n\n\n\f\ndef encode_noop(msg):\n    \"\"\"Do nothing.\"\"\"\n", 
    "email.errors": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"email package exception classes.\"\"\"\n\n\n\f\nclass MessageError(Exception):\n    \"\"\"Base class for errors in the email package.\"\"\"\n\n\nclass MessageParseError(MessageError):\n    \"\"\"Base class for message parsing errors.\"\"\"\n\n\nclass HeaderParseError(MessageParseError):\n    \"\"\"Error while parsing headers.\"\"\"\n\n\nclass BoundaryError(MessageParseError):\n    \"\"\"Couldn't find terminating boundary.\"\"\"\n\n\nclass MultipartConversionError(MessageError, TypeError):\n    \"\"\"Conversion to a multipart is prohibited.\"\"\"\n\n\nclass CharsetError(MessageError):\n    \"\"\"An illegal charset was given.\"\"\"\n\n\n\f\n# These are parsing defects which the parser was able to work around.\nclass MessageDefect:\n    \"\"\"Base class for a message defect.\"\"\"\n\n    def __init__(self, line=None):\n        self.line = line\n\nclass NoBoundaryInMultipartDefect(MessageDefect):\n    \"\"\"A message claimed to be a multipart but had no boundary parameter.\"\"\"\n\nclass StartBoundaryNotFoundDefect(MessageDefect):\n    \"\"\"The claimed start boundary was never found.\"\"\"\n\nclass FirstHeaderLineIsContinuationDefect(MessageDefect):\n    \"\"\"A message had a continuation line as its first header line.\"\"\"\n\nclass MisplacedEnvelopeHeaderDefect(MessageDefect):\n    \"\"\"A 'Unix-from' header was found in the middle of a header block.\"\"\"\n\nclass MalformedHeaderDefect(MessageDefect):\n    \"\"\"Found a header that was missing a colon, or was otherwise malformed.\"\"\"\n\nclass MultipartInvariantViolationDefect(MessageDefect):\n    \"\"\"A message claimed to be a multipart but no subparts were found.\"\"\"\n", 
    "email.feedparser": "# Copyright (C) 2004-2006 Python Software Foundation\n# Authors: Baxter, Wouters and Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"FeedParser - An email feed parser.\n\nThe feed parser implements an interface for incrementally parsing an email\nmessage, line by line.  This has advantages for certain applications, such as\nthose reading email messages off a socket.\n\nFeedParser.feed() is the primary interface for pushing new data into the\nparser.  It returns when there's nothing more it can do with the available\ndata.  When you have no more data to push into the parser, call .close().\nThis completes the parsing and returns the root message object.\n\nThe other advantage of this parser is that it will never raise a parsing\nexception.  Instead, when it finds something unexpected, it adds a 'defect' to\nthe current message.  Defects are just instances that live on the message\nobject's .defects attribute.\n\"\"\"\n\n__all__ = ['FeedParser']\n\nimport re\n\nfrom email import errors\nfrom email import message\n\nNLCRE = re.compile('\\r\\n|\\r|\\n')\nNLCRE_bol = re.compile('(\\r\\n|\\r|\\n)')\nNLCRE_eol = re.compile('(\\r\\n|\\r|\\n)\\Z')\nNLCRE_crack = re.compile('(\\r\\n|\\r|\\n)')\n# RFC 2822 $3.6.8 Optional fields.  ftext is %d33-57 / %d59-126, Any character\n# except controls, SP, and \":\".\nheaderRE = re.compile(r'^(From |[\\041-\\071\\073-\\176]{1,}:|[\\t ])')\nEMPTYSTRING = ''\nNL = '\\n'\n\nNeedMoreData = object()\n\n\n\f\nclass BufferedSubFile(object):\n    \"\"\"A file-ish object that can have new data loaded into it.\n\n    You can also push and pop line-matching predicates onto a stack.  When the\n    current predicate matches the current line, a false EOF response\n    (i.e. empty string) is returned instead.  This lets the parser adhere to a\n    simple abstraction -- it parses until EOF closes the current message.\n    \"\"\"\n    def __init__(self):\n        # The last partial line pushed into this object.\n        self._partial = ''\n        # The list of full, pushed lines, in reverse order\n        self._lines = []\n        # The stack of false-EOF checking predicates.\n        self._eofstack = []\n        # A flag indicating whether the file has been closed or not.\n        self._closed = False\n\n    def push_eof_matcher(self, pred):\n        self._eofstack.append(pred)\n\n    def pop_eof_matcher(self):\n        return self._eofstack.pop()\n\n    def close(self):\n        # Don't forget any trailing partial line.\n        self._lines.append(self._partial)\n        self._partial = ''\n        self._closed = True\n\n    def readline(self):\n        if not self._lines:\n            if self._closed:\n                return ''\n            return NeedMoreData\n        # Pop the line off the stack and see if it matches the current\n        # false-EOF predicate.\n        line = self._lines.pop()\n        # RFC 2046, section 5.1.2 requires us to recognize outer level\n        # boundaries at any level of inner nesting.  Do this, but be sure it's\n        # in the order of most to least nested.\n        for ateof in self._eofstack[::-1]:\n            if ateof(line):\n                # We're at the false EOF.  But push the last line back first.\n                self._lines.append(line)\n                return ''\n        return line\n\n    def unreadline(self, line):\n        # Let the consumer push a line back into the buffer.\n        assert line is not NeedMoreData\n        self._lines.append(line)\n\n    def push(self, data):\n        \"\"\"Push some new data into this object.\"\"\"\n        # Handle any previous leftovers\n        data, self._partial = self._partial + data, ''\n        # Crack into lines, but preserve the newlines on the end of each\n        parts = NLCRE_crack.split(data)\n        # The *ahem* interesting behaviour of re.split when supplied grouping\n        # parentheses is that the last element of the resulting list is the\n        # data after the final RE.  In the case of a NL/CR terminated string,\n        # this is the empty string.\n        self._partial = parts.pop()\n        #GAN 29Mar09  bugs 1555570, 1721862  Confusion at 8K boundary ending with \\r:\n        # is there a \\n to follow later?\n        if not self._partial and parts and parts[-1].endswith('\\r'):\n            self._partial = parts.pop(-2)+parts.pop()\n        # parts is a list of strings, alternating between the line contents\n        # and the eol character(s).  Gather up a list of lines after\n        # re-attaching the newlines.\n        lines = []\n        for i in range(len(parts) // 2):\n            lines.append(parts[i*2] + parts[i*2+1])\n        self.pushlines(lines)\n\n    def pushlines(self, lines):\n        # Reverse and insert at the front of the lines.\n        self._lines[:0] = lines[::-1]\n\n    def is_closed(self):\n        return self._closed\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        line = self.readline()\n        if line == '':\n            raise StopIteration\n        return line\n\n\n\f\nclass FeedParser:\n    \"\"\"A feed-style parser of email.\"\"\"\n\n    def __init__(self, _factory=message.Message):\n        \"\"\"_factory is called with no arguments to create a new message obj\"\"\"\n        self._factory = _factory\n        self._input = BufferedSubFile()\n        self._msgstack = []\n        self._parse = self._parsegen().next\n        self._cur = None\n        self._last = None\n        self._headersonly = False\n\n    # Non-public interface for supporting Parser's headersonly flag\n    def _set_headersonly(self):\n        self._headersonly = True\n\n    def feed(self, data):\n        \"\"\"Push more data into the parser.\"\"\"\n        self._input.push(data)\n        self._call_parse()\n\n    def _call_parse(self):\n        try:\n            self._parse()\n        except StopIteration:\n            pass\n\n    def close(self):\n        \"\"\"Parse all remaining data and return the root message object.\"\"\"\n        self._input.close()\n        self._call_parse()\n        root = self._pop_message()\n        assert not self._msgstack\n        # Look for final set of defects\n        if root.get_content_maintype() == 'multipart' \\\n               and not root.is_multipart():\n            root.defects.append(errors.MultipartInvariantViolationDefect())\n        return root\n\n    def _new_message(self):\n        msg = self._factory()\n        if self._cur and self._cur.get_content_type() == 'multipart/digest':\n            msg.set_default_type('message/rfc822')\n        if self._msgstack:\n            self._msgstack[-1].attach(msg)\n        self._msgstack.append(msg)\n        self._cur = msg\n        self._last = msg\n\n    def _pop_message(self):\n        retval = self._msgstack.pop()\n        if self._msgstack:\n            self._cur = self._msgstack[-1]\n        else:\n            self._cur = None\n        return retval\n\n    def _parsegen(self):\n        # Create a new message and start by parsing headers.\n        self._new_message()\n        headers = []\n        # Collect the headers, searching for a line that doesn't match the RFC\n        # 2822 header or continuation pattern (including an empty line).\n        for line in self._input:\n            if line is NeedMoreData:\n                yield NeedMoreData\n                continue\n            if not headerRE.match(line):\n                # If we saw the RFC defined header/body separator\n                # (i.e. newline), just throw it away. Otherwise the line is\n                # part of the body so push it back.\n                if not NLCRE.match(line):\n                    self._input.unreadline(line)\n                break\n            headers.append(line)\n        # Done with the headers, so parse them and figure out what we're\n        # supposed to see in the body of the message.\n        self._parse_headers(headers)\n        # Headers-only parsing is a backwards compatibility hack, which was\n        # necessary in the older parser, which could raise errors.  All\n        # remaining lines in the input are thrown into the message body.\n        if self._headersonly:\n            lines = []\n            while True:\n                line = self._input.readline()\n                if line is NeedMoreData:\n                    yield NeedMoreData\n                    continue\n                if line == '':\n                    break\n                lines.append(line)\n            self._cur.set_payload(EMPTYSTRING.join(lines))\n            return\n        if self._cur.get_content_type() == 'message/delivery-status':\n            # message/delivery-status contains blocks of headers separated by\n            # a blank line.  We'll represent each header block as a separate\n            # nested message object, but the processing is a bit different\n            # than standard message/* types because there is no body for the\n            # nested messages.  A blank line separates the subparts.\n            while True:\n                self._input.push_eof_matcher(NLCRE.match)\n                for retval in self._parsegen():\n                    if retval is NeedMoreData:\n                        yield NeedMoreData\n                        continue\n                    break\n                msg = self._pop_message()\n                # We need to pop the EOF matcher in order to tell if we're at\n                # the end of the current file, not the end of the last block\n                # of message headers.\n                self._input.pop_eof_matcher()\n                # The input stream must be sitting at the newline or at the\n                # EOF.  We want to see if we're at the end of this subpart, so\n                # first consume the blank line, then test the next line to see\n                # if we're at this subpart's EOF.\n                while True:\n                    line = self._input.readline()\n                    if line is NeedMoreData:\n                        yield NeedMoreData\n                        continue\n                    break\n                while True:\n                    line = self._input.readline()\n                    if line is NeedMoreData:\n                        yield NeedMoreData\n                        continue\n                    break\n                if line == '':\n                    break\n                # Not at EOF so this is a line we're going to need.\n                self._input.unreadline(line)\n            return\n        if self._cur.get_content_maintype() == 'message':\n            # The message claims to be a message/* type, then what follows is\n            # another RFC 2822 message.\n            for retval in self._parsegen():\n                if retval is NeedMoreData:\n                    yield NeedMoreData\n                    continue\n                break\n            self._pop_message()\n            return\n        if self._cur.get_content_maintype() == 'multipart':\n            boundary = self._cur.get_boundary()\n            if boundary is None:\n                # The message /claims/ to be a multipart but it has not\n                # defined a boundary.  That's a problem which we'll handle by\n                # reading everything until the EOF and marking the message as\n                # defective.\n                self._cur.defects.append(errors.NoBoundaryInMultipartDefect())\n                lines = []\n                for line in self._input:\n                    if line is NeedMoreData:\n                        yield NeedMoreData\n                        continue\n                    lines.append(line)\n                self._cur.set_payload(EMPTYSTRING.join(lines))\n                return\n            # Create a line match predicate which matches the inter-part\n            # boundary as well as the end-of-multipart boundary.  Don't push\n            # this onto the input stream until we've scanned past the\n            # preamble.\n            separator = '--' + boundary\n            boundaryre = re.compile(\n                '(?P<sep>' + re.escape(separator) +\n                r')(?P<end>--)?(?P<ws>[ \\t]*)(?P<linesep>\\r\\n|\\r|\\n)?$')\n            capturing_preamble = True\n            preamble = []\n            linesep = False\n            while True:\n                line = self._input.readline()\n                if line is NeedMoreData:\n                    yield NeedMoreData\n                    continue\n                if line == '':\n                    break\n                mo = boundaryre.match(line)\n                if mo:\n                    # If we're looking at the end boundary, we're done with\n                    # this multipart.  If there was a newline at the end of\n                    # the closing boundary, then we need to initialize the\n                    # epilogue with the empty string (see below).\n                    if mo.group('end'):\n                        linesep = mo.group('linesep')\n                        break\n                    # We saw an inter-part boundary.  Were we in the preamble?\n                    if capturing_preamble:\n                        if preamble:\n                            # According to RFC 2046, the last newline belongs\n                            # to the boundary.\n                            lastline = preamble[-1]\n                            eolmo = NLCRE_eol.search(lastline)\n                            if eolmo:\n                                preamble[-1] = lastline[:-len(eolmo.group(0))]\n                            self._cur.preamble = EMPTYSTRING.join(preamble)\n                        capturing_preamble = False\n                        self._input.unreadline(line)\n                        continue\n                    # We saw a boundary separating two parts.  Consume any\n                    # multiple boundary lines that may be following.  Our\n                    # interpretation of RFC 2046 BNF grammar does not produce\n                    # body parts within such double boundaries.\n                    while True:\n                        line = self._input.readline()\n                        if line is NeedMoreData:\n                            yield NeedMoreData\n                            continue\n                        mo = boundaryre.match(line)\n                        if not mo:\n                            self._input.unreadline(line)\n                            break\n                    # Recurse to parse this subpart; the input stream points\n                    # at the subpart's first line.\n                    self._input.push_eof_matcher(boundaryre.match)\n                    for retval in self._parsegen():\n                        if retval is NeedMoreData:\n                            yield NeedMoreData\n                            continue\n                        break\n                    # Because of RFC 2046, the newline preceding the boundary\n                    # separator actually belongs to the boundary, not the\n                    # previous subpart's payload (or epilogue if the previous\n                    # part is a multipart).\n                    if self._last.get_content_maintype() == 'multipart':\n                        epilogue = self._last.epilogue\n                        if epilogue == '':\n                            self._last.epilogue = None\n                        elif epilogue is not None:\n                            mo = NLCRE_eol.search(epilogue)\n                            if mo:\n                                end = len(mo.group(0))\n                                self._last.epilogue = epilogue[:-end]\n                    else:\n                        payload = self._last.get_payload()\n                        if isinstance(payload, basestring):\n                            mo = NLCRE_eol.search(payload)\n                            if mo:\n                                payload = payload[:-len(mo.group(0))]\n                                self._last.set_payload(payload)\n                    self._input.pop_eof_matcher()\n                    self._pop_message()\n                    # Set the multipart up for newline cleansing, which will\n                    # happen if we're in a nested multipart.\n                    self._last = self._cur\n                else:\n                    # I think we must be in the preamble\n                    assert capturing_preamble\n                    preamble.append(line)\n            # We've seen either the EOF or the end boundary.  If we're still\n            # capturing the preamble, we never saw the start boundary.  Note\n            # that as a defect and store the captured text as the payload.\n            # Everything from here to the EOF is epilogue.\n            if capturing_preamble:\n                self._cur.defects.append(errors.StartBoundaryNotFoundDefect())\n                self._cur.set_payload(EMPTYSTRING.join(preamble))\n                epilogue = []\n                for line in self._input:\n                    if line is NeedMoreData:\n                        yield NeedMoreData\n                        continue\n                self._cur.epilogue = EMPTYSTRING.join(epilogue)\n                return\n            # If the end boundary ended in a newline, we'll need to make sure\n            # the epilogue isn't None\n            if linesep:\n                epilogue = ['']\n            else:\n                epilogue = []\n            for line in self._input:\n                if line is NeedMoreData:\n                    yield NeedMoreData\n                    continue\n                epilogue.append(line)\n            # Any CRLF at the front of the epilogue is not technically part of\n            # the epilogue.  Also, watch out for an empty string epilogue,\n            # which means a single newline.\n            if epilogue:\n                firstline = epilogue[0]\n                bolmo = NLCRE_bol.match(firstline)\n                if bolmo:\n                    epilogue[0] = firstline[len(bolmo.group(0)):]\n            self._cur.epilogue = EMPTYSTRING.join(epilogue)\n            return\n        # Otherwise, it's some non-multipart type, so the entire rest of the\n        # file contents becomes the payload.\n        lines = []\n        for line in self._input:\n            if line is NeedMoreData:\n                yield NeedMoreData\n                continue\n            lines.append(line)\n        self._cur.set_payload(EMPTYSTRING.join(lines))\n\n    def _parse_headers(self, lines):\n        # Passed a list of lines that make up the headers for the current msg\n        lastheader = ''\n        lastvalue = []\n        for lineno, line in enumerate(lines):\n            # Check for continuation\n            if line[0] in ' \\t':\n                if not lastheader:\n                    # The first line of the headers was a continuation.  This\n                    # is illegal, so let's note the defect, store the illegal\n                    # line, and ignore it for purposes of headers.\n                    defect = errors.FirstHeaderLineIsContinuationDefect(line)\n                    self._cur.defects.append(defect)\n                    continue\n                lastvalue.append(line)\n                continue\n            if lastheader:\n                # XXX reconsider the joining of folded lines\n                lhdr = EMPTYSTRING.join(lastvalue)[:-1].rstrip('\\r\\n')\n                self._cur[lastheader] = lhdr\n                lastheader, lastvalue = '', []\n            # Check for envelope header, i.e. unix-from\n            if line.startswith('From '):\n                if lineno == 0:\n                    # Strip off the trailing newline\n                    mo = NLCRE_eol.search(line)\n                    if mo:\n                        line = line[:-len(mo.group(0))]\n                    self._cur.set_unixfrom(line)\n                    continue\n                elif lineno == len(lines) - 1:\n                    # Something looking like a unix-from at the end - it's\n                    # probably the first line of the body, so push back the\n                    # line and stop.\n                    self._input.unreadline(line)\n                    return\n                else:\n                    # Weirdly placed unix-from line.  Note this as a defect\n                    # and ignore it.\n                    defect = errors.MisplacedEnvelopeHeaderDefect(line)\n                    self._cur.defects.append(defect)\n                    continue\n            # Split the line on the colon separating field name from value.\n            i = line.find(':')\n            if i < 0:\n                defect = errors.MalformedHeaderDefect(line)\n                self._cur.defects.append(defect)\n                continue\n            lastheader = line[:i]\n            lastvalue = [line[i+1:].lstrip()]\n        # Done with all the lines, so handle the last header.\n        if lastheader:\n            # XXX reconsider the joining of folded lines\n            self._cur[lastheader] = EMPTYSTRING.join(lastvalue).rstrip('\\r\\n')\n", 
    "email.generator": "# Copyright (C) 2001-2010 Python Software Foundation\n# Contact: email-sig@python.org\n\n\"\"\"Classes to generate plain text from a message object tree.\"\"\"\n\n__all__ = ['Generator', 'DecodedGenerator']\n\nimport re\nimport sys\nimport time\nimport random\nimport warnings\n\nfrom cStringIO import StringIO\nfrom email.header import Header\n\nUNDERSCORE = '_'\nNL = '\\n'\n\nfcre = re.compile(r'^From ', re.MULTILINE)\n\ndef _is8bitstring(s):\n    if isinstance(s, str):\n        try:\n            unicode(s, 'us-ascii')\n        except UnicodeError:\n            return True\n    return False\n\n\n\f\nclass Generator:\n    \"\"\"Generates output from a Message object tree.\n\n    This basic generator writes the message to the given file object as plain\n    text.\n    \"\"\"\n    #\n    # Public interface\n    #\n\n    def __init__(self, outfp, mangle_from_=True, maxheaderlen=78):\n        \"\"\"Create the generator for message flattening.\n\n        outfp is the output file-like object for writing the message to.  It\n        must have a write() method.\n\n        Optional mangle_from_ is a flag that, when True (the default), escapes\n        From_ lines in the body of the message by putting a `>' in front of\n        them.\n\n        Optional maxheaderlen specifies the longest length for a non-continued\n        header.  When a header line is longer (in characters, with tabs\n        expanded to 8 spaces) than maxheaderlen, the header will split as\n        defined in the Header class.  Set maxheaderlen to zero to disable\n        header wrapping.  The default is 78, as recommended (but not required)\n        by RFC 2822.\n        \"\"\"\n        self._fp = outfp\n        self._mangle_from_ = mangle_from_\n        self._maxheaderlen = maxheaderlen\n\n    def write(self, s):\n        # Just delegate to the file object\n        self._fp.write(s)\n\n    def flatten(self, msg, unixfrom=False):\n        \"\"\"Print the message object tree rooted at msg to the output file\n        specified when the Generator instance was created.\n\n        unixfrom is a flag that forces the printing of a Unix From_ delimiter\n        before the first object in the message tree.  If the original message\n        has no From_ delimiter, a `standard' one is crafted.  By default, this\n        is False to inhibit the printing of any From_ delimiter.\n\n        Note that for subobjects, no From_ line is printed.\n        \"\"\"\n        if unixfrom:\n            ufrom = msg.get_unixfrom()\n            if not ufrom:\n                ufrom = 'From nobody ' + time.ctime(time.time())\n            print >> self._fp, ufrom\n        self._write(msg)\n\n    def clone(self, fp):\n        \"\"\"Clone this generator with the exact same options.\"\"\"\n        return self.__class__(fp, self._mangle_from_, self._maxheaderlen)\n\n    #\n    # Protected interface - undocumented ;/\n    #\n\n    def _write(self, msg):\n        # We can't write the headers yet because of the following scenario:\n        # say a multipart message includes the boundary string somewhere in\n        # its body.  We'd have to calculate the new boundary /before/ we write\n        # the headers so that we can write the correct Content-Type:\n        # parameter.\n        #\n        # The way we do this, so as to make the _handle_*() methods simpler,\n        # is to cache any subpart writes into a StringIO.  The we write the\n        # headers and the StringIO contents.  That way, subpart handlers can\n        # Do The Right Thing, and can still modify the Content-Type: header if\n        # necessary.\n        oldfp = self._fp\n        try:\n            self._fp = sfp = StringIO()\n            self._dispatch(msg)\n        finally:\n            self._fp = oldfp\n        # Write the headers.  First we see if the message object wants to\n        # handle that itself.  If not, we'll do it generically.\n        meth = getattr(msg, '_write_headers', None)\n        if meth is None:\n            self._write_headers(msg)\n        else:\n            meth(self)\n        self._fp.write(sfp.getvalue())\n\n    def _dispatch(self, msg):\n        # Get the Content-Type: for the message, then try to dispatch to\n        # self._handle_<maintype>_<subtype>().  If there's no handler for the\n        # full MIME type, then dispatch to self._handle_<maintype>().  If\n        # that's missing too, then dispatch to self._writeBody().\n        main = msg.get_content_maintype()\n        sub = msg.get_content_subtype()\n        specific = UNDERSCORE.join((main, sub)).replace('-', '_')\n        meth = getattr(self, '_handle_' + specific, None)\n        if meth is None:\n            generic = main.replace('-', '_')\n            meth = getattr(self, '_handle_' + generic, None)\n            if meth is None:\n                meth = self._writeBody\n        meth(msg)\n\n    #\n    # Default handlers\n    #\n\n    def _write_headers(self, msg):\n        for h, v in msg.items():\n            print >> self._fp, '%s:' % h,\n            if self._maxheaderlen == 0:\n                # Explicit no-wrapping\n                print >> self._fp, v\n            elif isinstance(v, Header):\n                # Header instances know what to do\n                print >> self._fp, v.encode()\n            elif _is8bitstring(v):\n                # If we have raw 8bit data in a byte string, we have no idea\n                # what the encoding is.  There is no safe way to split this\n                # string.  If it's ascii-subset, then we could do a normal\n                # ascii split, but if it's multibyte then we could break the\n                # string.  There's no way to know so the least harm seems to\n                # be to not split the string and risk it being too long.\n                print >> self._fp, v\n            else:\n                # Header's got lots of smarts, so use it.  Note that this is\n                # fundamentally broken though because we lose idempotency when\n                # the header string is continued with tabs.  It will now be\n                # continued with spaces.  This was reversedly broken before we\n                # fixed bug 1974.  Either way, we lose.\n                print >> self._fp, Header(\n                    v, maxlinelen=self._maxheaderlen, header_name=h).encode()\n        # A blank line always separates headers from body\n        print >> self._fp\n\n    #\n    # Handlers for writing types and subtypes\n    #\n\n    def _handle_text(self, msg):\n        payload = msg.get_payload()\n        if payload is None:\n            return\n        if not isinstance(payload, basestring):\n            raise TypeError('string payload expected: %s' % type(payload))\n        if self._mangle_from_:\n            payload = fcre.sub('>From ', payload)\n        self._fp.write(payload)\n\n    # Default body handler\n    _writeBody = _handle_text\n\n    def _handle_multipart(self, msg):\n        # The trick here is to write out each part separately, merge them all\n        # together, and then make sure that the boundary we've chosen isn't\n        # present in the payload.\n        msgtexts = []\n        subparts = msg.get_payload()\n        if subparts is None:\n            subparts = []\n        elif isinstance(subparts, basestring):\n            # e.g. a non-strict parse of a message with no starting boundary.\n            self._fp.write(subparts)\n            return\n        elif not isinstance(subparts, list):\n            # Scalar payload\n            subparts = [subparts]\n        for part in subparts:\n            s = StringIO()\n            g = self.clone(s)\n            g.flatten(part, unixfrom=False)\n            msgtexts.append(s.getvalue())\n        # BAW: What about boundaries that are wrapped in double-quotes?\n        boundary = msg.get_boundary()\n        if not boundary:\n            # Create a boundary that doesn't appear in any of the\n            # message texts.\n            alltext = NL.join(msgtexts)\n            boundary = _make_boundary(alltext)\n            msg.set_boundary(boundary)\n        # If there's a preamble, write it out, with a trailing CRLF\n        if msg.preamble is not None:\n            if self._mangle_from_:\n                preamble = fcre.sub('>From ', msg.preamble)\n            else:\n                preamble = msg.preamble\n            print >> self._fp, preamble\n        # dash-boundary transport-padding CRLF\n        print >> self._fp, '--' + boundary\n        # body-part\n        if msgtexts:\n            self._fp.write(msgtexts.pop(0))\n        # *encapsulation\n        # --> delimiter transport-padding\n        # --> CRLF body-part\n        for body_part in msgtexts:\n            # delimiter transport-padding CRLF\n            print >> self._fp, '\\n--' + boundary\n            # body-part\n            self._fp.write(body_part)\n        # close-delimiter transport-padding\n        self._fp.write('\\n--' + boundary + '--' + NL)\n        if msg.epilogue is not None:\n            if self._mangle_from_:\n                epilogue = fcre.sub('>From ', msg.epilogue)\n            else:\n                epilogue = msg.epilogue\n            self._fp.write(epilogue)\n\n    def _handle_multipart_signed(self, msg):\n        # The contents of signed parts has to stay unmodified in order to keep\n        # the signature intact per RFC1847 2.1, so we disable header wrapping.\n        # RDM: This isn't enough to completely preserve the part, but it helps.\n        old_maxheaderlen = self._maxheaderlen\n        try:\n            self._maxheaderlen = 0\n            self._handle_multipart(msg)\n        finally:\n            self._maxheaderlen = old_maxheaderlen\n\n    def _handle_message_delivery_status(self, msg):\n        # We can't just write the headers directly to self's file object\n        # because this will leave an extra newline between the last header\n        # block and the boundary.  Sigh.\n        blocks = []\n        for part in msg.get_payload():\n            s = StringIO()\n            g = self.clone(s)\n            g.flatten(part, unixfrom=False)\n            text = s.getvalue()\n            lines = text.split('\\n')\n            # Strip off the unnecessary trailing empty line\n            if lines and lines[-1] == '':\n                blocks.append(NL.join(lines[:-1]))\n            else:\n                blocks.append(text)\n        # Now join all the blocks with an empty line.  This has the lovely\n        # effect of separating each block with an empty line, but not adding\n        # an extra one after the last one.\n        self._fp.write(NL.join(blocks))\n\n    def _handle_message(self, msg):\n        s = StringIO()\n        g = self.clone(s)\n        # The payload of a message/rfc822 part should be a multipart sequence\n        # of length 1.  The zeroth element of the list should be the Message\n        # object for the subpart.  Extract that object, stringify it, and\n        # write it out.\n        # Except, it turns out, when it's a string instead, which happens when\n        # and only when HeaderParser is used on a message of mime type\n        # message/rfc822.  Such messages are generated by, for example,\n        # Groupwise when forwarding unadorned messages.  (Issue 7970.)  So\n        # in that case we just emit the string body.\n        payload = msg.get_payload()\n        if isinstance(payload, list):\n            g.flatten(msg.get_payload(0), unixfrom=False)\n            payload = s.getvalue()\n        self._fp.write(payload)\n\n\n\f\n_FMT = '[Non-text (%(type)s) part of message omitted, filename %(filename)s]'\n\nclass DecodedGenerator(Generator):\n    \"\"\"Generates a text representation of a message.\n\n    Like the Generator base class, except that non-text parts are substituted\n    with a format string representing the part.\n    \"\"\"\n    def __init__(self, outfp, mangle_from_=True, maxheaderlen=78, fmt=None):\n        \"\"\"Like Generator.__init__() except that an additional optional\n        argument is allowed.\n\n        Walks through all subparts of a message.  If the subpart is of main\n        type `text', then it prints the decoded payload of the subpart.\n\n        Otherwise, fmt is a format string that is used instead of the message\n        payload.  fmt is expanded with the following keywords (in\n        %(keyword)s format):\n\n        type       : Full MIME type of the non-text part\n        maintype   : Main MIME type of the non-text part\n        subtype    : Sub-MIME type of the non-text part\n        filename   : Filename of the non-text part\n        description: Description associated with the non-text part\n        encoding   : Content transfer encoding of the non-text part\n\n        The default value for fmt is None, meaning\n\n        [Non-text (%(type)s) part of message omitted, filename %(filename)s]\n        \"\"\"\n        Generator.__init__(self, outfp, mangle_from_, maxheaderlen)\n        if fmt is None:\n            self._fmt = _FMT\n        else:\n            self._fmt = fmt\n\n    def _dispatch(self, msg):\n        for part in msg.walk():\n            maintype = part.get_content_maintype()\n            if maintype == 'text':\n                print >> self, part.get_payload(decode=True)\n            elif maintype == 'multipart':\n                # Just skip this\n                pass\n            else:\n                print >> self, self._fmt % {\n                    'type'       : part.get_content_type(),\n                    'maintype'   : part.get_content_maintype(),\n                    'subtype'    : part.get_content_subtype(),\n                    'filename'   : part.get_filename('[no filename]'),\n                    'description': part.get('Content-Description',\n                                            '[no description]'),\n                    'encoding'   : part.get('Content-Transfer-Encoding',\n                                            '[no encoding]'),\n                    }\n\n\n\f\n# Helper\n_width = len(repr(sys.maxint-1))\n_fmt = '%%0%dd' % _width\n\ndef _make_boundary(text=None):\n    # Craft a random boundary.  If text is given, ensure that the chosen\n    # boundary doesn't appear in the text.\n    token = random.randrange(sys.maxint)\n    boundary = ('=' * 15) + (_fmt % token) + '=='\n    if text is None:\n        return boundary\n    b = boundary\n    counter = 0\n    while True:\n        cre = re.compile('^--' + re.escape(b) + '(--)?$', re.MULTILINE)\n        if not cre.search(text):\n            break\n        b = boundary + '.' + str(counter)\n        counter += 1\n    return b\n", 
    "email.header": "# Copyright (C) 2002-2006 Python Software Foundation\n# Author: Ben Gertzfield, Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Header encoding and decoding functionality.\"\"\"\n\n__all__ = [\n    'Header',\n    'decode_header',\n    'make_header',\n    ]\n\nimport re\nimport binascii\n\nimport email.quoprimime\nimport email.base64mime\n\nfrom email.errors import HeaderParseError\nfrom email.charset import Charset\n\nNL = '\\n'\nSPACE = ' '\nUSPACE = u' '\nSPACE8 = ' ' * 8\nUEMPTYSTRING = u''\n\nMAXLINELEN = 76\n\nUSASCII = Charset('us-ascii')\nUTF8 = Charset('utf-8')\n\n# Match encoded-word strings in the form =?charset?q?Hello_World?=\necre = re.compile(r'''\n  =\\?                   # literal =?\n  (?P<charset>[^?]*?)   # non-greedy up to the next ? is the charset\n  \\?                    # literal ?\n  (?P<encoding>[qb])    # either a \"q\" or a \"b\", case insensitive\n  \\?                    # literal ?\n  (?P<encoded>.*?)      # non-greedy up to the next ?= is the encoded string\n  \\?=                   # literal ?=\n  (?=[ \\t]|$)           # whitespace or the end of the string\n  ''', re.VERBOSE | re.IGNORECASE | re.MULTILINE)\n\n# Field name regexp, including trailing colon, but not separating whitespace,\n# according to RFC 2822.  Character range is from tilde to exclamation mark.\n# For use with .match()\nfcre = re.compile(r'[\\041-\\176]+:$')\n\n# Find a header embedded in a putative header value.  Used to check for\n# header injection attack.\n_embeded_header = re.compile(r'\\n[^ \\t]+:')\n\n\n\f\n# Helpers\n_max_append = email.quoprimime._max_append\n\n\n\f\ndef decode_header(header):\n    \"\"\"Decode a message header value without converting charset.\n\n    Returns a list of (decoded_string, charset) pairs containing each of the\n    decoded parts of the header.  Charset is None for non-encoded parts of the\n    header, otherwise a lower-case string containing the name of the character\n    set specified in the encoded string.\n\n    An email.errors.HeaderParseError may be raised when certain decoding error\n    occurs (e.g. a base64 decoding exception).\n    \"\"\"\n    # If no encoding, just return the header\n    header = str(header)\n    if not ecre.search(header):\n        return [(header, None)]\n    decoded = []\n    dec = ''\n    for line in header.splitlines():\n        # This line might not have an encoding in it\n        if not ecre.search(line):\n            decoded.append((line, None))\n            continue\n        parts = ecre.split(line)\n        while parts:\n            unenc = parts.pop(0).strip()\n            if unenc:\n                # Should we continue a long line?\n                if decoded and decoded[-1][1] is None:\n                    decoded[-1] = (decoded[-1][0] + SPACE + unenc, None)\n                else:\n                    decoded.append((unenc, None))\n            if parts:\n                charset, encoding = [s.lower() for s in parts[0:2]]\n                encoded = parts[2]\n                dec = None\n                if encoding == 'q':\n                    dec = email.quoprimime.header_decode(encoded)\n                elif encoding == 'b':\n                    paderr = len(encoded) % 4   # Postel's law: add missing padding\n                    if paderr:\n                        encoded += '==='[:4 - paderr]\n                    try:\n                        dec = email.base64mime.decode(encoded)\n                    except binascii.Error:\n                        # Turn this into a higher level exception.  BAW: Right\n                        # now we throw the lower level exception away but\n                        # when/if we get exception chaining, we'll preserve it.\n                        raise HeaderParseError\n                if dec is None:\n                    dec = encoded\n\n                if decoded and decoded[-1][1] == charset:\n                    decoded[-1] = (decoded[-1][0] + dec, decoded[-1][1])\n                else:\n                    decoded.append((dec, charset))\n            del parts[0:3]\n    return decoded\n\n\n\f\ndef make_header(decoded_seq, maxlinelen=None, header_name=None,\n                continuation_ws=' '):\n    \"\"\"Create a Header from a sequence of pairs as returned by decode_header()\n\n    decode_header() takes a header value string and returns a sequence of\n    pairs of the format (decoded_string, charset) where charset is the string\n    name of the character set.\n\n    This function takes one of those sequence of pairs and returns a Header\n    instance.  Optional maxlinelen, header_name, and continuation_ws are as in\n    the Header constructor.\n    \"\"\"\n    h = Header(maxlinelen=maxlinelen, header_name=header_name,\n               continuation_ws=continuation_ws)\n    for s, charset in decoded_seq:\n        # None means us-ascii but we can simply pass it on to h.append()\n        if charset is not None and not isinstance(charset, Charset):\n            charset = Charset(charset)\n        h.append(s, charset)\n    return h\n\n\n\f\nclass Header:\n    def __init__(self, s=None, charset=None,\n                 maxlinelen=None, header_name=None,\n                 continuation_ws=' ', errors='strict'):\n        \"\"\"Create a MIME-compliant header that can contain many character sets.\n\n        Optional s is the initial header value.  If None, the initial header\n        value is not set.  You can later append to the header with .append()\n        method calls.  s may be a byte string or a Unicode string, but see the\n        .append() documentation for semantics.\n\n        Optional charset serves two purposes: it has the same meaning as the\n        charset argument to the .append() method.  It also sets the default\n        character set for all subsequent .append() calls that omit the charset\n        argument.  If charset is not provided in the constructor, the us-ascii\n        charset is used both as s's initial charset and as the default for\n        subsequent .append() calls.\n\n        The maximum line length can be specified explicit via maxlinelen.  For\n        splitting the first line to a shorter value (to account for the field\n        header which isn't included in s, e.g. `Subject') pass in the name of\n        the field in header_name.  The default maxlinelen is 76.\n\n        continuation_ws must be RFC 2822 compliant folding whitespace (usually\n        either a space or a hard tab) which will be prepended to continuation\n        lines.\n\n        errors is passed through to the .append() call.\n        \"\"\"\n        if charset is None:\n            charset = USASCII\n        if not isinstance(charset, Charset):\n            charset = Charset(charset)\n        self._charset = charset\n        self._continuation_ws = continuation_ws\n        cws_expanded_len = len(continuation_ws.replace('\\t', SPACE8))\n        # BAW: I believe `chunks' and `maxlinelen' should be non-public.\n        self._chunks = []\n        if s is not None:\n            self.append(s, charset, errors)\n        if maxlinelen is None:\n            maxlinelen = MAXLINELEN\n        if header_name is None:\n            # We don't know anything about the field header so the first line\n            # is the same length as subsequent lines.\n            self._firstlinelen = maxlinelen\n        else:\n            # The first line should be shorter to take into account the field\n            # header.  Also subtract off 2 extra for the colon and space.\n            self._firstlinelen = maxlinelen - len(header_name) - 2\n        # Second and subsequent lines should subtract off the length in\n        # columns of the continuation whitespace prefix.\n        self._maxlinelen = maxlinelen - cws_expanded_len\n\n    def __str__(self):\n        \"\"\"A synonym for self.encode().\"\"\"\n        return self.encode()\n\n    def __unicode__(self):\n        \"\"\"Helper for the built-in unicode function.\"\"\"\n        uchunks = []\n        lastcs = None\n        for s, charset in self._chunks:\n            # We must preserve spaces between encoded and non-encoded word\n            # boundaries, which means for us we need to add a space when we go\n            # from a charset to None/us-ascii, or from None/us-ascii to a\n            # charset.  Only do this for the second and subsequent chunks.\n            nextcs = charset\n            if uchunks:\n                if lastcs not in (None, 'us-ascii'):\n                    if nextcs in (None, 'us-ascii'):\n                        uchunks.append(USPACE)\n                        nextcs = None\n                elif nextcs not in (None, 'us-ascii'):\n                    uchunks.append(USPACE)\n            lastcs = nextcs\n            uchunks.append(unicode(s, str(charset)))\n        return UEMPTYSTRING.join(uchunks)\n\n    # Rich comparison operators for equality only.  BAW: does it make sense to\n    # have or explicitly disable <, <=, >, >= operators?\n    def __eq__(self, other):\n        # other may be a Header or a string.  Both are fine so coerce\n        # ourselves to a string, swap the args and do another comparison.\n        return other == self.encode()\n\n    def __ne__(self, other):\n        return not self == other\n\n    def append(self, s, charset=None, errors='strict'):\n        \"\"\"Append a string to the MIME header.\n\n        Optional charset, if given, should be a Charset instance or the name\n        of a character set (which will be converted to a Charset instance).  A\n        value of None (the default) means that the charset given in the\n        constructor is used.\n\n        s may be a byte string or a Unicode string.  If it is a byte string\n        (i.e. isinstance(s, str) is true), then charset is the encoding of\n        that byte string, and a UnicodeError will be raised if the string\n        cannot be decoded with that charset.  If s is a Unicode string, then\n        charset is a hint specifying the character set of the characters in\n        the string.  In this case, when producing an RFC 2822 compliant header\n        using RFC 2047 rules, the Unicode string will be encoded using the\n        following charsets in order: us-ascii, the charset hint, utf-8.  The\n        first character set not to provoke a UnicodeError is used.\n\n        Optional `errors' is passed as the third argument to any unicode() or\n        ustr.encode() call.\n        \"\"\"\n        if charset is None:\n            charset = self._charset\n        elif not isinstance(charset, Charset):\n            charset = Charset(charset)\n        # If the charset is our faux 8bit charset, leave the string unchanged\n        if charset != '8bit':\n            # We need to test that the string can be converted to unicode and\n            # back to a byte string, given the input and output codecs of the\n            # charset.\n            if isinstance(s, str):\n                # Possibly raise UnicodeError if the byte string can't be\n                # converted to a unicode with the input codec of the charset.\n                incodec = charset.input_codec or 'us-ascii'\n                ustr = unicode(s, incodec, errors)\n                # Now make sure that the unicode could be converted back to a\n                # byte string with the output codec, which may be different\n                # than the iput coded.  Still, use the original byte string.\n                outcodec = charset.output_codec or 'us-ascii'\n                ustr.encode(outcodec, errors)\n            elif isinstance(s, unicode):\n                # Now we have to be sure the unicode string can be converted\n                # to a byte string with a reasonable output codec.  We want to\n                # use the byte string in the chunk.\n                for charset in USASCII, charset, UTF8:\n                    try:\n                        outcodec = charset.output_codec or 'us-ascii'\n                        s = s.encode(outcodec, errors)\n                        break\n                    except UnicodeError:\n                        pass\n                else:\n                    assert False, 'utf-8 conversion failed'\n        self._chunks.append((s, charset))\n\n    def _split(self, s, charset, maxlinelen, splitchars):\n        # Split up a header safely for use with encode_chunks.\n        splittable = charset.to_splittable(s)\n        encoded = charset.from_splittable(splittable, True)\n        elen = charset.encoded_header_len(encoded)\n        # If the line's encoded length first, just return it\n        if elen <= maxlinelen:\n            return [(encoded, charset)]\n        # If we have undetermined raw 8bit characters sitting in a byte\n        # string, we really don't know what the right thing to do is.  We\n        # can't really split it because it might be multibyte data which we\n        # could break if we split it between pairs.  The least harm seems to\n        # be to not split the header at all, but that means they could go out\n        # longer than maxlinelen.\n        if charset == '8bit':\n            return [(s, charset)]\n        # BAW: I'm not sure what the right test here is.  What we're trying to\n        # do is be faithful to RFC 2822's recommendation that ($2.2.3):\n        #\n        # \"Note: Though structured field bodies are defined in such a way that\n        #  folding can take place between many of the lexical tokens (and even\n        #  within some of the lexical tokens), folding SHOULD be limited to\n        #  placing the CRLF at higher-level syntactic breaks.\"\n        #\n        # For now, I can only imagine doing this when the charset is us-ascii,\n        # although it's possible that other charsets may also benefit from the\n        # higher-level syntactic breaks.\n        elif charset == 'us-ascii':\n            return self._split_ascii(s, charset, maxlinelen, splitchars)\n        # BAW: should we use encoded?\n        elif elen == len(s):\n            # We can split on _maxlinelen boundaries because we know that the\n            # encoding won't change the size of the string\n            splitpnt = maxlinelen\n            first = charset.from_splittable(splittable[:splitpnt], False)\n            last = charset.from_splittable(splittable[splitpnt:], False)\n        else:\n            # Binary search for split point\n            first, last = _binsplit(splittable, charset, maxlinelen)\n        # first is of the proper length so just wrap it in the appropriate\n        # chrome.  last must be recursively split.\n        fsplittable = charset.to_splittable(first)\n        fencoded = charset.from_splittable(fsplittable, True)\n        chunk = [(fencoded, charset)]\n        return chunk + self._split(last, charset, self._maxlinelen, splitchars)\n\n    def _split_ascii(self, s, charset, firstlen, splitchars):\n        chunks = _split_ascii(s, firstlen, self._maxlinelen,\n                              self._continuation_ws, splitchars)\n        return zip(chunks, [charset]*len(chunks))\n\n    def _encode_chunks(self, newchunks, maxlinelen):\n        # MIME-encode a header with many different charsets and/or encodings.\n        #\n        # Given a list of pairs (string, charset), return a MIME-encoded\n        # string suitable for use in a header field.  Each pair may have\n        # different charsets and/or encodings, and the resulting header will\n        # accurately reflect each setting.\n        #\n        # Each encoding can be email.utils.QP (quoted-printable, for\n        # ASCII-like character sets like iso-8859-1), email.utils.BASE64\n        # (Base64, for non-ASCII like character sets like KOI8-R and\n        # iso-2022-jp), or None (no encoding).\n        #\n        # Each pair will be represented on a separate line; the resulting\n        # string will be in the format:\n        #\n        # =?charset1?q?Mar=EDa_Gonz=E1lez_Alonso?=\\n\n        #  =?charset2?b?SvxyZ2VuIEL2aW5n?=\"\n        chunks = []\n        for header, charset in newchunks:\n            if not header:\n                continue\n            if charset is None or charset.header_encoding is None:\n                s = header\n            else:\n                s = charset.header_encode(header)\n            # Don't add more folding whitespace than necessary\n            if chunks and chunks[-1].endswith(' '):\n                extra = ''\n            else:\n                extra = ' '\n            _max_append(chunks, s, maxlinelen, extra)\n        joiner = NL + self._continuation_ws\n        return joiner.join(chunks)\n\n    def encode(self, splitchars=';, '):\n        \"\"\"Encode a message header into an RFC-compliant format.\n\n        There are many issues involved in converting a given string for use in\n        an email header.  Only certain character sets are readable in most\n        email clients, and as header strings can only contain a subset of\n        7-bit ASCII, care must be taken to properly convert and encode (with\n        Base64 or quoted-printable) header strings.  In addition, there is a\n        75-character length limit on any given encoded header field, so\n        line-wrapping must be performed, even with double-byte character sets.\n\n        This method will do its best to convert the string to the correct\n        character set used in email, and encode and line wrap it safely with\n        the appropriate scheme for that character set.\n\n        If the given charset is not known or an error occurs during\n        conversion, this function will return the header untouched.\n\n        Optional splitchars is a string containing characters to split long\n        ASCII lines on, in rough support of RFC 2822's `highest level\n        syntactic breaks'.  This doesn't affect RFC 2047 encoded lines.\n        \"\"\"\n        newchunks = []\n        maxlinelen = self._firstlinelen\n        lastlen = 0\n        for s, charset in self._chunks:\n            # The first bit of the next chunk should be just long enough to\n            # fill the next line.  Don't forget the space separating the\n            # encoded words.\n            targetlen = maxlinelen - lastlen - 1\n            if targetlen < charset.encoded_header_len(''):\n                # Stick it on the next line\n                targetlen = maxlinelen\n            newchunks += self._split(s, charset, targetlen, splitchars)\n            lastchunk, lastcharset = newchunks[-1]\n            lastlen = lastcharset.encoded_header_len(lastchunk)\n        value = self._encode_chunks(newchunks, maxlinelen)\n        if _embeded_header.search(value):\n            raise HeaderParseError(\"header value appears to contain \"\n                \"an embedded header: {!r}\".format(value))\n        return value\n\n\n\f\ndef _split_ascii(s, firstlen, restlen, continuation_ws, splitchars):\n    lines = []\n    maxlen = firstlen\n    for line in s.splitlines():\n        # Ignore any leading whitespace (i.e. continuation whitespace) already\n        # on the line, since we'll be adding our own.\n        line = line.lstrip()\n        if len(line) < maxlen:\n            lines.append(line)\n            maxlen = restlen\n            continue\n        # Attempt to split the line at the highest-level syntactic break\n        # possible.  Note that we don't have a lot of smarts about field\n        # syntax; we just try to break on semi-colons, then commas, then\n        # whitespace.\n        for ch in splitchars:\n            if ch in line:\n                break\n        else:\n            # There's nothing useful to split the line on, not even spaces, so\n            # just append this line unchanged\n            lines.append(line)\n            maxlen = restlen\n            continue\n        # Now split the line on the character plus trailing whitespace\n        cre = re.compile(r'%s\\s*' % ch)\n        if ch in ';,':\n            eol = ch\n        else:\n            eol = ''\n        joiner = eol + ' '\n        joinlen = len(joiner)\n        wslen = len(continuation_ws.replace('\\t', SPACE8))\n        this = []\n        linelen = 0\n        for part in cre.split(line):\n            curlen = linelen + max(0, len(this)-1) * joinlen\n            partlen = len(part)\n            onfirstline = not lines\n            # We don't want to split after the field name, if we're on the\n            # first line and the field name is present in the header string.\n            if ch == ' ' and onfirstline and \\\n                   len(this) == 1 and fcre.match(this[0]):\n                this.append(part)\n                linelen += partlen\n            elif curlen + partlen > maxlen:\n                if this:\n                    lines.append(joiner.join(this) + eol)\n                # If this part is longer than maxlen and we aren't already\n                # splitting on whitespace, try to recursively split this line\n                # on whitespace.\n                if partlen > maxlen and ch != ' ':\n                    subl = _split_ascii(part, maxlen, restlen,\n                                        continuation_ws, ' ')\n                    lines.extend(subl[:-1])\n                    this = [subl[-1]]\n                else:\n                    this = [part]\n                linelen = wslen + len(this[-1])\n                maxlen = restlen\n            else:\n                this.append(part)\n                linelen += partlen\n        # Put any left over parts on a line by themselves\n        if this:\n            lines.append(joiner.join(this))\n    return lines\n\n\n\f\ndef _binsplit(splittable, charset, maxlinelen):\n    i = 0\n    j = len(splittable)\n    while i < j:\n        # Invariants:\n        # 1. splittable[:k] fits for all k <= i (note that we *assume*,\n        #    at the start, that splittable[:0] fits).\n        # 2. splittable[:k] does not fit for any k > j (at the start,\n        #    this means we shouldn't look at any k > len(splittable)).\n        # 3. We don't know about splittable[:k] for k in i+1..j.\n        # 4. We want to set i to the largest k that fits, with i <= k <= j.\n        #\n        m = (i+j+1) >> 1  # ceiling((i+j)/2); i < m <= j\n        chunk = charset.from_splittable(splittable[:m], True)\n        chunklen = charset.encoded_header_len(chunk)\n        if chunklen <= maxlinelen:\n            # m is acceptable, so is a new lower bound.\n            i = m\n        else:\n            # m is not acceptable, so final i must be < m.\n            j = m - 1\n    # i == j.  Invariant #1 implies that splittable[:i] fits, and\n    # invariant #2 implies that splittable[:i+1] does not fit, so i\n    # is what we're looking for.\n    first = charset.from_splittable(splittable[:i], False)\n    last  = charset.from_splittable(splittable[i:], False)\n    return first, last\n", 
    "email.iterators": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Various types of useful iterators and generators.\"\"\"\n\n__all__ = [\n    'body_line_iterator',\n    'typed_subpart_iterator',\n    'walk',\n    # Do not include _structure() since it's part of the debugging API.\n    ]\n\nimport sys\nfrom cStringIO import StringIO\n\n\n\f\n# This function will become a method of the Message class\ndef walk(self):\n    \"\"\"Walk over the message tree, yielding each subpart.\n\n    The walk is performed in depth-first order.  This method is a\n    generator.\n    \"\"\"\n    yield self\n    if self.is_multipart():\n        for subpart in self.get_payload():\n            for subsubpart in subpart.walk():\n                yield subsubpart\n\n\n\f\n# These two functions are imported into the Iterators.py interface module.\ndef body_line_iterator(msg, decode=False):\n    \"\"\"Iterate over the parts, returning string payloads line-by-line.\n\n    Optional decode (default False) is passed through to .get_payload().\n    \"\"\"\n    for subpart in msg.walk():\n        payload = subpart.get_payload(decode=decode)\n        if isinstance(payload, basestring):\n            for line in StringIO(payload):\n                yield line\n\n\ndef typed_subpart_iterator(msg, maintype='text', subtype=None):\n    \"\"\"Iterate over the subparts with a given MIME type.\n\n    Use `maintype' as the main MIME type to match against; this defaults to\n    \"text\".  Optional `subtype' is the MIME subtype to match against; if\n    omitted, only the main type is matched.\n    \"\"\"\n    for subpart in msg.walk():\n        if subpart.get_content_maintype() == maintype:\n            if subtype is None or subpart.get_content_subtype() == subtype:\n                yield subpart\n\n\n\f\ndef _structure(msg, fp=None, level=0, include_default=False):\n    \"\"\"A handy debugging aid\"\"\"\n    if fp is None:\n        fp = sys.stdout\n    tab = ' ' * (level * 4)\n    print >> fp, tab + msg.get_content_type(),\n    if include_default:\n        print >> fp, '[%s]' % msg.get_default_type()\n    else:\n        print >> fp\n    if msg.is_multipart():\n        for subpart in msg.get_payload():\n            _structure(subpart, fp, level+1, include_default)\n", 
    "email.message": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Basic message object for the email package object model.\"\"\"\n\n__all__ = ['Message']\n\nimport re\nimport uu\nimport binascii\nimport warnings\nfrom cStringIO import StringIO\n\n# Intrapackage imports\nimport email.charset\nfrom email import utils\nfrom email import errors\n\nSEMISPACE = '; '\n\n# Regular expression that matches `special' characters in parameters, the\n# existence of which force quoting of the parameter value.\ntspecials = re.compile(r'[ \\(\\)<>@,;:\\\\\"/\\[\\]\\?=]')\n\n\n# Helper functions\ndef _splitparam(param):\n    # Split header parameters.  BAW: this may be too simple.  It isn't\n    # strictly RFC 2045 (section 5.1) compliant, but it catches most headers\n    # found in the wild.  We may eventually need a full fledged parser\n    # eventually.\n    a, sep, b = param.partition(';')\n    if not sep:\n        return a.strip(), None\n    return a.strip(), b.strip()\n\f\ndef _formatparam(param, value=None, quote=True):\n    \"\"\"Convenience function to format and return a key=value pair.\n\n    This will quote the value if needed or if quote is true.  If value is a\n    three tuple (charset, language, value), it will be encoded according\n    to RFC2231 rules.\n    \"\"\"\n    if value is not None and len(value) > 0:\n        # A tuple is used for RFC 2231 encoded parameter values where items\n        # are (charset, language, value).  charset is a string, not a Charset\n        # instance.\n        if isinstance(value, tuple):\n            # Encode as per RFC 2231\n            param += '*'\n            value = utils.encode_rfc2231(value[2], value[0], value[1])\n        # BAW: Please check this.  I think that if quote is set it should\n        # force quoting even if not necessary.\n        if quote or tspecials.search(value):\n            return '%s=\"%s\"' % (param, utils.quote(value))\n        else:\n            return '%s=%s' % (param, value)\n    else:\n        return param\n\ndef _parseparam(s):\n    plist = []\n    while s[:1] == ';':\n        s = s[1:]\n        end = s.find(';')\n        while end > 0 and (s.count('\"', 0, end) - s.count('\\\\\"', 0, end)) % 2:\n            end = s.find(';', end + 1)\n        if end < 0:\n            end = len(s)\n        f = s[:end]\n        if '=' in f:\n            i = f.index('=')\n            f = f[:i].strip().lower() + '=' + f[i+1:].strip()\n        plist.append(f.strip())\n        s = s[end:]\n    return plist\n\n\ndef _unquotevalue(value):\n    # This is different than utils.collapse_rfc2231_value() because it doesn't\n    # try to convert the value to a unicode.  Message.get_param() and\n    # Message.get_params() are both currently defined to return the tuple in\n    # the face of RFC 2231 parameters.\n    if isinstance(value, tuple):\n        return value[0], value[1], utils.unquote(value[2])\n    else:\n        return utils.unquote(value)\n\n\n\f\nclass Message:\n    \"\"\"Basic message object.\n\n    A message object is defined as something that has a bunch of RFC 2822\n    headers and a payload.  It may optionally have an envelope header\n    (a.k.a. Unix-From or From_ header).  If the message is a container (i.e. a\n    multipart or a message/rfc822), then the payload is a list of Message\n    objects, otherwise it is a string.\n\n    Message objects implement part of the `mapping' interface, which assumes\n    there is exactly one occurrence of the header per message.  Some headers\n    do in fact appear multiple times (e.g. Received) and for those headers,\n    you must use the explicit API to set or get all the headers.  Not all of\n    the mapping methods are implemented.\n    \"\"\"\n    def __init__(self):\n        self._headers = []\n        self._unixfrom = None\n        self._payload = None\n        self._charset = None\n        # Defaults for multipart messages\n        self.preamble = self.epilogue = None\n        self.defects = []\n        # Default content type\n        self._default_type = 'text/plain'\n\n    def __str__(self):\n        \"\"\"Return the entire formatted message as a string.\n        This includes the headers, body, and envelope header.\n        \"\"\"\n        return self.as_string(unixfrom=True)\n\n    def as_string(self, unixfrom=False):\n        \"\"\"Return the entire formatted message as a string.\n        Optional `unixfrom' when True, means include the Unix From_ envelope\n        header.\n\n        This is a convenience method and may not generate the message exactly\n        as you intend because by default it mangles lines that begin with\n        \"From \".  For more flexibility, use the flatten() method of a\n        Generator instance.\n        \"\"\"\n        from email.generator import Generator\n        fp = StringIO()\n        g = Generator(fp)\n        g.flatten(self, unixfrom=unixfrom)\n        return fp.getvalue()\n\n    def is_multipart(self):\n        \"\"\"Return True if the message consists of multiple parts.\"\"\"\n        return isinstance(self._payload, list)\n\n    #\n    # Unix From_ line\n    #\n    def set_unixfrom(self, unixfrom):\n        self._unixfrom = unixfrom\n\n    def get_unixfrom(self):\n        return self._unixfrom\n\n    #\n    # Payload manipulation.\n    #\n    def attach(self, payload):\n        \"\"\"Add the given payload to the current payload.\n\n        The current payload will always be a list of objects after this method\n        is called.  If you want to set the payload to a scalar object, use\n        set_payload() instead.\n        \"\"\"\n        if self._payload is None:\n            self._payload = [payload]\n        else:\n            self._payload.append(payload)\n\n    def get_payload(self, i=None, decode=False):\n        \"\"\"Return a reference to the payload.\n\n        The payload will either be a list object or a string.  If you mutate\n        the list object, you modify the message's payload in place.  Optional\n        i returns that index into the payload.\n\n        Optional decode is a flag indicating whether the payload should be\n        decoded or not, according to the Content-Transfer-Encoding header\n        (default is False).\n\n        When True and the message is not a multipart, the payload will be\n        decoded if this header's value is `quoted-printable' or `base64'.  If\n        some other encoding is used, or the header is missing, or if the\n        payload has bogus data (i.e. bogus base64 or uuencoded data), the\n        payload is returned as-is.\n\n        If the message is a multipart and the decode flag is True, then None\n        is returned.\n        \"\"\"\n        if i is None:\n            payload = self._payload\n        elif not isinstance(self._payload, list):\n            raise TypeError('Expected list, got %s' % type(self._payload))\n        else:\n            payload = self._payload[i]\n        if decode:\n            if self.is_multipart():\n                return None\n            cte = self.get('content-transfer-encoding', '').lower()\n            if cte == 'quoted-printable':\n                return utils._qdecode(payload)\n            elif cte == 'base64':\n                try:\n                    return utils._bdecode(payload)\n                except binascii.Error:\n                    # Incorrect padding\n                    return payload\n            elif cte in ('x-uuencode', 'uuencode', 'uue', 'x-uue'):\n                sfp = StringIO()\n                try:\n                    uu.decode(StringIO(payload+'\\n'), sfp, quiet=True)\n                    payload = sfp.getvalue()\n                except uu.Error:\n                    # Some decoding problem\n                    return payload\n        # Everything else, including encodings with 8bit or 7bit are returned\n        # unchanged.\n        return payload\n\n    def set_payload(self, payload, charset=None):\n        \"\"\"Set the payload to the given value.\n\n        Optional charset sets the message's default character set.  See\n        set_charset() for details.\n        \"\"\"\n        self._payload = payload\n        if charset is not None:\n            self.set_charset(charset)\n\n    def set_charset(self, charset):\n        \"\"\"Set the charset of the payload to a given character set.\n\n        charset can be a Charset instance, a string naming a character set, or\n        None.  If it is a string it will be converted to a Charset instance.\n        If charset is None, the charset parameter will be removed from the\n        Content-Type field.  Anything else will generate a TypeError.\n\n        The message will be assumed to be of type text/* encoded with\n        charset.input_charset.  It will be converted to charset.output_charset\n        and encoded properly, if needed, when generating the plain text\n        representation of the message.  MIME headers (MIME-Version,\n        Content-Type, Content-Transfer-Encoding) will be added as needed.\n\n        \"\"\"\n        if charset is None:\n            self.del_param('charset')\n            self._charset = None\n            return\n        if isinstance(charset, basestring):\n            charset = email.charset.Charset(charset)\n        if not isinstance(charset, email.charset.Charset):\n            raise TypeError(charset)\n        # BAW: should we accept strings that can serve as arguments to the\n        # Charset constructor?\n        self._charset = charset\n        if 'MIME-Version' not in self:\n            self.add_header('MIME-Version', '1.0')\n        if 'Content-Type' not in self:\n            self.add_header('Content-Type', 'text/plain',\n                            charset=charset.get_output_charset())\n        else:\n            self.set_param('charset', charset.get_output_charset())\n        if isinstance(self._payload, unicode):\n            self._payload = self._payload.encode(charset.output_charset)\n        if str(charset) != charset.get_output_charset():\n            self._payload = charset.body_encode(self._payload)\n        if 'Content-Transfer-Encoding' not in self:\n            cte = charset.get_body_encoding()\n            try:\n                cte(self)\n            except TypeError:\n                self._payload = charset.body_encode(self._payload)\n                self.add_header('Content-Transfer-Encoding', cte)\n\n    def get_charset(self):\n        \"\"\"Return the Charset instance associated with the message's payload.\n        \"\"\"\n        return self._charset\n\n    #\n    # MAPPING INTERFACE (partial)\n    #\n    def __len__(self):\n        \"\"\"Return the total number of headers, including duplicates.\"\"\"\n        return len(self._headers)\n\n    def __getitem__(self, name):\n        \"\"\"Get a header value.\n\n        Return None if the header is missing instead of raising an exception.\n\n        Note that if the header appeared multiple times, exactly which\n        occurrence gets returned is undefined.  Use get_all() to get all\n        the values matching a header field name.\n        \"\"\"\n        return self.get(name)\n\n    def __setitem__(self, name, val):\n        \"\"\"Set the value of a header.\n\n        Note: this does not overwrite an existing header with the same field\n        name.  Use __delitem__() first to delete any existing headers.\n        \"\"\"\n        self._headers.append((name, val))\n\n    def __delitem__(self, name):\n        \"\"\"Delete all occurrences of a header, if present.\n\n        Does not raise an exception if the header is missing.\n        \"\"\"\n        name = name.lower()\n        newheaders = []\n        for k, v in self._headers:\n            if k.lower() != name:\n                newheaders.append((k, v))\n        self._headers = newheaders\n\n    def __contains__(self, name):\n        return name.lower() in [k.lower() for k, v in self._headers]\n\n    def has_key(self, name):\n        \"\"\"Return true if the message contains the header.\"\"\"\n        missing = object()\n        return self.get(name, missing) is not missing\n\n    def keys(self):\n        \"\"\"Return a list of all the message's header field names.\n\n        These will be sorted in the order they appeared in the original\n        message, or were added to the message, and may contain duplicates.\n        Any fields deleted and re-inserted are always appended to the header\n        list.\n        \"\"\"\n        return [k for k, v in self._headers]\n\n    def values(self):\n        \"\"\"Return a list of all the message's header values.\n\n        These will be sorted in the order they appeared in the original\n        message, or were added to the message, and may contain duplicates.\n        Any fields deleted and re-inserted are always appended to the header\n        list.\n        \"\"\"\n        return [v for k, v in self._headers]\n\n    def items(self):\n        \"\"\"Get all the message's header fields and values.\n\n        These will be sorted in the order they appeared in the original\n        message, or were added to the message, and may contain duplicates.\n        Any fields deleted and re-inserted are always appended to the header\n        list.\n        \"\"\"\n        return self._headers[:]\n\n    def get(self, name, failobj=None):\n        \"\"\"Get a header value.\n\n        Like __getitem__() but return failobj instead of None when the field\n        is missing.\n        \"\"\"\n        name = name.lower()\n        for k, v in self._headers:\n            if k.lower() == name:\n                return v\n        return failobj\n\n    #\n    # Additional useful stuff\n    #\n\n    def get_all(self, name, failobj=None):\n        \"\"\"Return a list of all the values for the named field.\n\n        These will be sorted in the order they appeared in the original\n        message, and may contain duplicates.  Any fields deleted and\n        re-inserted are always appended to the header list.\n\n        If no such fields exist, failobj is returned (defaults to None).\n        \"\"\"\n        values = []\n        name = name.lower()\n        for k, v in self._headers:\n            if k.lower() == name:\n                values.append(v)\n        if not values:\n            return failobj\n        return values\n\n    def add_header(self, _name, _value, **_params):\n        \"\"\"Extended header setting.\n\n        name is the header field to add.  keyword arguments can be used to set\n        additional parameters for the header field, with underscores converted\n        to dashes.  Normally the parameter will be added as key=\"value\" unless\n        value is None, in which case only the key will be added.  If a\n        parameter value contains non-ASCII characters it must be specified as a\n        three-tuple of (charset, language, value), in which case it will be\n        encoded according to RFC2231 rules.\n\n        Example:\n\n        msg.add_header('content-disposition', 'attachment', filename='bud.gif')\n        \"\"\"\n        parts = []\n        for k, v in _params.items():\n            if v is None:\n                parts.append(k.replace('_', '-'))\n            else:\n                parts.append(_formatparam(k.replace('_', '-'), v))\n        if _value is not None:\n            parts.insert(0, _value)\n        self._headers.append((_name, SEMISPACE.join(parts)))\n\n    def replace_header(self, _name, _value):\n        \"\"\"Replace a header.\n\n        Replace the first matching header found in the message, retaining\n        header order and case.  If no matching header was found, a KeyError is\n        raised.\n        \"\"\"\n        _name = _name.lower()\n        for i, (k, v) in zip(range(len(self._headers)), self._headers):\n            if k.lower() == _name:\n                self._headers[i] = (k, _value)\n                break\n        else:\n            raise KeyError(_name)\n\n    #\n    # Use these three methods instead of the three above.\n    #\n\n    def get_content_type(self):\n        \"\"\"Return the message's content type.\n\n        The returned string is coerced to lower case of the form\n        `maintype/subtype'.  If there was no Content-Type header in the\n        message, the default type as given by get_default_type() will be\n        returned.  Since according to RFC 2045, messages always have a default\n        type this will always return a value.\n\n        RFC 2045 defines a message's default type to be text/plain unless it\n        appears inside a multipart/digest container, in which case it would be\n        message/rfc822.\n        \"\"\"\n        missing = object()\n        value = self.get('content-type', missing)\n        if value is missing:\n            # This should have no parameters\n            return self.get_default_type()\n        ctype = _splitparam(value)[0].lower()\n        # RFC 2045, section 5.2 says if its invalid, use text/plain\n        if ctype.count('/') != 1:\n            return 'text/plain'\n        return ctype\n\n    def get_content_maintype(self):\n        \"\"\"Return the message's main content type.\n\n        This is the `maintype' part of the string returned by\n        get_content_type().\n        \"\"\"\n        ctype = self.get_content_type()\n        return ctype.split('/')[0]\n\n    def get_content_subtype(self):\n        \"\"\"Returns the message's sub-content type.\n\n        This is the `subtype' part of the string returned by\n        get_content_type().\n        \"\"\"\n        ctype = self.get_content_type()\n        return ctype.split('/')[1]\n\n    def get_default_type(self):\n        \"\"\"Return the `default' content type.\n\n        Most messages have a default content type of text/plain, except for\n        messages that are subparts of multipart/digest containers.  Such\n        subparts have a default content type of message/rfc822.\n        \"\"\"\n        return self._default_type\n\n    def set_default_type(self, ctype):\n        \"\"\"Set the `default' content type.\n\n        ctype should be either \"text/plain\" or \"message/rfc822\", although this\n        is not enforced.  The default content type is not stored in the\n        Content-Type header.\n        \"\"\"\n        self._default_type = ctype\n\n    def _get_params_preserve(self, failobj, header):\n        # Like get_params() but preserves the quoting of values.  BAW:\n        # should this be part of the public interface?\n        missing = object()\n        value = self.get(header, missing)\n        if value is missing:\n            return failobj\n        params = []\n        for p in _parseparam(';' + value):\n            try:\n                name, val = p.split('=', 1)\n                name = name.strip()\n                val = val.strip()\n            except ValueError:\n                # Must have been a bare attribute\n                name = p.strip()\n                val = ''\n            params.append((name, val))\n        params = utils.decode_params(params)\n        return params\n\n    def get_params(self, failobj=None, header='content-type', unquote=True):\n        \"\"\"Return the message's Content-Type parameters, as a list.\n\n        The elements of the returned list are 2-tuples of key/value pairs, as\n        split on the `=' sign.  The left hand side of the `=' is the key,\n        while the right hand side is the value.  If there is no `=' sign in\n        the parameter the value is the empty string.  The value is as\n        described in the get_param() method.\n\n        Optional failobj is the object to return if there is no Content-Type\n        header.  Optional header is the header to search instead of\n        Content-Type.  If unquote is True, the value is unquoted.\n        \"\"\"\n        missing = object()\n        params = self._get_params_preserve(missing, header)\n        if params is missing:\n            return failobj\n        if unquote:\n            return [(k, _unquotevalue(v)) for k, v in params]\n        else:\n            return params\n\n    def get_param(self, param, failobj=None, header='content-type',\n                  unquote=True):\n        \"\"\"Return the parameter value if found in the Content-Type header.\n\n        Optional failobj is the object to return if there is no Content-Type\n        header, or the Content-Type header has no such parameter.  Optional\n        header is the header to search instead of Content-Type.\n\n        Parameter keys are always compared case insensitively.  The return\n        value can either be a string, or a 3-tuple if the parameter was RFC\n        2231 encoded.  When it's a 3-tuple, the elements of the value are of\n        the form (CHARSET, LANGUAGE, VALUE).  Note that both CHARSET and\n        LANGUAGE can be None, in which case you should consider VALUE to be\n        encoded in the us-ascii charset.  You can usually ignore LANGUAGE.\n\n        Your application should be prepared to deal with 3-tuple return\n        values, and can convert the parameter to a Unicode string like so:\n\n            param = msg.get_param('foo')\n            if isinstance(param, tuple):\n                param = unicode(param[2], param[0] or 'us-ascii')\n\n        In any case, the parameter value (either the returned string, or the\n        VALUE item in the 3-tuple) is always unquoted, unless unquote is set\n        to False.\n        \"\"\"\n        if header not in self:\n            return failobj\n        for k, v in self._get_params_preserve(failobj, header):\n            if k.lower() == param.lower():\n                if unquote:\n                    return _unquotevalue(v)\n                else:\n                    return v\n        return failobj\n\n    def set_param(self, param, value, header='Content-Type', requote=True,\n                  charset=None, language=''):\n        \"\"\"Set a parameter in the Content-Type header.\n\n        If the parameter already exists in the header, its value will be\n        replaced with the new value.\n\n        If header is Content-Type and has not yet been defined for this\n        message, it will be set to \"text/plain\" and the new parameter and\n        value will be appended as per RFC 2045.\n\n        An alternate header can specified in the header argument, and all\n        parameters will be quoted as necessary unless requote is False.\n\n        If charset is specified, the parameter will be encoded according to RFC\n        2231.  Optional language specifies the RFC 2231 language, defaulting\n        to the empty string.  Both charset and language should be strings.\n        \"\"\"\n        if not isinstance(value, tuple) and charset:\n            value = (charset, language, value)\n\n        if header not in self and header.lower() == 'content-type':\n            ctype = 'text/plain'\n        else:\n            ctype = self.get(header)\n        if not self.get_param(param, header=header):\n            if not ctype:\n                ctype = _formatparam(param, value, requote)\n            else:\n                ctype = SEMISPACE.join(\n                    [ctype, _formatparam(param, value, requote)])\n        else:\n            ctype = ''\n            for old_param, old_value in self.get_params(header=header,\n                                                        unquote=requote):\n                append_param = ''\n                if old_param.lower() == param.lower():\n                    append_param = _formatparam(param, value, requote)\n                else:\n                    append_param = _formatparam(old_param, old_value, requote)\n                if not ctype:\n                    ctype = append_param\n                else:\n                    ctype = SEMISPACE.join([ctype, append_param])\n        if ctype != self.get(header):\n            del self[header]\n            self[header] = ctype\n\n    def del_param(self, param, header='content-type', requote=True):\n        \"\"\"Remove the given parameter completely from the Content-Type header.\n\n        The header will be re-written in place without the parameter or its\n        value. All values will be quoted as necessary unless requote is\n        False.  Optional header specifies an alternative to the Content-Type\n        header.\n        \"\"\"\n        if header not in self:\n            return\n        new_ctype = ''\n        for p, v in self.get_params(header=header, unquote=requote):\n            if p.lower() != param.lower():\n                if not new_ctype:\n                    new_ctype = _formatparam(p, v, requote)\n                else:\n                    new_ctype = SEMISPACE.join([new_ctype,\n                                                _formatparam(p, v, requote)])\n        if new_ctype != self.get(header):\n            del self[header]\n            self[header] = new_ctype\n\n    def set_type(self, type, header='Content-Type', requote=True):\n        \"\"\"Set the main type and subtype for the Content-Type header.\n\n        type must be a string in the form \"maintype/subtype\", otherwise a\n        ValueError is raised.\n\n        This method replaces the Content-Type header, keeping all the\n        parameters in place.  If requote is False, this leaves the existing\n        header's quoting as is.  Otherwise, the parameters will be quoted (the\n        default).\n\n        An alternative header can be specified in the header argument.  When\n        the Content-Type header is set, we'll always also add a MIME-Version\n        header.\n        \"\"\"\n        # BAW: should we be strict?\n        if not type.count('/') == 1:\n            raise ValueError\n        # Set the Content-Type, you get a MIME-Version\n        if header.lower() == 'content-type':\n            del self['mime-version']\n            self['MIME-Version'] = '1.0'\n        if header not in self:\n            self[header] = type\n            return\n        params = self.get_params(header=header, unquote=requote)\n        del self[header]\n        self[header] = type\n        # Skip the first param; it's the old type.\n        for p, v in params[1:]:\n            self.set_param(p, v, header, requote)\n\n    def get_filename(self, failobj=None):\n        \"\"\"Return the filename associated with the payload if present.\n\n        The filename is extracted from the Content-Disposition header's\n        `filename' parameter, and it is unquoted.  If that header is missing\n        the `filename' parameter, this method falls back to looking for the\n        `name' parameter.\n        \"\"\"\n        missing = object()\n        filename = self.get_param('filename', missing, 'content-disposition')\n        if filename is missing:\n            filename = self.get_param('name', missing, 'content-type')\n        if filename is missing:\n            return failobj\n        return utils.collapse_rfc2231_value(filename).strip()\n\n    def get_boundary(self, failobj=None):\n        \"\"\"Return the boundary associated with the payload if present.\n\n        The boundary is extracted from the Content-Type header's `boundary'\n        parameter, and it is unquoted.\n        \"\"\"\n        missing = object()\n        boundary = self.get_param('boundary', missing)\n        if boundary is missing:\n            return failobj\n        # RFC 2046 says that boundaries may begin but not end in w/s\n        return utils.collapse_rfc2231_value(boundary).rstrip()\n\n    def set_boundary(self, boundary):\n        \"\"\"Set the boundary parameter in Content-Type to 'boundary'.\n\n        This is subtly different than deleting the Content-Type header and\n        adding a new one with a new boundary parameter via add_header().  The\n        main difference is that using the set_boundary() method preserves the\n        order of the Content-Type header in the original message.\n\n        HeaderParseError is raised if the message has no Content-Type header.\n        \"\"\"\n        missing = object()\n        params = self._get_params_preserve(missing, 'content-type')\n        if params is missing:\n            # There was no Content-Type header, and we don't know what type\n            # to set it to, so raise an exception.\n            raise errors.HeaderParseError('No Content-Type header found')\n        newparams = []\n        foundp = False\n        for pk, pv in params:\n            if pk.lower() == 'boundary':\n                newparams.append(('boundary', '\"%s\"' % boundary))\n                foundp = True\n            else:\n                newparams.append((pk, pv))\n        if not foundp:\n            # The original Content-Type header had no boundary attribute.\n            # Tack one on the end.  BAW: should we raise an exception\n            # instead???\n            newparams.append(('boundary', '\"%s\"' % boundary))\n        # Replace the existing Content-Type header with the new value\n        newheaders = []\n        for h, v in self._headers:\n            if h.lower() == 'content-type':\n                parts = []\n                for k, v in newparams:\n                    if v == '':\n                        parts.append(k)\n                    else:\n                        parts.append('%s=%s' % (k, v))\n                newheaders.append((h, SEMISPACE.join(parts)))\n\n            else:\n                newheaders.append((h, v))\n        self._headers = newheaders\n\n    def get_content_charset(self, failobj=None):\n        \"\"\"Return the charset parameter of the Content-Type header.\n\n        The returned string is always coerced to lower case.  If there is no\n        Content-Type header, or if that header has no charset parameter,\n        failobj is returned.\n        \"\"\"\n        missing = object()\n        charset = self.get_param('charset', missing)\n        if charset is missing:\n            return failobj\n        if isinstance(charset, tuple):\n            # RFC 2231 encoded, so decode it, and it better end up as ascii.\n            pcharset = charset[0] or 'us-ascii'\n            try:\n                # LookupError will be raised if the charset isn't known to\n                # Python.  UnicodeError will be raised if the encoded text\n                # contains a character not in the charset.\n                charset = unicode(charset[2], pcharset).encode('us-ascii')\n            except (LookupError, UnicodeError):\n                charset = charset[2]\n        # charset character must be in us-ascii range\n        try:\n            if isinstance(charset, str):\n                charset = unicode(charset, 'us-ascii')\n            charset = charset.encode('us-ascii')\n        except UnicodeError:\n            return failobj\n        # RFC 2046, $4.1.2 says charsets are not case sensitive\n        return charset.lower()\n\n    def get_charsets(self, failobj=None):\n        \"\"\"Return a list containing the charset(s) used in this message.\n\n        The returned list of items describes the Content-Type headers'\n        charset parameter for this message and all the subparts in its\n        payload.\n\n        Each item will either be a string (the value of the charset parameter\n        in the Content-Type header of that part) or the value of the\n        'failobj' parameter (defaults to None), if the part does not have a\n        main MIME type of \"text\", or the charset is not defined.\n\n        The list will contain one string for each part of the message, plus\n        one for the container message (i.e. self), so that a non-multipart\n        message will still return a list of length 1.\n        \"\"\"\n        return [part.get_content_charset(failobj) for part in self.walk()]\n\n    # I.e. def walk(self): ...\n    from email.iterators import walk\n", 
    "email.mime.__init__": "", 
    "email.parser": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Barry Warsaw, Thomas Wouters, Anthony Baxter\n# Contact: email-sig@python.org\n\n\"\"\"A parser of RFC 2822 and MIME email messages.\"\"\"\n\n__all__ = ['Parser', 'HeaderParser']\n\nimport warnings\nfrom cStringIO import StringIO\n\nfrom email.feedparser import FeedParser\nfrom email.message import Message\n\n\n\f\nclass Parser:\n    def __init__(self, *args, **kws):\n        \"\"\"Parser of RFC 2822 and MIME email messages.\n\n        Creates an in-memory object tree representing the email message, which\n        can then be manipulated and turned over to a Generator to return the\n        textual representation of the message.\n\n        The string must be formatted as a block of RFC 2822 headers and header\n        continuation lines, optionally preceeded by a `Unix-from' header.  The\n        header block is terminated either by the end of the string or by a\n        blank line.\n\n        _class is the class to instantiate for new message objects when they\n        must be created.  This class must have a constructor that can take\n        zero arguments.  Default is Message.Message.\n        \"\"\"\n        if len(args) >= 1:\n            if '_class' in kws:\n                raise TypeError(\"Multiple values for keyword arg '_class'\")\n            kws['_class'] = args[0]\n        if len(args) == 2:\n            if 'strict' in kws:\n                raise TypeError(\"Multiple values for keyword arg 'strict'\")\n            kws['strict'] = args[1]\n        if len(args) > 2:\n            raise TypeError('Too many arguments')\n        if '_class' in kws:\n            self._class = kws['_class']\n            del kws['_class']\n        else:\n            self._class = Message\n        if 'strict' in kws:\n            warnings.warn(\"'strict' argument is deprecated (and ignored)\",\n                          DeprecationWarning, 2)\n            del kws['strict']\n        if kws:\n            raise TypeError('Unexpected keyword arguments')\n\n    def parse(self, fp, headersonly=False):\n        \"\"\"Create a message structure from the data in a file.\n\n        Reads all the data from the file and returns the root of the message\n        structure.  Optional headersonly is a flag specifying whether to stop\n        parsing after reading the headers or not.  The default is False,\n        meaning it parses the entire contents of the file.\n        \"\"\"\n        feedparser = FeedParser(self._class)\n        if headersonly:\n            feedparser._set_headersonly()\n        while True:\n            data = fp.read(8192)\n            if not data:\n                break\n            feedparser.feed(data)\n        return feedparser.close()\n\n    def parsestr(self, text, headersonly=False):\n        \"\"\"Create a message structure from a string.\n\n        Returns the root of the message structure.  Optional headersonly is a\n        flag specifying whether to stop parsing after reading the headers or\n        not.  The default is False, meaning it parses the entire contents of\n        the file.\n        \"\"\"\n        return self.parse(StringIO(text), headersonly=headersonly)\n\n\n\f\nclass HeaderParser(Parser):\n    def parse(self, fp, headersonly=True):\n        return Parser.parse(self, fp, True)\n\n    def parsestr(self, text, headersonly=True):\n        return Parser.parsestr(self, text, True)\n", 
    "email.quoprimime": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Ben Gertzfield\n# Contact: email-sig@python.org\n\n\"\"\"Quoted-printable content transfer encoding per RFCs 2045-2047.\n\nThis module handles the content transfer encoding method defined in RFC 2045\nto encode US ASCII-like 8-bit data called `quoted-printable'.  It is used to\nsafely encode text that is in a character set similar to the 7-bit US ASCII\ncharacter set, but that includes some 8-bit characters that are normally not\nallowed in email bodies or headers.\n\nQuoted-printable is very space-inefficient for encoding binary files; use the\nemail.base64mime module for that instead.\n\nThis module provides an interface to encode and decode both headers and bodies\nwith quoted-printable encoding.\n\nRFC 2045 defines a method for including character set information in an\n`encoded-word' in a header.  This method is commonly used for 8-bit real names\nin To:/From:/Cc: etc. fields, as well as Subject: lines.\n\nThis module does not do the line wrapping or end-of-line character\nconversion necessary for proper internationalized headers; it only\ndoes dumb encoding and decoding.  To deal with the various line\nwrapping issues, use the email.header module.\n\"\"\"\n\n__all__ = [\n    'body_decode',\n    'body_encode',\n    'body_quopri_check',\n    'body_quopri_len',\n    'decode',\n    'decodestring',\n    'encode',\n    'encodestring',\n    'header_decode',\n    'header_encode',\n    'header_quopri_check',\n    'header_quopri_len',\n    'quote',\n    'unquote',\n    ]\n\nimport re\n\nfrom string import hexdigits\nfrom email.utils import fix_eols\n\nCRLF = '\\r\\n'\nNL = '\\n'\n\n# See also Charset.py\nMISC_LEN = 7\n\nhqre = re.compile(r'[^-a-zA-Z0-9!*+/ ]')\nbqre = re.compile(r'[^ !-<>-~\\t]')\n\n\n\f\n# Helpers\ndef header_quopri_check(c):\n    \"\"\"Return True if the character should be escaped with header quopri.\"\"\"\n    return bool(hqre.match(c))\n\n\ndef body_quopri_check(c):\n    \"\"\"Return True if the character should be escaped with body quopri.\"\"\"\n    return bool(bqre.match(c))\n\n\ndef header_quopri_len(s):\n    \"\"\"Return the length of str when it is encoded with header quopri.\"\"\"\n    count = 0\n    for c in s:\n        if hqre.match(c):\n            count += 3\n        else:\n            count += 1\n    return count\n\n\ndef body_quopri_len(str):\n    \"\"\"Return the length of str when it is encoded with body quopri.\"\"\"\n    count = 0\n    for c in str:\n        if bqre.match(c):\n            count += 3\n        else:\n            count += 1\n    return count\n\n\ndef _max_append(L, s, maxlen, extra=''):\n    if not L:\n        L.append(s.lstrip())\n    elif len(L[-1]) + len(s) <= maxlen:\n        L[-1] += extra + s\n    else:\n        L.append(s.lstrip())\n\n\ndef unquote(s):\n    \"\"\"Turn a string in the form =AB to the ASCII character with value 0xab\"\"\"\n    return chr(int(s[1:3], 16))\n\n\ndef quote(c):\n    return \"=%02X\" % ord(c)\n\n\n\f\ndef header_encode(header, charset=\"iso-8859-1\", keep_eols=False,\n                  maxlinelen=76, eol=NL):\n    \"\"\"Encode a single header line with quoted-printable (like) encoding.\n\n    Defined in RFC 2045, this `Q' encoding is similar to quoted-printable, but\n    used specifically for email header fields to allow charsets with mostly 7\n    bit characters (and some 8 bit) to remain more or less readable in non-RFC\n    2045 aware mail clients.\n\n    charset names the character set to use to encode the header.  It defaults\n    to iso-8859-1.\n\n    The resulting string will be in the form:\n\n    \"=?charset?q?I_f=E2rt_in_your_g=E8n=E8ral_dire=E7tion?\\\\n\n      =?charset?q?Silly_=C8nglish_Kn=EEghts?=\"\n\n    with each line wrapped safely at, at most, maxlinelen characters (defaults\n    to 76 characters).  If maxlinelen is None, the entire string is encoded in\n    one chunk with no splitting.\n\n    End-of-line characters (\\\\r, \\\\n, \\\\r\\\\n) will be automatically converted\n    to the canonical email line separator \\\\r\\\\n unless the keep_eols\n    parameter is True (the default is False).\n\n    Each line of the header will be terminated in the value of eol, which\n    defaults to \"\\\\n\".  Set this to \"\\\\r\\\\n\" if you are using the result of\n    this function directly in email.\n    \"\"\"\n    # Return empty headers unchanged\n    if not header:\n        return header\n\n    if not keep_eols:\n        header = fix_eols(header)\n\n    # Quopri encode each line, in encoded chunks no greater than maxlinelen in\n    # length, after the RFC chrome is added in.\n    quoted = []\n    if maxlinelen is None:\n        # An obnoxiously large number that's good enough\n        max_encoded = 100000\n    else:\n        max_encoded = maxlinelen - len(charset) - MISC_LEN - 1\n\n    for c in header:\n        # Space may be represented as _ instead of =20 for readability\n        if c == ' ':\n            _max_append(quoted, '_', max_encoded)\n        # These characters can be included verbatim\n        elif not hqre.match(c):\n            _max_append(quoted, c, max_encoded)\n        # Otherwise, replace with hex value like =E2\n        else:\n            _max_append(quoted, \"=%02X\" % ord(c), max_encoded)\n\n    # Now add the RFC chrome to each encoded chunk and glue the chunks\n    # together.  BAW: should we be able to specify the leading whitespace in\n    # the joiner?\n    joiner = eol + ' '\n    return joiner.join(['=?%s?q?%s?=' % (charset, line) for line in quoted])\n\n\n\f\ndef encode(body, binary=False, maxlinelen=76, eol=NL):\n    \"\"\"Encode with quoted-printable, wrapping at maxlinelen characters.\n\n    If binary is False (the default), end-of-line characters will be converted\n    to the canonical email end-of-line sequence \\\\r\\\\n.  Otherwise they will\n    be left verbatim.\n\n    Each line of encoded text will end with eol, which defaults to \"\\\\n\".  Set\n    this to \"\\\\r\\\\n\" if you will be using the result of this function directly\n    in an email.\n\n    Each line will be wrapped at, at most, maxlinelen characters (defaults to\n    76 characters).  Long lines will have the `soft linefeed' quoted-printable\n    character \"=\" appended to them, so the decoded text will be identical to\n    the original text.\n    \"\"\"\n    if not body:\n        return body\n\n    if not binary:\n        body = fix_eols(body)\n\n    # BAW: We're accumulating the body text by string concatenation.  That\n    # can't be very efficient, but I don't have time now to rewrite it.  It\n    # just feels like this algorithm could be more efficient.\n    encoded_body = ''\n    lineno = -1\n    # Preserve line endings here so we can check later to see an eol needs to\n    # be added to the output later.\n    lines = body.splitlines(1)\n    for line in lines:\n        # But strip off line-endings for processing this line.\n        if line.endswith(CRLF):\n            line = line[:-2]\n        elif line[-1] in CRLF:\n            line = line[:-1]\n\n        lineno += 1\n        encoded_line = ''\n        prev = None\n        linelen = len(line)\n        # Now we need to examine every character to see if it needs to be\n        # quopri encoded.  BAW: again, string concatenation is inefficient.\n        for j in range(linelen):\n            c = line[j]\n            prev = c\n            if bqre.match(c):\n                c = quote(c)\n            elif j+1 == linelen:\n                # Check for whitespace at end of line; special case\n                if c not in ' \\t':\n                    encoded_line += c\n                prev = c\n                continue\n            # Check to see to see if the line has reached its maximum length\n            if len(encoded_line) + len(c) >= maxlinelen:\n                encoded_body += encoded_line + '=' + eol\n                encoded_line = ''\n            encoded_line += c\n        # Now at end of line..\n        if prev and prev in ' \\t':\n            # Special case for whitespace at end of file\n            if lineno + 1 == len(lines):\n                prev = quote(prev)\n                if len(encoded_line) + len(prev) > maxlinelen:\n                    encoded_body += encoded_line + '=' + eol + prev\n                else:\n                    encoded_body += encoded_line + prev\n            # Just normal whitespace at end of line\n            else:\n                encoded_body += encoded_line + prev + '=' + eol\n            encoded_line = ''\n        # Now look at the line we just finished and it has a line ending, we\n        # need to add eol to the end of the line.\n        if lines[lineno].endswith(CRLF) or lines[lineno][-1] in CRLF:\n            encoded_body += encoded_line + eol\n        else:\n            encoded_body += encoded_line\n        encoded_line = ''\n    return encoded_body\n\n\n# For convenience and backwards compatibility w/ standard base64 module\nbody_encode = encode\nencodestring = encode\n\n\n\f\n# BAW: I'm not sure if the intent was for the signature of this function to be\n# the same as base64MIME.decode() or not...\ndef decode(encoded, eol=NL):\n    \"\"\"Decode a quoted-printable string.\n\n    Lines are separated with eol, which defaults to \\\\n.\n    \"\"\"\n    if not encoded:\n        return encoded\n    # BAW: see comment in encode() above.  Again, we're building up the\n    # decoded string with string concatenation, which could be done much more\n    # efficiently.\n    decoded = ''\n\n    for line in encoded.splitlines():\n        line = line.rstrip()\n        if not line:\n            decoded += eol\n            continue\n\n        i = 0\n        n = len(line)\n        while i < n:\n            c = line[i]\n            if c != '=':\n                decoded += c\n                i += 1\n            # Otherwise, c == \"=\".  Are we at the end of the line?  If so, add\n            # a soft line break.\n            elif i+1 == n:\n                i += 1\n                continue\n            # Decode if in form =AB\n            elif i+2 < n and line[i+1] in hexdigits and line[i+2] in hexdigits:\n                decoded += unquote(line[i:i+3])\n                i += 3\n            # Otherwise, not in form =AB, pass literally\n            else:\n                decoded += c\n                i += 1\n\n            if i == n:\n                decoded += eol\n    # Special case if original string did not end with eol\n    if not encoded.endswith(eol) and decoded.endswith(eol):\n        decoded = decoded[:-1]\n    return decoded\n\n\n# For convenience and backwards compatibility w/ standard base64 module\nbody_decode = decode\ndecodestring = decode\n\n\n\f\ndef _unquote_match(match):\n    \"\"\"Turn a match in the form =AB to the ASCII character with value 0xab\"\"\"\n    s = match.group(0)\n    return unquote(s)\n\n\n# Header decoding is done a bit differently\ndef header_decode(s):\n    \"\"\"Decode a string encoded with RFC 2045 MIME header `Q' encoding.\n\n    This function does not parse a full MIME header value encoded with\n    quoted-printable (like =?iso-8895-1?q?Hello_World?=) -- please use\n    the high level email.header class for that functionality.\n    \"\"\"\n    s = s.replace('_', ' ')\n    return re.sub(r'=[a-fA-F0-9]{2}', _unquote_match, s)\n", 
    "email.utils": "# Copyright (C) 2001-2010 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Miscellaneous utilities.\"\"\"\n\n__all__ = [\n    'collapse_rfc2231_value',\n    'decode_params',\n    'decode_rfc2231',\n    'encode_rfc2231',\n    'formataddr',\n    'formatdate',\n    'getaddresses',\n    'make_msgid',\n    'mktime_tz',\n    'parseaddr',\n    'parsedate',\n    'parsedate_tz',\n    'unquote',\n    ]\n\nimport os\nimport re\nimport time\nimport base64\nimport random\nimport socket\nimport urllib\nimport warnings\n\nfrom email._parseaddr import quote\nfrom email._parseaddr import AddressList as _AddressList\nfrom email._parseaddr import mktime_tz\n\n# We need wormarounds for bugs in these methods in older Pythons (see below)\nfrom email._parseaddr import parsedate as _parsedate\nfrom email._parseaddr import parsedate_tz as _parsedate_tz\n\nfrom quopri import decodestring as _qdecode\n\n# Intrapackage imports\nfrom email.encoders import _bencode, _qencode\n\nCOMMASPACE = ', '\nEMPTYSTRING = ''\nUEMPTYSTRING = u''\nCRLF = '\\r\\n'\nTICK = \"'\"\n\nspecialsre = re.compile(r'[][\\\\()<>@,:;\".]')\nescapesre = re.compile(r'[][\\\\()\"]')\n\n\n\f\n# Helpers\n\ndef _identity(s):\n    return s\n\n\ndef _bdecode(s):\n    \"\"\"Decodes a base64 string.\n\n    This function is equivalent to base64.decodestring and it's retained only\n    for backward compatibility. It used to remove the last \\\\n of the decoded\n    string, if it had any (see issue 7143).\n    \"\"\"\n    if not s:\n        return s\n    return base64.decodestring(s)\n\n\n\f\ndef fix_eols(s):\n    \"\"\"Replace all line-ending characters with \\\\r\\\\n.\"\"\"\n    # Fix newlines with no preceding carriage return\n    s = re.sub(r'(?<!\\r)\\n', CRLF, s)\n    # Fix carriage returns with no following newline\n    s = re.sub(r'\\r(?!\\n)', CRLF, s)\n    return s\n\n\n\f\ndef formataddr(pair):\n    \"\"\"The inverse of parseaddr(), this takes a 2-tuple of the form\n    (realname, email_address) and returns the string value suitable\n    for an RFC 2822 From, To or Cc header.\n\n    If the first element of pair is false, then the second element is\n    returned unmodified.\n    \"\"\"\n    name, address = pair\n    if name:\n        quotes = ''\n        if specialsre.search(name):\n            quotes = '\"'\n        name = escapesre.sub(r'\\\\\\g<0>', name)\n        return '%s%s%s <%s>' % (quotes, name, quotes, address)\n    return address\n\n\n\f\ndef getaddresses(fieldvalues):\n    \"\"\"Return a list of (REALNAME, EMAIL) for each fieldvalue.\"\"\"\n    all = COMMASPACE.join(fieldvalues)\n    a = _AddressList(all)\n    return a.addresslist\n\n\n\f\necre = re.compile(r'''\n  =\\?                   # literal =?\n  (?P<charset>[^?]*?)   # non-greedy up to the next ? is the charset\n  \\?                    # literal ?\n  (?P<encoding>[qb])    # either a \"q\" or a \"b\", case insensitive\n  \\?                    # literal ?\n  (?P<atom>.*?)         # non-greedy up to the next ?= is the atom\n  \\?=                   # literal ?=\n  ''', re.VERBOSE | re.IGNORECASE)\n\n\n\f\ndef formatdate(timeval=None, localtime=False, usegmt=False):\n    \"\"\"Returns a date string as specified by RFC 2822, e.g.:\n\n    Fri, 09 Nov 2001 01:08:47 -0000\n\n    Optional timeval if given is a floating point time value as accepted by\n    gmtime() and localtime(), otherwise the current time is used.\n\n    Optional localtime is a flag that when True, interprets timeval, and\n    returns a date relative to the local timezone instead of UTC, properly\n    taking daylight savings time into account.\n\n    Optional argument usegmt means that the timezone is written out as\n    an ascii string, not numeric one (so \"GMT\" instead of \"+0000\"). This\n    is needed for HTTP, and is only used when localtime==False.\n    \"\"\"\n    # Note: we cannot use strftime() because that honors the locale and RFC\n    # 2822 requires that day and month names be the English abbreviations.\n    if timeval is None:\n        timeval = time.time()\n    if localtime:\n        now = time.localtime(timeval)\n        # Calculate timezone offset, based on whether the local zone has\n        # daylight savings time, and whether DST is in effect.\n        if time.daylight and now[-1]:\n            offset = time.altzone\n        else:\n            offset = time.timezone\n        hours, minutes = divmod(abs(offset), 3600)\n        # Remember offset is in seconds west of UTC, but the timezone is in\n        # minutes east of UTC, so the signs differ.\n        if offset > 0:\n            sign = '-'\n        else:\n            sign = '+'\n        zone = '%s%02d%02d' % (sign, hours, minutes // 60)\n    else:\n        now = time.gmtime(timeval)\n        # Timezone offset is always -0000\n        if usegmt:\n            zone = 'GMT'\n        else:\n            zone = '-0000'\n    return '%s, %02d %s %04d %02d:%02d:%02d %s' % (\n        ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][now[6]],\n        now[2],\n        ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n         'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][now[1] - 1],\n        now[0], now[3], now[4], now[5],\n        zone)\n\n\n\f\ndef make_msgid(idstring=None):\n    \"\"\"Returns a string suitable for RFC 2822 compliant Message-ID, e.g:\n\n    <20020201195627.33539.96671@nightshade.la.mastaler.com>\n\n    Optional idstring if given is a string used to strengthen the\n    uniqueness of the message id.\n    \"\"\"\n    timeval = time.time()\n    utcdate = time.strftime('%Y%m%d%H%M%S', time.gmtime(timeval))\n    pid = os.getpid()\n    randint = random.randrange(100000)\n    if idstring is None:\n        idstring = ''\n    else:\n        idstring = '.' + idstring\n    idhost = socket.getfqdn()\n    msgid = '<%s.%s.%s%s@%s>' % (utcdate, pid, randint, idstring, idhost)\n    return msgid\n\n\n\f\n# These functions are in the standalone mimelib version only because they've\n# subsequently been fixed in the latest Python versions.  We use this to worm\n# around broken older Pythons.\ndef parsedate(data):\n    if not data:\n        return None\n    return _parsedate(data)\n\n\ndef parsedate_tz(data):\n    if not data:\n        return None\n    return _parsedate_tz(data)\n\n\ndef parseaddr(addr):\n    addrs = _AddressList(addr).addresslist\n    if not addrs:\n        return '', ''\n    return addrs[0]\n\n\n# rfc822.unquote() doesn't properly de-backslash-ify in Python pre-2.3.\ndef unquote(str):\n    \"\"\"Remove quotes from a string.\"\"\"\n    if len(str) > 1:\n        if str.startswith('\"') and str.endswith('\"'):\n            return str[1:-1].replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n        if str.startswith('<') and str.endswith('>'):\n            return str[1:-1]\n    return str\n\n\n\f\n# RFC2231-related functions - parameter encoding and decoding\ndef decode_rfc2231(s):\n    \"\"\"Decode string according to RFC 2231\"\"\"\n    parts = s.split(TICK, 2)\n    if len(parts) <= 2:\n        return None, None, s\n    return parts\n\n\ndef encode_rfc2231(s, charset=None, language=None):\n    \"\"\"Encode string according to RFC 2231.\n\n    If neither charset nor language is given, then s is returned as-is.  If\n    charset is given but not language, the string is encoded using the empty\n    string for language.\n    \"\"\"\n    import urllib\n    s = urllib.quote(s, safe='')\n    if charset is None and language is None:\n        return s\n    if language is None:\n        language = ''\n    return \"%s'%s'%s\" % (charset, language, s)\n\n\nrfc2231_continuation = re.compile(r'^(?P<name>\\w+)\\*((?P<num>[0-9]+)\\*?)?$')\n\ndef decode_params(params):\n    \"\"\"Decode parameters list according to RFC 2231.\n\n    params is a sequence of 2-tuples containing (param name, string value).\n    \"\"\"\n    # Copy params so we don't mess with the original\n    params = params[:]\n    new_params = []\n    # Map parameter's name to a list of continuations.  The values are a\n    # 3-tuple of the continuation number, the string value, and a flag\n    # specifying whether a particular segment is %-encoded.\n    rfc2231_params = {}\n    name, value = params.pop(0)\n    new_params.append((name, value))\n    while params:\n        name, value = params.pop(0)\n        if name.endswith('*'):\n            encoded = True\n        else:\n            encoded = False\n        value = unquote(value)\n        mo = rfc2231_continuation.match(name)\n        if mo:\n            name, num = mo.group('name', 'num')\n            if num is not None:\n                num = int(num)\n            rfc2231_params.setdefault(name, []).append((num, value, encoded))\n        else:\n            new_params.append((name, '\"%s\"' % quote(value)))\n    if rfc2231_params:\n        for name, continuations in rfc2231_params.items():\n            value = []\n            extended = False\n            # Sort by number\n            continuations.sort()\n            # And now append all values in numerical order, converting\n            # %-encodings for the encoded segments.  If any of the\n            # continuation names ends in a *, then the entire string, after\n            # decoding segments and concatenating, must have the charset and\n            # language specifiers at the beginning of the string.\n            for num, s, encoded in continuations:\n                if encoded:\n                    s = urllib.unquote(s)\n                    extended = True\n                value.append(s)\n            value = quote(EMPTYSTRING.join(value))\n            if extended:\n                charset, language, value = decode_rfc2231(value)\n                new_params.append((name, (charset, language, '\"%s\"' % value)))\n            else:\n                new_params.append((name, '\"%s\"' % value))\n    return new_params\n\ndef collapse_rfc2231_value(value, errors='replace',\n                           fallback_charset='us-ascii'):\n    if isinstance(value, tuple):\n        rawval = unquote(value[2])\n        charset = value[0] or 'us-ascii'\n        try:\n            return unicode(rawval, charset, errors)\n        except LookupError:\n            # XXX charset is unknown to Python.\n            return unicode(rawval, fallback_charset, errors)\n    else:\n        return unquote(value)\n", 
    "encodings.__init__": "\"\"\" Standard \"encodings\" Package\n\n    Standard Python encoding modules are stored in this package\n    directory.\n\n    Codec modules must have names corresponding to normalized encoding\n    names as defined in the normalize_encoding() function below, e.g.\n    'utf-8' must be implemented by the module 'utf_8.py'.\n\n    Each codec module must export the following interface:\n\n    * getregentry() -> codecs.CodecInfo object\n    The getregentry() API must a CodecInfo object with encoder, decoder,\n    incrementalencoder, incrementaldecoder, streamwriter and streamreader\n    atttributes which adhere to the Python Codec Interface Standard.\n\n    In addition, a module may optionally also define the following\n    APIs which are then used by the package's codec search function:\n\n    * getaliases() -> sequence of encoding name strings to use as aliases\n\n    Alias names returned by getaliases() must be normalized encoding\n    names as defined by normalize_encoding().\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"#\"\n\nimport codecs\nfrom encodings import aliases\nimport __builtin__\n\n_cache = {}\n_unknown = '--unknown--'\n_import_tail = ['*']\n_norm_encoding_map = ('                                              . '\n                      '0123456789       ABCDEFGHIJKLMNOPQRSTUVWXYZ     '\n                      ' abcdefghijklmnopqrstuvwxyz                     '\n                      '                                                '\n                      '                                                '\n                      '                ')\n_aliases = aliases.aliases\n\nclass CodecRegistryError(LookupError, SystemError):\n    pass\n\ndef normalize_encoding(encoding):\n\n    \"\"\" Normalize an encoding name.\n\n        Normalization works as follows: all non-alphanumeric\n        characters except the dot used for Python package names are\n        collapsed and replaced with a single underscore, e.g. '  -;#'\n        becomes '_'. Leading and trailing underscores are removed.\n\n        Note that encoding names should be ASCII only; if they do use\n        non-ASCII characters, these must be Latin-1 compatible.\n\n    \"\"\"\n    # Make sure we have an 8-bit string, because .translate() works\n    # differently for Unicode strings.\n    if hasattr(__builtin__, \"unicode\") and isinstance(encoding, unicode):\n        # Note that .encode('latin-1') does *not* use the codec\n        # registry, so this call doesn't recurse. (See unicodeobject.c\n        # PyUnicode_AsEncodedString() for details)\n        encoding = encoding.encode('latin-1')\n    return '_'.join(encoding.translate(_norm_encoding_map).split())\n\ndef search_function(encoding):\n\n    # Cache lookup\n    entry = _cache.get(encoding, _unknown)\n    if entry is not _unknown:\n        return entry\n\n    # Import the module:\n    #\n    # First try to find an alias for the normalized encoding\n    # name and lookup the module using the aliased name, then try to\n    # lookup the module using the standard import scheme, i.e. first\n    # try in the encodings package, then at top-level.\n    #\n    norm_encoding = normalize_encoding(encoding)\n    aliased_encoding = _aliases.get(norm_encoding) or \\\n                       _aliases.get(norm_encoding.replace('.', '_'))\n    if aliased_encoding is not None:\n        modnames = [aliased_encoding,\n                    norm_encoding]\n    else:\n        modnames = [norm_encoding]\n    for modname in modnames:\n        if not modname or '.' in modname:\n            continue\n        try:\n            # Import is absolute to prevent the possibly malicious import of a\n            # module with side-effects that is not in the 'encodings' package.\n            mod = __import__('encodings.' + modname, fromlist=_import_tail,\n                             level=0)\n        except ImportError:\n            pass\n        else:\n            break\n    else:\n        mod = None\n\n    try:\n        getregentry = mod.getregentry\n    except AttributeError:\n        # Not a codec module\n        mod = None\n\n    if mod is None:\n        # Cache misses\n        _cache[encoding] = None\n        return None\n\n    # Now ask the module for the registry entry\n    entry = getregentry()\n    if not isinstance(entry, codecs.CodecInfo):\n        if not 4 <= len(entry) <= 7:\n            raise CodecRegistryError,\\\n                 'module \"%s\" (%s) failed to register' % \\\n                  (mod.__name__, mod.__file__)\n        if not hasattr(entry[0], '__call__') or \\\n           not hasattr(entry[1], '__call__') or \\\n           (entry[2] is not None and not hasattr(entry[2], '__call__')) or \\\n           (entry[3] is not None and not hasattr(entry[3], '__call__')) or \\\n           (len(entry) > 4 and entry[4] is not None and not hasattr(entry[4], '__call__')) or \\\n           (len(entry) > 5 and entry[5] is not None and not hasattr(entry[5], '__call__')):\n            raise CodecRegistryError,\\\n                'incompatible codecs in module \"%s\" (%s)' % \\\n                (mod.__name__, mod.__file__)\n        if len(entry)<7 or entry[6] is None:\n            entry += (None,)*(6-len(entry)) + (mod.__name__.split(\".\", 1)[1],)\n        entry = codecs.CodecInfo(*entry)\n\n    # Cache the codec registry entry\n    _cache[encoding] = entry\n\n    # Register its aliases (without overwriting previously registered\n    # aliases)\n    try:\n        codecaliases = mod.getaliases()\n    except AttributeError:\n        pass\n    else:\n        for alias in codecaliases:\n            if alias not in _aliases:\n                _aliases[alias] = modname\n\n    # Return the registry entry\n    return entry\n\n# Register the search_function in the Python codec registry\ncodecs.register(search_function)\n", 
    "encodings.aliases": "\"\"\" Encoding Aliases Support\n\n    This module is used by the encodings package search function to\n    map encodings names to module names.\n\n    Note that the search function normalizes the encoding names before\n    doing the lookup, so the mapping will have to map normalized\n    encoding names to module names.\n\n    Contents:\n\n        The following aliases dictionary contains mappings of all IANA\n        character set names for which the Python core library provides\n        codecs. In addition to these, a few Python specific codec\n        aliases have also been added.\n\n\"\"\"\naliases = {\n\n    # Please keep this list sorted alphabetically by value !\n\n    # ascii codec\n    '646'                : 'ascii',\n    'ansi_x3.4_1968'     : 'ascii',\n    'ansi_x3_4_1968'     : 'ascii', # some email headers use this non-standard name\n    'ansi_x3.4_1986'     : 'ascii',\n    'cp367'              : 'ascii',\n    'csascii'            : 'ascii',\n    'ibm367'             : 'ascii',\n    'iso646_us'          : 'ascii',\n    'iso_646.irv_1991'   : 'ascii',\n    'iso_ir_6'           : 'ascii',\n    'us'                 : 'ascii',\n    'us_ascii'           : 'ascii',\n\n    # base64_codec codec\n    'base64'             : 'base64_codec',\n    'base_64'            : 'base64_codec',\n\n    # big5 codec\n    'big5_tw'            : 'big5',\n    'csbig5'             : 'big5',\n\n    # big5hkscs codec\n    'big5_hkscs'         : 'big5hkscs',\n    'hkscs'              : 'big5hkscs',\n\n    # bz2_codec codec\n    'bz2'                : 'bz2_codec',\n\n    # cp037 codec\n    '037'                : 'cp037',\n    'csibm037'           : 'cp037',\n    'ebcdic_cp_ca'       : 'cp037',\n    'ebcdic_cp_nl'       : 'cp037',\n    'ebcdic_cp_us'       : 'cp037',\n    'ebcdic_cp_wt'       : 'cp037',\n    'ibm037'             : 'cp037',\n    'ibm039'             : 'cp037',\n\n    # cp1026 codec\n    '1026'               : 'cp1026',\n    'csibm1026'          : 'cp1026',\n    'ibm1026'            : 'cp1026',\n\n    # cp1140 codec\n    '1140'               : 'cp1140',\n    'ibm1140'            : 'cp1140',\n\n    # cp1250 codec\n    '1250'               : 'cp1250',\n    'windows_1250'       : 'cp1250',\n\n    # cp1251 codec\n    '1251'               : 'cp1251',\n    'windows_1251'       : 'cp1251',\n\n    # cp1252 codec\n    '1252'               : 'cp1252',\n    'windows_1252'       : 'cp1252',\n\n    # cp1253 codec\n    '1253'               : 'cp1253',\n    'windows_1253'       : 'cp1253',\n\n    # cp1254 codec\n    '1254'               : 'cp1254',\n    'windows_1254'       : 'cp1254',\n\n    # cp1255 codec\n    '1255'               : 'cp1255',\n    'windows_1255'       : 'cp1255',\n\n    # cp1256 codec\n    '1256'               : 'cp1256',\n    'windows_1256'       : 'cp1256',\n\n    # cp1257 codec\n    '1257'               : 'cp1257',\n    'windows_1257'       : 'cp1257',\n\n    # cp1258 codec\n    '1258'               : 'cp1258',\n    'windows_1258'       : 'cp1258',\n\n    # cp424 codec\n    '424'                : 'cp424',\n    'csibm424'           : 'cp424',\n    'ebcdic_cp_he'       : 'cp424',\n    'ibm424'             : 'cp424',\n\n    # cp437 codec\n    '437'                : 'cp437',\n    'cspc8codepage437'   : 'cp437',\n    'ibm437'             : 'cp437',\n\n    # cp500 codec\n    '500'                : 'cp500',\n    'csibm500'           : 'cp500',\n    'ebcdic_cp_be'       : 'cp500',\n    'ebcdic_cp_ch'       : 'cp500',\n    'ibm500'             : 'cp500',\n\n    # cp775 codec\n    '775'                : 'cp775',\n    'cspc775baltic'      : 'cp775',\n    'ibm775'             : 'cp775',\n\n    # cp850 codec\n    '850'                : 'cp850',\n    'cspc850multilingual' : 'cp850',\n    'ibm850'             : 'cp850',\n\n    # cp852 codec\n    '852'                : 'cp852',\n    'cspcp852'           : 'cp852',\n    'ibm852'             : 'cp852',\n\n    # cp855 codec\n    '855'                : 'cp855',\n    'csibm855'           : 'cp855',\n    'ibm855'             : 'cp855',\n\n    # cp857 codec\n    '857'                : 'cp857',\n    'csibm857'           : 'cp857',\n    'ibm857'             : 'cp857',\n\n    # cp858 codec\n    '858'                : 'cp858',\n    'csibm858'           : 'cp858',\n    'ibm858'             : 'cp858',\n\n    # cp860 codec\n    '860'                : 'cp860',\n    'csibm860'           : 'cp860',\n    'ibm860'             : 'cp860',\n\n    # cp861 codec\n    '861'                : 'cp861',\n    'cp_is'              : 'cp861',\n    'csibm861'           : 'cp861',\n    'ibm861'             : 'cp861',\n\n    # cp862 codec\n    '862'                : 'cp862',\n    'cspc862latinhebrew' : 'cp862',\n    'ibm862'             : 'cp862',\n\n    # cp863 codec\n    '863'                : 'cp863',\n    'csibm863'           : 'cp863',\n    'ibm863'             : 'cp863',\n\n    # cp864 codec\n    '864'                : 'cp864',\n    'csibm864'           : 'cp864',\n    'ibm864'             : 'cp864',\n\n    # cp865 codec\n    '865'                : 'cp865',\n    'csibm865'           : 'cp865',\n    'ibm865'             : 'cp865',\n\n    # cp866 codec\n    '866'                : 'cp866',\n    'csibm866'           : 'cp866',\n    'ibm866'             : 'cp866',\n\n    # cp869 codec\n    '869'                : 'cp869',\n    'cp_gr'              : 'cp869',\n    'csibm869'           : 'cp869',\n    'ibm869'             : 'cp869',\n\n    # cp932 codec\n    '932'                : 'cp932',\n    'ms932'              : 'cp932',\n    'mskanji'            : 'cp932',\n    'ms_kanji'           : 'cp932',\n\n    # cp949 codec\n    '949'                : 'cp949',\n    'ms949'              : 'cp949',\n    'uhc'                : 'cp949',\n\n    # cp950 codec\n    '950'                : 'cp950',\n    'ms950'              : 'cp950',\n\n    # euc_jis_2004 codec\n    'jisx0213'           : 'euc_jis_2004',\n    'eucjis2004'         : 'euc_jis_2004',\n    'euc_jis2004'        : 'euc_jis_2004',\n\n    # euc_jisx0213 codec\n    'eucjisx0213'        : 'euc_jisx0213',\n\n    # euc_jp codec\n    'eucjp'              : 'euc_jp',\n    'ujis'               : 'euc_jp',\n    'u_jis'              : 'euc_jp',\n\n    # euc_kr codec\n    'euckr'              : 'euc_kr',\n    'korean'             : 'euc_kr',\n    'ksc5601'            : 'euc_kr',\n    'ks_c_5601'          : 'euc_kr',\n    'ks_c_5601_1987'     : 'euc_kr',\n    'ksx1001'            : 'euc_kr',\n    'ks_x_1001'          : 'euc_kr',\n\n    # gb18030 codec\n    'gb18030_2000'       : 'gb18030',\n\n    # gb2312 codec\n    'chinese'            : 'gb2312',\n    'csiso58gb231280'    : 'gb2312',\n    'euc_cn'             : 'gb2312',\n    'euccn'              : 'gb2312',\n    'eucgb2312_cn'       : 'gb2312',\n    'gb2312_1980'        : 'gb2312',\n    'gb2312_80'          : 'gb2312',\n    'iso_ir_58'          : 'gb2312',\n\n    # gbk codec\n    '936'                : 'gbk',\n    'cp936'              : 'gbk',\n    'ms936'              : 'gbk',\n\n    # hex_codec codec\n    'hex'                : 'hex_codec',\n\n    # hp_roman8 codec\n    'roman8'             : 'hp_roman8',\n    'r8'                 : 'hp_roman8',\n    'csHPRoman8'         : 'hp_roman8',\n\n    # hz codec\n    'hzgb'               : 'hz',\n    'hz_gb'              : 'hz',\n    'hz_gb_2312'         : 'hz',\n\n    # iso2022_jp codec\n    'csiso2022jp'        : 'iso2022_jp',\n    'iso2022jp'          : 'iso2022_jp',\n    'iso_2022_jp'        : 'iso2022_jp',\n\n    # iso2022_jp_1 codec\n    'iso2022jp_1'        : 'iso2022_jp_1',\n    'iso_2022_jp_1'      : 'iso2022_jp_1',\n\n    # iso2022_jp_2 codec\n    'iso2022jp_2'        : 'iso2022_jp_2',\n    'iso_2022_jp_2'      : 'iso2022_jp_2',\n\n    # iso2022_jp_2004 codec\n    'iso_2022_jp_2004'   : 'iso2022_jp_2004',\n    'iso2022jp_2004'     : 'iso2022_jp_2004',\n\n    # iso2022_jp_3 codec\n    'iso2022jp_3'        : 'iso2022_jp_3',\n    'iso_2022_jp_3'      : 'iso2022_jp_3',\n\n    # iso2022_jp_ext codec\n    'iso2022jp_ext'      : 'iso2022_jp_ext',\n    'iso_2022_jp_ext'    : 'iso2022_jp_ext',\n\n    # iso2022_kr codec\n    'csiso2022kr'        : 'iso2022_kr',\n    'iso2022kr'          : 'iso2022_kr',\n    'iso_2022_kr'        : 'iso2022_kr',\n\n    # iso8859_10 codec\n    'csisolatin6'        : 'iso8859_10',\n    'iso_8859_10'        : 'iso8859_10',\n    'iso_8859_10_1992'   : 'iso8859_10',\n    'iso_ir_157'         : 'iso8859_10',\n    'l6'                 : 'iso8859_10',\n    'latin6'             : 'iso8859_10',\n\n    # iso8859_11 codec\n    'thai'               : 'iso8859_11',\n    'iso_8859_11'        : 'iso8859_11',\n    'iso_8859_11_2001'   : 'iso8859_11',\n\n    # iso8859_13 codec\n    'iso_8859_13'        : 'iso8859_13',\n    'l7'                 : 'iso8859_13',\n    'latin7'             : 'iso8859_13',\n\n    # iso8859_14 codec\n    'iso_8859_14'        : 'iso8859_14',\n    'iso_8859_14_1998'   : 'iso8859_14',\n    'iso_celtic'         : 'iso8859_14',\n    'iso_ir_199'         : 'iso8859_14',\n    'l8'                 : 'iso8859_14',\n    'latin8'             : 'iso8859_14',\n\n    # iso8859_15 codec\n    'iso_8859_15'        : 'iso8859_15',\n    'l9'                 : 'iso8859_15',\n    'latin9'             : 'iso8859_15',\n\n    # iso8859_16 codec\n    'iso_8859_16'        : 'iso8859_16',\n    'iso_8859_16_2001'   : 'iso8859_16',\n    'iso_ir_226'         : 'iso8859_16',\n    'l10'                : 'iso8859_16',\n    'latin10'            : 'iso8859_16',\n\n    # iso8859_2 codec\n    'csisolatin2'        : 'iso8859_2',\n    'iso_8859_2'         : 'iso8859_2',\n    'iso_8859_2_1987'    : 'iso8859_2',\n    'iso_ir_101'         : 'iso8859_2',\n    'l2'                 : 'iso8859_2',\n    'latin2'             : 'iso8859_2',\n\n    # iso8859_3 codec\n    'csisolatin3'        : 'iso8859_3',\n    'iso_8859_3'         : 'iso8859_3',\n    'iso_8859_3_1988'    : 'iso8859_3',\n    'iso_ir_109'         : 'iso8859_3',\n    'l3'                 : 'iso8859_3',\n    'latin3'             : 'iso8859_3',\n\n    # iso8859_4 codec\n    'csisolatin4'        : 'iso8859_4',\n    'iso_8859_4'         : 'iso8859_4',\n    'iso_8859_4_1988'    : 'iso8859_4',\n    'iso_ir_110'         : 'iso8859_4',\n    'l4'                 : 'iso8859_4',\n    'latin4'             : 'iso8859_4',\n\n    # iso8859_5 codec\n    'csisolatincyrillic' : 'iso8859_5',\n    'cyrillic'           : 'iso8859_5',\n    'iso_8859_5'         : 'iso8859_5',\n    'iso_8859_5_1988'    : 'iso8859_5',\n    'iso_ir_144'         : 'iso8859_5',\n\n    # iso8859_6 codec\n    'arabic'             : 'iso8859_6',\n    'asmo_708'           : 'iso8859_6',\n    'csisolatinarabic'   : 'iso8859_6',\n    'ecma_114'           : 'iso8859_6',\n    'iso_8859_6'         : 'iso8859_6',\n    'iso_8859_6_1987'    : 'iso8859_6',\n    'iso_ir_127'         : 'iso8859_6',\n\n    # iso8859_7 codec\n    'csisolatingreek'    : 'iso8859_7',\n    'ecma_118'           : 'iso8859_7',\n    'elot_928'           : 'iso8859_7',\n    'greek'              : 'iso8859_7',\n    'greek8'             : 'iso8859_7',\n    'iso_8859_7'         : 'iso8859_7',\n    'iso_8859_7_1987'    : 'iso8859_7',\n    'iso_ir_126'         : 'iso8859_7',\n\n    # iso8859_8 codec\n    'csisolatinhebrew'   : 'iso8859_8',\n    'hebrew'             : 'iso8859_8',\n    'iso_8859_8'         : 'iso8859_8',\n    'iso_8859_8_1988'    : 'iso8859_8',\n    'iso_ir_138'         : 'iso8859_8',\n\n    # iso8859_9 codec\n    'csisolatin5'        : 'iso8859_9',\n    'iso_8859_9'         : 'iso8859_9',\n    'iso_8859_9_1989'    : 'iso8859_9',\n    'iso_ir_148'         : 'iso8859_9',\n    'l5'                 : 'iso8859_9',\n    'latin5'             : 'iso8859_9',\n\n    # johab codec\n    'cp1361'             : 'johab',\n    'ms1361'             : 'johab',\n\n    # koi8_r codec\n    'cskoi8r'            : 'koi8_r',\n\n    # latin_1 codec\n    #\n    # Note that the latin_1 codec is implemented internally in C and a\n    # lot faster than the charmap codec iso8859_1 which uses the same\n    # encoding. This is why we discourage the use of the iso8859_1\n    # codec and alias it to latin_1 instead.\n    #\n    '8859'               : 'latin_1',\n    'cp819'              : 'latin_1',\n    'csisolatin1'        : 'latin_1',\n    'ibm819'             : 'latin_1',\n    'iso8859'            : 'latin_1',\n    'iso8859_1'          : 'latin_1',\n    'iso_8859_1'         : 'latin_1',\n    'iso_8859_1_1987'    : 'latin_1',\n    'iso_ir_100'         : 'latin_1',\n    'l1'                 : 'latin_1',\n    'latin'              : 'latin_1',\n    'latin1'             : 'latin_1',\n\n    # mac_cyrillic codec\n    'maccyrillic'        : 'mac_cyrillic',\n\n    # mac_greek codec\n    'macgreek'           : 'mac_greek',\n\n    # mac_iceland codec\n    'maciceland'         : 'mac_iceland',\n\n    # mac_latin2 codec\n    'maccentraleurope'   : 'mac_latin2',\n    'maclatin2'          : 'mac_latin2',\n\n    # mac_roman codec\n    'macroman'           : 'mac_roman',\n\n    # mac_turkish codec\n    'macturkish'         : 'mac_turkish',\n\n    # mbcs codec\n    'dbcs'               : 'mbcs',\n\n    # ptcp154 codec\n    'csptcp154'          : 'ptcp154',\n    'pt154'              : 'ptcp154',\n    'cp154'              : 'ptcp154',\n    'cyrillic_asian'     : 'ptcp154',\n\n    # quopri_codec codec\n    'quopri'             : 'quopri_codec',\n    'quoted_printable'   : 'quopri_codec',\n    'quotedprintable'    : 'quopri_codec',\n\n    # rot_13 codec\n    'rot13'              : 'rot_13',\n\n    # shift_jis codec\n    'csshiftjis'         : 'shift_jis',\n    'shiftjis'           : 'shift_jis',\n    'sjis'               : 'shift_jis',\n    's_jis'              : 'shift_jis',\n\n    # shift_jis_2004 codec\n    'shiftjis2004'       : 'shift_jis_2004',\n    'sjis_2004'          : 'shift_jis_2004',\n    's_jis_2004'         : 'shift_jis_2004',\n\n    # shift_jisx0213 codec\n    'shiftjisx0213'      : 'shift_jisx0213',\n    'sjisx0213'          : 'shift_jisx0213',\n    's_jisx0213'         : 'shift_jisx0213',\n\n    # tactis codec\n    'tis260'             : 'tactis',\n\n    # tis_620 codec\n    'tis620'             : 'tis_620',\n    'tis_620_0'          : 'tis_620',\n    'tis_620_2529_0'     : 'tis_620',\n    'tis_620_2529_1'     : 'tis_620',\n    'iso_ir_166'         : 'tis_620',\n\n    # utf_16 codec\n    'u16'                : 'utf_16',\n    'utf16'              : 'utf_16',\n\n    # utf_16_be codec\n    'unicodebigunmarked' : 'utf_16_be',\n    'utf_16be'           : 'utf_16_be',\n\n    # utf_16_le codec\n    'unicodelittleunmarked' : 'utf_16_le',\n    'utf_16le'           : 'utf_16_le',\n\n    # utf_32 codec\n    'u32'                : 'utf_32',\n    'utf32'              : 'utf_32',\n\n    # utf_32_be codec\n    'utf_32be'           : 'utf_32_be',\n\n    # utf_32_le codec\n    'utf_32le'           : 'utf_32_le',\n\n    # utf_7 codec\n    'u7'                 : 'utf_7',\n    'utf7'               : 'utf_7',\n    'unicode_1_1_utf_7'  : 'utf_7',\n\n    # utf_8 codec\n    'u8'                 : 'utf_8',\n    'utf'                : 'utf_8',\n    'utf8'               : 'utf_8',\n    'utf8_ucs2'          : 'utf_8',\n    'utf8_ucs4'          : 'utf_8',\n\n    # uu_codec codec\n    'uu'                 : 'uu_codec',\n\n    # zlib_codec codec\n    'zip'                : 'zlib_codec',\n    'zlib'               : 'zlib_codec',\n\n}\n", 
    "encodings.ascii": "\"\"\" Python 'ascii' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.ascii_encode\n    decode = codecs.ascii_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.ascii_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.ascii_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\nclass StreamConverter(StreamWriter,StreamReader):\n\n    encode = codecs.ascii_decode\n    decode = codecs.ascii_encode\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='ascii',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.base64_codec": "\"\"\" Python 'base64_codec' Codec - base64 content transfer encoding\n\n    Unlike most of the other codecs which target Unicode, this codec\n    will return Python string objects for both encode and decode.\n\n    Written by Marc-Andre Lemburg (mal@lemburg.com).\n\n\"\"\"\nimport codecs, base64\n\n### Codec APIs\n\ndef base64_encode(input,errors='strict'):\n\n    \"\"\" Encodes the object input and returns a tuple (output\n        object, length consumed).\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = base64.encodestring(input)\n    return (output, len(input))\n\ndef base64_decode(input,errors='strict'):\n\n    \"\"\" Decodes the object input and returns a tuple (output\n        object, length consumed).\n\n        input must be an object which provides the bf_getreadbuf\n        buffer slot. Python strings, buffer objects and memory\n        mapped files are examples of objects providing this slot.\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = base64.decodestring(input)\n    return (output, len(input))\n\nclass Codec(codecs.Codec):\n\n    def encode(self, input,errors='strict'):\n        return base64_encode(input,errors)\n    def decode(self, input,errors='strict'):\n        return base64_decode(input,errors)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        assert self.errors == 'strict'\n        return base64.encodestring(input)\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        assert self.errors == 'strict'\n        return base64.decodestring(input)\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='base64',\n        encode=base64_encode,\n        decode=base64_decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.hex_codec": "\"\"\" Python 'hex_codec' Codec - 2-digit hex content transfer encoding\n\n    Unlike most of the other codecs which target Unicode, this codec\n    will return Python string objects for both encode and decode.\n\n    Written by Marc-Andre Lemburg (mal@lemburg.com).\n\n\"\"\"\nimport codecs, binascii\n\n### Codec APIs\n\ndef hex_encode(input,errors='strict'):\n\n    \"\"\" Encodes the object input and returns a tuple (output\n        object, length consumed).\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = binascii.b2a_hex(input)\n    return (output, len(input))\n\ndef hex_decode(input,errors='strict'):\n\n    \"\"\" Decodes the object input and returns a tuple (output\n        object, length consumed).\n\n        input must be an object which provides the bf_getreadbuf\n        buffer slot. Python strings, buffer objects and memory\n        mapped files are examples of objects providing this slot.\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = binascii.a2b_hex(input)\n    return (output, len(input))\n\nclass Codec(codecs.Codec):\n\n    def encode(self, input,errors='strict'):\n        return hex_encode(input,errors)\n    def decode(self, input,errors='strict'):\n        return hex_decode(input,errors)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        assert self.errors == 'strict'\n        return binascii.b2a_hex(input)\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        assert self.errors == 'strict'\n        return binascii.a2b_hex(input)\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='hex',\n        encode=hex_encode,\n        decode=hex_decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.latin_1": "\"\"\" Python 'latin-1' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.latin_1_encode\n    decode = codecs.latin_1_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.latin_1_encode(input,self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.latin_1_decode(input,self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\nclass StreamConverter(StreamWriter,StreamReader):\n\n    encode = codecs.latin_1_decode\n    decode = codecs.latin_1_encode\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='iso8859-1',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n", 
    "encodings.raw_unicode_escape": "\"\"\" Python 'raw-unicode-escape' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.raw_unicode_escape_encode\n    decode = codecs.raw_unicode_escape_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.raw_unicode_escape_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.raw_unicode_escape_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='raw-unicode-escape',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.string_escape": "# -*- coding: utf-8 -*-\n\"\"\" Python 'escape' Codec\n\n\nWritten by Martin v. L\u00f6wis (martin@v.loewis.de).\n\n\"\"\"\nimport codecs\n\nclass Codec(codecs.Codec):\n\n    encode = codecs.escape_encode\n    decode = codecs.escape_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.escape_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.escape_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='string-escape',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.unicode_escape": "\"\"\" Python 'unicode-escape' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.unicode_escape_encode\n    decode = codecs.unicode_escape_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.unicode_escape_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.unicode_escape_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='unicode-escape',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.unicode_internal": "\"\"\" Python 'unicode-internal' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.unicode_internal_encode\n    decode = codecs.unicode_internal_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.unicode_internal_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.unicode_internal_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='unicode-internal',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.utf_16": "\"\"\" Python 'utf-16' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs, sys\n\n### Codec APIs\n\nencode = codecs.utf_16_encode\n\ndef decode(input, errors='strict'):\n    return codecs.utf_16_decode(input, errors, True)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def __init__(self, errors='strict'):\n        codecs.IncrementalEncoder.__init__(self, errors)\n        self.encoder = None\n\n    def encode(self, input, final=False):\n        if self.encoder is None:\n            result = codecs.utf_16_encode(input, self.errors)[0]\n            if sys.byteorder == 'little':\n                self.encoder = codecs.utf_16_le_encode\n            else:\n                self.encoder = codecs.utf_16_be_encode\n            return result\n        return self.encoder(input, self.errors)[0]\n\n    def reset(self):\n        codecs.IncrementalEncoder.reset(self)\n        self.encoder = None\n\n    def getstate(self):\n        # state info we return to the caller:\n        # 0: stream is in natural order for this platform\n        # 2: endianness hasn't been determined yet\n        # (we're never writing in unnatural order)\n        return (2 if self.encoder is None else 0)\n\n    def setstate(self, state):\n        if state:\n            self.encoder = None\n        else:\n            if sys.byteorder == 'little':\n                self.encoder = codecs.utf_16_le_encode\n            else:\n                self.encoder = codecs.utf_16_be_encode\n\nclass IncrementalDecoder(codecs.BufferedIncrementalDecoder):\n    def __init__(self, errors='strict'):\n        codecs.BufferedIncrementalDecoder.__init__(self, errors)\n        self.decoder = None\n\n    def _buffer_decode(self, input, errors, final):\n        if self.decoder is None:\n            (output, consumed, byteorder) = \\\n                codecs.utf_16_ex_decode(input, errors, 0, final)\n            if byteorder == -1:\n                self.decoder = codecs.utf_16_le_decode\n            elif byteorder == 1:\n                self.decoder = codecs.utf_16_be_decode\n            elif consumed >= 2:\n                raise UnicodeError(\"UTF-16 stream does not start with BOM\")\n            return (output, consumed)\n        return self.decoder(input, self.errors, final)\n\n    def reset(self):\n        codecs.BufferedIncrementalDecoder.reset(self)\n        self.decoder = None\n\nclass StreamWriter(codecs.StreamWriter):\n    def __init__(self, stream, errors='strict'):\n        codecs.StreamWriter.__init__(self, stream, errors)\n        self.encoder = None\n\n    def reset(self):\n        codecs.StreamWriter.reset(self)\n        self.encoder = None\n\n    def encode(self, input, errors='strict'):\n        if self.encoder is None:\n            result = codecs.utf_16_encode(input, errors)\n            if sys.byteorder == 'little':\n                self.encoder = codecs.utf_16_le_encode\n            else:\n                self.encoder = codecs.utf_16_be_encode\n            return result\n        else:\n            return self.encoder(input, errors)\n\nclass StreamReader(codecs.StreamReader):\n\n    def reset(self):\n        codecs.StreamReader.reset(self)\n        try:\n            del self.decode\n        except AttributeError:\n            pass\n\n    def decode(self, input, errors='strict'):\n        (object, consumed, byteorder) = \\\n            codecs.utf_16_ex_decode(input, errors, 0, False)\n        if byteorder == -1:\n            self.decode = codecs.utf_16_le_decode\n        elif byteorder == 1:\n            self.decode = codecs.utf_16_be_decode\n        elif consumed>=2:\n            raise UnicodeError,\"UTF-16 stream does not start with BOM\"\n        return (object, consumed)\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='utf-16',\n        encode=encode,\n        decode=decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n", 
    "encodings.utf_8": "\"\"\" Python 'utf-8' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nencode = codecs.utf_8_encode\n\ndef decode(input, errors='strict'):\n    return codecs.utf_8_decode(input, errors, True)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.utf_8_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.BufferedIncrementalDecoder):\n    _buffer_decode = codecs.utf_8_decode\n\nclass StreamWriter(codecs.StreamWriter):\n    encode = codecs.utf_8_encode\n\nclass StreamReader(codecs.StreamReader):\n    decode = codecs.utf_8_decode\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='utf-8',\n        encode=encode,\n        decode=decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n", 
    "fnmatch": "\"\"\"Filename matching with shell patterns.\n\nfnmatch(FILENAME, PATTERN) matches according to the local convention.\nfnmatchcase(FILENAME, PATTERN) always takes case in account.\n\nThe functions operate by translating the pattern into a regular\nexpression.  They cache the compiled regular expressions for speed.\n\nThe function translate(PATTERN) returns a regular expression\ncorresponding to PATTERN.  (It does not compile it.)\n\"\"\"\n\nimport re\n\n__all__ = [\"filter\", \"fnmatch\", \"fnmatchcase\", \"translate\"]\n\n_cache = {}\n_MAXCACHE = 100\n\ndef _purge():\n    \"\"\"Clear the pattern cache\"\"\"\n    _cache.clear()\n\ndef fnmatch(name, pat):\n    \"\"\"Test whether FILENAME matches PATTERN.\n\n    Patterns are Unix shell style:\n\n    *       matches everything\n    ?       matches any single character\n    [seq]   matches any character in seq\n    [!seq]  matches any char not in seq\n\n    An initial period in FILENAME is not special.\n    Both FILENAME and PATTERN are first case-normalized\n    if the operating system requires it.\n    If you don't want this, use fnmatchcase(FILENAME, PATTERN).\n    \"\"\"\n\n    import os\n    name = os.path.normcase(name)\n    pat = os.path.normcase(pat)\n    return fnmatchcase(name, pat)\n\ndef filter(names, pat):\n    \"\"\"Return the subset of the list NAMES that match PAT\"\"\"\n    import os,posixpath\n    result=[]\n    pat=os.path.normcase(pat)\n    if not pat in _cache:\n        res = translate(pat)\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        _cache[pat] = re.compile(res)\n    match=_cache[pat].match\n    if os.path is posixpath:\n        # normcase on posix is NOP. Optimize it away from the loop.\n        for name in names:\n            if match(name):\n                result.append(name)\n    else:\n        for name in names:\n            if match(os.path.normcase(name)):\n                result.append(name)\n    return result\n\ndef fnmatchcase(name, pat):\n    \"\"\"Test whether FILENAME matches PATTERN, including case.\n\n    This is a version of fnmatch() which doesn't case-normalize\n    its arguments.\n    \"\"\"\n\n    if not pat in _cache:\n        res = translate(pat)\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        _cache[pat] = re.compile(res)\n    return _cache[pat].match(name) is not None\n\ndef translate(pat):\n    \"\"\"Translate a shell PATTERN to a regular expression.\n\n    There is no way to quote meta-characters.\n    \"\"\"\n\n    i, n = 0, len(pat)\n    res = ''\n    while i < n:\n        c = pat[i]\n        i = i+1\n        if c == '*':\n            res = res + '.*'\n        elif c == '?':\n            res = res + '.'\n        elif c == '[':\n            j = i\n            if j < n and pat[j] == '!':\n                j = j+1\n            if j < n and pat[j] == ']':\n                j = j+1\n            while j < n and pat[j] != ']':\n                j = j+1\n            if j >= n:\n                res = res + '\\\\['\n            else:\n                stuff = pat[i:j].replace('\\\\','\\\\\\\\')\n                i = j+1\n                if stuff[0] == '!':\n                    stuff = '^' + stuff[1:]\n                elif stuff[0] == '^':\n                    stuff = '\\\\' + stuff\n                res = '%s[%s]' % (res, stuff)\n        else:\n            res = res + re.escape(c)\n    return res + '\\Z(?ms)'\n", 
    "fractions": "# Originally contributed by Sjoerd Mullender.\n# Significantly modified by Jeffrey Yasskin <jyasskin at gmail.com>.\n\n\"\"\"Rational, infinite-precision, real numbers.\"\"\"\n\nfrom __future__ import division\nfrom decimal import Decimal\nimport math\nimport numbers\nimport operator\nimport re\n\n__all__ = ['Fraction', 'gcd']\n\nRational = numbers.Rational\n\n\ndef gcd(a, b):\n    \"\"\"Calculate the Greatest Common Divisor of a and b.\n\n    Unless b==0, the result will have the same sign as b (so that when\n    b is divided by it, the result comes out positive).\n    \"\"\"\n    while b:\n        a, b = b, a%b\n    return a\n\n\n_RATIONAL_FORMAT = re.compile(r\"\"\"\n    \\A\\s*                      # optional whitespace at the start, then\n    (?P<sign>[-+]?)            # an optional sign, then\n    (?=\\d|\\.\\d)                # lookahead for digit or .digit\n    (?P<num>\\d*)               # numerator (possibly empty)\n    (?:                        # followed by\n       (?:/(?P<denom>\\d+))?    # an optional denominator\n    |                          # or\n       (?:\\.(?P<decimal>\\d*))? # an optional fractional part\n       (?:E(?P<exp>[-+]?\\d+))? # and optional exponent\n    )\n    \\s*\\Z                      # and optional whitespace to finish\n\"\"\", re.VERBOSE | re.IGNORECASE)\n\n\nclass Fraction(Rational):\n    \"\"\"This class implements rational numbers.\n\n    In the two-argument form of the constructor, Fraction(8, 6) will\n    produce a rational number equivalent to 4/3. Both arguments must\n    be Rational. The numerator defaults to 0 and the denominator\n    defaults to 1 so that Fraction(3) == 3 and Fraction() == 0.\n\n    Fractions can also be constructed from:\n\n      - numeric strings similar to those accepted by the\n        float constructor (for example, '-2.3' or '1e10')\n\n      - strings of the form '123/456'\n\n      - float and Decimal instances\n\n      - other Rational instances (including integers)\n\n    \"\"\"\n\n    __slots__ = ('_numerator', '_denominator')\n\n    # We're immutable, so use __new__ not __init__\n    def __new__(cls, numerator=0, denominator=None):\n        \"\"\"Constructs a Fraction.\n\n        Takes a string like '3/2' or '1.5', another Rational instance, a\n        numerator/denominator pair, or a float.\n\n        Examples\n        --------\n\n        >>> Fraction(10, -8)\n        Fraction(-5, 4)\n        >>> Fraction(Fraction(1, 7), 5)\n        Fraction(1, 35)\n        >>> Fraction(Fraction(1, 7), Fraction(2, 3))\n        Fraction(3, 14)\n        >>> Fraction('314')\n        Fraction(314, 1)\n        >>> Fraction('-35/4')\n        Fraction(-35, 4)\n        >>> Fraction('3.1415') # conversion from numeric string\n        Fraction(6283, 2000)\n        >>> Fraction('-47e-2') # string may include a decimal exponent\n        Fraction(-47, 100)\n        >>> Fraction(1.47)  # direct construction from float (exact conversion)\n        Fraction(6620291452234629, 4503599627370496)\n        >>> Fraction(2.25)\n        Fraction(9, 4)\n        >>> Fraction(Decimal('1.47'))\n        Fraction(147, 100)\n\n        \"\"\"\n        self = super(Fraction, cls).__new__(cls)\n\n        if denominator is None:\n            if isinstance(numerator, Rational):\n                self._numerator = numerator.numerator\n                self._denominator = numerator.denominator\n                return self\n\n            elif isinstance(numerator, float):\n                # Exact conversion from float\n                value = Fraction.from_float(numerator)\n                self._numerator = value._numerator\n                self._denominator = value._denominator\n                return self\n\n            elif isinstance(numerator, Decimal):\n                value = Fraction.from_decimal(numerator)\n                self._numerator = value._numerator\n                self._denominator = value._denominator\n                return self\n\n            elif isinstance(numerator, basestring):\n                # Handle construction from strings.\n                m = _RATIONAL_FORMAT.match(numerator)\n                if m is None:\n                    raise ValueError('Invalid literal for Fraction: %r' %\n                                     numerator)\n                numerator = int(m.group('num') or '0')\n                denom = m.group('denom')\n                if denom:\n                    denominator = int(denom)\n                else:\n                    denominator = 1\n                    decimal = m.group('decimal')\n                    if decimal:\n                        scale = 10**len(decimal)\n                        numerator = numerator * scale + int(decimal)\n                        denominator *= scale\n                    exp = m.group('exp')\n                    if exp:\n                        exp = int(exp)\n                        if exp >= 0:\n                            numerator *= 10**exp\n                        else:\n                            denominator *= 10**-exp\n                if m.group('sign') == '-':\n                    numerator = -numerator\n\n            else:\n                raise TypeError(\"argument should be a string \"\n                                \"or a Rational instance\")\n\n        elif (isinstance(numerator, Rational) and\n            isinstance(denominator, Rational)):\n            numerator, denominator = (\n                numerator.numerator * denominator.denominator,\n                denominator.numerator * numerator.denominator\n                )\n        else:\n            raise TypeError(\"both arguments should be \"\n                            \"Rational instances\")\n\n        if denominator == 0:\n            raise ZeroDivisionError('Fraction(%s, 0)' % numerator)\n        g = gcd(numerator, denominator)\n        self._numerator = numerator // g\n        self._denominator = denominator // g\n        return self\n\n    @classmethod\n    def from_float(cls, f):\n        \"\"\"Converts a finite float to a rational number, exactly.\n\n        Beware that Fraction.from_float(0.3) != Fraction(3, 10).\n\n        \"\"\"\n        if isinstance(f, numbers.Integral):\n            return cls(f)\n        elif not isinstance(f, float):\n            raise TypeError(\"%s.from_float() only takes floats, not %r (%s)\" %\n                            (cls.__name__, f, type(f).__name__))\n        if math.isnan(f) or math.isinf(f):\n            raise TypeError(\"Cannot convert %r to %s.\" % (f, cls.__name__))\n        return cls(*f.as_integer_ratio())\n\n    @classmethod\n    def from_decimal(cls, dec):\n        \"\"\"Converts a finite Decimal instance to a rational number, exactly.\"\"\"\n        from decimal import Decimal\n        if isinstance(dec, numbers.Integral):\n            dec = Decimal(int(dec))\n        elif not isinstance(dec, Decimal):\n            raise TypeError(\n                \"%s.from_decimal() only takes Decimals, not %r (%s)\" %\n                (cls.__name__, dec, type(dec).__name__))\n        if not dec.is_finite():\n            # Catches infinities and nans.\n            raise TypeError(\"Cannot convert %s to %s.\" % (dec, cls.__name__))\n        sign, digits, exp = dec.as_tuple()\n        digits = int(''.join(map(str, digits)))\n        if sign:\n            digits = -digits\n        if exp >= 0:\n            return cls(digits * 10 ** exp)\n        else:\n            return cls(digits, 10 ** -exp)\n\n    def limit_denominator(self, max_denominator=1000000):\n        \"\"\"Closest Fraction to self with denominator at most max_denominator.\n\n        >>> Fraction('3.141592653589793').limit_denominator(10)\n        Fraction(22, 7)\n        >>> Fraction('3.141592653589793').limit_denominator(100)\n        Fraction(311, 99)\n        >>> Fraction(4321, 8765).limit_denominator(10000)\n        Fraction(4321, 8765)\n\n        \"\"\"\n        # Algorithm notes: For any real number x, define a *best upper\n        # approximation* to x to be a rational number p/q such that:\n        #\n        #   (1) p/q >= x, and\n        #   (2) if p/q > r/s >= x then s > q, for any rational r/s.\n        #\n        # Define *best lower approximation* similarly.  Then it can be\n        # proved that a rational number is a best upper or lower\n        # approximation to x if, and only if, it is a convergent or\n        # semiconvergent of the (unique shortest) continued fraction\n        # associated to x.\n        #\n        # To find a best rational approximation with denominator <= M,\n        # we find the best upper and lower approximations with\n        # denominator <= M and take whichever of these is closer to x.\n        # In the event of a tie, the bound with smaller denominator is\n        # chosen.  If both denominators are equal (which can happen\n        # only when max_denominator == 1 and self is midway between\n        # two integers) the lower bound---i.e., the floor of self, is\n        # taken.\n\n        if max_denominator < 1:\n            raise ValueError(\"max_denominator should be at least 1\")\n        if self._denominator <= max_denominator:\n            return Fraction(self)\n\n        p0, q0, p1, q1 = 0, 1, 1, 0\n        n, d = self._numerator, self._denominator\n        while True:\n            a = n//d\n            q2 = q0+a*q1\n            if q2 > max_denominator:\n                break\n            p0, q0, p1, q1 = p1, q1, p0+a*p1, q2\n            n, d = d, n-a*d\n\n        k = (max_denominator-q0)//q1\n        bound1 = Fraction(p0+k*p1, q0+k*q1)\n        bound2 = Fraction(p1, q1)\n        if abs(bound2 - self) <= abs(bound1-self):\n            return bound2\n        else:\n            return bound1\n\n    @property\n    def numerator(a):\n        return a._numerator\n\n    @property\n    def denominator(a):\n        return a._denominator\n\n    def __repr__(self):\n        \"\"\"repr(self)\"\"\"\n        return ('Fraction(%s, %s)' % (self._numerator, self._denominator))\n\n    def __str__(self):\n        \"\"\"str(self)\"\"\"\n        if self._denominator == 1:\n            return str(self._numerator)\n        else:\n            return '%s/%s' % (self._numerator, self._denominator)\n\n    def _operator_fallbacks(monomorphic_operator, fallback_operator):\n        \"\"\"Generates forward and reverse operators given a purely-rational\n        operator and a function from the operator module.\n\n        Use this like:\n        __op__, __rop__ = _operator_fallbacks(just_rational_op, operator.op)\n\n        In general, we want to implement the arithmetic operations so\n        that mixed-mode operations either call an implementation whose\n        author knew about the types of both arguments, or convert both\n        to the nearest built in type and do the operation there. In\n        Fraction, that means that we define __add__ and __radd__ as:\n\n            def __add__(self, other):\n                # Both types have numerators/denominator attributes,\n                # so do the operation directly\n                if isinstance(other, (int, long, Fraction)):\n                    return Fraction(self.numerator * other.denominator +\n                                    other.numerator * self.denominator,\n                                    self.denominator * other.denominator)\n                # float and complex don't have those operations, but we\n                # know about those types, so special case them.\n                elif isinstance(other, float):\n                    return float(self) + other\n                elif isinstance(other, complex):\n                    return complex(self) + other\n                # Let the other type take over.\n                return NotImplemented\n\n            def __radd__(self, other):\n                # radd handles more types than add because there's\n                # nothing left to fall back to.\n                if isinstance(other, Rational):\n                    return Fraction(self.numerator * other.denominator +\n                                    other.numerator * self.denominator,\n                                    self.denominator * other.denominator)\n                elif isinstance(other, Real):\n                    return float(other) + float(self)\n                elif isinstance(other, Complex):\n                    return complex(other) + complex(self)\n                return NotImplemented\n\n\n        There are 5 different cases for a mixed-type addition on\n        Fraction. I'll refer to all of the above code that doesn't\n        refer to Fraction, float, or complex as \"boilerplate\". 'r'\n        will be an instance of Fraction, which is a subtype of\n        Rational (r : Fraction <: Rational), and b : B <:\n        Complex. The first three involve 'r + b':\n\n            1. If B <: Fraction, int, float, or complex, we handle\n               that specially, and all is well.\n            2. If Fraction falls back to the boilerplate code, and it\n               were to return a value from __add__, we'd miss the\n               possibility that B defines a more intelligent __radd__,\n               so the boilerplate should return NotImplemented from\n               __add__. In particular, we don't handle Rational\n               here, even though we could get an exact answer, in case\n               the other type wants to do something special.\n            3. If B <: Fraction, Python tries B.__radd__ before\n               Fraction.__add__. This is ok, because it was\n               implemented with knowledge of Fraction, so it can\n               handle those instances before delegating to Real or\n               Complex.\n\n        The next two situations describe 'b + r'. We assume that b\n        didn't know about Fraction in its implementation, and that it\n        uses similar boilerplate code:\n\n            4. If B <: Rational, then __radd_ converts both to the\n               builtin rational type (hey look, that's us) and\n               proceeds.\n            5. Otherwise, __radd__ tries to find the nearest common\n               base ABC, and fall back to its builtin type. Since this\n               class doesn't subclass a concrete type, there's no\n               implementation to fall back to, so we need to try as\n               hard as possible to return an actual value, or the user\n               will get a TypeError.\n\n        \"\"\"\n        def forward(a, b):\n            if isinstance(b, (int, long, Fraction)):\n                return monomorphic_operator(a, b)\n            elif isinstance(b, float):\n                return fallback_operator(float(a), b)\n            elif isinstance(b, complex):\n                return fallback_operator(complex(a), b)\n            else:\n                return NotImplemented\n        forward.__name__ = '__' + fallback_operator.__name__ + '__'\n        forward.__doc__ = monomorphic_operator.__doc__\n\n        def reverse(b, a):\n            if isinstance(a, Rational):\n                # Includes ints.\n                return monomorphic_operator(a, b)\n            elif isinstance(a, numbers.Real):\n                return fallback_operator(float(a), float(b))\n            elif isinstance(a, numbers.Complex):\n                return fallback_operator(complex(a), complex(b))\n            else:\n                return NotImplemented\n        reverse.__name__ = '__r' + fallback_operator.__name__ + '__'\n        reverse.__doc__ = monomorphic_operator.__doc__\n\n        return forward, reverse\n\n    def _add(a, b):\n        \"\"\"a + b\"\"\"\n        return Fraction(a.numerator * b.denominator +\n                        b.numerator * a.denominator,\n                        a.denominator * b.denominator)\n\n    __add__, __radd__ = _operator_fallbacks(_add, operator.add)\n\n    def _sub(a, b):\n        \"\"\"a - b\"\"\"\n        return Fraction(a.numerator * b.denominator -\n                        b.numerator * a.denominator,\n                        a.denominator * b.denominator)\n\n    __sub__, __rsub__ = _operator_fallbacks(_sub, operator.sub)\n\n    def _mul(a, b):\n        \"\"\"a * b\"\"\"\n        return Fraction(a.numerator * b.numerator, a.denominator * b.denominator)\n\n    __mul__, __rmul__ = _operator_fallbacks(_mul, operator.mul)\n\n    def _div(a, b):\n        \"\"\"a / b\"\"\"\n        return Fraction(a.numerator * b.denominator,\n                        a.denominator * b.numerator)\n\n    __truediv__, __rtruediv__ = _operator_fallbacks(_div, operator.truediv)\n    __div__, __rdiv__ = _operator_fallbacks(_div, operator.div)\n\n    def __floordiv__(a, b):\n        \"\"\"a // b\"\"\"\n        # Will be math.floor(a / b) in 3.0.\n        div = a / b\n        if isinstance(div, Rational):\n            # trunc(math.floor(div)) doesn't work if the rational is\n            # more precise than a float because the intermediate\n            # rounding may cross an integer boundary.\n            return div.numerator // div.denominator\n        else:\n            return math.floor(div)\n\n    def __rfloordiv__(b, a):\n        \"\"\"a // b\"\"\"\n        # Will be math.floor(a / b) in 3.0.\n        div = a / b\n        if isinstance(div, Rational):\n            # trunc(math.floor(div)) doesn't work if the rational is\n            # more precise than a float because the intermediate\n            # rounding may cross an integer boundary.\n            return div.numerator // div.denominator\n        else:\n            return math.floor(div)\n\n    def __mod__(a, b):\n        \"\"\"a % b\"\"\"\n        div = a // b\n        return a - b * div\n\n    def __rmod__(b, a):\n        \"\"\"a % b\"\"\"\n        div = a // b\n        return a - b * div\n\n    def __pow__(a, b):\n        \"\"\"a ** b\n\n        If b is not an integer, the result will be a float or complex\n        since roots are generally irrational. If b is an integer, the\n        result will be rational.\n\n        \"\"\"\n        if isinstance(b, Rational):\n            if b.denominator == 1:\n                power = b.numerator\n                if power >= 0:\n                    return Fraction(a._numerator ** power,\n                                    a._denominator ** power)\n                else:\n                    return Fraction(a._denominator ** -power,\n                                    a._numerator ** -power)\n            else:\n                # A fractional power will generally produce an\n                # irrational number.\n                return float(a) ** float(b)\n        else:\n            return float(a) ** b\n\n    def __rpow__(b, a):\n        \"\"\"a ** b\"\"\"\n        if b._denominator == 1 and b._numerator >= 0:\n            # If a is an int, keep it that way if possible.\n            return a ** b._numerator\n\n        if isinstance(a, Rational):\n            return Fraction(a.numerator, a.denominator) ** b\n\n        if b._denominator == 1:\n            return a ** b._numerator\n\n        return a ** float(b)\n\n    def __pos__(a):\n        \"\"\"+a: Coerces a subclass instance to Fraction\"\"\"\n        return Fraction(a._numerator, a._denominator)\n\n    def __neg__(a):\n        \"\"\"-a\"\"\"\n        return Fraction(-a._numerator, a._denominator)\n\n    def __abs__(a):\n        \"\"\"abs(a)\"\"\"\n        return Fraction(abs(a._numerator), a._denominator)\n\n    def __trunc__(a):\n        \"\"\"trunc(a)\"\"\"\n        if a._numerator < 0:\n            return -(-a._numerator // a._denominator)\n        else:\n            return a._numerator // a._denominator\n\n    def __hash__(self):\n        \"\"\"hash(self)\n\n        Tricky because values that are exactly representable as a\n        float must have the same hash as that float.\n\n        \"\"\"\n        # XXX since this method is expensive, consider caching the result\n        if self._denominator == 1:\n            # Get integers right.\n            return hash(self._numerator)\n        # Expensive check, but definitely correct.\n        if self == float(self):\n            return hash(float(self))\n        else:\n            # Use tuple's hash to avoid a high collision rate on\n            # simple fractions.\n            return hash((self._numerator, self._denominator))\n\n    def __eq__(a, b):\n        \"\"\"a == b\"\"\"\n        if isinstance(b, Rational):\n            return (a._numerator == b.numerator and\n                    a._denominator == b.denominator)\n        if isinstance(b, numbers.Complex) and b.imag == 0:\n            b = b.real\n        if isinstance(b, float):\n            if math.isnan(b) or math.isinf(b):\n                # comparisons with an infinity or nan should behave in\n                # the same way for any finite a, so treat a as zero.\n                return 0.0 == b\n            else:\n                return a == a.from_float(b)\n        else:\n            # Since a doesn't know how to compare with b, let's give b\n            # a chance to compare itself with a.\n            return NotImplemented\n\n    def _richcmp(self, other, op):\n        \"\"\"Helper for comparison operators, for internal use only.\n\n        Implement comparison between a Rational instance `self`, and\n        either another Rational instance or a float `other`.  If\n        `other` is not a Rational instance or a float, return\n        NotImplemented. `op` should be one of the six standard\n        comparison operators.\n\n        \"\"\"\n        # convert other to a Rational instance where reasonable.\n        if isinstance(other, Rational):\n            return op(self._numerator * other.denominator,\n                      self._denominator * other.numerator)\n        # comparisons with complex should raise a TypeError, for consistency\n        # with int<->complex, float<->complex, and complex<->complex comparisons.\n        if isinstance(other, complex):\n            raise TypeError(\"no ordering relation is defined for complex numbers\")\n        if isinstance(other, float):\n            if math.isnan(other) or math.isinf(other):\n                return op(0.0, other)\n            else:\n                return op(self, self.from_float(other))\n        else:\n            return NotImplemented\n\n    def __lt__(a, b):\n        \"\"\"a < b\"\"\"\n        return a._richcmp(b, operator.lt)\n\n    def __gt__(a, b):\n        \"\"\"a > b\"\"\"\n        return a._richcmp(b, operator.gt)\n\n    def __le__(a, b):\n        \"\"\"a <= b\"\"\"\n        return a._richcmp(b, operator.le)\n\n    def __ge__(a, b):\n        \"\"\"a >= b\"\"\"\n        return a._richcmp(b, operator.ge)\n\n    def __nonzero__(a):\n        \"\"\"a != 0\"\"\"\n        return a._numerator != 0\n\n    # support for pickling, copy, and deepcopy\n\n    def __reduce__(self):\n        return (self.__class__, (str(self),))\n\n    def __copy__(self):\n        if type(self) == Fraction:\n            return self     # I'm immutable; therefore I am my own clone\n        return self.__class__(self._numerator, self._denominator)\n\n    def __deepcopy__(self, memo):\n        if type(self) == Fraction:\n            return self     # My components are also immutable\n        return self.__class__(self._numerator, self._denominator)\n", 
    "ftplib": "\"\"\"An FTP client class and some helper functions.\n\nBased on RFC 959: File Transfer Protocol (FTP), by J. Postel and J. Reynolds\n\nExample:\n\n>>> from ftplib import FTP\n>>> ftp = FTP('ftp.python.org') # connect to host, default port\n>>> ftp.login() # default, i.e.: user anonymous, passwd anonymous@\n'230 Guest login ok, access restrictions apply.'\n>>> ftp.retrlines('LIST') # list directory contents\ntotal 9\ndrwxr-xr-x   8 root     wheel        1024 Jan  3  1994 .\ndrwxr-xr-x   8 root     wheel        1024 Jan  3  1994 ..\ndrwxr-xr-x   2 root     wheel        1024 Jan  3  1994 bin\ndrwxr-xr-x   2 root     wheel        1024 Jan  3  1994 etc\nd-wxrwxr-x   2 ftp      wheel        1024 Sep  5 13:43 incoming\ndrwxr-xr-x   2 root     wheel        1024 Nov 17  1993 lib\ndrwxr-xr-x   6 1094     wheel        1024 Sep 13 19:07 pub\ndrwxr-xr-x   3 root     wheel        1024 Jan  3  1994 usr\n-rw-r--r--   1 root     root          312 Aug  1  1994 welcome.msg\n'226 Transfer complete.'\n>>> ftp.quit()\n'221 Goodbye.'\n>>>\n\nA nice test that reveals some of the network dialogue would be:\npython ftplib.py -d localhost -l -p -l\n\"\"\"\n\n#\n# Changes and improvements suggested by Steve Majewski.\n# Modified by Jack to work on the mac.\n# Modified by Siebren to support docstrings and PASV.\n# Modified by Phil Schwartz to add storbinary and storlines callbacks.\n# Modified by Giampaolo Rodola' to add TLS support.\n#\n\nimport os\nimport sys\n\n# Import SOCKS module if it exists, else standard socket module socket\ntry:\n    import SOCKS; socket = SOCKS; del SOCKS # import SOCKS as socket\n    from socket import getfqdn; socket.getfqdn = getfqdn; del getfqdn\nexcept ImportError:\n    import socket\nfrom socket import _GLOBAL_DEFAULT_TIMEOUT\n\n__all__ = [\"FTP\",\"Netrc\"]\n\n# Magic number from <socket.h>\nMSG_OOB = 0x1                           # Process data out of band\n\n\n# The standard FTP server control port\nFTP_PORT = 21\n# The sizehint parameter passed to readline() calls\nMAXLINE = 8192\n\n\n# Exception raised when an error or invalid response is received\nclass Error(Exception): pass\nclass error_reply(Error): pass          # unexpected [123]xx reply\nclass error_temp(Error): pass           # 4xx errors\nclass error_perm(Error): pass           # 5xx errors\nclass error_proto(Error): pass          # response does not begin with [1-5]\n\n\n# All exceptions (hopefully) that may be raised here and that aren't\n# (always) programming errors on our side\nall_errors = (Error, IOError, EOFError)\n\n\n# Line terminators (we always output CRLF, but accept any of CRLF, CR, LF)\nCRLF = '\\r\\n'\n\n# The class itself\nclass FTP:\n\n    '''An FTP client class.\n\n    To create a connection, call the class using these arguments:\n            host, user, passwd, acct, timeout\n\n    The first four arguments are all strings, and have default value ''.\n    timeout must be numeric and defaults to None if not passed,\n    meaning that no timeout will be set on any ftp socket(s)\n    If a timeout is passed, then this is now the default timeout for all ftp\n    socket operations for this instance.\n\n    Then use self.connect() with optional host and port argument.\n\n    To download a file, use ftp.retrlines('RETR ' + filename),\n    or ftp.retrbinary() with slightly different arguments.\n    To upload a file, use ftp.storlines() or ftp.storbinary(),\n    which have an open file as argument (see their definitions\n    below for details).\n    The download/upload functions first issue appropriate TYPE\n    and PORT or PASV commands.\n'''\n\n    debugging = 0\n    host = ''\n    port = FTP_PORT\n    maxline = MAXLINE\n    sock = None\n    file = None\n    welcome = None\n    passiveserver = 1\n\n    # Initialization method (called by class instantiation).\n    # Initialize host to localhost, port to standard ftp port\n    # Optional arguments are host (for connect()),\n    # and user, passwd, acct (for login())\n    def __init__(self, host='', user='', passwd='', acct='',\n                 timeout=_GLOBAL_DEFAULT_TIMEOUT):\n        self.timeout = timeout\n        if host:\n            self.connect(host)\n            if user:\n                self.login(user, passwd, acct)\n\n    def connect(self, host='', port=0, timeout=-999):\n        '''Connect to host.  Arguments are:\n         - host: hostname to connect to (string, default previous host)\n         - port: port to connect to (integer, default previous port)\n        '''\n        if host != '':\n            self.host = host\n        if port > 0:\n            self.port = port\n        if timeout != -999:\n            self.timeout = timeout\n        self.sock = socket.create_connection((self.host, self.port), self.timeout)\n        self.af = self.sock.family\n        self.file = self.sock.makefile('rb')\n        self.welcome = self.getresp()\n        return self.welcome\n\n    def getwelcome(self):\n        '''Get the welcome message from the server.\n        (this is read and squirreled away by connect())'''\n        if self.debugging:\n            print '*welcome*', self.sanitize(self.welcome)\n        return self.welcome\n\n    def set_debuglevel(self, level):\n        '''Set the debugging level.\n        The required argument level means:\n        0: no debugging output (default)\n        1: print commands and responses but not body text etc.\n        2: also print raw lines read and sent before stripping CR/LF'''\n        self.debugging = level\n    debug = set_debuglevel\n\n    def set_pasv(self, val):\n        '''Use passive or active mode for data transfers.\n        With a false argument, use the normal PORT mode,\n        With a true argument, use the PASV command.'''\n        self.passiveserver = val\n\n    # Internal: \"sanitize\" a string for printing\n    def sanitize(self, s):\n        if s[:5] == 'pass ' or s[:5] == 'PASS ':\n            i = len(s)\n            while i > 5 and s[i-1] in '\\r\\n':\n                i = i-1\n            s = s[:5] + '*'*(i-5) + s[i:]\n        return repr(s)\n\n    # Internal: send one line to the server, appending CRLF\n    def putline(self, line):\n        line = line + CRLF\n        if self.debugging > 1: print '*put*', self.sanitize(line)\n        self.sock.sendall(line)\n\n    # Internal: send one command to the server (through putline())\n    def putcmd(self, line):\n        if self.debugging: print '*cmd*', self.sanitize(line)\n        self.putline(line)\n\n    # Internal: return one line from the server, stripping CRLF.\n    # Raise EOFError if the connection is closed\n    def getline(self):\n        line = self.file.readline(self.maxline + 1)\n        if len(line) > self.maxline:\n            raise Error(\"got more than %d bytes\" % self.maxline)\n        if self.debugging > 1:\n            print '*get*', self.sanitize(line)\n        if not line: raise EOFError\n        if line[-2:] == CRLF: line = line[:-2]\n        elif line[-1:] in CRLF: line = line[:-1]\n        return line\n\n    # Internal: get a response from the server, which may possibly\n    # consist of multiple lines.  Return a single string with no\n    # trailing CRLF.  If the response consists of multiple lines,\n    # these are separated by '\\n' characters in the string\n    def getmultiline(self):\n        line = self.getline()\n        if line[3:4] == '-':\n            code = line[:3]\n            while 1:\n                nextline = self.getline()\n                line = line + ('\\n' + nextline)\n                if nextline[:3] == code and \\\n                        nextline[3:4] != '-':\n                    break\n        return line\n\n    # Internal: get a response from the server.\n    # Raise various errors if the response indicates an error\n    def getresp(self):\n        resp = self.getmultiline()\n        if self.debugging: print '*resp*', self.sanitize(resp)\n        self.lastresp = resp[:3]\n        c = resp[:1]\n        if c in ('1', '2', '3'):\n            return resp\n        if c == '4':\n            raise error_temp, resp\n        if c == '5':\n            raise error_perm, resp\n        raise error_proto, resp\n\n    def voidresp(self):\n        \"\"\"Expect a response beginning with '2'.\"\"\"\n        resp = self.getresp()\n        if resp[:1] != '2':\n            raise error_reply, resp\n        return resp\n\n    def abort(self):\n        '''Abort a file transfer.  Uses out-of-band data.\n        This does not follow the procedure from the RFC to send Telnet\n        IP and Synch; that doesn't seem to work with the servers I've\n        tried.  Instead, just send the ABOR command as OOB data.'''\n        line = 'ABOR' + CRLF\n        if self.debugging > 1: print '*put urgent*', self.sanitize(line)\n        self.sock.sendall(line, MSG_OOB)\n        resp = self.getmultiline()\n        if resp[:3] not in ('426', '225', '226'):\n            raise error_proto, resp\n\n    def sendcmd(self, cmd):\n        '''Send a command and return the response.'''\n        self.putcmd(cmd)\n        return self.getresp()\n\n    def voidcmd(self, cmd):\n        \"\"\"Send a command and expect a response beginning with '2'.\"\"\"\n        self.putcmd(cmd)\n        return self.voidresp()\n\n    def sendport(self, host, port):\n        '''Send a PORT command with the current host and the given\n        port number.\n        '''\n        hbytes = host.split('.')\n        pbytes = [repr(port//256), repr(port%256)]\n        bytes = hbytes + pbytes\n        cmd = 'PORT ' + ','.join(bytes)\n        return self.voidcmd(cmd)\n\n    def sendeprt(self, host, port):\n        '''Send a EPRT command with the current host and the given port number.'''\n        af = 0\n        if self.af == socket.AF_INET:\n            af = 1\n        if self.af == socket.AF_INET6:\n            af = 2\n        if af == 0:\n            raise error_proto, 'unsupported address family'\n        fields = ['', repr(af), host, repr(port), '']\n        cmd = 'EPRT ' + '|'.join(fields)\n        return self.voidcmd(cmd)\n\n    def makeport(self):\n        '''Create a new socket and send a PORT command for it.'''\n        err = None\n        sock = None\n        for res in socket.getaddrinfo(None, 0, self.af, socket.SOCK_STREAM, 0, socket.AI_PASSIVE):\n            af, socktype, proto, canonname, sa = res\n            try:\n                sock = socket.socket(af, socktype, proto)\n                sock.bind(sa)\n            except socket.error, err:\n                if sock:\n                    sock.close()\n                sock = None\n                continue\n            break\n        if sock is None:\n            if err is not None:\n                raise err\n            else:\n                raise socket.error(\"getaddrinfo returns an empty list\")\n        sock.listen(1)\n        port = sock.getsockname()[1] # Get proper port\n        host = self.sock.getsockname()[0] # Get proper host\n        if self.af == socket.AF_INET:\n            resp = self.sendport(host, port)\n        else:\n            resp = self.sendeprt(host, port)\n        if self.timeout is not _GLOBAL_DEFAULT_TIMEOUT:\n            sock.settimeout(self.timeout)\n        return sock\n\n    def makepasv(self):\n        if self.af == socket.AF_INET:\n            host, port = parse227(self.sendcmd('PASV'))\n        else:\n            host, port = parse229(self.sendcmd('EPSV'), self.sock.getpeername())\n        return host, port\n\n    def ntransfercmd(self, cmd, rest=None):\n        \"\"\"Initiate a transfer over the data connection.\n\n        If the transfer is active, send a port command and the\n        transfer command, and accept the connection.  If the server is\n        passive, send a pasv command, connect to it, and start the\n        transfer command.  Either way, return the socket for the\n        connection and the expected size of the transfer.  The\n        expected size may be None if it could not be determined.\n\n        Optional `rest' argument can be a string that is sent as the\n        argument to a REST command.  This is essentially a server\n        marker used to tell the server to skip over any data up to the\n        given marker.\n        \"\"\"\n        size = None\n        if self.passiveserver:\n            host, port = self.makepasv()\n            conn = socket.create_connection((host, port), self.timeout)\n            try:\n                if rest is not None:\n                    self.sendcmd(\"REST %s\" % rest)\n                resp = self.sendcmd(cmd)\n                # Some servers apparently send a 200 reply to\n                # a LIST or STOR command, before the 150 reply\n                # (and way before the 226 reply). This seems to\n                # be in violation of the protocol (which only allows\n                # 1xx or error messages for LIST), so we just discard\n                # this response.\n                if resp[0] == '2':\n                    resp = self.getresp()\n                if resp[0] != '1':\n                    raise error_reply, resp\n            except:\n                conn.close()\n                raise\n        else:\n            sock = self.makeport()\n            try:\n                if rest is not None:\n                    self.sendcmd(\"REST %s\" % rest)\n                resp = self.sendcmd(cmd)\n                # See above.\n                if resp[0] == '2':\n                    resp = self.getresp()\n                if resp[0] != '1':\n                    raise error_reply, resp\n                conn, sockaddr = sock.accept()\n                if self.timeout is not _GLOBAL_DEFAULT_TIMEOUT:\n                    conn.settimeout(self.timeout)\n            finally:\n                sock.close()\n        if resp[:3] == '150':\n            # this is conditional in case we received a 125\n            size = parse150(resp)\n        return conn, size\n\n    def transfercmd(self, cmd, rest=None):\n        \"\"\"Like ntransfercmd() but returns only the socket.\"\"\"\n        return self.ntransfercmd(cmd, rest)[0]\n\n    def login(self, user = '', passwd = '', acct = ''):\n        '''Login, default anonymous.'''\n        if not user: user = 'anonymous'\n        if not passwd: passwd = ''\n        if not acct: acct = ''\n        if user == 'anonymous' and passwd in ('', '-'):\n            # If there is no anonymous ftp password specified\n            # then we'll just use anonymous@\n            # We don't send any other thing because:\n            # - We want to remain anonymous\n            # - We want to stop SPAM\n            # - We don't want to let ftp sites to discriminate by the user,\n            #   host or country.\n            passwd = passwd + 'anonymous@'\n        resp = self.sendcmd('USER ' + user)\n        if resp[0] == '3': resp = self.sendcmd('PASS ' + passwd)\n        if resp[0] == '3': resp = self.sendcmd('ACCT ' + acct)\n        if resp[0] != '2':\n            raise error_reply, resp\n        return resp\n\n    def retrbinary(self, cmd, callback, blocksize=8192, rest=None):\n        \"\"\"Retrieve data in binary mode.  A new port is created for you.\n\n        Args:\n          cmd: A RETR command.\n          callback: A single parameter callable to be called on each\n                    block of data read.\n          blocksize: The maximum number of bytes to read from the\n                     socket at one time.  [default: 8192]\n          rest: Passed to transfercmd().  [default: None]\n\n        Returns:\n          The response code.\n        \"\"\"\n        self.voidcmd('TYPE I')\n        conn = self.transfercmd(cmd, rest)\n        while 1:\n            data = conn.recv(blocksize)\n            if not data:\n                break\n            callback(data)\n        conn.close()\n        return self.voidresp()\n\n    def retrlines(self, cmd, callback = None):\n        \"\"\"Retrieve data in line mode.  A new port is created for you.\n\n        Args:\n          cmd: A RETR, LIST, NLST, or MLSD command.\n          callback: An optional single parameter callable that is called\n                    for each line with the trailing CRLF stripped.\n                    [default: print_line()]\n\n        Returns:\n          The response code.\n        \"\"\"\n        if callback is None: callback = print_line\n        resp = self.sendcmd('TYPE A')\n        conn = self.transfercmd(cmd)\n        fp = conn.makefile('rb')\n        while 1:\n            line = fp.readline(self.maxline + 1)\n            if len(line) > self.maxline:\n                raise Error(\"got more than %d bytes\" % self.maxline)\n            if self.debugging > 2: print '*retr*', repr(line)\n            if not line:\n                break\n            if line[-2:] == CRLF:\n                line = line[:-2]\n            elif line[-1:] == '\\n':\n                line = line[:-1]\n            callback(line)\n        fp.close()\n        conn.close()\n        return self.voidresp()\n\n    def storbinary(self, cmd, fp, blocksize=8192, callback=None, rest=None):\n        \"\"\"Store a file in binary mode.  A new port is created for you.\n\n        Args:\n          cmd: A STOR command.\n          fp: A file-like object with a read(num_bytes) method.\n          blocksize: The maximum data size to read from fp and send over\n                     the connection at once.  [default: 8192]\n          callback: An optional single parameter callable that is called on\n                    each block of data after it is sent.  [default: None]\n          rest: Passed to transfercmd().  [default: None]\n\n        Returns:\n          The response code.\n        \"\"\"\n        self.voidcmd('TYPE I')\n        conn = self.transfercmd(cmd, rest)\n        while 1:\n            buf = fp.read(blocksize)\n            if not buf: break\n            conn.sendall(buf)\n            if callback: callback(buf)\n        conn.close()\n        return self.voidresp()\n\n    def storlines(self, cmd, fp, callback=None):\n        \"\"\"Store a file in line mode.  A new port is created for you.\n\n        Args:\n          cmd: A STOR command.\n          fp: A file-like object with a readline() method.\n          callback: An optional single parameter callable that is called on\n                    each line after it is sent.  [default: None]\n\n        Returns:\n          The response code.\n        \"\"\"\n        self.voidcmd('TYPE A')\n        conn = self.transfercmd(cmd)\n        while 1:\n            buf = fp.readline(self.maxline + 1)\n            if len(buf) > self.maxline:\n                raise Error(\"got more than %d bytes\" % self.maxline)\n            if not buf: break\n            if buf[-2:] != CRLF:\n                if buf[-1] in CRLF: buf = buf[:-1]\n                buf = buf + CRLF\n            conn.sendall(buf)\n            if callback: callback(buf)\n        conn.close()\n        return self.voidresp()\n\n    def acct(self, password):\n        '''Send new account name.'''\n        cmd = 'ACCT ' + password\n        return self.voidcmd(cmd)\n\n    def nlst(self, *args):\n        '''Return a list of files in a given directory (default the current).'''\n        cmd = 'NLST'\n        for arg in args:\n            cmd = cmd + (' ' + arg)\n        files = []\n        self.retrlines(cmd, files.append)\n        return files\n\n    def dir(self, *args):\n        '''List a directory in long form.\n        By default list current directory to stdout.\n        Optional last argument is callback function; all\n        non-empty arguments before it are concatenated to the\n        LIST command.  (This *should* only be used for a pathname.)'''\n        cmd = 'LIST'\n        func = None\n        if args[-1:] and type(args[-1]) != type(''):\n            args, func = args[:-1], args[-1]\n        for arg in args:\n            if arg:\n                cmd = cmd + (' ' + arg)\n        self.retrlines(cmd, func)\n\n    def rename(self, fromname, toname):\n        '''Rename a file.'''\n        resp = self.sendcmd('RNFR ' + fromname)\n        if resp[0] != '3':\n            raise error_reply, resp\n        return self.voidcmd('RNTO ' + toname)\n\n    def delete(self, filename):\n        '''Delete a file.'''\n        resp = self.sendcmd('DELE ' + filename)\n        if resp[:3] in ('250', '200'):\n            return resp\n        else:\n            raise error_reply, resp\n\n    def cwd(self, dirname):\n        '''Change to a directory.'''\n        if dirname == '..':\n            try:\n                return self.voidcmd('CDUP')\n            except error_perm, msg:\n                if msg.args[0][:3] != '500':\n                    raise\n        elif dirname == '':\n            dirname = '.'  # does nothing, but could return error\n        cmd = 'CWD ' + dirname\n        return self.voidcmd(cmd)\n\n    def size(self, filename):\n        '''Retrieve the size of a file.'''\n        # The SIZE command is defined in RFC-3659\n        resp = self.sendcmd('SIZE ' + filename)\n        if resp[:3] == '213':\n            s = resp[3:].strip()\n            try:\n                return int(s)\n            except (OverflowError, ValueError):\n                return long(s)\n\n    def mkd(self, dirname):\n        '''Make a directory, return its full pathname.'''\n        resp = self.sendcmd('MKD ' + dirname)\n        return parse257(resp)\n\n    def rmd(self, dirname):\n        '''Remove a directory.'''\n        return self.voidcmd('RMD ' + dirname)\n\n    def pwd(self):\n        '''Return current working directory.'''\n        resp = self.sendcmd('PWD')\n        return parse257(resp)\n\n    def quit(self):\n        '''Quit, and close the connection.'''\n        resp = self.voidcmd('QUIT')\n        self.close()\n        return resp\n\n    def close(self):\n        '''Close the connection without assuming anything about it.'''\n        if self.file is not None:\n            self.file.close()\n        if self.sock is not None:\n            self.sock.close()\n        self.file = self.sock = None\n\ntry:\n    import ssl\nexcept ImportError:\n    pass\nelse:\n    class FTP_TLS(FTP):\n        '''A FTP subclass which adds TLS support to FTP as described\n        in RFC-4217.\n\n        Connect as usual to port 21 implicitly securing the FTP control\n        connection before authenticating.\n\n        Securing the data connection requires user to explicitly ask\n        for it by calling prot_p() method.\n\n        Usage example:\n        >>> from ftplib import FTP_TLS\n        >>> ftps = FTP_TLS('ftp.python.org')\n        >>> ftps.login()  # login anonymously previously securing control channel\n        '230 Guest login ok, access restrictions apply.'\n        >>> ftps.prot_p()  # switch to secure data connection\n        '200 Protection level set to P'\n        >>> ftps.retrlines('LIST')  # list directory content securely\n        total 9\n        drwxr-xr-x   8 root     wheel        1024 Jan  3  1994 .\n        drwxr-xr-x   8 root     wheel        1024 Jan  3  1994 ..\n        drwxr-xr-x   2 root     wheel        1024 Jan  3  1994 bin\n        drwxr-xr-x   2 root     wheel        1024 Jan  3  1994 etc\n        d-wxrwxr-x   2 ftp      wheel        1024 Sep  5 13:43 incoming\n        drwxr-xr-x   2 root     wheel        1024 Nov 17  1993 lib\n        drwxr-xr-x   6 1094     wheel        1024 Sep 13 19:07 pub\n        drwxr-xr-x   3 root     wheel        1024 Jan  3  1994 usr\n        -rw-r--r--   1 root     root          312 Aug  1  1994 welcome.msg\n        '226 Transfer complete.'\n        >>> ftps.quit()\n        '221 Goodbye.'\n        >>>\n        '''\n        ssl_version = ssl.PROTOCOL_TLSv1\n\n        def __init__(self, host='', user='', passwd='', acct='', keyfile=None,\n                     certfile=None, timeout=_GLOBAL_DEFAULT_TIMEOUT):\n            self.keyfile = keyfile\n            self.certfile = certfile\n            self._prot_p = False\n            FTP.__init__(self, host, user, passwd, acct, timeout)\n\n        def login(self, user='', passwd='', acct='', secure=True):\n            if secure and not isinstance(self.sock, ssl.SSLSocket):\n                self.auth()\n            return FTP.login(self, user, passwd, acct)\n\n        def auth(self):\n            '''Set up secure control connection by using TLS/SSL.'''\n            if isinstance(self.sock, ssl.SSLSocket):\n                raise ValueError(\"Already using TLS\")\n            if self.ssl_version == ssl.PROTOCOL_TLSv1:\n                resp = self.voidcmd('AUTH TLS')\n            else:\n                resp = self.voidcmd('AUTH SSL')\n            self.sock = ssl.wrap_socket(self.sock, self.keyfile, self.certfile,\n                                        ssl_version=self.ssl_version)\n            self.file = self.sock.makefile(mode='rb')\n            return resp\n\n        def prot_p(self):\n            '''Set up secure data connection.'''\n            # PROT defines whether or not the data channel is to be protected.\n            # Though RFC-2228 defines four possible protection levels,\n            # RFC-4217 only recommends two, Clear and Private.\n            # Clear (PROT C) means that no security is to be used on the\n            # data-channel, Private (PROT P) means that the data-channel\n            # should be protected by TLS.\n            # PBSZ command MUST still be issued, but must have a parameter of\n            # '0' to indicate that no buffering is taking place and the data\n            # connection should not be encapsulated.\n            self.voidcmd('PBSZ 0')\n            resp = self.voidcmd('PROT P')\n            self._prot_p = True\n            return resp\n\n        def prot_c(self):\n            '''Set up clear text data connection.'''\n            resp = self.voidcmd('PROT C')\n            self._prot_p = False\n            return resp\n\n        # --- Overridden FTP methods\n\n        def ntransfercmd(self, cmd, rest=None):\n            conn, size = FTP.ntransfercmd(self, cmd, rest)\n            if self._prot_p:\n                conn = ssl.wrap_socket(conn, self.keyfile, self.certfile,\n                                       ssl_version=self.ssl_version)\n            return conn, size\n\n        def retrbinary(self, cmd, callback, blocksize=8192, rest=None):\n            self.voidcmd('TYPE I')\n            conn = self.transfercmd(cmd, rest)\n            try:\n                while 1:\n                    data = conn.recv(blocksize)\n                    if not data:\n                        break\n                    callback(data)\n                # shutdown ssl layer\n                if isinstance(conn, ssl.SSLSocket):\n                    conn.unwrap()\n            finally:\n                conn.close()\n            return self.voidresp()\n\n        def retrlines(self, cmd, callback = None):\n            if callback is None: callback = print_line\n            resp = self.sendcmd('TYPE A')\n            conn = self.transfercmd(cmd)\n            fp = conn.makefile('rb')\n            try:\n                while 1:\n                    line = fp.readline(self.maxline + 1)\n                    if len(line) > self.maxline:\n                        raise Error(\"got more than %d bytes\" % self.maxline)\n                    if self.debugging > 2: print '*retr*', repr(line)\n                    if not line:\n                        break\n                    if line[-2:] == CRLF:\n                        line = line[:-2]\n                    elif line[-1:] == '\\n':\n                        line = line[:-1]\n                    callback(line)\n                # shutdown ssl layer\n                if isinstance(conn, ssl.SSLSocket):\n                    conn.unwrap()\n            finally:\n                fp.close()\n                conn.close()\n            return self.voidresp()\n\n        def storbinary(self, cmd, fp, blocksize=8192, callback=None, rest=None):\n            self.voidcmd('TYPE I')\n            conn = self.transfercmd(cmd, rest)\n            try:\n                while 1:\n                    buf = fp.read(blocksize)\n                    if not buf: break\n                    conn.sendall(buf)\n                    if callback: callback(buf)\n                # shutdown ssl layer\n                if isinstance(conn, ssl.SSLSocket):\n                    conn.unwrap()\n            finally:\n                conn.close()\n            return self.voidresp()\n\n        def storlines(self, cmd, fp, callback=None):\n            self.voidcmd('TYPE A')\n            conn = self.transfercmd(cmd)\n            try:\n                while 1:\n                    buf = fp.readline(self.maxline + 1)\n                    if len(buf) > self.maxline:\n                        raise Error(\"got more than %d bytes\" % self.maxline)\n                    if not buf: break\n                    if buf[-2:] != CRLF:\n                        if buf[-1] in CRLF: buf = buf[:-1]\n                        buf = buf + CRLF\n                    conn.sendall(buf)\n                    if callback: callback(buf)\n                # shutdown ssl layer\n                if isinstance(conn, ssl.SSLSocket):\n                    conn.unwrap()\n            finally:\n                conn.close()\n            return self.voidresp()\n\n    __all__.append('FTP_TLS')\n    all_errors = (Error, IOError, EOFError, ssl.SSLError)\n\n\n_150_re = None\n\ndef parse150(resp):\n    '''Parse the '150' response for a RETR request.\n    Returns the expected transfer size or None; size is not guaranteed to\n    be present in the 150 message.\n    '''\n    if resp[:3] != '150':\n        raise error_reply, resp\n    global _150_re\n    if _150_re is None:\n        import re\n        _150_re = re.compile(\"150 .* \\((\\d+) bytes\\)\", re.IGNORECASE)\n    m = _150_re.match(resp)\n    if not m:\n        return None\n    s = m.group(1)\n    try:\n        return int(s)\n    except (OverflowError, ValueError):\n        return long(s)\n\n\n_227_re = None\n\ndef parse227(resp):\n    '''Parse the '227' response for a PASV request.\n    Raises error_proto if it does not contain '(h1,h2,h3,h4,p1,p2)'\n    Return ('host.addr.as.numbers', port#) tuple.'''\n\n    if resp[:3] != '227':\n        raise error_reply, resp\n    global _227_re\n    if _227_re is None:\n        import re\n        _227_re = re.compile(r'(\\d+),(\\d+),(\\d+),(\\d+),(\\d+),(\\d+)')\n    m = _227_re.search(resp)\n    if not m:\n        raise error_proto, resp\n    numbers = m.groups()\n    host = '.'.join(numbers[:4])\n    port = (int(numbers[4]) << 8) + int(numbers[5])\n    return host, port\n\n\ndef parse229(resp, peer):\n    '''Parse the '229' response for a EPSV request.\n    Raises error_proto if it does not contain '(|||port|)'\n    Return ('host.addr.as.numbers', port#) tuple.'''\n\n    if resp[:3] != '229':\n        raise error_reply, resp\n    left = resp.find('(')\n    if left < 0: raise error_proto, resp\n    right = resp.find(')', left + 1)\n    if right < 0:\n        raise error_proto, resp # should contain '(|||port|)'\n    if resp[left + 1] != resp[right - 1]:\n        raise error_proto, resp\n    parts = resp[left + 1:right].split(resp[left+1])\n    if len(parts) != 5:\n        raise error_proto, resp\n    host = peer[0]\n    port = int(parts[3])\n    return host, port\n\n\ndef parse257(resp):\n    '''Parse the '257' response for a MKD or PWD request.\n    This is a response to a MKD or PWD request: a directory name.\n    Returns the directoryname in the 257 reply.'''\n\n    if resp[:3] != '257':\n        raise error_reply, resp\n    if resp[3:5] != ' \"':\n        return '' # Not compliant to RFC 959, but UNIX ftpd does this\n    dirname = ''\n    i = 5\n    n = len(resp)\n    while i < n:\n        c = resp[i]\n        i = i+1\n        if c == '\"':\n            if i >= n or resp[i] != '\"':\n                break\n            i = i+1\n        dirname = dirname + c\n    return dirname\n\n\ndef print_line(line):\n    '''Default retrlines callback to print a line.'''\n    print line\n\n\ndef ftpcp(source, sourcename, target, targetname = '', type = 'I'):\n    '''Copy file from one FTP-instance to another.'''\n    if not targetname: targetname = sourcename\n    type = 'TYPE ' + type\n    source.voidcmd(type)\n    target.voidcmd(type)\n    sourcehost, sourceport = parse227(source.sendcmd('PASV'))\n    target.sendport(sourcehost, sourceport)\n    # RFC 959: the user must \"listen\" [...] BEFORE sending the\n    # transfer request.\n    # So: STOR before RETR, because here the target is a \"user\".\n    treply = target.sendcmd('STOR ' + targetname)\n    if treply[:3] not in ('125', '150'): raise error_proto  # RFC 959\n    sreply = source.sendcmd('RETR ' + sourcename)\n    if sreply[:3] not in ('125', '150'): raise error_proto  # RFC 959\n    source.voidresp()\n    target.voidresp()\n\n\nclass Netrc:\n    \"\"\"Class to parse & provide access to 'netrc' format files.\n\n    See the netrc(4) man page for information on the file format.\n\n    WARNING: This class is obsolete -- use module netrc instead.\n\n    \"\"\"\n    __defuser = None\n    __defpasswd = None\n    __defacct = None\n\n    def __init__(self, filename=None):\n        if filename is None:\n            if \"HOME\" in os.environ:\n                filename = os.path.join(os.environ[\"HOME\"],\n                                        \".netrc\")\n            else:\n                raise IOError, \\\n                      \"specify file to load or set $HOME\"\n        self.__hosts = {}\n        self.__macros = {}\n        fp = open(filename, \"r\")\n        in_macro = 0\n        while 1:\n            line = fp.readline(self.maxline + 1)\n            if len(line) > self.maxline:\n                raise Error(\"got more than %d bytes\" % self.maxline)\n            if not line: break\n            if in_macro and line.strip():\n                macro_lines.append(line)\n                continue\n            elif in_macro:\n                self.__macros[macro_name] = tuple(macro_lines)\n                in_macro = 0\n            words = line.split()\n            host = user = passwd = acct = None\n            default = 0\n            i = 0\n            while i < len(words):\n                w1 = words[i]\n                if i+1 < len(words):\n                    w2 = words[i + 1]\n                else:\n                    w2 = None\n                if w1 == 'default':\n                    default = 1\n                elif w1 == 'machine' and w2:\n                    host = w2.lower()\n                    i = i + 1\n                elif w1 == 'login' and w2:\n                    user = w2\n                    i = i + 1\n                elif w1 == 'password' and w2:\n                    passwd = w2\n                    i = i + 1\n                elif w1 == 'account' and w2:\n                    acct = w2\n                    i = i + 1\n                elif w1 == 'macdef' and w2:\n                    macro_name = w2\n                    macro_lines = []\n                    in_macro = 1\n                    break\n                i = i + 1\n            if default:\n                self.__defuser = user or self.__defuser\n                self.__defpasswd = passwd or self.__defpasswd\n                self.__defacct = acct or self.__defacct\n            if host:\n                if host in self.__hosts:\n                    ouser, opasswd, oacct = \\\n                           self.__hosts[host]\n                    user = user or ouser\n                    passwd = passwd or opasswd\n                    acct = acct or oacct\n                self.__hosts[host] = user, passwd, acct\n        fp.close()\n\n    def get_hosts(self):\n        \"\"\"Return a list of hosts mentioned in the .netrc file.\"\"\"\n        return self.__hosts.keys()\n\n    def get_account(self, host):\n        \"\"\"Returns login information for the named host.\n\n        The return value is a triple containing userid,\n        password, and the accounting field.\n\n        \"\"\"\n        host = host.lower()\n        user = passwd = acct = None\n        if host in self.__hosts:\n            user, passwd, acct = self.__hosts[host]\n        user = user or self.__defuser\n        passwd = passwd or self.__defpasswd\n        acct = acct or self.__defacct\n        return user, passwd, acct\n\n    def get_macros(self):\n        \"\"\"Return a list of all defined macro names.\"\"\"\n        return self.__macros.keys()\n\n    def get_macro(self, macro):\n        \"\"\"Return a sequence of lines which define a named macro.\"\"\"\n        return self.__macros[macro]\n\n\n\ndef test():\n    '''Test program.\n    Usage: ftp [-d] [-r[file]] host [-l[dir]] [-d[dir]] [-p] [file] ...\n\n    -d dir\n    -l list\n    -p password\n    '''\n\n    if len(sys.argv) < 2:\n        print test.__doc__\n        sys.exit(0)\n\n    debugging = 0\n    rcfile = None\n    while sys.argv[1] == '-d':\n        debugging = debugging+1\n        del sys.argv[1]\n    if sys.argv[1][:2] == '-r':\n        # get name of alternate ~/.netrc file:\n        rcfile = sys.argv[1][2:]\n        del sys.argv[1]\n    host = sys.argv[1]\n    ftp = FTP(host)\n    ftp.set_debuglevel(debugging)\n    userid = passwd = acct = ''\n    try:\n        netrc = Netrc(rcfile)\n    except IOError:\n        if rcfile is not None:\n            sys.stderr.write(\"Could not open account file\"\n                             \" -- using anonymous login.\")\n    else:\n        try:\n            userid, passwd, acct = netrc.get_account(host)\n        except KeyError:\n            # no account for host\n            sys.stderr.write(\n                    \"No account -- using anonymous login.\")\n    ftp.login(userid, passwd, acct)\n    for file in sys.argv[2:]:\n        if file[:2] == '-l':\n            ftp.dir(file[2:])\n        elif file[:2] == '-d':\n            cmd = 'CWD'\n            if file[2:]: cmd = cmd + ' ' + file[2:]\n            resp = ftp.sendcmd(cmd)\n        elif file == '-p':\n            ftp.set_pasv(not ftp.passiveserver)\n        else:\n            ftp.retrbinary('RETR ' + file, \\\n                           sys.stdout.write, 1024)\n    ftp.quit()\n\n\nif __name__ == '__main__':\n    test()\n", 
    "functools": "\"\"\"functools.py - Tools for working with functions and callable objects\n\"\"\"\n# Python module wrapper for _functools C module\n# to allow utilities written in Python to be added\n# to the functools module.\n# Written by Nick Coghlan <ncoghlan at gmail.com>\n#   Copyright (C) 2006 Python Software Foundation.\n# See C source code for _functools credits/copyright\n\nfrom _functools import partial, reduce\n\n# update_wrapper() and wraps() are tools to help write\n# wrapper functions that can handle naive introspection\n\nWRAPPER_ASSIGNMENTS = ('__module__', '__name__', '__doc__')\nWRAPPER_UPDATES = ('__dict__',)\ndef update_wrapper(wrapper,\n                   wrapped,\n                   assigned = WRAPPER_ASSIGNMENTS,\n                   updated = WRAPPER_UPDATES):\n    \"\"\"Update a wrapper function to look like the wrapped function\n\n       wrapper is the function to be updated\n       wrapped is the original function\n       assigned is a tuple naming the attributes assigned directly\n       from the wrapped function to the wrapper function (defaults to\n       functools.WRAPPER_ASSIGNMENTS)\n       updated is a tuple naming the attributes of the wrapper that\n       are updated with the corresponding attribute from the wrapped\n       function (defaults to functools.WRAPPER_UPDATES)\n    \"\"\"\n    for attr in assigned:\n        setattr(wrapper, attr, getattr(wrapped, attr))\n    for attr in updated:\n        getattr(wrapper, attr).update(getattr(wrapped, attr, {}))\n    # Return the wrapper so this can be used as a decorator via partial()\n    return wrapper\n\ndef wraps(wrapped,\n          assigned = WRAPPER_ASSIGNMENTS,\n          updated = WRAPPER_UPDATES):\n    \"\"\"Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    \"\"\"\n    return partial(update_wrapper, wrapped=wrapped,\n                   assigned=assigned, updated=updated)\n\ndef total_ordering(cls):\n    \"\"\"Class decorator that fills in missing ordering methods\"\"\"\n    convert = {\n        '__lt__': [('__gt__', lambda self, other: not (self < other or self == other)),\n                   ('__le__', lambda self, other: self < other or self == other),\n                   ('__ge__', lambda self, other: not self < other)],\n        '__le__': [('__ge__', lambda self, other: not self <= other or self == other),\n                   ('__lt__', lambda self, other: self <= other and not self == other),\n                   ('__gt__', lambda self, other: not self <= other)],\n        '__gt__': [('__lt__', lambda self, other: not (self > other or self == other)),\n                   ('__ge__', lambda self, other: self > other or self == other),\n                   ('__le__', lambda self, other: not self > other)],\n        '__ge__': [('__le__', lambda self, other: (not self >= other) or self == other),\n                   ('__gt__', lambda self, other: self >= other and not self == other),\n                   ('__lt__', lambda self, other: not self >= other)]\n    }\n    roots = set(dir(cls)) & set(convert)\n    if not roots:\n        raise ValueError('must define at least one ordering operation: < > <= >=')\n    root = max(roots)       # prefer __lt__ to __le__ to __gt__ to __ge__\n    for opname, opfunc in convert[root]:\n        if opname not in roots:\n            opfunc.__name__ = opname\n            opfunc.__doc__ = getattr(int, opname).__doc__\n            setattr(cls, opname, opfunc)\n    return cls\n\ndef cmp_to_key(mycmp):\n    \"\"\"Convert a cmp= function into a key= function\"\"\"\n    class K(object):\n        __slots__ = ['obj']\n        def __init__(self, obj, *args):\n            self.obj = obj\n        def __lt__(self, other):\n            return mycmp(self.obj, other.obj) < 0\n        def __gt__(self, other):\n            return mycmp(self.obj, other.obj) > 0\n        def __eq__(self, other):\n            return mycmp(self.obj, other.obj) == 0\n        def __le__(self, other):\n            return mycmp(self.obj, other.obj) <= 0\n        def __ge__(self, other):\n            return mycmp(self.obj, other.obj) >= 0\n        def __ne__(self, other):\n            return mycmp(self.obj, other.obj) != 0\n        def __hash__(self):\n            raise TypeError('hash not implemented')\n    return K\n", 
    "genericpath": "\"\"\"\nPath operations common to more than one OS\nDo not use directly.  The OS specific modules import the appropriate\nfunctions from this module themselves.\n\"\"\"\nimport os\nimport stat\n\n__all__ = ['commonprefix', 'exists', 'getatime', 'getctime', 'getmtime',\n           'getsize', 'isdir', 'isfile']\n\n\n# Does a path exist?\n# This is false for dangling symbolic links on systems that support them.\ndef exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        os.stat(path)\n    except os.error:\n        return False\n    return True\n\n\n# This follows symbolic links, so both islink() and isdir() can be true\n# for the same path on systems that support symlinks\ndef isfile(path):\n    \"\"\"Test whether a path is a regular file\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return stat.S_ISREG(st.st_mode)\n\n\n# Is a path a directory?\n# This follows symbolic links, so both islink() and isdir()\n# can be true for the same path on systems that support symlinks\ndef isdir(s):\n    \"\"\"Return true if the pathname refers to an existing directory.\"\"\"\n    try:\n        st = os.stat(s)\n    except os.error:\n        return False\n    return stat.S_ISDIR(st.st_mode)\n\n\ndef getsize(filename):\n    \"\"\"Return the size of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_size\n\n\ndef getmtime(filename):\n    \"\"\"Return the last modification time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_mtime\n\n\ndef getatime(filename):\n    \"\"\"Return the last access time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_atime\n\n\ndef getctime(filename):\n    \"\"\"Return the metadata change time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_ctime\n\n\n# Return the longest prefix of all list elements.\ndef commonprefix(m):\n    \"Given a list of pathnames, returns the longest common leading component\"\n    if not m: return ''\n    s1 = min(m)\n    s2 = max(m)\n    for i, c in enumerate(s1):\n        if c != s2[i]:\n            return s1[:i]\n    return s1\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\n# Generic implementation of splitext, to be parametrized with\n# the separators\ndef _splitext(p, sep, altsep, extsep):\n    \"\"\"Split the extension from a pathname.\n\n    Extension is everything from the last dot to the end, ignoring\n    leading dots.  Returns \"(root, ext)\"; ext may be empty.\"\"\"\n\n    sepIndex = p.rfind(sep)\n    if altsep:\n        altsepIndex = p.rfind(altsep)\n        sepIndex = max(sepIndex, altsepIndex)\n\n    dotIndex = p.rfind(extsep)\n    if dotIndex > sepIndex:\n        # skip all leading dots\n        filenameIndex = sepIndex + 1\n        while filenameIndex < dotIndex:\n            if p[filenameIndex] != extsep:\n                return p[:dotIndex], p[dotIndex:]\n            filenameIndex += 1\n\n    return p, ''\n", 
    "getopt": "\"\"\"Parser for command line options.\n\nThis module helps scripts to parse the command line arguments in\nsys.argv.  It supports the same conventions as the Unix getopt()\nfunction (including the special meanings of arguments of the form `-'\nand `--').  Long options similar to those supported by GNU software\nmay be used as well via an optional third argument.  This module\nprovides two functions and an exception:\n\ngetopt() -- Parse command line options\ngnu_getopt() -- Like getopt(), but allow option and non-option arguments\nto be intermixed.\nGetoptError -- exception (class) raised with 'opt' attribute, which is the\noption involved with the exception.\n\"\"\"\n\n# Long option support added by Lars Wirzenius <liw@iki.fi>.\n#\n# Gerrit Holl <gerrit@nl.linux.org> moved the string-based exceptions\n# to class-based exceptions.\n#\n# Peter Astrand <astrand@lysator.liu.se> added gnu_getopt().\n#\n# TODO for gnu_getopt():\n#\n# - GNU getopt_long_only mechanism\n# - allow the caller to specify ordering\n# - RETURN_IN_ORDER option\n# - GNU extension with '-' as first character of option string\n# - optional arguments, specified by double colons\n# - a option string with a W followed by semicolon should\n#   treat \"-W foo\" as \"--foo\"\n\n__all__ = [\"GetoptError\",\"error\",\"getopt\",\"gnu_getopt\"]\n\nimport os\n\nclass GetoptError(Exception):\n    opt = ''\n    msg = ''\n    def __init__(self, msg, opt=''):\n        self.msg = msg\n        self.opt = opt\n        Exception.__init__(self, msg, opt)\n\n    def __str__(self):\n        return self.msg\n\nerror = GetoptError # backward compatibility\n\ndef getopt(args, shortopts, longopts = []):\n    \"\"\"getopt(args, options[, long_options]) -> opts, args\n\n    Parses command line options and parameter list.  args is the\n    argument list to be parsed, without the leading reference to the\n    running program.  Typically, this means \"sys.argv[1:]\".  shortopts\n    is the string of option letters that the script wants to\n    recognize, with options that require an argument followed by a\n    colon (i.e., the same format that Unix getopt() uses).  If\n    specified, longopts is a list of strings with the names of the\n    long options which should be supported.  The leading '--'\n    characters should not be included in the option name.  Options\n    which require an argument should be followed by an equal sign\n    ('=').\n\n    The return value consists of two elements: the first is a list of\n    (option, value) pairs; the second is the list of program arguments\n    left after the option list was stripped (this is a trailing slice\n    of the first argument).  Each option-and-value pair returned has\n    the option as its first element, prefixed with a hyphen (e.g.,\n    '-x'), and the option argument as its second element, or an empty\n    string if the option has no argument.  The options occur in the\n    list in the same order in which they were found, thus allowing\n    multiple occurrences.  Long and short options may be mixed.\n\n    \"\"\"\n\n    opts = []\n    if type(longopts) == type(\"\"):\n        longopts = [longopts]\n    else:\n        longopts = list(longopts)\n    while args and args[0].startswith('-') and args[0] != '-':\n        if args[0] == '--':\n            args = args[1:]\n            break\n        if args[0].startswith('--'):\n            opts, args = do_longs(opts, args[0][2:], longopts, args[1:])\n        else:\n            opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])\n\n    return opts, args\n\ndef gnu_getopt(args, shortopts, longopts = []):\n    \"\"\"getopt(args, options[, long_options]) -> opts, args\n\n    This function works like getopt(), except that GNU style scanning\n    mode is used by default. This means that option and non-option\n    arguments may be intermixed. The getopt() function stops\n    processing options as soon as a non-option argument is\n    encountered.\n\n    If the first character of the option string is `+', or if the\n    environment variable POSIXLY_CORRECT is set, then option\n    processing stops as soon as a non-option argument is encountered.\n\n    \"\"\"\n\n    opts = []\n    prog_args = []\n    if isinstance(longopts, str):\n        longopts = [longopts]\n    else:\n        longopts = list(longopts)\n\n    # Allow options after non-option arguments?\n    if shortopts.startswith('+'):\n        shortopts = shortopts[1:]\n        all_options_first = True\n    elif os.environ.get(\"POSIXLY_CORRECT\"):\n        all_options_first = True\n    else:\n        all_options_first = False\n\n    while args:\n        if args[0] == '--':\n            prog_args += args[1:]\n            break\n\n        if args[0][:2] == '--':\n            opts, args = do_longs(opts, args[0][2:], longopts, args[1:])\n        elif args[0][:1] == '-' and args[0] != '-':\n            opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])\n        else:\n            if all_options_first:\n                prog_args += args\n                break\n            else:\n                prog_args.append(args[0])\n                args = args[1:]\n\n    return opts, prog_args\n\ndef do_longs(opts, opt, longopts, args):\n    try:\n        i = opt.index('=')\n    except ValueError:\n        optarg = None\n    else:\n        opt, optarg = opt[:i], opt[i+1:]\n\n    has_arg, opt = long_has_args(opt, longopts)\n    if has_arg:\n        if optarg is None:\n            if not args:\n                raise GetoptError('option --%s requires argument' % opt, opt)\n            optarg, args = args[0], args[1:]\n    elif optarg is not None:\n        raise GetoptError('option --%s must not have an argument' % opt, opt)\n    opts.append(('--' + opt, optarg or ''))\n    return opts, args\n\n# Return:\n#   has_arg?\n#   full option name\ndef long_has_args(opt, longopts):\n    possibilities = [o for o in longopts if o.startswith(opt)]\n    if not possibilities:\n        raise GetoptError('option --%s not recognized' % opt, opt)\n    # Is there an exact match?\n    if opt in possibilities:\n        return False, opt\n    elif opt + '=' in possibilities:\n        return True, opt\n    # No exact match, so better be unique.\n    if len(possibilities) > 1:\n        # XXX since possibilities contains all valid continuations, might be\n        # nice to work them into the error msg\n        raise GetoptError('option --%s not a unique prefix' % opt, opt)\n    assert len(possibilities) == 1\n    unique_match = possibilities[0]\n    has_arg = unique_match.endswith('=')\n    if has_arg:\n        unique_match = unique_match[:-1]\n    return has_arg, unique_match\n\ndef do_shorts(opts, optstring, shortopts, args):\n    while optstring != '':\n        opt, optstring = optstring[0], optstring[1:]\n        if short_has_arg(opt, shortopts):\n            if optstring == '':\n                if not args:\n                    raise GetoptError('option -%s requires argument' % opt,\n                                      opt)\n                optstring, args = args[0], args[1:]\n            optarg, optstring = optstring, ''\n        else:\n            optarg = ''\n        opts.append(('-' + opt, optarg))\n    return opts, args\n\ndef short_has_arg(opt, shortopts):\n    for i in range(len(shortopts)):\n        if opt == shortopts[i] != ':':\n            return shortopts.startswith(':', i+1)\n    raise GetoptError('option -%s not recognized' % opt, opt)\n\nif __name__ == '__main__':\n    import sys\n    print getopt(sys.argv[1:], \"a:b\", [\"alpha=\", \"beta\"])\n", 
    "getpass": "\"\"\"Utilities to get a password and/or the current user name.\n\ngetpass(prompt[, stream]) - Prompt for a password, with echo turned off.\ngetuser() - Get the user name from the environment or password database.\n\nGetPassWarning - This UserWarning is issued when getpass() cannot prevent\n                 echoing of the password contents while reading.\n\nOn Windows, the msvcrt module will be used.\nOn the Mac EasyDialogs.AskPassword is used, if available.\n\n\"\"\"\n\n# Authors: Piers Lauder (original)\n#          Guido van Rossum (Windows support and cleanup)\n#          Gregory P. Smith (tty support & GetPassWarning)\n\nimport os, sys, warnings\n\n__all__ = [\"getpass\",\"getuser\",\"GetPassWarning\"]\n\n\nclass GetPassWarning(UserWarning): pass\n\n\ndef unix_getpass(prompt='Password: ', stream=None):\n    \"\"\"Prompt for a password, with echo turned off.\n\n    Args:\n      prompt: Written on stream to ask for the input.  Default: 'Password: '\n      stream: A writable file object to display the prompt.  Defaults to\n              the tty.  If no tty is available defaults to sys.stderr.\n    Returns:\n      The seKr3t input.\n    Raises:\n      EOFError: If our input tty or stdin was closed.\n      GetPassWarning: When we were unable to turn echo off on the input.\n\n    Always restores terminal settings before returning.\n    \"\"\"\n    fd = None\n    tty = None\n    try:\n        # Always try reading and writing directly on the tty first.\n        fd = os.open('/dev/tty', os.O_RDWR|os.O_NOCTTY)\n        tty = os.fdopen(fd, 'w+', 1)\n        input = tty\n        if not stream:\n            stream = tty\n    except EnvironmentError, e:\n        # If that fails, see if stdin can be controlled.\n        try:\n            fd = sys.stdin.fileno()\n        except (AttributeError, ValueError):\n            passwd = fallback_getpass(prompt, stream)\n        input = sys.stdin\n        if not stream:\n            stream = sys.stderr\n\n    if fd is not None:\n        passwd = None\n        try:\n            old = termios.tcgetattr(fd)     # a copy to save\n            new = old[:]\n            new[3] &= ~termios.ECHO  # 3 == 'lflags'\n            tcsetattr_flags = termios.TCSAFLUSH\n            if hasattr(termios, 'TCSASOFT'):\n                tcsetattr_flags |= termios.TCSASOFT\n            try:\n                termios.tcsetattr(fd, tcsetattr_flags, new)\n                passwd = _raw_input(prompt, stream, input=input)\n            finally:\n                termios.tcsetattr(fd, tcsetattr_flags, old)\n                stream.flush()  # issue7208\n        except termios.error, e:\n            if passwd is not None:\n                # _raw_input succeeded.  The final tcsetattr failed.  Reraise\n                # instead of leaving the terminal in an unknown state.\n                raise\n            # We can't control the tty or stdin.  Give up and use normal IO.\n            # fallback_getpass() raises an appropriate warning.\n            del input, tty  # clean up unused file objects before blocking\n            passwd = fallback_getpass(prompt, stream)\n\n    stream.write('\\n')\n    return passwd\n\n\ndef win_getpass(prompt='Password: ', stream=None):\n    \"\"\"Prompt for password with echo off, using Windows getch().\"\"\"\n    if sys.stdin is not sys.__stdin__:\n        return fallback_getpass(prompt, stream)\n    import msvcrt\n    for c in prompt:\n        msvcrt.putch(c)\n    pw = \"\"\n    while 1:\n        c = msvcrt.getch()\n        if c == '\\r' or c == '\\n':\n            break\n        if c == '\\003':\n            raise KeyboardInterrupt\n        if c == '\\b':\n            pw = pw[:-1]\n        else:\n            pw = pw + c\n    msvcrt.putch('\\r')\n    msvcrt.putch('\\n')\n    return pw\n\n\ndef fallback_getpass(prompt='Password: ', stream=None):\n    warnings.warn(\"Can not control echo on the terminal.\", GetPassWarning,\n                  stacklevel=2)\n    if not stream:\n        stream = sys.stderr\n    print >>stream, \"Warning: Password input may be echoed.\"\n    return _raw_input(prompt, stream)\n\n\ndef _raw_input(prompt=\"\", stream=None, input=None):\n    # A raw_input() replacement that doesn't save the string in the\n    # GNU readline history.\n    if not stream:\n        stream = sys.stderr\n    if not input:\n        input = sys.stdin\n    prompt = str(prompt)\n    if prompt:\n        stream.write(prompt)\n        stream.flush()\n    # NOTE: The Python C API calls flockfile() (and unlock) during readline.\n    line = input.readline()\n    if not line:\n        raise EOFError\n    if line[-1] == '\\n':\n        line = line[:-1]\n    return line\n\n\ndef getuser():\n    \"\"\"Get the username from the environment or password database.\n\n    First try various environment variables, then the password\n    database.  This works on Windows as long as USERNAME is set.\n\n    \"\"\"\n\n    import os\n\n    for name in ('LOGNAME', 'USER', 'LNAME', 'USERNAME'):\n        user = os.environ.get(name)\n        if user:\n            return user\n\n    # If this fails, the exception will \"explain\" why\n    import pwd\n    return pwd.getpwuid(os.getuid())[0]\n\n# Bind the name getpass to the appropriate function\ntry:\n    import termios\n    # it's possible there is an incompatible termios from the\n    # McMillan Installer, make sure we have a UNIX-compatible termios\n    termios.tcgetattr, termios.tcsetattr\nexcept (ImportError, AttributeError):\n    try:\n        import msvcrt\n    except ImportError:\n        try:\n            from EasyDialogs import AskPassword\n        except ImportError:\n            getpass = fallback_getpass\n        else:\n            getpass = AskPassword\n    else:\n        getpass = win_getpass\nelse:\n    getpass = unix_getpass\n", 
    "gettext": "\"\"\"Internationalization and localization support.\n\nThis module provides internationalization (I18N) and localization (L10N)\nsupport for your Python programs by providing an interface to the GNU gettext\nmessage catalog library.\n\nI18N refers to the operation by which a program is made aware of multiple\nlanguages.  L10N refers to the adaptation of your program, once\ninternationalized, to the local language and cultural habits.\n\n\"\"\"\n\n# This module represents the integration of work, contributions, feedback, and\n# suggestions from the following people:\n#\n# Martin von Loewis, who wrote the initial implementation of the underlying\n# C-based libintlmodule (later renamed _gettext), along with a skeletal\n# gettext.py implementation.\n#\n# Peter Funk, who wrote fintl.py, a fairly complete wrapper around intlmodule,\n# which also included a pure-Python implementation to read .mo files if\n# intlmodule wasn't available.\n#\n# James Henstridge, who also wrote a gettext.py module, which has some\n# interesting, but currently unsupported experimental features: the notion of\n# a Catalog class and instances, and the ability to add to a catalog file via\n# a Python API.\n#\n# Barry Warsaw integrated these modules, wrote the .install() API and code,\n# and conformed all C and Python code to Python's coding standards.\n#\n# Francois Pinard and Marc-Andre Lemburg also contributed valuably to this\n# module.\n#\n# J. David Ibanez implemented plural forms. Bruno Haible fixed some bugs.\n#\n# TODO:\n# - Lazy loading of .mo files.  Currently the entire catalog is loaded into\n#   memory, but that's probably bad for large translated programs.  Instead,\n#   the lexical sort of original strings in GNU .mo files should be exploited\n#   to do binary searches and lazy initializations.  Or you might want to use\n#   the undocumented double-hash algorithm for .mo files with hash tables, but\n#   you'll need to study the GNU gettext code to do this.\n#\n# - Support Solaris .mo file formats.  Unfortunately, we've been unable to\n#   find this format documented anywhere.\n\n\nimport locale, copy, os, re, struct, sys\nfrom errno import ENOENT\n\n\n__all__ = ['NullTranslations', 'GNUTranslations', 'Catalog',\n           'find', 'translation', 'install', 'textdomain', 'bindtextdomain',\n           'dgettext', 'dngettext', 'gettext', 'ngettext',\n           ]\n\n_default_localedir = os.path.join(sys.prefix, 'share', 'locale')\n\n\ndef test(condition, true, false):\n    \"\"\"\n    Implements the C expression:\n\n      condition ? true : false\n\n    Required to correctly interpret plural forms.\n    \"\"\"\n    if condition:\n        return true\n    else:\n        return false\n\n\ndef c2py(plural):\n    \"\"\"Gets a C expression as used in PO files for plural forms and returns a\n    Python lambda function that implements an equivalent expression.\n    \"\"\"\n    # Security check, allow only the \"n\" identifier\n    try:\n        from cStringIO import StringIO\n    except ImportError:\n        from StringIO import StringIO\n    import token, tokenize\n    tokens = tokenize.generate_tokens(StringIO(plural).readline)\n    try:\n        danger = [x for x in tokens if x[0] == token.NAME and x[1] != 'n']\n    except tokenize.TokenError:\n        raise ValueError, \\\n              'plural forms expression error, maybe unbalanced parenthesis'\n    else:\n        if danger:\n            raise ValueError, 'plural forms expression could be dangerous'\n\n    # Replace some C operators by their Python equivalents\n    plural = plural.replace('&&', ' and ')\n    plural = plural.replace('||', ' or ')\n\n    expr = re.compile(r'\\!([^=])')\n    plural = expr.sub(' not \\\\1', plural)\n\n    # Regular expression and replacement function used to transform\n    # \"a?b:c\" to \"test(a,b,c)\".\n    expr = re.compile(r'(.*?)\\?(.*?):(.*)')\n    def repl(x):\n        return \"test(%s, %s, %s)\" % (x.group(1), x.group(2),\n                                     expr.sub(repl, x.group(3)))\n\n    # Code to transform the plural expression, taking care of parentheses\n    stack = ['']\n    for c in plural:\n        if c == '(':\n            stack.append('')\n        elif c == ')':\n            if len(stack) == 1:\n                # Actually, we never reach this code, because unbalanced\n                # parentheses get caught in the security check at the\n                # beginning.\n                raise ValueError, 'unbalanced parenthesis in plural form'\n            s = expr.sub(repl, stack.pop())\n            stack[-1] += '(%s)' % s\n        else:\n            stack[-1] += c\n    plural = expr.sub(repl, stack.pop())\n\n    return eval('lambda n: int(%s)' % plural)\n\n\n\ndef _expand_lang(locale):\n    from locale import normalize\n    locale = normalize(locale)\n    COMPONENT_CODESET   = 1 << 0\n    COMPONENT_TERRITORY = 1 << 1\n    COMPONENT_MODIFIER  = 1 << 2\n    # split up the locale into its base components\n    mask = 0\n    pos = locale.find('@')\n    if pos >= 0:\n        modifier = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_MODIFIER\n    else:\n        modifier = ''\n    pos = locale.find('.')\n    if pos >= 0:\n        codeset = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_CODESET\n    else:\n        codeset = ''\n    pos = locale.find('_')\n    if pos >= 0:\n        territory = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_TERRITORY\n    else:\n        territory = ''\n    language = locale\n    ret = []\n    for i in range(mask+1):\n        if not (i & ~mask):  # if all components for this combo exist ...\n            val = language\n            if i & COMPONENT_TERRITORY: val += territory\n            if i & COMPONENT_CODESET:   val += codeset\n            if i & COMPONENT_MODIFIER:  val += modifier\n            ret.append(val)\n    ret.reverse()\n    return ret\n\n\n\nclass NullTranslations:\n    def __init__(self, fp=None):\n        self._info = {}\n        self._charset = None\n        self._output_charset = None\n        self._fallback = None\n        if fp is not None:\n            self._parse(fp)\n\n    def _parse(self, fp):\n        pass\n\n    def add_fallback(self, fallback):\n        if self._fallback:\n            self._fallback.add_fallback(fallback)\n        else:\n            self._fallback = fallback\n\n    def gettext(self, message):\n        if self._fallback:\n            return self._fallback.gettext(message)\n        return message\n\n    def lgettext(self, message):\n        if self._fallback:\n            return self._fallback.lgettext(message)\n        return message\n\n    def ngettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.ngettext(msgid1, msgid2, n)\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n\n    def lngettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.lngettext(msgid1, msgid2, n)\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n\n    def ugettext(self, message):\n        if self._fallback:\n            return self._fallback.ugettext(message)\n        return unicode(message)\n\n    def ungettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.ungettext(msgid1, msgid2, n)\n        if n == 1:\n            return unicode(msgid1)\n        else:\n            return unicode(msgid2)\n\n    def info(self):\n        return self._info\n\n    def charset(self):\n        return self._charset\n\n    def output_charset(self):\n        return self._output_charset\n\n    def set_output_charset(self, charset):\n        self._output_charset = charset\n\n    def install(self, unicode=False, names=None):\n        import __builtin__\n        __builtin__.__dict__['_'] = unicode and self.ugettext or self.gettext\n        if hasattr(names, \"__contains__\"):\n            if \"gettext\" in names:\n                __builtin__.__dict__['gettext'] = __builtin__.__dict__['_']\n            if \"ngettext\" in names:\n                __builtin__.__dict__['ngettext'] = (unicode and self.ungettext\n                                                             or self.ngettext)\n            if \"lgettext\" in names:\n                __builtin__.__dict__['lgettext'] = self.lgettext\n            if \"lngettext\" in names:\n                __builtin__.__dict__['lngettext'] = self.lngettext\n\n\nclass GNUTranslations(NullTranslations):\n    # Magic number of .mo files\n    LE_MAGIC = 0x950412deL\n    BE_MAGIC = 0xde120495L\n\n    def _parse(self, fp):\n        \"\"\"Override this method to support alternative .mo formats.\"\"\"\n        unpack = struct.unpack\n        filename = getattr(fp, 'name', '')\n        # Parse the .mo file header, which consists of 5 little endian 32\n        # bit words.\n        self._catalog = catalog = {}\n        self.plural = lambda n: int(n != 1) # germanic plural by default\n        buf = fp.read()\n        buflen = len(buf)\n        # Are we big endian or little endian?\n        magic = unpack('<I', buf[:4])[0]\n        if magic == self.LE_MAGIC:\n            version, msgcount, masteridx, transidx = unpack('<4I', buf[4:20])\n            ii = '<II'\n        elif magic == self.BE_MAGIC:\n            version, msgcount, masteridx, transidx = unpack('>4I', buf[4:20])\n            ii = '>II'\n        else:\n            raise IOError(0, 'Bad magic number', filename)\n        # Now put all messages from the .mo file buffer into the catalog\n        # dictionary.\n        for i in xrange(0, msgcount):\n            mlen, moff = unpack(ii, buf[masteridx:masteridx+8])\n            mend = moff + mlen\n            tlen, toff = unpack(ii, buf[transidx:transidx+8])\n            tend = toff + tlen\n            if mend < buflen and tend < buflen:\n                msg = buf[moff:mend]\n                tmsg = buf[toff:tend]\n            else:\n                raise IOError(0, 'File is corrupt', filename)\n            # See if we're looking at GNU .mo conventions for metadata\n            if mlen == 0:\n                # Catalog description\n                lastk = k = None\n                for item in tmsg.splitlines():\n                    item = item.strip()\n                    if not item:\n                        continue\n                    if ':' in item:\n                        k, v = item.split(':', 1)\n                        k = k.strip().lower()\n                        v = v.strip()\n                        self._info[k] = v\n                        lastk = k\n                    elif lastk:\n                        self._info[lastk] += '\\n' + item\n                    if k == 'content-type':\n                        self._charset = v.split('charset=')[1]\n                    elif k == 'plural-forms':\n                        v = v.split(';')\n                        plural = v[1].split('plural=')[1]\n                        self.plural = c2py(plural)\n            # Note: we unconditionally convert both msgids and msgstrs to\n            # Unicode using the character encoding specified in the charset\n            # parameter of the Content-Type header.  The gettext documentation\n            # strongly encourages msgids to be us-ascii, but some applications\n            # require alternative encodings (e.g. Zope's ZCML and ZPT).  For\n            # traditional gettext applications, the msgid conversion will\n            # cause no problems since us-ascii should always be a subset of\n            # the charset encoding.  We may want to fall back to 8-bit msgids\n            # if the Unicode conversion fails.\n            if '\\x00' in msg:\n                # Plural forms\n                msgid1, msgid2 = msg.split('\\x00')\n                tmsg = tmsg.split('\\x00')\n                if self._charset:\n                    msgid1 = unicode(msgid1, self._charset)\n                    tmsg = [unicode(x, self._charset) for x in tmsg]\n                for i in range(len(tmsg)):\n                    catalog[(msgid1, i)] = tmsg[i]\n            else:\n                if self._charset:\n                    msg = unicode(msg, self._charset)\n                    tmsg = unicode(tmsg, self._charset)\n                catalog[msg] = tmsg\n            # advance to next entry in the seek tables\n            masteridx += 8\n            transidx += 8\n\n    def gettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.gettext(message)\n            return message\n        # Encode the Unicode tmsg back to an 8-bit string, if possible\n        if self._output_charset:\n            return tmsg.encode(self._output_charset)\n        elif self._charset:\n            return tmsg.encode(self._charset)\n        return tmsg\n\n    def lgettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.lgettext(message)\n            return message\n        if self._output_charset:\n            return tmsg.encode(self._output_charset)\n        return tmsg.encode(locale.getpreferredencoding())\n\n    def ngettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n            if self._output_charset:\n                return tmsg.encode(self._output_charset)\n            elif self._charset:\n                return tmsg.encode(self._charset)\n            return tmsg\n        except KeyError:\n            if self._fallback:\n                return self._fallback.ngettext(msgid1, msgid2, n)\n            if n == 1:\n                return msgid1\n            else:\n                return msgid2\n\n    def lngettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n            if self._output_charset:\n                return tmsg.encode(self._output_charset)\n            return tmsg.encode(locale.getpreferredencoding())\n        except KeyError:\n            if self._fallback:\n                return self._fallback.lngettext(msgid1, msgid2, n)\n            if n == 1:\n                return msgid1\n            else:\n                return msgid2\n\n    def ugettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.ugettext(message)\n            return unicode(message)\n        return tmsg\n\n    def ungettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n        except KeyError:\n            if self._fallback:\n                return self._fallback.ungettext(msgid1, msgid2, n)\n            if n == 1:\n                tmsg = unicode(msgid1)\n            else:\n                tmsg = unicode(msgid2)\n        return tmsg\n\n\n# Locate a .mo file using the gettext strategy\ndef find(domain, localedir=None, languages=None, all=0):\n    # Get some reasonable defaults for arguments that were not supplied\n    if localedir is None:\n        localedir = _default_localedir\n    if languages is None:\n        languages = []\n        for envar in ('LANGUAGE', 'LC_ALL', 'LC_MESSAGES', 'LANG'):\n            val = os.environ.get(envar)\n            if val:\n                languages = val.split(':')\n                break\n        if 'C' not in languages:\n            languages.append('C')\n    # now normalize and expand the languages\n    nelangs = []\n    for lang in languages:\n        for nelang in _expand_lang(lang):\n            if nelang not in nelangs:\n                nelangs.append(nelang)\n    # select a language\n    if all:\n        result = []\n    else:\n        result = None\n    for lang in nelangs:\n        if lang == 'C':\n            break\n        mofile = os.path.join(localedir, lang, 'LC_MESSAGES', '%s.mo' % domain)\n        if os.path.exists(mofile):\n            if all:\n                result.append(mofile)\n            else:\n                return mofile\n    return result\n\n\n\n# a mapping between absolute .mo file path and Translation object\n_translations = {}\n\ndef translation(domain, localedir=None, languages=None,\n                class_=None, fallback=False, codeset=None):\n    if class_ is None:\n        class_ = GNUTranslations\n    mofiles = find(domain, localedir, languages, all=1)\n    if not mofiles:\n        if fallback:\n            return NullTranslations()\n        raise IOError(ENOENT, 'No translation file found for domain', domain)\n    # Avoid opening, reading, and parsing the .mo file after it's been done\n    # once.\n    result = None\n    for mofile in mofiles:\n        key = (class_, os.path.abspath(mofile))\n        t = _translations.get(key)\n        if t is None:\n            with open(mofile, 'rb') as fp:\n                t = _translations.setdefault(key, class_(fp))\n        # Copy the translation object to allow setting fallbacks and\n        # output charset. All other instance data is shared with the\n        # cached object.\n        t = copy.copy(t)\n        if codeset:\n            t.set_output_charset(codeset)\n        if result is None:\n            result = t\n        else:\n            result.add_fallback(t)\n    return result\n\n\ndef install(domain, localedir=None, unicode=False, codeset=None, names=None):\n    t = translation(domain, localedir, fallback=True, codeset=codeset)\n    t.install(unicode, names)\n\n\n\n# a mapping b/w domains and locale directories\n_localedirs = {}\n# a mapping b/w domains and codesets\n_localecodesets = {}\n# current global domain, `messages' used for compatibility w/ GNU gettext\n_current_domain = 'messages'\n\n\ndef textdomain(domain=None):\n    global _current_domain\n    if domain is not None:\n        _current_domain = domain\n    return _current_domain\n\n\ndef bindtextdomain(domain, localedir=None):\n    global _localedirs\n    if localedir is not None:\n        _localedirs[domain] = localedir\n    return _localedirs.get(domain, _default_localedir)\n\n\ndef bind_textdomain_codeset(domain, codeset=None):\n    global _localecodesets\n    if codeset is not None:\n        _localecodesets[domain] = codeset\n    return _localecodesets.get(domain)\n\n\ndef dgettext(domain, message):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        return message\n    return t.gettext(message)\n\ndef ldgettext(domain, message):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        return message\n    return t.lgettext(message)\n\ndef dngettext(domain, msgid1, msgid2, n):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n    return t.ngettext(msgid1, msgid2, n)\n\ndef ldngettext(domain, msgid1, msgid2, n):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n    return t.lngettext(msgid1, msgid2, n)\n\ndef gettext(message):\n    return dgettext(_current_domain, message)\n\ndef lgettext(message):\n    return ldgettext(_current_domain, message)\n\ndef ngettext(msgid1, msgid2, n):\n    return dngettext(_current_domain, msgid1, msgid2, n)\n\ndef lngettext(msgid1, msgid2, n):\n    return ldngettext(_current_domain, msgid1, msgid2, n)\n\n# dcgettext() has been deemed unnecessary and is not implemented.\n\n# James Henstridge's Catalog constructor from GNOME gettext.  Documented usage\n# was:\n#\n#    import gettext\n#    cat = gettext.Catalog(PACKAGE, localedir=LOCALEDIR)\n#    _ = cat.gettext\n#    print _('Hello World')\n\n# The resulting catalog object currently don't support access through a\n# dictionary API, which was supported (but apparently unused) in GNOME\n# gettext.\n\nCatalog = translation\n", 
    "hashlib": "# $Id$\n#\n#  Copyright (C) 2005   Gregory P. Smith (greg@krypto.org)\n#  Licensed to PSF under a Contributor Agreement.\n#\n\n__doc__ = \"\"\"hashlib module - A common interface to many hash functions.\n\nnew(name, string='') - returns a new hash object implementing the\n                       given hash function; initializing the hash\n                       using the given string data.\n\nNamed constructor functions are also available, these are much faster\nthan using new():\n\nmd5(), sha1(), sha224(), sha256(), sha384(), and sha512()\n\nMore algorithms may be available on your platform but the above are\nguaranteed to exist.\n\nNOTE: If you want the adler32 or crc32 hash functions they are available in\nthe zlib module.\n\nChoose your hash function wisely.  Some have known collision weaknesses.\nsha384 and sha512 will be slow on 32 bit platforms.\n\nHash objects have these methods:\n - update(arg): Update the hash object with the string arg. Repeated calls\n                are equivalent to a single call with the concatenation of all\n                the arguments.\n - digest():    Return the digest of the strings passed to the update() method\n                so far. This may contain non-ASCII characters, including\n                NUL bytes.\n - hexdigest(): Like digest() except the digest is returned as a string of\n                double length, containing only hexadecimal digits.\n - copy():      Return a copy (clone) of the hash object. This can be used to\n                efficiently compute the digests of strings that share a common\n                initial substring.\n\nFor example, to obtain the digest of the string 'Nobody inspects the\nspammish repetition':\n\n    >>> import hashlib\n    >>> m = hashlib.md5()\n    >>> m.update(\"Nobody inspects\")\n    >>> m.update(\" the spammish repetition\")\n    >>> m.digest()\n    '\\\\xbbd\\\\x9c\\\\x83\\\\xdd\\\\x1e\\\\xa5\\\\xc9\\\\xd9\\\\xde\\\\xc9\\\\xa1\\\\x8d\\\\xf0\\\\xff\\\\xe9'\n\nMore condensed:\n\n    >>> hashlib.sha224(\"Nobody inspects the spammish repetition\").hexdigest()\n    'a4337bc45a8fc544c03f52dc550cd6e1e87021bc896588bd79e901e2'\n\n\"\"\"\n\n# This tuple and __get_builtin_constructor() must be modified if a new\n# always available algorithm is added.\n__always_supported = ('md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512')\n\nalgorithms = __always_supported\n\n__all__ = __always_supported + ('new', 'algorithms', 'pbkdf2_hmac')\n\n\ndef __get_builtin_constructor(name):\n    try:\n        if name in ('SHA1', 'sha1'):\n            import _sha\n            return _sha.new\n        elif name in ('MD5', 'md5'):\n            import _md5\n            return _md5.new\n        elif name in ('SHA256', 'sha256', 'SHA224', 'sha224'):\n            import _sha256\n            bs = name[3:]\n            if bs == '256':\n                return _sha256.sha256\n            elif bs == '224':\n                return _sha256.sha224\n        elif name in ('SHA512', 'sha512', 'SHA384', 'sha384'):\n            import _sha512\n            bs = name[3:]\n            if bs == '512':\n                return _sha512.sha512\n            elif bs == '384':\n                return _sha512.sha384\n    except ImportError:\n        pass  # no extension module, this hash is unsupported.\n\n    raise ValueError('unsupported hash type ' + name)\n\n\ndef __get_openssl_constructor(name):\n    try:\n        f = getattr(_hashlib, 'openssl_' + name)\n        # Allow the C module to raise ValueError.  The function will be\n        # defined but the hash not actually available thanks to OpenSSL.\n        f()\n        # Use the C function directly (very fast)\n        return f\n    except (AttributeError, ValueError):\n        return __get_builtin_constructor(name)\n\n\ndef __py_new(name, string=''):\n    \"\"\"new(name, string='') - Return a new hashing object using the named algorithm;\n    optionally initialized with a string.\n    \"\"\"\n    return __get_builtin_constructor(name)(string)\n\n\ndef __hash_new(name, string=''):\n    \"\"\"new(name, string='') - Return a new hashing object using the named algorithm;\n    optionally initialized with a string.\n    \"\"\"\n    try:\n        return _hashlib.new(name, string)\n    except ValueError:\n        # If the _hashlib module (OpenSSL) doesn't support the named\n        # hash, try using our builtin implementations.\n        # This allows for SHA224/256 and SHA384/512 support even though\n        # the OpenSSL library prior to 0.9.8 doesn't provide them.\n        return __get_builtin_constructor(name)(string)\n\n\ntry:\n    import _hashlib\n    new = __hash_new\n    __get_hash = __get_openssl_constructor\nexcept ImportError:\n    new = __py_new\n    __get_hash = __get_builtin_constructor\n\nfor __func_name in __always_supported:\n    # try them all, some may not work due to the OpenSSL\n    # version not supporting that algorithm.\n    try:\n        globals()[__func_name] = __get_hash(__func_name)\n    except ValueError:\n        import logging\n        logging.exception('code for hash %s was not found.', __func_name)\n\n\ntry:\n    # OpenSSL's PKCS5_PBKDF2_HMAC requires OpenSSL 1.0+ with HMAC and SHA\n    from _hashlib import pbkdf2_hmac\nexcept ImportError:\n    import binascii\n    import struct\n\n    _trans_5C = b\"\".join(chr(x ^ 0x5C) for x in range(256))\n    _trans_36 = b\"\".join(chr(x ^ 0x36) for x in range(256))\n\n    def pbkdf2_hmac(hash_name, password, salt, iterations, dklen=None):\n        \"\"\"Password based key derivation function 2 (PKCS #5 v2.0)\n\n        This Python implementations based on the hmac module about as fast\n        as OpenSSL's PKCS5_PBKDF2_HMAC for short passwords and much faster\n        for long passwords.\n        \"\"\"\n        if not isinstance(hash_name, str):\n            raise TypeError(hash_name)\n\n        if not isinstance(password, (bytes, bytearray)):\n            password = bytes(buffer(password))\n        if not isinstance(salt, (bytes, bytearray)):\n            salt = bytes(buffer(salt))\n\n        # Fast inline HMAC implementation\n        inner = new(hash_name)\n        outer = new(hash_name)\n        blocksize = getattr(inner, 'block_size', 64)\n        if len(password) > blocksize:\n            password = new(hash_name, password).digest()\n        password = password + b'\\x00' * (blocksize - len(password))\n        inner.update(password.translate(_trans_36))\n        outer.update(password.translate(_trans_5C))\n\n        def prf(msg, inner=inner, outer=outer):\n            # PBKDF2_HMAC uses the password as key. We can re-use the same\n            # digest objects and and just update copies to skip initialization.\n            icpy = inner.copy()\n            ocpy = outer.copy()\n            icpy.update(msg)\n            ocpy.update(icpy.digest())\n            return ocpy.digest()\n\n        if iterations < 1:\n            raise ValueError(iterations)\n        if dklen is None:\n            dklen = outer.digest_size\n        if dklen < 1:\n            raise ValueError(dklen)\n\n        hex_format_string = \"%%0%ix\" % (new(hash_name).digest_size * 2)\n\n        dkey = b''\n        loop = 1\n        while len(dkey) < dklen:\n            prev = prf(salt + struct.pack(b'>I', loop))\n            rkey = int(binascii.hexlify(prev), 16)\n            for i in xrange(iterations - 1):\n                prev = prf(prev)\n                rkey ^= int(binascii.hexlify(prev), 16)\n            loop += 1\n            dkey += binascii.unhexlify(hex_format_string % rkey)\n\n        return dkey[:dklen]\n\n# Cleanup locals()\ndel __always_supported, __func_name, __get_hash\ndel __py_new, __hash_new, __get_openssl_constructor\n", 
    "heapq": "# -*- coding: utf-8 -*-\n\n\"\"\"Heap queue algorithm (a.k.a. priority queue).\n\nHeaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\nall k, counting elements from 0.  For the sake of comparison,\nnon-existing elements are considered to be infinite.  The interesting\nproperty of a heap is that a[0] is always its smallest element.\n\nUsage:\n\nheap = []            # creates an empty heap\nheappush(heap, item) # pushes a new item on the heap\nitem = heappop(heap) # pops the smallest item from the heap\nitem = heap[0]       # smallest item on the heap without popping it\nheapify(x)           # transforms list into a heap, in-place, in linear time\nitem = heapreplace(heap, item) # pops and returns smallest item, and adds\n                               # new item; the heap size is unchanged\n\nOur API differs from textbook heap algorithms as follows:\n\n- We use 0-based indexing.  This makes the relationship between the\n  index for a node and the indexes for its children slightly less\n  obvious, but is more suitable since Python uses 0-based indexing.\n\n- Our heappop() method returns the smallest item, not the largest.\n\nThese two make it possible to view the heap as a regular Python list\nwithout surprises: heap[0] is the smallest item, and heap.sort()\nmaintains the heap invariant!\n\"\"\"\n\n# Original code by Kevin O'Connor, augmented by Tim Peters and Raymond Hettinger\n\n__about__ = \"\"\"Heap queues\n\n[explanation by Fran\u00e7ois Pinard]\n\nHeaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\nall k, counting elements from 0.  For the sake of comparison,\nnon-existing elements are considered to be infinite.  The interesting\nproperty of a heap is that a[0] is always its smallest element.\n\nThe strange invariant above is meant to be an efficient memory\nrepresentation for a tournament.  The numbers below are `k', not a[k]:\n\n                                   0\n\n                  1                                 2\n\n          3               4                5               6\n\n      7       8       9       10      11      12      13      14\n\n    15 16   17 18   19 20   21 22   23 24   25 26   27 28   29 30\n\n\nIn the tree above, each cell `k' is topping `2*k+1' and `2*k+2'.  In\nan usual binary tournament we see in sports, each cell is the winner\nover the two cells it tops, and we can trace the winner down the tree\nto see all opponents s/he had.  However, in many computer applications\nof such tournaments, we do not need to trace the history of a winner.\nTo be more memory efficient, when a winner is promoted, we try to\nreplace it by something else at a lower level, and the rule becomes\nthat a cell and the two cells it tops contain three different items,\nbut the top cell \"wins\" over the two topped cells.\n\nIf this heap invariant is protected at all time, index 0 is clearly\nthe overall winner.  The simplest algorithmic way to remove it and\nfind the \"next\" winner is to move some loser (let's say cell 30 in the\ndiagram above) into the 0 position, and then percolate this new 0 down\nthe tree, exchanging values, until the invariant is re-established.\nThis is clearly logarithmic on the total number of items in the tree.\nBy iterating over all items, you get an O(n ln n) sort.\n\nA nice feature of this sort is that you can efficiently insert new\nitems while the sort is going on, provided that the inserted items are\nnot \"better\" than the last 0'th element you extracted.  This is\nespecially useful in simulation contexts, where the tree holds all\nincoming events, and the \"win\" condition means the smallest scheduled\ntime.  When an event schedule other events for execution, they are\nscheduled into the future, so they can easily go into the heap.  So, a\nheap is a good structure for implementing schedulers (this is what I\nused for my MIDI sequencer :-).\n\nVarious structures for implementing schedulers have been extensively\nstudied, and heaps are good for this, as they are reasonably speedy,\nthe speed is almost constant, and the worst case is not much different\nthan the average case.  However, there are other representations which\nare more efficient overall, yet the worst cases might be terrible.\n\nHeaps are also very useful in big disk sorts.  You most probably all\nknow that a big sort implies producing \"runs\" (which are pre-sorted\nsequences, which size is usually related to the amount of CPU memory),\nfollowed by a merging passes for these runs, which merging is often\nvery cleverly organised[1].  It is very important that the initial\nsort produces the longest runs possible.  Tournaments are a good way\nto that.  If, using all the memory available to hold a tournament, you\nreplace and percolate items that happen to fit the current run, you'll\nproduce runs which are twice the size of the memory for random input,\nand much better for input fuzzily ordered.\n\nMoreover, if you output the 0'th item on disk and get an input which\nmay not fit in the current tournament (because the value \"wins\" over\nthe last output value), it cannot fit in the heap, so the size of the\nheap decreases.  The freed memory could be cleverly reused immediately\nfor progressively building a second heap, which grows at exactly the\nsame rate the first heap is melting.  When the first heap completely\nvanishes, you switch heaps and start a new run.  Clever and quite\neffective!\n\nIn a word, heaps are useful memory structures to know.  I use them in\na few applications, and I think it is good to keep a `heap' module\naround. :-)\n\n--------------------\n[1] The disk balancing algorithms which are current, nowadays, are\nmore annoying than clever, and this is a consequence of the seeking\ncapabilities of the disks.  On devices which cannot seek, like big\ntape drives, the story was quite different, and one had to be very\nclever to ensure (far in advance) that each tape movement will be the\nmost effective possible (that is, will best participate at\n\"progressing\" the merge).  Some tapes were even able to read\nbackwards, and this was also used to avoid the rewinding time.\nBelieve me, real good tape sorts were quite spectacular to watch!\nFrom all times, sorting has always been a Great Art! :-)\n\"\"\"\n\n__all__ = ['heappush', 'heappop', 'heapify', 'heapreplace', 'merge',\n           'nlargest', 'nsmallest', 'heappushpop']\n\nfrom itertools import islice, count, imap, izip, tee, chain\nfrom operator import itemgetter\n\ndef cmp_lt(x, y):\n    # Use __lt__ if available; otherwise, try __le__.\n    # In Py3.x, only __lt__ will be called.\n    return (x < y) if hasattr(x, '__lt__') else (not y <= x)\n\ndef heappush(heap, item):\n    \"\"\"Push item onto heap, maintaining the heap invariant.\"\"\"\n    heap.append(item)\n    _siftdown(heap, 0, len(heap)-1)\n\ndef heappop(heap):\n    \"\"\"Pop the smallest item off the heap, maintaining the heap invariant.\"\"\"\n    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty\n    if heap:\n        returnitem = heap[0]\n        heap[0] = lastelt\n        _siftup(heap, 0)\n    else:\n        returnitem = lastelt\n    return returnitem\n\ndef heapreplace(heap, item):\n    \"\"\"Pop and return the current smallest value, and add the new item.\n\n    This is more efficient than heappop() followed by heappush(), and can be\n    more appropriate when using a fixed-size heap.  Note that the value\n    returned may be larger than item!  That constrains reasonable uses of\n    this routine unless written as part of a conditional replacement:\n\n        if item > heap[0]:\n            item = heapreplace(heap, item)\n    \"\"\"\n    returnitem = heap[0]    # raises appropriate IndexError if heap is empty\n    heap[0] = item\n    _siftup(heap, 0)\n    return returnitem\n\ndef heappushpop(heap, item):\n    \"\"\"Fast version of a heappush followed by a heappop.\"\"\"\n    if heap and cmp_lt(heap[0], item):\n        item, heap[0] = heap[0], item\n        _siftup(heap, 0)\n    return item\n\ndef heapify(x):\n    \"\"\"Transform list into a heap, in-place, in O(len(x)) time.\"\"\"\n    n = len(x)\n    # Transform bottom-up.  The largest index there's any point to looking at\n    # is the largest with a child index in-range, so must have 2*i + 1 < n,\n    # or i < (n-1)/2.  If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so\n    # j-1 is the largest, which is n//2 - 1.  If n is odd = 2*j+1, this is\n    # (2*j+1-1)/2 = j so j-1 is the largest, and that's again n//2-1.\n    for i in reversed(xrange(n//2)):\n        _siftup(x, i)\n\ndef _heappushpop_max(heap, item):\n    \"\"\"Maxheap version of a heappush followed by a heappop.\"\"\"\n    if heap and cmp_lt(item, heap[0]):\n        item, heap[0] = heap[0], item\n        _siftup_max(heap, 0)\n    return item\n\ndef _heapify_max(x):\n    \"\"\"Transform list into a maxheap, in-place, in O(len(x)) time.\"\"\"\n    n = len(x)\n    for i in reversed(range(n//2)):\n        _siftup_max(x, i)\n\ndef nlargest(n, iterable):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, reverse=True)[:n]\n    \"\"\"\n    if n < 0:\n        return []\n    it = iter(iterable)\n    result = list(islice(it, n))\n    if not result:\n        return result\n    heapify(result)\n    _heappushpop = heappushpop\n    for elem in it:\n        _heappushpop(result, elem)\n    result.sort(reverse=True)\n    return result\n\ndef nsmallest(n, iterable):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable)[:n]\n    \"\"\"\n    if n < 0:\n        return []\n    it = iter(iterable)\n    result = list(islice(it, n))\n    if not result:\n        return result\n    _heapify_max(result)\n    _heappushpop = _heappushpop_max\n    for elem in it:\n        _heappushpop(result, elem)\n    result.sort()\n    return result\n\n# 'heap' is a heap at all indices >= startpos, except possibly for pos.  pos\n# is the index of a leaf with a possibly out-of-order value.  Restore the\n# heap invariant.\ndef _siftdown(heap, startpos, pos):\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if cmp_lt(newitem, parent):\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem\n\n# The child indices of heap index pos are already heaps, and we want to make\n# a heap at index pos too.  We do this by bubbling the smaller child of\n# pos up (and so on with that child's children, etc) until hitting a leaf,\n# then using _siftdown to move the oddball originally at index pos into place.\n#\n# We *could* break out of the loop as soon as we find a pos where newitem <=\n# both its children, but turns out that's not a good idea, and despite that\n# many books write the algorithm that way.  During a heap pop, the last array\n# element is sifted in, and that tends to be large, so that comparing it\n# against values starting from the root usually doesn't pay (= usually doesn't\n# get us out of the loop early).  See Knuth, Volume 3, where this is\n# explained and quantified in an exercise.\n#\n# Cutting the # of comparisons is important, since these routines have no\n# way to extract \"the priority\" from an array element, so that intelligence\n# is likely to be hiding in custom __cmp__ methods, or in array elements\n# storing (priority, record) tuples.  Comparisons are thus potentially\n# expensive.\n#\n# On random arrays of length 1000, making this change cut the number of\n# comparisons made by heapify() a little, and those made by exhaustive\n# heappop() a lot, in accord with theory.  Here are typical results from 3\n# runs (3 just to demonstrate how small the variance is):\n#\n# Compares needed by heapify     Compares needed by 1000 heappops\n# --------------------------     --------------------------------\n# 1837 cut to 1663               14996 cut to 8680\n# 1855 cut to 1659               14966 cut to 8678\n# 1847 cut to 1660               15024 cut to 8703\n#\n# Building the heap by using heappush() 1000 times instead required\n# 2198, 2148, and 2219 compares:  heapify() is more efficient, when\n# you can use it.\n#\n# The total compares needed by list.sort() on the same lists were 8627,\n# 8627, and 8632 (this should be compared to the sum of heapify() and\n# heappop() compares):  list.sort() is (unsurprisingly!) more efficient\n# for sorting.\n\ndef _siftup(heap, pos):\n    endpos = len(heap)\n    startpos = pos\n    newitem = heap[pos]\n    # Bubble up the smaller child until hitting a leaf.\n    childpos = 2*pos + 1    # leftmost child position\n    while childpos < endpos:\n        # Set childpos to index of smaller child.\n        rightpos = childpos + 1\n        if rightpos < endpos and not cmp_lt(heap[childpos], heap[rightpos]):\n            childpos = rightpos\n        # Move the smaller child up.\n        heap[pos] = heap[childpos]\n        pos = childpos\n        childpos = 2*pos + 1\n    # The leaf at pos is empty now.  Put newitem there, and bubble it up\n    # to its final resting place (by sifting its parents down).\n    heap[pos] = newitem\n    _siftdown(heap, startpos, pos)\n\ndef _siftdown_max(heap, startpos, pos):\n    'Maxheap variant of _siftdown'\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if cmp_lt(parent, newitem):\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem\n\ndef _siftup_max(heap, pos):\n    'Maxheap variant of _siftup'\n    endpos = len(heap)\n    startpos = pos\n    newitem = heap[pos]\n    # Bubble up the larger child until hitting a leaf.\n    childpos = 2*pos + 1    # leftmost child position\n    while childpos < endpos:\n        # Set childpos to index of larger child.\n        rightpos = childpos + 1\n        if rightpos < endpos and not cmp_lt(heap[rightpos], heap[childpos]):\n            childpos = rightpos\n        # Move the larger child up.\n        heap[pos] = heap[childpos]\n        pos = childpos\n        childpos = 2*pos + 1\n    # The leaf at pos is empty now.  Put newitem there, and bubble it up\n    # to its final resting place (by sifting its parents down).\n    heap[pos] = newitem\n    _siftdown_max(heap, startpos, pos)\n\n# If available, use C implementation\ntry:\n    from _heapq import *\nexcept ImportError:\n    pass\n\ndef merge(*iterables):\n    '''Merge multiple sorted inputs into a single sorted output.\n\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\n    does not pull the data into memory all at once, and assumes that each of\n    the input streams is already sorted (smallest to largest).\n\n    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\n    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\n\n    '''\n    _heappop, _heapreplace, _StopIteration = heappop, heapreplace, StopIteration\n    _len = len\n\n    h = []\n    h_append = h.append\n    for itnum, it in enumerate(map(iter, iterables)):\n        try:\n            next = it.next\n            h_append([next(), itnum, next])\n        except _StopIteration:\n            pass\n    heapify(h)\n\n    while _len(h) > 1:\n        try:\n            while 1:\n                v, itnum, next = s = h[0]\n                yield v\n                s[0] = next()               # raises StopIteration when exhausted\n                _heapreplace(h, s)          # restore heap condition\n        except _StopIteration:\n            _heappop(h)                     # remove empty iterator\n    if h:\n        # fast case when only a single iterator remains\n        v, itnum, next = h[0]\n        yield v\n        for v in next.__self__:\n            yield v\n\n# Extend the implementations of nsmallest and nlargest to use a key= argument\n_nsmallest = nsmallest\ndef nsmallest(n, iterable, key=None):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key)[:n]\n    \"\"\"\n    # Short-cut for n==1 is to use min() when len(iterable)>0\n    if n == 1:\n        it = iter(iterable)\n        head = list(islice(it, 1))\n        if not head:\n            return []\n        if key is None:\n            return [min(chain(head, it))]\n        return [min(chain(head, it), key=key)]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = izip(iterable, count())                        # decorate\n        result = _nsmallest(n, it)\n        return map(itemgetter(0), result)                   # undecorate\n\n    # General case, slowest method\n    in1, in2 = tee(iterable)\n    it = izip(imap(key, in1), count(), in2)                 # decorate\n    result = _nsmallest(n, it)\n    return map(itemgetter(2), result)                       # undecorate\n\n_nlargest = nlargest\ndef nlargest(n, iterable, key=None):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]\n    \"\"\"\n\n    # Short-cut for n==1 is to use max() when len(iterable)>0\n    if n == 1:\n        it = iter(iterable)\n        head = list(islice(it, 1))\n        if not head:\n            return []\n        if key is None:\n            return [max(chain(head, it))]\n        return [max(chain(head, it), key=key)]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key, reverse=True)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = izip(iterable, count(0,-1))                    # decorate\n        result = _nlargest(n, it)\n        return map(itemgetter(0), result)                   # undecorate\n\n    # General case, slowest method\n    in1, in2 = tee(iterable)\n    it = izip(imap(key, in1), count(0,-1), in2)             # decorate\n    result = _nlargest(n, it)\n    return map(itemgetter(2), result)                       # undecorate\n\nif __name__ == \"__main__\":\n    # Simple sanity test\n    heap = []\n    data = [1, 3, 5, 7, 9, 2, 4, 6, 8, 0]\n    for item in data:\n        heappush(heap, item)\n    sort = []\n    while heap:\n        sort.append(heappop(heap))\n    print sort\n\n    import doctest\n    doctest.testmod()\n", 
    "httplib": "r\"\"\"HTTP/1.1 client library\n\n<intro stuff goes here>\n<other stuff, too>\n\nHTTPConnection goes through a number of \"states\", which define when a client\nmay legally make another request or fetch the response for a particular\nrequest. This diagram details these state transitions:\n\n    (null)\n      |\n      | HTTPConnection()\n      v\n    Idle\n      |\n      | putrequest()\n      v\n    Request-started\n      |\n      | ( putheader() )*  endheaders()\n      v\n    Request-sent\n      |\n      | response = getresponse()\n      v\n    Unread-response   [Response-headers-read]\n      |\\____________________\n      |                     |\n      | response.read()     | putrequest()\n      v                     v\n    Idle                  Req-started-unread-response\n                     ______/|\n                   /        |\n   response.read() |        | ( putheader() )*  endheaders()\n                   v        v\n       Request-started    Req-sent-unread-response\n                            |\n                            | response.read()\n                            v\n                          Request-sent\n\nThis diagram presents the following rules:\n  -- a second request may not be started until {response-headers-read}\n  -- a response [object] cannot be retrieved until {request-sent}\n  -- there is no differentiation between an unread response body and a\n     partially read response body\n\nNote: this enforcement is applied by the HTTPConnection class. The\n      HTTPResponse class does not enforce this state machine, which\n      implies sophisticated clients may accelerate the request/response\n      pipeline. Caution should be taken, though: accelerating the states\n      beyond the above pattern may imply knowledge of the server's\n      connection-close behavior for certain requests. For example, it\n      is impossible to tell whether the server will close the connection\n      UNTIL the response headers have been read; this means that further\n      requests cannot be placed into the pipeline until it is known that\n      the server will NOT be closing the connection.\n\nLogical State                  __state            __response\n-------------                  -------            ----------\nIdle                           _CS_IDLE           None\nRequest-started                _CS_REQ_STARTED    None\nRequest-sent                   _CS_REQ_SENT       None\nUnread-response                _CS_IDLE           <response_class>\nReq-started-unread-response    _CS_REQ_STARTED    <response_class>\nReq-sent-unread-response       _CS_REQ_SENT       <response_class>\n\"\"\"\n\nfrom array import array\nimport os\nimport socket\nfrom sys import py3kwarning\nfrom urlparse import urlsplit\nimport warnings\nwith warnings.catch_warnings():\n    if py3kwarning:\n        warnings.filterwarnings(\"ignore\", \".*mimetools has been removed\",\n                                DeprecationWarning)\n    import mimetools\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\n__all__ = [\"HTTP\", \"HTTPResponse\", \"HTTPConnection\",\n           \"HTTPException\", \"NotConnected\", \"UnknownProtocol\",\n           \"UnknownTransferEncoding\", \"UnimplementedFileMode\",\n           \"IncompleteRead\", \"InvalidURL\", \"ImproperConnectionState\",\n           \"CannotSendRequest\", \"CannotSendHeader\", \"ResponseNotReady\",\n           \"BadStatusLine\", \"error\", \"responses\"]\n\nHTTP_PORT = 80\nHTTPS_PORT = 443\n\n_UNKNOWN = 'UNKNOWN'\n\n# connection states\n_CS_IDLE = 'Idle'\n_CS_REQ_STARTED = 'Request-started'\n_CS_REQ_SENT = 'Request-sent'\n\n# status codes\n# informational\nCONTINUE = 100\nSWITCHING_PROTOCOLS = 101\nPROCESSING = 102\n\n# successful\nOK = 200\nCREATED = 201\nACCEPTED = 202\nNON_AUTHORITATIVE_INFORMATION = 203\nNO_CONTENT = 204\nRESET_CONTENT = 205\nPARTIAL_CONTENT = 206\nMULTI_STATUS = 207\nIM_USED = 226\n\n# redirection\nMULTIPLE_CHOICES = 300\nMOVED_PERMANENTLY = 301\nFOUND = 302\nSEE_OTHER = 303\nNOT_MODIFIED = 304\nUSE_PROXY = 305\nTEMPORARY_REDIRECT = 307\n\n# client error\nBAD_REQUEST = 400\nUNAUTHORIZED = 401\nPAYMENT_REQUIRED = 402\nFORBIDDEN = 403\nNOT_FOUND = 404\nMETHOD_NOT_ALLOWED = 405\nNOT_ACCEPTABLE = 406\nPROXY_AUTHENTICATION_REQUIRED = 407\nREQUEST_TIMEOUT = 408\nCONFLICT = 409\nGONE = 410\nLENGTH_REQUIRED = 411\nPRECONDITION_FAILED = 412\nREQUEST_ENTITY_TOO_LARGE = 413\nREQUEST_URI_TOO_LONG = 414\nUNSUPPORTED_MEDIA_TYPE = 415\nREQUESTED_RANGE_NOT_SATISFIABLE = 416\nEXPECTATION_FAILED = 417\nUNPROCESSABLE_ENTITY = 422\nLOCKED = 423\nFAILED_DEPENDENCY = 424\nUPGRADE_REQUIRED = 426\n\n# server error\nINTERNAL_SERVER_ERROR = 500\nNOT_IMPLEMENTED = 501\nBAD_GATEWAY = 502\nSERVICE_UNAVAILABLE = 503\nGATEWAY_TIMEOUT = 504\nHTTP_VERSION_NOT_SUPPORTED = 505\nINSUFFICIENT_STORAGE = 507\nNOT_EXTENDED = 510\n\n# Mapping status codes to official W3C names\nresponses = {\n    100: 'Continue',\n    101: 'Switching Protocols',\n\n    200: 'OK',\n    201: 'Created',\n    202: 'Accepted',\n    203: 'Non-Authoritative Information',\n    204: 'No Content',\n    205: 'Reset Content',\n    206: 'Partial Content',\n\n    300: 'Multiple Choices',\n    301: 'Moved Permanently',\n    302: 'Found',\n    303: 'See Other',\n    304: 'Not Modified',\n    305: 'Use Proxy',\n    306: '(Unused)',\n    307: 'Temporary Redirect',\n\n    400: 'Bad Request',\n    401: 'Unauthorized',\n    402: 'Payment Required',\n    403: 'Forbidden',\n    404: 'Not Found',\n    405: 'Method Not Allowed',\n    406: 'Not Acceptable',\n    407: 'Proxy Authentication Required',\n    408: 'Request Timeout',\n    409: 'Conflict',\n    410: 'Gone',\n    411: 'Length Required',\n    412: 'Precondition Failed',\n    413: 'Request Entity Too Large',\n    414: 'Request-URI Too Long',\n    415: 'Unsupported Media Type',\n    416: 'Requested Range Not Satisfiable',\n    417: 'Expectation Failed',\n\n    500: 'Internal Server Error',\n    501: 'Not Implemented',\n    502: 'Bad Gateway',\n    503: 'Service Unavailable',\n    504: 'Gateway Timeout',\n    505: 'HTTP Version Not Supported',\n}\n\n# maximal amount of data to read at one time in _safe_read\nMAXAMOUNT = 1048576\n\n# maximal line length when calling readline().\n_MAXLINE = 65536\n\nclass HTTPMessage(mimetools.Message):\n\n    def addheader(self, key, value):\n        \"\"\"Add header for field key handling repeats.\"\"\"\n        prev = self.dict.get(key)\n        if prev is None:\n            self.dict[key] = value\n        else:\n            combined = \", \".join((prev, value))\n            self.dict[key] = combined\n\n    def addcontinue(self, key, more):\n        \"\"\"Add more field data from a continuation line.\"\"\"\n        prev = self.dict[key]\n        self.dict[key] = prev + \"\\n \" + more\n\n    def readheaders(self):\n        \"\"\"Read header lines.\n\n        Read header lines up to the entirely blank line that terminates them.\n        The (normally blank) line that ends the headers is skipped, but not\n        included in the returned list.  If a non-header line ends the headers,\n        (which is an error), an attempt is made to backspace over it; it is\n        never included in the returned list.\n\n        The variable self.status is set to the empty string if all went well,\n        otherwise it is an error message.  The variable self.headers is a\n        completely uninterpreted list of lines contained in the header (so\n        printing them will reproduce the header exactly as it appears in the\n        file).\n\n        If multiple header fields with the same name occur, they are combined\n        according to the rules in RFC 2616 sec 4.2:\n\n        Appending each subsequent field-value to the first, each separated\n        by a comma. The order in which header fields with the same field-name\n        are received is significant to the interpretation of the combined\n        field value.\n        \"\"\"\n        # XXX The implementation overrides the readheaders() method of\n        # rfc822.Message.  The base class design isn't amenable to\n        # customized behavior here so the method here is a copy of the\n        # base class code with a few small changes.\n\n        self.dict = {}\n        self.unixfrom = ''\n        self.headers = hlist = []\n        self.status = ''\n        headerseen = \"\"\n        firstline = 1\n        startofline = unread = tell = None\n        if hasattr(self.fp, 'unread'):\n            unread = self.fp.unread\n        elif self.seekable:\n            tell = self.fp.tell\n        while True:\n            if tell:\n                try:\n                    startofline = tell()\n                except IOError:\n                    startofline = tell = None\n                    self.seekable = 0\n            line = self.fp.readline(_MAXLINE + 1)\n            if len(line) > _MAXLINE:\n                raise LineTooLong(\"header line\")\n            if not line:\n                self.status = 'EOF in headers'\n                break\n            # Skip unix From name time lines\n            if firstline and line.startswith('From '):\n                self.unixfrom = self.unixfrom + line\n                continue\n            firstline = 0\n            if headerseen and line[0] in ' \\t':\n                # XXX Not sure if continuation lines are handled properly\n                # for http and/or for repeating headers\n                # It's a continuation line.\n                hlist.append(line)\n                self.addcontinue(headerseen, line.strip())\n                continue\n            elif self.iscomment(line):\n                # It's a comment.  Ignore it.\n                continue\n            elif self.islast(line):\n                # Note! No pushback here!  The delimiter line gets eaten.\n                break\n            headerseen = self.isheader(line)\n            if headerseen:\n                # It's a legal header line, save it.\n                hlist.append(line)\n                self.addheader(headerseen, line[len(headerseen)+1:].strip())\n                continue\n            else:\n                # It's not a header line; throw it back and stop here.\n                if not self.dict:\n                    self.status = 'No headers'\n                else:\n                    self.status = 'Non-header line where header expected'\n                # Try to undo the read.\n                if unread:\n                    unread(line)\n                elif tell:\n                    self.fp.seek(startofline)\n                else:\n                    self.status = self.status + '; bad seek'\n                break\n\nclass HTTPResponse:\n\n    # strict: If true, raise BadStatusLine if the status line can't be\n    # parsed as a valid HTTP/1.0 or 1.1 status line.  By default it is\n    # false because it prevents clients from talking to HTTP/0.9\n    # servers.  Note that a response with a sufficiently corrupted\n    # status line will look like an HTTP/0.9 response.\n\n    # See RFC 2616 sec 19.6 and RFC 1945 sec 6 for details.\n\n    def __init__(self, sock, debuglevel=0, strict=0, method=None, buffering=False):\n        if buffering:\n            # The caller won't be using any sock.recv() calls, so buffering\n            # is fine and recommended for performance.\n            self.fp = sock.makefile('rb')\n        else:\n            # The buffer size is specified as zero, because the headers of\n            # the response are read with readline().  If the reads were\n            # buffered the readline() calls could consume some of the\n            # response, which make be read via a recv() on the underlying\n            # socket.\n            self.fp = sock.makefile('rb', 0)\n        self.debuglevel = debuglevel\n        self.strict = strict\n        self._method = method\n\n        self.msg = None\n\n        # from the Status-Line of the response\n        self.version = _UNKNOWN # HTTP-Version\n        self.status = _UNKNOWN  # Status-Code\n        self.reason = _UNKNOWN  # Reason-Phrase\n\n        self.chunked = _UNKNOWN         # is \"chunked\" being used?\n        self.chunk_left = _UNKNOWN      # bytes left to read in current chunk\n        self.length = _UNKNOWN          # number of bytes left in response\n        self.will_close = _UNKNOWN      # conn will close at end of response\n\n    def _read_status(self):\n        # Initialize with Simple-Response defaults\n        line = self.fp.readline(_MAXLINE + 1)\n        if len(line) > _MAXLINE:\n            raise LineTooLong(\"header line\")\n        if self.debuglevel > 0:\n            print \"reply:\", repr(line)\n        if not line:\n            # Presumably, the server closed the connection before\n            # sending a valid response.\n            raise BadStatusLine(line)\n        try:\n            [version, status, reason] = line.split(None, 2)\n        except ValueError:\n            try:\n                [version, status] = line.split(None, 1)\n                reason = \"\"\n            except ValueError:\n                # empty version will cause next test to fail and status\n                # will be treated as 0.9 response.\n                version = \"\"\n        if not version.startswith('HTTP/'):\n            if self.strict:\n                self.close()\n                raise BadStatusLine(line)\n            else:\n                # assume it's a Simple-Response from an 0.9 server\n                self.fp = LineAndFileWrapper(line, self.fp)\n                return \"HTTP/0.9\", 200, \"\"\n\n        # The status code is a three-digit number\n        try:\n            status = int(status)\n            if status < 100 or status > 999:\n                raise BadStatusLine(line)\n        except ValueError:\n            raise BadStatusLine(line)\n        return version, status, reason\n\n    def begin(self):\n        if self.msg is not None:\n            # we've already started reading the response\n            return\n\n        # read until we get a non-100 response\n        while True:\n            version, status, reason = self._read_status()\n            if status != CONTINUE:\n                break\n            # skip the header from the 100 response\n            while True:\n                skip = self.fp.readline(_MAXLINE + 1)\n                if len(skip) > _MAXLINE:\n                    raise LineTooLong(\"header line\")\n                skip = skip.strip()\n                if not skip:\n                    break\n                if self.debuglevel > 0:\n                    print \"header:\", skip\n\n        self.status = status\n        self.reason = reason.strip()\n        if version == 'HTTP/1.0':\n            self.version = 10\n        elif version.startswith('HTTP/1.'):\n            self.version = 11   # use HTTP/1.1 code for HTTP/1.x where x>=1\n        elif version == 'HTTP/0.9':\n            self.version = 9\n        else:\n            raise UnknownProtocol(version)\n\n        if self.version == 9:\n            self.length = None\n            self.chunked = 0\n            self.will_close = 1\n            self.msg = HTTPMessage(StringIO())\n            return\n\n        self.msg = HTTPMessage(self.fp, 0)\n        if self.debuglevel > 0:\n            for hdr in self.msg.headers:\n                print \"header:\", hdr,\n\n        # don't let the msg keep an fp\n        self.msg.fp = None\n\n        # are we using the chunked-style of transfer encoding?\n        tr_enc = self.msg.getheader('transfer-encoding')\n        if tr_enc and tr_enc.lower() == \"chunked\":\n            self.chunked = 1\n            self.chunk_left = None\n        else:\n            self.chunked = 0\n\n        # will the connection close at the end of the response?\n        self.will_close = self._check_close()\n\n        # do we have a Content-Length?\n        # NOTE: RFC 2616, S4.4, #3 says we ignore this if tr_enc is \"chunked\"\n        length = self.msg.getheader('content-length')\n        if length and not self.chunked:\n            try:\n                self.length = int(length)\n            except ValueError:\n                self.length = None\n            else:\n                if self.length < 0:  # ignore nonsensical negative lengths\n                    self.length = None\n        else:\n            self.length = None\n\n        # does the body have a fixed length? (of zero)\n        if (status == NO_CONTENT or status == NOT_MODIFIED or\n            100 <= status < 200 or      # 1xx codes\n            self._method == 'HEAD'):\n            self.length = 0\n\n        # if the connection remains open, and we aren't using chunked, and\n        # a content-length was not provided, then assume that the connection\n        # WILL close.\n        if not self.will_close and \\\n           not self.chunked and \\\n           self.length is None:\n            self.will_close = 1\n\n    def _check_close(self):\n        conn = self.msg.getheader('connection')\n        if self.version == 11:\n            # An HTTP/1.1 proxy is assumed to stay open unless\n            # explicitly closed.\n            conn = self.msg.getheader('connection')\n            if conn and \"close\" in conn.lower():\n                return True\n            return False\n\n        # Some HTTP/1.0 implementations have support for persistent\n        # connections, using rules different than HTTP/1.1.\n\n        # For older HTTP, Keep-Alive indicates persistent connection.\n        if self.msg.getheader('keep-alive'):\n            return False\n\n        # At least Akamai returns a \"Connection: Keep-Alive\" header,\n        # which was supposed to be sent by the client.\n        if conn and \"keep-alive\" in conn.lower():\n            return False\n\n        # Proxy-Connection is a netscape hack.\n        pconn = self.msg.getheader('proxy-connection')\n        if pconn and \"keep-alive\" in pconn.lower():\n            return False\n\n        # otherwise, assume it will close\n        return True\n\n    def close(self):\n        if self.fp:\n            self.fp.close()\n            self.fp = None\n\n    def isclosed(self):\n        # NOTE: it is possible that we will not ever call self.close(). This\n        #       case occurs when will_close is TRUE, length is None, and we\n        #       read up to the last byte, but NOT past it.\n        #\n        # IMPLIES: if will_close is FALSE, then self.close() will ALWAYS be\n        #          called, meaning self.isclosed() is meaningful.\n        return self.fp is None\n\n    # XXX It would be nice to have readline and __iter__ for this, too.\n\n    def read(self, amt=None):\n        if self.fp is None:\n            return ''\n\n        if self._method == 'HEAD':\n            self.close()\n            return ''\n\n        if self.chunked:\n            return self._read_chunked(amt)\n\n        if amt is None:\n            # unbounded read\n            if self.length is None:\n                s = self.fp.read()\n            else:\n                try:\n                    s = self._safe_read(self.length)\n                except IncompleteRead:\n                    self.close()\n                    raise\n                self.length = 0\n            self.close()        # we read everything\n            return s\n\n        if self.length is not None:\n            if amt > self.length:\n                # clip the read to the \"end of response\"\n                amt = self.length\n\n        # we do not use _safe_read() here because this may be a .will_close\n        # connection, and the user is reading more bytes than will be provided\n        # (for example, reading in 1k chunks)\n        s = self.fp.read(amt)\n        if not s and amt:\n            # Ideally, we would raise IncompleteRead if the content-length\n            # wasn't satisfied, but it might break compatibility.\n            self.close()\n        if self.length is not None:\n            self.length -= len(s)\n            if not self.length:\n                self.close()\n\n        return s\n\n    def _read_chunked(self, amt):\n        assert self.chunked != _UNKNOWN\n        chunk_left = self.chunk_left\n        value = []\n        while True:\n            if chunk_left is None:\n                line = self.fp.readline(_MAXLINE + 1)\n                if len(line) > _MAXLINE:\n                    raise LineTooLong(\"chunk size\")\n                i = line.find(';')\n                if i >= 0:\n                    line = line[:i] # strip chunk-extensions\n                try:\n                    chunk_left = int(line, 16)\n                except ValueError:\n                    # close the connection as protocol synchronisation is\n                    # probably lost\n                    self.close()\n                    raise IncompleteRead(''.join(value))\n                if chunk_left == 0:\n                    break\n            if amt is None:\n                value.append(self._safe_read(chunk_left))\n            elif amt < chunk_left:\n                value.append(self._safe_read(amt))\n                self.chunk_left = chunk_left - amt\n                return ''.join(value)\n            elif amt == chunk_left:\n                value.append(self._safe_read(amt))\n                self._safe_read(2)  # toss the CRLF at the end of the chunk\n                self.chunk_left = None\n                return ''.join(value)\n            else:\n                value.append(self._safe_read(chunk_left))\n                amt -= chunk_left\n\n            # we read the whole chunk, get another\n            self._safe_read(2)      # toss the CRLF at the end of the chunk\n            chunk_left = None\n\n        # read and discard trailer up to the CRLF terminator\n        ### note: we shouldn't have any trailers!\n        while True:\n            line = self.fp.readline(_MAXLINE + 1)\n            if len(line) > _MAXLINE:\n                raise LineTooLong(\"trailer line\")\n            if not line:\n                # a vanishingly small number of sites EOF without\n                # sending the trailer\n                break\n            if line == '\\r\\n':\n                break\n\n        # we read everything; close the \"file\"\n        self.close()\n\n        return ''.join(value)\n\n    def _safe_read(self, amt):\n        \"\"\"Read the number of bytes requested, compensating for partial reads.\n\n        Normally, we have a blocking socket, but a read() can be interrupted\n        by a signal (resulting in a partial read).\n\n        Note that we cannot distinguish between EOF and an interrupt when zero\n        bytes have been read. IncompleteRead() will be raised in this\n        situation.\n\n        This function should be used when <amt> bytes \"should\" be present for\n        reading. If the bytes are truly not available (due to EOF), then the\n        IncompleteRead exception can be used to detect the problem.\n        \"\"\"\n        # NOTE(gps): As of svn r74426 socket._fileobject.read(x) will never\n        # return less than x bytes unless EOF is encountered.  It now handles\n        # signal interruptions (socket.error EINTR) internally.  This code\n        # never caught that exception anyways.  It seems largely pointless.\n        # self.fp.read(amt) will work fine.\n        s = []\n        while amt > 0:\n            chunk = self.fp.read(min(amt, MAXAMOUNT))\n            if not chunk:\n                raise IncompleteRead(''.join(s), amt)\n            s.append(chunk)\n            amt -= len(chunk)\n        return ''.join(s)\n\n    def fileno(self):\n        return self.fp.fileno()\n\n    def getheader(self, name, default=None):\n        if self.msg is None:\n            raise ResponseNotReady()\n        return self.msg.getheader(name, default)\n\n    def getheaders(self):\n        \"\"\"Return list of (header, value) tuples.\"\"\"\n        if self.msg is None:\n            raise ResponseNotReady()\n        return self.msg.items()\n\n\nclass HTTPConnection:\n\n    _http_vsn = 11\n    _http_vsn_str = 'HTTP/1.1'\n\n    response_class = HTTPResponse\n    default_port = HTTP_PORT\n    auto_open = 1\n    debuglevel = 0\n    strict = 0\n\n    def __init__(self, host, port=None, strict=None,\n                 timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None):\n        self.timeout = timeout\n        self.source_address = source_address\n        self.sock = None\n        self._buffer = []\n        self.__response = None\n        self.__state = _CS_IDLE\n        self._method = None\n        self._tunnel_host = None\n        self._tunnel_port = None\n        self._tunnel_headers = {}\n        if strict is not None:\n            self.strict = strict\n\n        (self.host, self.port) = self._get_hostport(host, port)\n\n        # This is stored as an instance variable to allow unittests\n        # to replace with a suitable mock\n        self._create_connection = socket.create_connection\n\n    def set_tunnel(self, host, port=None, headers=None):\n        \"\"\" Set up host and port for HTTP CONNECT tunnelling.\n\n        In a connection that uses HTTP Connect tunneling, the host passed to the\n        constructor is used as proxy server that relays all communication to the\n        endpoint passed to set_tunnel. This is done by sending a HTTP CONNECT\n        request to the proxy server when the connection is established.\n\n        This method must be called before the HTML connection has been\n        established.\n\n        The headers argument should be a mapping of extra HTTP headers\n        to send with the CONNECT request.\n        \"\"\"\n        # Verify if this is required.\n        if self.sock:\n            raise RuntimeError(\"Can't setup tunnel for established connection.\")\n\n        self._tunnel_host = host\n        self._tunnel_port = port\n        if headers:\n            self._tunnel_headers = headers\n        else:\n            self._tunnel_headers.clear()\n\n    def _get_hostport(self, host, port):\n        if port is None:\n            i = host.rfind(':')\n            j = host.rfind(']')         # ipv6 addresses have [...]\n            if i > j:\n                try:\n                    port = int(host[i+1:])\n                except ValueError:\n                    if host[i+1:] == \"\":  # http://foo.com:/ == http://foo.com/\n                        port = self.default_port\n                    else:\n                        raise InvalidURL(\"nonnumeric port: '%s'\" % host[i+1:])\n                host = host[:i]\n            else:\n                port = self.default_port\n            if host and host[0] == '[' and host[-1] == ']':\n                host = host[1:-1]\n        return (host, port)\n\n    def set_debuglevel(self, level):\n        self.debuglevel = level\n\n    def _tunnel(self):\n        (host, port) = self._get_hostport(self._tunnel_host, self._tunnel_port)\n        self.send(\"CONNECT %s:%d HTTP/1.0\\r\\n\" % (host, port))\n        for header, value in self._tunnel_headers.iteritems():\n            self.send(\"%s: %s\\r\\n\" % (header, value))\n        self.send(\"\\r\\n\")\n        response = self.response_class(self.sock, strict = self.strict,\n                                       method = self._method)\n        (version, code, message) = response._read_status()\n\n        if code != 200:\n            self.close()\n            raise socket.error(\"Tunnel connection failed: %d %s\" % (code,\n                                                                    message.strip()))\n        while True:\n            line = response.fp.readline(_MAXLINE + 1)\n            if len(line) > _MAXLINE:\n                raise LineTooLong(\"header line\")\n            if not line:\n                # for sites which EOF without sending trailer\n                break\n            if line == '\\r\\n':\n                break\n\n\n    def connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        self.sock = self._create_connection((self.host,self.port),\n                                           self.timeout, self.source_address)\n\n        if self._tunnel_host:\n            self._tunnel()\n\n    def close(self):\n        \"\"\"Close the connection to the HTTP server.\"\"\"\n        if self.sock:\n            self.sock.close()   # close it manually... there may be other refs\n            self.sock = None\n        if self.__response:\n            self.__response.close()\n            self.__response = None\n        self.__state = _CS_IDLE\n\n    def send(self, data):\n        \"\"\"Send `data' to the server.\"\"\"\n        if self.sock is None:\n            if self.auto_open:\n                self.connect()\n            else:\n                raise NotConnected()\n\n        if self.debuglevel > 0:\n            print \"send:\", repr(data)\n        blocksize = 8192\n        if hasattr(data,'read') and not isinstance(data, array):\n            if self.debuglevel > 0: print \"sendIng a read()able\"\n            datablock = data.read(blocksize)\n            while datablock:\n                self.sock.sendall(datablock)\n                datablock = data.read(blocksize)\n        else:\n            self.sock.sendall(data)\n\n    def _output(self, s):\n        \"\"\"Add a line of output to the current request buffer.\n\n        Assumes that the line does *not* end with \\\\r\\\\n.\n        \"\"\"\n        self._buffer.append(s)\n\n    def _send_output(self, message_body=None):\n        \"\"\"Send the currently buffered request and clear the buffer.\n\n        Appends an extra \\\\r\\\\n to the buffer.\n        A message_body may be specified, to be appended to the request.\n        \"\"\"\n        self._buffer.extend((\"\", \"\"))\n        msg = \"\\r\\n\".join(self._buffer)\n        del self._buffer[:]\n        # If msg and message_body are sent in a single send() call,\n        # it will avoid performance problems caused by the interaction\n        # between delayed ack and the Nagle algorithm.\n        if isinstance(message_body, str):\n            msg += message_body\n            message_body = None\n        self.send(msg)\n        if message_body is not None:\n            #message_body was not a string (i.e. it is a file) and\n            #we must run the risk of Nagle\n            self.send(message_body)\n\n    def putrequest(self, method, url, skip_host=0, skip_accept_encoding=0):\n        \"\"\"Send a request to the server.\n\n        `method' specifies an HTTP request method, e.g. 'GET'.\n        `url' specifies the object being requested, e.g. '/index.html'.\n        `skip_host' if True does not add automatically a 'Host:' header\n        `skip_accept_encoding' if True does not add automatically an\n           'Accept-Encoding:' header\n        \"\"\"\n\n        # if a prior response has been completed, then forget about it.\n        if self.__response and self.__response.isclosed():\n            self.__response = None\n\n\n        # in certain cases, we cannot issue another request on this connection.\n        # this occurs when:\n        #   1) we are in the process of sending a request.   (_CS_REQ_STARTED)\n        #   2) a response to a previous request has signalled that it is going\n        #      to close the connection upon completion.\n        #   3) the headers for the previous response have not been read, thus\n        #      we cannot determine whether point (2) is true.   (_CS_REQ_SENT)\n        #\n        # if there is no prior response, then we can request at will.\n        #\n        # if point (2) is true, then we will have passed the socket to the\n        # response (effectively meaning, \"there is no prior response\"), and\n        # will open a new one when a new request is made.\n        #\n        # Note: if a prior response exists, then we *can* start a new request.\n        #       We are not allowed to begin fetching the response to this new\n        #       request, however, until that prior response is complete.\n        #\n        if self.__state == _CS_IDLE:\n            self.__state = _CS_REQ_STARTED\n        else:\n            raise CannotSendRequest()\n\n        # Save the method we use, we need it later in the response phase\n        self._method = method\n        if not url:\n            url = '/'\n        hdr = '%s %s %s' % (method, url, self._http_vsn_str)\n\n        self._output(hdr)\n\n        if self._http_vsn == 11:\n            # Issue some standard headers for better HTTP/1.1 compliance\n\n            if not skip_host:\n                # this header is issued *only* for HTTP/1.1\n                # connections. more specifically, this means it is\n                # only issued when the client uses the new\n                # HTTPConnection() class. backwards-compat clients\n                # will be using HTTP/1.0 and those clients may be\n                # issuing this header themselves. we should NOT issue\n                # it twice; some web servers (such as Apache) barf\n                # when they see two Host: headers\n\n                # If we need a non-standard port,include it in the\n                # header.  If the request is going through a proxy,\n                # but the host of the actual URL, not the host of the\n                # proxy.\n\n                netloc = ''\n                if url.startswith('http'):\n                    nil, netloc, nil, nil, nil = urlsplit(url)\n\n                if netloc:\n                    try:\n                        netloc_enc = netloc.encode(\"ascii\")\n                    except UnicodeEncodeError:\n                        netloc_enc = netloc.encode(\"idna\")\n                    self.putheader('Host', netloc_enc)\n                else:\n                    if self._tunnel_host:\n                        host = self._tunnel_host\n                        port = self._tunnel_port\n                    else:\n                        host = self.host\n                        port = self.port\n\n                    try:\n                        host_enc = host.encode(\"ascii\")\n                    except UnicodeEncodeError:\n                        host_enc = host.encode(\"idna\")\n                    # Wrap the IPv6 Host Header with [] (RFC 2732)\n                    if host_enc.find(':') >= 0:\n                        host_enc = \"[\" + host_enc + \"]\"\n                    if port == self.default_port:\n                        self.putheader('Host', host_enc)\n                    else:\n                        self.putheader('Host', \"%s:%s\" % (host_enc, port))\n\n            # note: we are assuming that clients will not attempt to set these\n            #       headers since *this* library must deal with the\n            #       consequences. this also means that when the supporting\n            #       libraries are updated to recognize other forms, then this\n            #       code should be changed (removed or updated).\n\n            # we only want a Content-Encoding of \"identity\" since we don't\n            # support encodings such as x-gzip or x-deflate.\n            if not skip_accept_encoding:\n                self.putheader('Accept-Encoding', 'identity')\n\n            # we can accept \"chunked\" Transfer-Encodings, but no others\n            # NOTE: no TE header implies *only* \"chunked\"\n            #self.putheader('TE', 'chunked')\n\n            # if TE is supplied in the header, then it must appear in a\n            # Connection header.\n            #self.putheader('Connection', 'TE')\n\n        else:\n            # For HTTP/1.0, the server will assume \"not chunked\"\n            pass\n\n    def putheader(self, header, *values):\n        \"\"\"Send a request header line to the server.\n\n        For example: h.putheader('Accept', 'text/html')\n        \"\"\"\n        if self.__state != _CS_REQ_STARTED:\n            raise CannotSendHeader()\n\n        hdr = '%s: %s' % (header, '\\r\\n\\t'.join([str(v) for v in values]))\n        self._output(hdr)\n\n    def endheaders(self, message_body=None):\n        \"\"\"Indicate that the last header line has been sent to the server.\n\n        This method sends the request to the server.  The optional\n        message_body argument can be used to pass a message body\n        associated with the request.  The message body will be sent in\n        the same packet as the message headers if it is string, otherwise it is\n        sent as a separate packet.\n        \"\"\"\n        if self.__state == _CS_REQ_STARTED:\n            self.__state = _CS_REQ_SENT\n        else:\n            raise CannotSendHeader()\n        self._send_output(message_body)\n\n    def request(self, method, url, body=None, headers={}):\n        \"\"\"Send a complete request to the server.\"\"\"\n        self._send_request(method, url, body, headers)\n\n    def _set_content_length(self, body):\n        # Set the content-length based on the body.\n        thelen = None\n        try:\n            thelen = str(len(body))\n        except TypeError, te:\n            # If this is a file-like object, try to\n            # fstat its file descriptor\n            try:\n                thelen = str(os.fstat(body.fileno()).st_size)\n            except (AttributeError, OSError):\n                # Don't send a length if this failed\n                if self.debuglevel > 0: print \"Cannot stat!!\"\n\n        if thelen is not None:\n            self.putheader('Content-Length', thelen)\n\n    def _send_request(self, method, url, body, headers):\n        # Honor explicitly requested Host: and Accept-Encoding: headers.\n        header_names = dict.fromkeys([k.lower() for k in headers])\n        skips = {}\n        if 'host' in header_names:\n            skips['skip_host'] = 1\n        if 'accept-encoding' in header_names:\n            skips['skip_accept_encoding'] = 1\n\n        self.putrequest(method, url, **skips)\n\n        if body is not None and 'content-length' not in header_names:\n            self._set_content_length(body)\n        for hdr, value in headers.iteritems():\n            self.putheader(hdr, value)\n        self.endheaders(body)\n\n    def getresponse(self, buffering=False):\n        \"Get the response from the server.\"\n\n        # if a prior response has been completed, then forget about it.\n        if self.__response and self.__response.isclosed():\n            self.__response = None\n\n        #\n        # if a prior response exists, then it must be completed (otherwise, we\n        # cannot read this response's header to determine the connection-close\n        # behavior)\n        #\n        # note: if a prior response existed, but was connection-close, then the\n        # socket and response were made independent of this HTTPConnection\n        # object since a new request requires that we open a whole new\n        # connection\n        #\n        # this means the prior response had one of two states:\n        #   1) will_close: this connection was reset and the prior socket and\n        #                  response operate independently\n        #   2) persistent: the response was retained and we await its\n        #                  isclosed() status to become true.\n        #\n        if self.__state != _CS_REQ_SENT or self.__response:\n            raise ResponseNotReady()\n\n        args = (self.sock,)\n        kwds = {\"strict\":self.strict, \"method\":self._method}\n        if self.debuglevel > 0:\n            args += (self.debuglevel,)\n        if buffering:\n            #only add this keyword if non-default, for compatibility with\n            #other response_classes.\n            kwds[\"buffering\"] = True;\n        response = self.response_class(*args, **kwds)\n\n        try:\n            response.begin()\n        except:\n            response.close()\n            raise\n        assert response.will_close != _UNKNOWN\n        self.__state = _CS_IDLE\n\n        if response.will_close:\n            # this effectively passes the connection to the response\n            self.close()\n        else:\n            # remember this, so we can tell when it is complete\n            self.__response = response\n\n        return response\n\n\nclass HTTP:\n    \"Compatibility class with httplib.py from 1.5.\"\n\n    _http_vsn = 10\n    _http_vsn_str = 'HTTP/1.0'\n\n    debuglevel = 0\n\n    _connection_class = HTTPConnection\n\n    def __init__(self, host='', port=None, strict=None):\n        \"Provide a default host, since the superclass requires one.\"\n\n        # some joker passed 0 explicitly, meaning default port\n        if port == 0:\n            port = None\n\n        # Note that we may pass an empty string as the host; this will raise\n        # an error when we attempt to connect. Presumably, the client code\n        # will call connect before then, with a proper host.\n        self._setup(self._connection_class(host, port, strict))\n\n    def _setup(self, conn):\n        self._conn = conn\n\n        # set up delegation to flesh out interface\n        self.send = conn.send\n        self.putrequest = conn.putrequest\n        self.putheader = conn.putheader\n        self.endheaders = conn.endheaders\n        self.set_debuglevel = conn.set_debuglevel\n\n        conn._http_vsn = self._http_vsn\n        conn._http_vsn_str = self._http_vsn_str\n\n        self.file = None\n\n    def connect(self, host=None, port=None):\n        \"Accept arguments to set the host/port, since the superclass doesn't.\"\n\n        if host is not None:\n            self._conn._set_hostport(host, port)\n        self._conn.connect()\n\n    def getfile(self):\n        \"Provide a getfile, since the superclass' does not use this concept.\"\n        return self.file\n\n    def getreply(self, buffering=False):\n        \"\"\"Compat definition since superclass does not define it.\n\n        Returns a tuple consisting of:\n        - server status code (e.g. '200' if all goes well)\n        - server \"reason\" corresponding to status code\n        - any RFC822 headers in the response from the server\n        \"\"\"\n        try:\n            if not buffering:\n                response = self._conn.getresponse()\n            else:\n                #only add this keyword if non-default for compatibility\n                #with other connection classes\n                response = self._conn.getresponse(buffering)\n        except BadStatusLine, e:\n            ### hmm. if getresponse() ever closes the socket on a bad request,\n            ### then we are going to have problems with self.sock\n\n            ### should we keep this behavior? do people use it?\n            # keep the socket open (as a file), and return it\n            self.file = self._conn.sock.makefile('rb', 0)\n\n            # close our socket -- we want to restart after any protocol error\n            self.close()\n\n            self.headers = None\n            return -1, e.line, None\n\n        self.headers = response.msg\n        self.file = response.fp\n        return response.status, response.reason, response.msg\n\n    def close(self):\n        self._conn.close()\n\n        # note that self.file == response.fp, which gets closed by the\n        # superclass. just clear the object ref here.\n        ### hmm. messy. if status==-1, then self.file is owned by us.\n        ### well... we aren't explicitly closing, but losing this ref will\n        ### do it\n        self.file = None\n\ntry:\n    import ssl\nexcept ImportError:\n    pass\nelse:\n    class HTTPSConnection(HTTPConnection):\n        \"This class allows communication via SSL.\"\n\n        default_port = HTTPS_PORT\n\n        def __init__(self, host, port=None, key_file=None, cert_file=None,\n                     strict=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\n                     source_address=None):\n            HTTPConnection.__init__(self, host, port, strict, timeout,\n                                    source_address)\n            self.key_file = key_file\n            self.cert_file = cert_file\n\n        def connect(self):\n            \"Connect to a host on a given (SSL) port.\"\n\n            sock = self._create_connection((self.host, self.port),\n                                          self.timeout, self.source_address)\n            if self._tunnel_host:\n                self.sock = sock\n                self._tunnel()\n            self.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file)\n\n    __all__.append(\"HTTPSConnection\")\n\n    class HTTPS(HTTP):\n        \"\"\"Compatibility with 1.5 httplib interface\n\n        Python 1.5.2 did not have an HTTPS class, but it defined an\n        interface for sending http requests that is also useful for\n        https.\n        \"\"\"\n\n        _connection_class = HTTPSConnection\n\n        def __init__(self, host='', port=None, key_file=None, cert_file=None,\n                     strict=None):\n            # provide a default host, pass the X509 cert info\n\n            # urf. compensate for bad input.\n            if port == 0:\n                port = None\n            self._setup(self._connection_class(host, port, key_file,\n                                               cert_file, strict))\n\n            # we never actually use these for anything, but we keep them\n            # here for compatibility with post-1.5.2 CVS.\n            self.key_file = key_file\n            self.cert_file = cert_file\n\n\n    def FakeSocket (sock, sslobj):\n        warnings.warn(\"FakeSocket is deprecated, and won't be in 3.x.  \" +\n                      \"Use the result of ssl.wrap_socket() directly instead.\",\n                      DeprecationWarning, stacklevel=2)\n        return sslobj\n\n\nclass HTTPException(Exception):\n    # Subclasses that define an __init__ must call Exception.__init__\n    # or define self.args.  Otherwise, str() will fail.\n    pass\n\nclass NotConnected(HTTPException):\n    pass\n\nclass InvalidURL(HTTPException):\n    pass\n\nclass UnknownProtocol(HTTPException):\n    def __init__(self, version):\n        self.args = version,\n        self.version = version\n\nclass UnknownTransferEncoding(HTTPException):\n    pass\n\nclass UnimplementedFileMode(HTTPException):\n    pass\n\nclass IncompleteRead(HTTPException):\n    def __init__(self, partial, expected=None):\n        self.args = partial,\n        self.partial = partial\n        self.expected = expected\n    def __repr__(self):\n        if self.expected is not None:\n            e = ', %i more expected' % self.expected\n        else:\n            e = ''\n        return 'IncompleteRead(%i bytes read%s)' % (len(self.partial), e)\n    def __str__(self):\n        return repr(self)\n\nclass ImproperConnectionState(HTTPException):\n    pass\n\nclass CannotSendRequest(ImproperConnectionState):\n    pass\n\nclass CannotSendHeader(ImproperConnectionState):\n    pass\n\nclass ResponseNotReady(ImproperConnectionState):\n    pass\n\nclass BadStatusLine(HTTPException):\n    def __init__(self, line):\n        if not line:\n            line = repr(line)\n        self.args = line,\n        self.line = line\n\nclass LineTooLong(HTTPException):\n    def __init__(self, line_type):\n        HTTPException.__init__(self, \"got more than %d bytes when reading %s\"\n                                     % (_MAXLINE, line_type))\n\n# for backwards compatibility\nerror = HTTPException\n\nclass LineAndFileWrapper:\n    \"\"\"A limited file-like object for HTTP/0.9 responses.\"\"\"\n\n    # The status-line parsing code calls readline(), which normally\n    # get the HTTP status line.  For a 0.9 response, however, this is\n    # actually the first line of the body!  Clients need to get a\n    # readable file object that contains that line.\n\n    def __init__(self, line, file):\n        self._line = line\n        self._file = file\n        self._line_consumed = 0\n        self._line_offset = 0\n        self._line_left = len(line)\n\n    def __getattr__(self, attr):\n        return getattr(self._file, attr)\n\n    def _done(self):\n        # called when the last byte is read from the line.  After the\n        # call, all read methods are delegated to the underlying file\n        # object.\n        self._line_consumed = 1\n        self.read = self._file.read\n        self.readline = self._file.readline\n        self.readlines = self._file.readlines\n\n    def read(self, amt=None):\n        if self._line_consumed:\n            return self._file.read(amt)\n        assert self._line_left\n        if amt is None or amt > self._line_left:\n            s = self._line[self._line_offset:]\n            self._done()\n            if amt is None:\n                return s + self._file.read()\n            else:\n                return s + self._file.read(amt - len(s))\n        else:\n            assert amt <= self._line_left\n            i = self._line_offset\n            j = i + amt\n            s = self._line[i:j]\n            self._line_offset = j\n            self._line_left -= amt\n            if self._line_left == 0:\n                self._done()\n            return s\n\n    def readline(self):\n        if self._line_consumed:\n            return self._file.readline()\n        assert self._line_left\n        s = self._line[self._line_offset:]\n        self._done()\n        return s\n\n    def readlines(self, size=None):\n        if self._line_consumed:\n            return self._file.readlines(size)\n        assert self._line_left\n        L = [self._line[self._line_offset:]]\n        self._done()\n        if size is None:\n            return L + self._file.readlines()\n        else:\n            return L + self._file.readlines(size)\n", 
    "importlib.__init__": "\"\"\"Backport of importlib.import_module from 3.x.\"\"\"\n# While not critical (and in no way guaranteed!), it would be nice to keep this\n# code compatible with Python 2.3.\nimport sys\n\ndef _resolve_name(name, package, level):\n    \"\"\"Return the absolute name of the module to be imported.\"\"\"\n    if not hasattr(package, 'rindex'):\n        raise ValueError(\"'package' not set to a string\")\n    dot = len(package)\n    for x in xrange(level, 1, -1):\n        try:\n            dot = package.rindex('.', 0, dot)\n        except ValueError:\n            raise ValueError(\"attempted relative import beyond top-level \"\n                              \"package\")\n    return \"%s.%s\" % (package[:dot], name)\n\n\ndef import_module(name, package=None):\n    \"\"\"Import a module.\n\n    The 'package' argument is required when performing a relative import. It\n    specifies the package to use as the anchor point from which to resolve the\n    relative import to an absolute import.\n\n    \"\"\"\n    if name.startswith('.'):\n        if not package:\n            raise TypeError(\"relative imports require the 'package' argument\")\n        level = 0\n        for character in name:\n            if character != '.':\n                break\n            level += 1\n        name = _resolve_name(name[level:], package, level)\n    __import__(name)\n    return sys.modules[name]\n", 
    "inspect": "# -*- coding: utf-8 -*-\n\"\"\"Get useful information from live Python objects.\n\nThis module encapsulates the interface provided by the internal special\nattributes (func_*, co_*, im_*, tb_*, etc.) in a friendlier fashion.\nIt also provides some help for examining source code and class layout.\n\nHere are some of the useful functions provided by this module:\n\n    ismodule(), isclass(), ismethod(), isfunction(), isgeneratorfunction(),\n        isgenerator(), istraceback(), isframe(), iscode(), isbuiltin(),\n        isroutine() - check object types\n    getmembers() - get members of an object that satisfy a given condition\n\n    getfile(), getsourcefile(), getsource() - find an object's source code\n    getdoc(), getcomments() - get documentation on an object\n    getmodule() - determine the module that an object came from\n    getclasstree() - arrange classes so as to represent their hierarchy\n\n    getargspec(), getargvalues(), getcallargs() - get info about function arguments\n    formatargspec(), formatargvalues() - format an argument spec\n    getouterframes(), getinnerframes() - get info about frames\n    currentframe() - get the current stack frame\n    stack(), trace() - get info about frames on the stack or in a traceback\n\"\"\"\n\n# This module is in the public domain.  No warranties.\n\n__author__ = 'Ka-Ping Yee <ping@lfw.org>'\n__date__ = '1 Jan 2001'\n\nimport sys\nimport os\nimport types\nimport string\nimport re\nimport dis\nimport imp\nimport tokenize\nimport linecache\nfrom operator import attrgetter\nfrom collections import namedtuple\n\n# These constants are from Include/code.h.\nCO_OPTIMIZED, CO_NEWLOCALS, CO_VARARGS, CO_VARKEYWORDS = 0x1, 0x2, 0x4, 0x8\nCO_NESTED, CO_GENERATOR, CO_NOFREE = 0x10, 0x20, 0x40\n# See Include/object.h\nTPFLAGS_IS_ABSTRACT = 1 << 20\n\n# ----------------------------------------------------------- type-checking\ndef ismodule(object):\n    \"\"\"Return true if the object is a module.\n\n    Module objects provide these attributes:\n        __doc__         documentation string\n        __file__        filename (missing for built-in modules)\"\"\"\n    return isinstance(object, types.ModuleType)\n\ndef isclass(object):\n    \"\"\"Return true if the object is a class.\n\n    Class objects provide these attributes:\n        __doc__         documentation string\n        __module__      name of module in which this class was defined\"\"\"\n    return isinstance(object, (type, types.ClassType))\n\ndef ismethod(object):\n    \"\"\"Return true if the object is an instance method.\n\n    Instance method objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this method was defined\n        im_class        class object in which this method belongs\n        im_func         function object containing implementation of method\n        im_self         instance to which this method is bound, or None\"\"\"\n    return isinstance(object, types.MethodType)\n\ndef ismethoddescriptor(object):\n    \"\"\"Return true if the object is a method descriptor.\n\n    But not if ismethod() or isclass() or isfunction() are true.\n\n    This is new in Python 2.2, and, for example, is true of int.__add__.\n    An object passing this test has a __get__ attribute but not a __set__\n    attribute, but beyond that the set of attributes varies.  __name__ is\n    usually sensible, and __doc__ often is.\n\n    Methods implemented via descriptors that also pass one of the other\n    tests return false from the ismethoddescriptor() test, simply because\n    the other tests promise more -- you can, e.g., count on having the\n    im_func attribute (etc) when an object passes ismethod().\"\"\"\n    return (hasattr(object, \"__get__\")\n            and not hasattr(object, \"__set__\") # else it's a data descriptor\n            and not ismethod(object)           # mutual exclusion\n            and not isfunction(object)\n            and not isclass(object))\n\ndef isdatadescriptor(object):\n    \"\"\"Return true if the object is a data descriptor.\n\n    Data descriptors have both a __get__ and a __set__ attribute.  Examples are\n    properties (defined in Python) and getsets and members (defined in C).\n    Typically, data descriptors will also have __name__ and __doc__ attributes\n    (properties, getsets, and members have both of these attributes), but this\n    is not guaranteed.\"\"\"\n    return (hasattr(object, \"__set__\") and hasattr(object, \"__get__\"))\n\nif hasattr(types, 'MemberDescriptorType'):\n    # CPython and equivalent\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.MemberDescriptorType)\nelse:\n    # Other implementations\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\nif hasattr(types, 'GetSetDescriptorType'):\n    # CPython and equivalent\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.GetSetDescriptorType)\nelse:\n    # Other implementations\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\ndef isfunction(object):\n    \"\"\"Return true if the object is a user-defined function.\n\n    Function objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this function was defined\n        func_code       code object containing compiled function bytecode\n        func_defaults   tuple of any default values for arguments\n        func_doc        (same as __doc__)\n        func_globals    global namespace in which this function was defined\n        func_name       (same as __name__)\"\"\"\n    return isinstance(object, types.FunctionType)\n\ndef isgeneratorfunction(object):\n    \"\"\"Return true if the object is a user-defined generator function.\n\n    Generator function objects provides same attributes as functions.\n\n    See help(isfunction) for attributes listing.\"\"\"\n    return bool((isfunction(object) or ismethod(object)) and\n                object.func_code.co_flags & CO_GENERATOR)\n\ndef isgenerator(object):\n    \"\"\"Return true if the object is a generator.\n\n    Generator objects provide these attributes:\n        __iter__        defined to support iteration over container\n        close           raises a new GeneratorExit exception inside the\n                        generator to terminate the iteration\n        gi_code         code object\n        gi_frame        frame object or possibly None once the generator has\n                        been exhausted\n        gi_running      set to 1 when generator is executing, 0 otherwise\n        next            return the next item from the container\n        send            resumes the generator and \"sends\" a value that becomes\n                        the result of the current yield-expression\n        throw           used to raise an exception inside the generator\"\"\"\n    return isinstance(object, types.GeneratorType)\n\ndef istraceback(object):\n    \"\"\"Return true if the object is a traceback.\n\n    Traceback objects provide these attributes:\n        tb_frame        frame object at this level\n        tb_lasti        index of last attempted instruction in bytecode\n        tb_lineno       current line number in Python source code\n        tb_next         next inner traceback object (called by this level)\"\"\"\n    return isinstance(object, types.TracebackType)\n\ndef isframe(object):\n    \"\"\"Return true if the object is a frame object.\n\n    Frame objects provide these attributes:\n        f_back          next outer frame object (this frame's caller)\n        f_builtins      built-in namespace seen by this frame\n        f_code          code object being executed in this frame\n        f_exc_traceback traceback if raised in this frame, or None\n        f_exc_type      exception type if raised in this frame, or None\n        f_exc_value     exception value if raised in this frame, or None\n        f_globals       global namespace seen by this frame\n        f_lasti         index of last attempted instruction in bytecode\n        f_lineno        current line number in Python source code\n        f_locals        local namespace seen by this frame\n        f_restricted    0 or 1 if frame is in restricted execution mode\n        f_trace         tracing function for this frame, or None\"\"\"\n    return isinstance(object, types.FrameType)\n\ndef iscode(object):\n    \"\"\"Return true if the object is a code object.\n\n    Code objects provide these attributes:\n        co_argcount     number of arguments (not including * or ** args)\n        co_code         string of raw compiled bytecode\n        co_consts       tuple of constants used in the bytecode\n        co_filename     name of file in which this code object was created\n        co_firstlineno  number of first line in Python source code\n        co_flags        bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg\n        co_lnotab       encoded mapping of line numbers to bytecode indices\n        co_name         name with which this code object was defined\n        co_names        tuple of names of local variables\n        co_nlocals      number of local variables\n        co_stacksize    virtual machine stack space required\n        co_varnames     tuple of names of arguments and local variables\"\"\"\n    return isinstance(object, types.CodeType)\n\ndef isbuiltin(object):\n    \"\"\"Return true if the object is a built-in function or method.\n\n    Built-in functions and methods provide these attributes:\n        __doc__         documentation string\n        __name__        original name of this function or method\n        __self__        instance to which a method is bound, or None\"\"\"\n    return isinstance(object, types.BuiltinFunctionType)\n\ndef isroutine(object):\n    \"\"\"Return true if the object is any kind of function or method.\"\"\"\n    return (isbuiltin(object)\n            or isfunction(object)\n            or ismethod(object)\n            or ismethoddescriptor(object))\n\ndef isabstract(object):\n    \"\"\"Return true if the object is an abstract base class (ABC).\"\"\"\n    return bool(isinstance(object, type) and object.__flags__ & TPFLAGS_IS_ABSTRACT)\n\ndef getmembers(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name.\n    Optionally, only return members that satisfy a given predicate.\"\"\"\n    results = []\n    for key in dir(object):\n        try:\n            value = getattr(object, key)\n        except AttributeError:\n            continue\n        if not predicate or predicate(value):\n            results.append((key, value))\n    results.sort()\n    return results\n\nAttribute = namedtuple('Attribute', 'name kind defining_class object')\n\ndef classify_class_attrs(cls):\n    \"\"\"Return list of attribute-descriptor tuples.\n\n    For each name in dir(cls), the return list contains a 4-tuple\n    with these elements:\n\n        0. The name (a string).\n\n        1. The kind of attribute this is, one of these strings:\n               'class method'    created via classmethod()\n               'static method'   created via staticmethod()\n               'property'        created via property()\n               'method'          any other flavor of method\n               'data'            not a method\n\n        2. The class which defined this attribute (a class).\n\n        3. The object as obtained directly from the defining class's\n           __dict__, not via getattr.  This is especially important for\n           data attributes:  C.data is just a data object, but\n           C.__dict__['data'] may be a data descriptor with additional\n           info, like a __doc__ string.\n    \"\"\"\n\n    mro = getmro(cls)\n    names = dir(cls)\n    result = []\n    for name in names:\n        # Get the object associated with the name, and where it was defined.\n        # Getting an obj from the __dict__ sometimes reveals more than\n        # using getattr.  Static and class methods are dramatic examples.\n        # Furthermore, some objects may raise an Exception when fetched with\n        # getattr(). This is the case with some descriptors (bug #1785).\n        # Thus, we only use getattr() as a last resort.\n        homecls = None\n        for base in (cls,) + mro:\n            if name in base.__dict__:\n                obj = base.__dict__[name]\n                homecls = base\n                break\n        else:\n            obj = getattr(cls, name)\n            homecls = getattr(obj, \"__objclass__\", homecls)\n\n        # Classify the object.\n        if isinstance(obj, staticmethod):\n            kind = \"static method\"\n        elif isinstance(obj, classmethod):\n            kind = \"class method\"\n        elif isinstance(obj, property):\n            kind = \"property\"\n        elif ismethoddescriptor(obj):\n            kind = \"method\"\n        elif isdatadescriptor(obj):\n            kind = \"data\"\n        else:\n            obj_via_getattr = getattr(cls, name)\n            if (ismethod(obj_via_getattr) or\n                ismethoddescriptor(obj_via_getattr)):\n                kind = \"method\"\n            else:\n                kind = \"data\"\n            obj = obj_via_getattr\n\n        result.append(Attribute(name, kind, homecls, obj))\n\n    return result\n\n# ----------------------------------------------------------- class helpers\ndef _searchbases(cls, accum):\n    # Simulate the \"classic class\" search order.\n    if cls in accum:\n        return\n    accum.append(cls)\n    for base in cls.__bases__:\n        _searchbases(base, accum)\n\ndef getmro(cls):\n    \"Return tuple of base classes (including cls) in method resolution order.\"\n    if hasattr(cls, \"__mro__\"):\n        return cls.__mro__\n    else:\n        result = []\n        _searchbases(cls, result)\n        return tuple(result)\n\n# -------------------------------------------------- source code extraction\ndef indentsize(line):\n    \"\"\"Return the indent size, in spaces, at the start of a line of text.\"\"\"\n    expline = string.expandtabs(line)\n    return len(expline) - len(string.lstrip(expline))\n\ndef getdoc(object):\n    \"\"\"Get the documentation string for an object.\n\n    All tabs are expanded to spaces.  To clean up docstrings that are\n    indented to line up with blocks of code, any whitespace than can be\n    uniformly removed from the second line onwards is removed.\"\"\"\n    try:\n        doc = object.__doc__\n    except AttributeError:\n        return None\n    if not isinstance(doc, types.StringTypes):\n        return None\n    return cleandoc(doc)\n\ndef cleandoc(doc):\n    \"\"\"Clean up indentation from docstrings.\n\n    Any whitespace that can be uniformly removed from the second line\n    onwards is removed.\"\"\"\n    try:\n        lines = string.split(string.expandtabs(doc), '\\n')\n    except UnicodeError:\n        return None\n    else:\n        # Find minimum indentation of any non-blank lines after first line.\n        margin = sys.maxint\n        for line in lines[1:]:\n            content = len(string.lstrip(line))\n            if content:\n                indent = len(line) - content\n                margin = min(margin, indent)\n        # Remove indentation.\n        if lines:\n            lines[0] = lines[0].lstrip()\n        if margin < sys.maxint:\n            for i in range(1, len(lines)): lines[i] = lines[i][margin:]\n        # Remove any trailing or leading blank lines.\n        while lines and not lines[-1]:\n            lines.pop()\n        while lines and not lines[0]:\n            lines.pop(0)\n        return string.join(lines, '\\n')\n\ndef getfile(object):\n    \"\"\"Work out which source or compiled file an object was defined in.\"\"\"\n    if ismodule(object):\n        if hasattr(object, '__file__'):\n            return object.__file__\n        raise TypeError('{!r} is a built-in module'.format(object))\n    if isclass(object):\n        object = sys.modules.get(object.__module__)\n        if hasattr(object, '__file__'):\n            return object.__file__\n        raise TypeError('{!r} is a built-in class'.format(object))\n    if ismethod(object):\n        object = object.im_func\n    if isfunction(object):\n        object = object.func_code\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        return object.co_filename\n    raise TypeError('{!r} is not a module, class, method, '\n                    'function, traceback, frame, or code object'.format(object))\n\nModuleInfo = namedtuple('ModuleInfo', 'name suffix mode module_type')\n\ndef getmoduleinfo(path):\n    \"\"\"Get the module name, suffix, mode, and module type for a given file.\"\"\"\n    filename = os.path.basename(path)\n    suffixes = map(lambda info:\n                   (-len(info[0]), info[0], info[1], info[2]),\n                    imp.get_suffixes())\n    suffixes.sort() # try longest suffixes first, in case they overlap\n    for neglen, suffix, mode, mtype in suffixes:\n        if filename[neglen:] == suffix:\n            return ModuleInfo(filename[:neglen], suffix, mode, mtype)\n\ndef getmodulename(path):\n    \"\"\"Return the module name for a given file, or None.\"\"\"\n    info = getmoduleinfo(path)\n    if info: return info[0]\n\ndef getsourcefile(object):\n    \"\"\"Return the filename that can be used to locate an object's source.\n    Return None if no way can be identified to get the source.\n    \"\"\"\n    filename = getfile(object)\n    if string.lower(filename[-4:]) in ('.pyc', '.pyo'):\n        filename = filename[:-4] + '.py'\n    for suffix, mode, kind in imp.get_suffixes():\n        if 'b' in mode and string.lower(filename[-len(suffix):]) == suffix:\n            # Looks like a binary file.  We want to only return a text file.\n            return None\n    if os.path.exists(filename):\n        return filename\n    # only return a non-existent filename if the module has a PEP 302 loader\n    if hasattr(getmodule(object, filename), '__loader__'):\n        return filename\n    # or it is in the linecache\n    if filename in linecache.cache:\n        return filename\n\ndef getabsfile(object, _filename=None):\n    \"\"\"Return an absolute path to the source or compiled file for an object.\n\n    The idea is for each object to have a unique origin, so this routine\n    normalizes the result as much as possible.\"\"\"\n    if _filename is None:\n        _filename = getsourcefile(object) or getfile(object)\n    return os.path.normcase(os.path.abspath(_filename))\n\nmodulesbyfile = {}\n_filesbymodname = {}\n\ndef getmodule(object, _filename=None):\n    \"\"\"Return the module an object was defined in, or None if not found.\"\"\"\n    if ismodule(object):\n        return object\n    if hasattr(object, '__module__'):\n        return sys.modules.get(object.__module__)\n    # Try the filename to modulename cache\n    if _filename is not None and _filename in modulesbyfile:\n        return sys.modules.get(modulesbyfile[_filename])\n    # Try the cache again with the absolute file name\n    try:\n        file = getabsfile(object, _filename)\n    except TypeError:\n        return None\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Update the filename to module name cache and check yet again\n    # Copy sys.modules in order to cope with changes while iterating\n    for modname, module in sys.modules.items():\n        if ismodule(module) and hasattr(module, '__file__'):\n            f = module.__file__\n            if f == _filesbymodname.get(modname, None):\n                # Have already mapped this module, so skip it\n                continue\n            _filesbymodname[modname] = f\n            f = getabsfile(module)\n            # Always map to the name the module knows itself by\n            modulesbyfile[f] = modulesbyfile[\n                os.path.realpath(f)] = module.__name__\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Check the main module\n    main = sys.modules['__main__']\n    if not hasattr(object, '__name__'):\n        return None\n    if hasattr(main, object.__name__):\n        mainobject = getattr(main, object.__name__)\n        if mainobject is object:\n            return main\n    # Check builtins\n    builtin = sys.modules['__builtin__']\n    if hasattr(builtin, object.__name__):\n        builtinobject = getattr(builtin, object.__name__)\n        if builtinobject is object:\n            return builtin\n\ndef findsource(object):\n    \"\"\"Return the entire source file and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of all the lines\n    in the file and the line number indexes a line in that list.  An IOError\n    is raised if the source code cannot be retrieved.\"\"\"\n\n    file = getfile(object)\n    sourcefile = getsourcefile(object)\n    if not sourcefile and file[:1] + file[-1:] != '<>':\n        raise IOError('source code not available')\n    file = sourcefile if sourcefile else file\n\n    module = getmodule(object, file)\n    if module:\n        lines = linecache.getlines(file, module.__dict__)\n    else:\n        lines = linecache.getlines(file)\n    if not lines:\n        raise IOError('could not get source code')\n\n    if ismodule(object):\n        return lines, 0\n\n    if isclass(object):\n        name = object.__name__\n        pat = re.compile(r'^(\\s*)class\\s*' + name + r'\\b')\n        # make some effort to find the best matching class definition:\n        # use the one with the least indentation, which is the one\n        # that's most probably not inside a function definition.\n        candidates = []\n        for i in range(len(lines)):\n            match = pat.match(lines[i])\n            if match:\n                # if it's at toplevel, it's already the best one\n                if lines[i][0] == 'c':\n                    return lines, i\n                # else add whitespace to candidate list\n                candidates.append((match.group(1), i))\n        if candidates:\n            # this will sort by whitespace, and by line number,\n            # less whitespace first\n            candidates.sort()\n            return lines, candidates[0][1]\n        else:\n            raise IOError('could not find class definition')\n\n    if ismethod(object):\n        object = object.im_func\n    if isfunction(object):\n        object = object.func_code\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        if not hasattr(object, 'co_firstlineno'):\n            raise IOError('could not find function definition')\n        lnum = object.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            if pat.match(lines[lnum]): break\n            lnum = lnum - 1\n        return lines, lnum\n    raise IOError('could not find code object')\n\ndef getcomments(object):\n    \"\"\"Get lines of comments immediately preceding an object's source code.\n\n    Returns None when source can't be found.\n    \"\"\"\n    try:\n        lines, lnum = findsource(object)\n    except (IOError, TypeError):\n        return None\n\n    if ismodule(object):\n        # Look for a comment block at the top of the file.\n        start = 0\n        if lines and lines[0][:2] == '#!': start = 1\n        while start < len(lines) and string.strip(lines[start]) in ('', '#'):\n            start = start + 1\n        if start < len(lines) and lines[start][:1] == '#':\n            comments = []\n            end = start\n            while end < len(lines) and lines[end][:1] == '#':\n                comments.append(string.expandtabs(lines[end]))\n                end = end + 1\n            return string.join(comments, '')\n\n    # Look for a preceding block of comments at the same indentation.\n    elif lnum > 0:\n        indent = indentsize(lines[lnum])\n        end = lnum - 1\n        if end >= 0 and string.lstrip(lines[end])[:1] == '#' and \\\n            indentsize(lines[end]) == indent:\n            comments = [string.lstrip(string.expandtabs(lines[end]))]\n            if end > 0:\n                end = end - 1\n                comment = string.lstrip(string.expandtabs(lines[end]))\n                while comment[:1] == '#' and indentsize(lines[end]) == indent:\n                    comments[:0] = [comment]\n                    end = end - 1\n                    if end < 0: break\n                    comment = string.lstrip(string.expandtabs(lines[end]))\n            while comments and string.strip(comments[0]) == '#':\n                comments[:1] = []\n            while comments and string.strip(comments[-1]) == '#':\n                comments[-1:] = []\n            return string.join(comments, '')\n\nclass EndOfBlock(Exception): pass\n\nclass BlockFinder:\n    \"\"\"Provide a tokeneater() method to detect the end of a code block.\"\"\"\n    def __init__(self):\n        self.indent = 0\n        self.islambda = False\n        self.started = False\n        self.passline = False\n        self.last = 1\n\n    def tokeneater(self, type, token, srow_scol, erow_ecol, line):\n        srow, scol = srow_scol\n        erow, ecol = erow_ecol\n        if not self.started:\n            # look for the first \"def\", \"class\" or \"lambda\"\n            if token in (\"def\", \"class\", \"lambda\"):\n                if token == \"lambda\":\n                    self.islambda = True\n                self.started = True\n            self.passline = True    # skip to the end of the line\n        elif type == tokenize.NEWLINE:\n            self.passline = False   # stop skipping when a NEWLINE is seen\n            self.last = srow\n            if self.islambda:       # lambdas always end at the first NEWLINE\n                raise EndOfBlock\n        elif self.passline:\n            pass\n        elif type == tokenize.INDENT:\n            self.indent = self.indent + 1\n            self.passline = True\n        elif type == tokenize.DEDENT:\n            self.indent = self.indent - 1\n            # the end of matching indent/dedent pairs end a block\n            # (note that this only works for \"def\"/\"class\" blocks,\n            #  not e.g. for \"if: else:\" or \"try: finally:\" blocks)\n            if self.indent <= 0:\n                raise EndOfBlock\n        elif self.indent == 0 and type not in (tokenize.COMMENT, tokenize.NL):\n            # any other token on the same indentation level end the previous\n            # block as well, except the pseudo-tokens COMMENT and NL.\n            raise EndOfBlock\n\ndef getblock(lines):\n    \"\"\"Extract the block of code at the top of the given list of lines.\"\"\"\n    blockfinder = BlockFinder()\n    try:\n        tokenize.tokenize(iter(lines).next, blockfinder.tokeneater)\n    except (EndOfBlock, IndentationError):\n        pass\n    return lines[:blockfinder.last]\n\ndef getsourcelines(object):\n    \"\"\"Return a list of source lines and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of the lines\n    corresponding to the object and the line number indicates where in the\n    original source file the first line of code was found.  An IOError is\n    raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = findsource(object)\n\n    if ismodule(object): return lines, 0\n    else: return getblock(lines[lnum:]), lnum + 1\n\ndef getsource(object):\n    \"\"\"Return the text of the source code for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a single string.  An\n    IOError is raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = getsourcelines(object)\n    return string.join(lines, '')\n\n# --------------------------------------------------- class tree extraction\ndef walktree(classes, children, parent):\n    \"\"\"Recursive helper function for getclasstree().\"\"\"\n    results = []\n    classes.sort(key=attrgetter('__module__', '__name__'))\n    for c in classes:\n        results.append((c, c.__bases__))\n        if c in children:\n            results.append(walktree(children[c], children, c))\n    return results\n\ndef getclasstree(classes, unique=0):\n    \"\"\"Arrange the given list of classes into a hierarchy of nested lists.\n\n    Where a nested list appears, it contains classes derived from the class\n    whose entry immediately precedes the list.  Each entry is a 2-tuple\n    containing a class and a tuple of its base classes.  If the 'unique'\n    argument is true, exactly one entry appears in the returned structure\n    for each class in the given list.  Otherwise, classes using multiple\n    inheritance and their descendants will appear multiple times.\"\"\"\n    children = {}\n    roots = []\n    for c in classes:\n        if c.__bases__:\n            for parent in c.__bases__:\n                if not parent in children:\n                    children[parent] = []\n                if c not in children[parent]:\n                    children[parent].append(c)\n                if unique and parent in classes: break\n        elif c not in roots:\n            roots.append(c)\n    for parent in children:\n        if parent not in classes:\n            roots.append(parent)\n    return walktree(roots, children, None)\n\n# ------------------------------------------------ argument list extraction\nArguments = namedtuple('Arguments', 'args varargs keywords')\n\ndef getargs(co):\n    \"\"\"Get information about the arguments accepted by a code object.\n\n    Three things are returned: (args, varargs, varkw), where 'args' is\n    a list of argument names (possibly containing nested lists), and\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\"\"\"\n\n    if not iscode(co):\n        if hasattr(len, 'func_code') and type(co) is type(len.func_code):\n            # PyPy extension: built-in function objects have a func_code too.\n            # There is no co_code on it, but co_argcount and co_varnames and\n            # co_flags are present.\n            pass\n        else:\n            raise TypeError('{!r} is not a code object'.format(co))\n\n    code = getattr(co, 'co_code', '')\n    nargs = co.co_argcount\n    names = co.co_varnames\n    args = list(names[:nargs])\n    step = 0\n\n    # The following acrobatics are for anonymous (tuple) arguments.\n    for i in range(nargs):\n        if args[i][:1] in ('', '.'):\n            stack, remain, count = [], [], []\n            while step < len(code):\n                op = ord(code[step])\n                step = step + 1\n                if op >= dis.HAVE_ARGUMENT:\n                    opname = dis.opname[op]\n                    value = ord(code[step]) + ord(code[step+1])*256\n                    step = step + 2\n                    if opname in ('UNPACK_TUPLE', 'UNPACK_SEQUENCE'):\n                        remain.append(value)\n                        count.append(value)\n                    elif opname == 'STORE_FAST':\n                        stack.append(names[value])\n\n                        # Special case for sublists of length 1: def foo((bar))\n                        # doesn't generate the UNPACK_TUPLE bytecode, so if\n                        # `remain` is empty here, we have such a sublist.\n                        if not remain:\n                            stack[0] = [stack[0]]\n                            break\n                        else:\n                            remain[-1] = remain[-1] - 1\n                            while remain[-1] == 0:\n                                remain.pop()\n                                size = count.pop()\n                                stack[-size:] = [stack[-size:]]\n                                if not remain: break\n                                remain[-1] = remain[-1] - 1\n                            if not remain: break\n            args[i] = stack[0]\n\n    varargs = None\n    if co.co_flags & CO_VARARGS:\n        varargs = co.co_varnames[nargs]\n        nargs = nargs + 1\n    varkw = None\n    if co.co_flags & CO_VARKEYWORDS:\n        varkw = co.co_varnames[nargs]\n    return Arguments(args, varargs, varkw)\n\nArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')\n\ndef getargspec(func):\n    \"\"\"Get the names and default values of a function's arguments.\n\n    A tuple of four things is returned: (args, varargs, varkw, defaults).\n    'args' is a list of the argument names (it may contain nested lists).\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'defaults' is an n-tuple of the default values of the last n arguments.\n    \"\"\"\n\n    if ismethod(func):\n        func = func.im_func\n    if not (isfunction(func) or\n            isbuiltin(func) and hasattr(func, 'func_code')):\n            # PyPy extension: this works for built-in functions too\n        raise TypeError('{!r} is not a Python function'.format(func))\n    args, varargs, varkw = getargs(func.func_code)\n    return ArgSpec(args, varargs, varkw, func.func_defaults)\n\nArgInfo = namedtuple('ArgInfo', 'args varargs keywords locals')\n\ndef getargvalues(frame):\n    \"\"\"Get information about arguments passed into a particular frame.\n\n    A tuple of four things is returned: (args, varargs, varkw, locals).\n    'args' is a list of the argument names (it may contain nested lists).\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'locals' is the locals dictionary of the given frame.\"\"\"\n    args, varargs, varkw = getargs(frame.f_code)\n    return ArgInfo(args, varargs, varkw, frame.f_locals)\n\ndef joinseq(seq):\n    if len(seq) == 1:\n        return '(' + seq[0] + ',)'\n    else:\n        return '(' + string.join(seq, ', ') + ')'\n\ndef strseq(object, convert, join=joinseq):\n    \"\"\"Recursively walk a sequence, stringifying each element.\"\"\"\n    if type(object) in (list, tuple):\n        return join(map(lambda o, c=convert, j=join: strseq(o, c, j), object))\n    else:\n        return convert(object)\n\ndef formatargspec(args, varargs=None, varkw=None, defaults=None,\n                  formatarg=str,\n                  formatvarargs=lambda name: '*' + name,\n                  formatvarkw=lambda name: '**' + name,\n                  formatvalue=lambda value: '=' + repr(value),\n                  join=joinseq):\n    \"\"\"Format an argument spec from the 4 values returned by getargspec.\n\n    The first four arguments are (args, varargs, varkw, defaults).  The\n    other four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    specs = []\n    if defaults:\n        firstdefault = len(args) - len(defaults)\n    for i, arg in enumerate(args):\n        spec = strseq(arg, formatarg, join)\n        if defaults and i >= firstdefault:\n            spec = spec + formatvalue(defaults[i - firstdefault])\n        specs.append(spec)\n    if varargs is not None:\n        specs.append(formatvarargs(varargs))\n    if varkw is not None:\n        specs.append(formatvarkw(varkw))\n    return '(' + string.join(specs, ', ') + ')'\n\ndef formatargvalues(args, varargs, varkw, locals,\n                    formatarg=str,\n                    formatvarargs=lambda name: '*' + name,\n                    formatvarkw=lambda name: '**' + name,\n                    formatvalue=lambda value: '=' + repr(value),\n                    join=joinseq):\n    \"\"\"Format an argument spec from the 4 values returned by getargvalues.\n\n    The first four arguments are (args, varargs, varkw, locals).  The\n    next four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    def convert(name, locals=locals,\n                formatarg=formatarg, formatvalue=formatvalue):\n        return formatarg(name) + formatvalue(locals[name])\n    specs = []\n    for i in range(len(args)):\n        specs.append(strseq(args[i], convert, join))\n    if varargs:\n        specs.append(formatvarargs(varargs) + formatvalue(locals[varargs]))\n    if varkw:\n        specs.append(formatvarkw(varkw) + formatvalue(locals[varkw]))\n    return '(' + string.join(specs, ', ') + ')'\n\ndef getcallargs(func, *positional, **named):\n    \"\"\"Get the mapping of arguments to values.\n\n    A dict is returned, with keys the function argument names (including the\n    names of the * and ** arguments, if any), and values the respective bound\n    values from 'positional' and 'named'.\"\"\"\n    args, varargs, varkw, defaults = getargspec(func)\n    f_name = func.__name__\n    arg2value = {}\n\n    # The following closures are basically because of tuple parameter unpacking.\n    assigned_tuple_params = []\n    def assign(arg, value):\n        if isinstance(arg, str):\n            arg2value[arg] = value\n        else:\n            assigned_tuple_params.append(arg)\n            value = iter(value)\n            for i, subarg in enumerate(arg):\n                try:\n                    subvalue = next(value)\n                except StopIteration:\n                    raise ValueError('need more than %d %s to unpack' %\n                                     (i, 'values' if i > 1 else 'value'))\n                assign(subarg,subvalue)\n            try:\n                next(value)\n            except StopIteration:\n                pass\n            else:\n                raise ValueError('too many values to unpack')\n    def is_assigned(arg):\n        if isinstance(arg,str):\n            return arg in arg2value\n        return arg in assigned_tuple_params\n    if ismethod(func) and func.im_self is not None:\n        # implicit 'self' (or 'cls' for classmethods) argument\n        positional = (func.im_self,) + positional\n    num_pos = len(positional)\n    num_total = num_pos + len(named)\n    num_args = len(args)\n    num_defaults = len(defaults) if defaults else 0\n    for arg, value in zip(args, positional):\n        assign(arg, value)\n    if varargs:\n        if num_pos > num_args:\n            assign(varargs, positional[-(num_pos-num_args):])\n        else:\n            assign(varargs, ())\n    elif 0 < num_args < num_pos:\n        raise TypeError('%s() takes %s %d %s (%d given)' % (\n            f_name, 'at most' if defaults else 'exactly', num_args,\n            'arguments' if num_args > 1 else 'argument', num_total))\n    elif num_args == 0 and num_total:\n        if varkw:\n            if num_pos:\n                # XXX: We should use num_pos, but Python also uses num_total:\n                raise TypeError('%s() takes exactly 0 arguments '\n                                '(%d given)' % (f_name, num_total))\n        else:\n            raise TypeError('%s() takes no arguments (%d given)' %\n                            (f_name, num_total))\n    for arg in args:\n        if isinstance(arg, str) and arg in named:\n            if is_assigned(arg):\n                raise TypeError(\"%s() got multiple values for keyword \"\n                                \"argument '%s'\" % (f_name, arg))\n            else:\n                assign(arg, named.pop(arg))\n    if defaults:    # fill in any missing values with the defaults\n        for arg, value in zip(args[-num_defaults:], defaults):\n            if not is_assigned(arg):\n                assign(arg, value)\n    if varkw:\n        assign(varkw, named)\n    elif named:\n        unexpected = next(iter(named))\n        if isinstance(unexpected, unicode):\n            unexpected = unexpected.encode(sys.getdefaultencoding(), 'replace')\n        raise TypeError(\"%s() got an unexpected keyword argument '%s'\" %\n                        (f_name, unexpected))\n    unassigned = num_args - len([arg for arg in args if is_assigned(arg)])\n    if unassigned:\n        num_required = num_args - num_defaults\n        raise TypeError('%s() takes %s %d %s (%d given)' % (\n            f_name, 'at least' if defaults else 'exactly', num_required,\n            'arguments' if num_required > 1 else 'argument', num_total))\n    return arg2value\n\n# -------------------------------------------------- stack frame extraction\n\nTraceback = namedtuple('Traceback', 'filename lineno function code_context index')\n\ndef getframeinfo(frame, context=1):\n    \"\"\"Get information about a frame or traceback object.\n\n    A tuple of five things is returned: the filename, the line number of\n    the current line, the function name, a list of lines of context from\n    the source code, and the index of the current line within that list.\n    The optional second argument specifies the number of lines of context\n    to return, which are centered around the current line.\"\"\"\n    if istraceback(frame):\n        lineno = frame.tb_lineno\n        frame = frame.tb_frame\n    else:\n        lineno = frame.f_lineno\n    if not isframe(frame):\n        raise TypeError('{!r} is not a frame or traceback object'.format(frame))\n\n    filename = getsourcefile(frame) or getfile(frame)\n    if context > 0:\n        start = lineno - 1 - context//2\n        try:\n            lines, lnum = findsource(frame)\n        except IOError:\n            lines = index = None\n        else:\n            start = max(start, 1)\n            start = max(0, min(start, len(lines) - context))\n            lines = lines[start:start+context]\n            index = lineno - 1 - start\n    else:\n        lines = index = None\n\n    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)\n\ndef getlineno(frame):\n    \"\"\"Get the line number from a frame object, allowing for optimization.\"\"\"\n    # FrameType.f_lineno is now a descriptor that grovels co_lnotab\n    return frame.f_lineno\n\ndef getouterframes(frame, context=1):\n    \"\"\"Get a list of records for a frame and all higher (calling) frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while frame:\n        framelist.append((frame,) + getframeinfo(frame, context))\n        frame = frame.f_back\n    return framelist\n\ndef getinnerframes(tb, context=1):\n    \"\"\"Get a list of records for a traceback's frame and all lower frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while tb:\n        framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n        tb = tb.tb_next\n    return framelist\n\nif hasattr(sys, '_getframe'):\n    currentframe = sys._getframe\nelse:\n    currentframe = lambda _=None: None\n\ndef stack(context=1):\n    \"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\n    return getouterframes(sys._getframe(1), context)\n\ndef trace(context=1):\n    \"\"\"Return a list of records for the stack below the current exception.\"\"\"\n    return getinnerframes(sys.exc_info()[2], context)\n", 
    "io": "\"\"\"The io module provides the Python interfaces to stream handling. The\nbuiltin open function is defined in this module.\n\nAt the top of the I/O hierarchy is the abstract base class IOBase. It\ndefines the basic interface to a stream. Note, however, that there is no\nseparation between reading and writing to streams; implementations are\nallowed to raise an IOError if they do not support a given operation.\n\nExtending IOBase is RawIOBase which deals simply with the reading and\nwriting of raw bytes to a stream. FileIO subclasses RawIOBase to provide\nan interface to OS files.\n\nBufferedIOBase deals with buffering on a raw byte stream (RawIOBase). Its\nsubclasses, BufferedWriter, BufferedReader, and BufferedRWPair buffer\nstreams that are readable, writable, and both respectively.\nBufferedRandom provides a buffered interface to random access\nstreams. BytesIO is a simple stream of in-memory bytes.\n\nAnother IOBase subclass, TextIOBase, deals with the encoding and decoding\nof streams into text. TextIOWrapper, which extends it, is a buffered text\ninterface to a buffered raw stream (`BufferedIOBase`). Finally, StringIO\nis a in-memory stream for text.\n\nArgument names are not part of the specification, and only the arguments\nof open() are intended to be used as keyword arguments.\n\ndata:\n\nDEFAULT_BUFFER_SIZE\n\n   An int containing the default buffer size used by the module's buffered\n   I/O classes. open() uses the file's blksize (as obtained by os.stat) if\n   possible.\n\"\"\"\n# New I/O library conforming to PEP 3116.\n\n__author__ = (\"Guido van Rossum <guido@python.org>, \"\n              \"Mike Verdone <mike.verdone@gmail.com>, \"\n              \"Mark Russell <mark.russell@zen.co.uk>, \"\n              \"Antoine Pitrou <solipsis@pitrou.net>, \"\n              \"Amaury Forgeot d'Arc <amauryfa@gmail.com>, \"\n              \"Benjamin Peterson <benjamin@python.org>\")\n\n__all__ = [\"BlockingIOError\", \"open\", \"IOBase\", \"RawIOBase\", \"FileIO\",\n           \"BytesIO\", \"StringIO\", \"BufferedIOBase\",\n           \"BufferedReader\", \"BufferedWriter\", \"BufferedRWPair\",\n           \"BufferedRandom\", \"TextIOBase\", \"TextIOWrapper\",\n           \"UnsupportedOperation\", \"SEEK_SET\", \"SEEK_CUR\", \"SEEK_END\"]\n\n\nimport _io\nimport abc\n\nfrom _io import (DEFAULT_BUFFER_SIZE, BlockingIOError, UnsupportedOperation,\n                 open, FileIO, BytesIO, StringIO, BufferedReader,\n                 BufferedWriter, BufferedRWPair, BufferedRandom,\n                 IncrementalNewlineDecoder, TextIOWrapper)\n\nOpenWrapper = _io.open # for compatibility with _pyio\n\n# for seek()\nSEEK_SET = 0\nSEEK_CUR = 1\nSEEK_END = 2\n\n# Declaring ABCs in C is tricky so we do it here.\n# Method descriptions and default implementations are inherited from the C\n# version however.\nclass IOBase(_io._IOBase):\n    __metaclass__ = abc.ABCMeta\n    __doc__ = _io._IOBase.__doc__\n\nclass RawIOBase(_io._RawIOBase, IOBase):\n    __doc__ = _io._RawIOBase.__doc__\n\nclass BufferedIOBase(_io._BufferedIOBase, IOBase):\n    __doc__ = _io._BufferedIOBase.__doc__\n\nclass TextIOBase(_io._TextIOBase, IOBase):\n    __doc__ = _io._TextIOBase.__doc__\n\nRawIOBase.register(FileIO)\n\nfor klass in (BytesIO, BufferedReader, BufferedWriter, BufferedRandom,\n              BufferedRWPair):\n    BufferedIOBase.register(klass)\n\nfor klass in (StringIO, TextIOWrapper):\n    TextIOBase.register(klass)\ndel klass\n", 
    "keyword": "#! /usr/bin/env python\n\n\"\"\"Keywords (from \"graminit.c\")\n\nThis file is automatically generated; please don't muck it up!\n\nTo update the symbols in this file, 'cd' to the top directory of\nthe python source tree after building the interpreter and run:\n\n    ./python Lib/keyword.py\n\"\"\"\n\n__all__ = [\"iskeyword\", \"kwlist\"]\n\nkwlist = [\n#--start keywords--\n        'and',\n        'as',\n        'assert',\n        'break',\n        'class',\n        'continue',\n        'def',\n        'del',\n        'elif',\n        'else',\n        'except',\n        'exec',\n        'finally',\n        'for',\n        'from',\n        'global',\n        'if',\n        'import',\n        'in',\n        'is',\n        'lambda',\n        'not',\n        'or',\n        'pass',\n        'print',\n        'raise',\n        'return',\n        'try',\n        'while',\n        'with',\n        'yield',\n#--end keywords--\n        ]\n\niskeyword = frozenset(kwlist).__contains__\n\ndef main():\n    import sys, re\n\n    args = sys.argv[1:]\n    iptfile = args and args[0] or \"Python/graminit.c\"\n    if len(args) > 1: optfile = args[1]\n    else: optfile = \"Lib/keyword.py\"\n\n    # scan the source file for keywords\n    fp = open(iptfile)\n    strprog = re.compile('\"([^\"]+)\"')\n    lines = []\n    for line in fp:\n        if '{1, \"' in line:\n            match = strprog.search(line)\n            if match:\n                lines.append(\"        '\" + match.group(1) + \"',\\n\")\n    fp.close()\n    lines.sort()\n\n    # load the output skeleton from the target\n    fp = open(optfile)\n    format = fp.readlines()\n    fp.close()\n\n    # insert the lines of keywords\n    try:\n        start = format.index(\"#--start keywords--\\n\") + 1\n        end = format.index(\"#--end keywords--\\n\")\n        format[start:end] = lines\n    except ValueError:\n        sys.stderr.write(\"target does not contain format markers\\n\")\n        sys.exit(1)\n\n    # write the output file\n    fp = open(optfile, 'w')\n    fp.write(''.join(format))\n    fp.close()\n\nif __name__ == \"__main__\":\n    main()\n", 
    "linecache": "\"\"\"Cache lines from files.\n\nThis is intended to read lines from modules imported -- hence if a filename\nis not found, it will look down the module search path for a file by\nthat name.\n\"\"\"\n\nimport sys\nimport os\n\n__all__ = [\"getline\", \"clearcache\", \"checkcache\"]\n\ndef getline(filename, lineno, module_globals=None):\n    lines = getlines(filename, module_globals)\n    if 1 <= lineno <= len(lines):\n        return lines[lineno-1]\n    else:\n        return ''\n\n\n# The cache\n\ncache = {} # The cache\n\n\ndef clearcache():\n    \"\"\"Clear the cache entirely.\"\"\"\n\n    global cache\n    cache = {}\n\n\ndef getlines(filename, module_globals=None):\n    \"\"\"Get the lines for a file from the cache.\n    Update the cache if it doesn't contain an entry for this file already.\"\"\"\n\n    if filename in cache:\n        return cache[filename][2]\n    else:\n        return updatecache(filename, module_globals)\n\n\ndef checkcache(filename=None):\n    \"\"\"Discard cache entries that are out of date.\n    (This is not checked upon each call!)\"\"\"\n\n    if filename is None:\n        filenames = cache.keys()\n    else:\n        if filename in cache:\n            filenames = [filename]\n        else:\n            return\n\n    for filename in filenames:\n        size, mtime, lines, fullname = cache[filename]\n        if mtime is None:\n            continue   # no-op for files loaded via a __loader__\n        try:\n            stat = os.stat(fullname)\n        except os.error:\n            del cache[filename]\n            continue\n        if size != stat.st_size or mtime != stat.st_mtime:\n            del cache[filename]\n\n\ndef updatecache(filename, module_globals=None):\n    \"\"\"Update a cache entry and return its list of lines.\n    If something's wrong, print a message, discard the cache entry,\n    and return an empty list.\"\"\"\n\n    if filename in cache:\n        del cache[filename]\n    if not filename or (filename.startswith('<') and filename.endswith('>')):\n        return []\n\n    fullname = filename\n    try:\n        stat = os.stat(fullname)\n    except OSError:\n        basename = filename\n\n        # Try for a __loader__, if available\n        if module_globals and '__loader__' in module_globals:\n            name = module_globals.get('__name__')\n            loader = module_globals['__loader__']\n            get_source = getattr(loader, 'get_source', None)\n\n            if name and get_source:\n                try:\n                    data = get_source(name)\n                except (ImportError, IOError):\n                    pass\n                else:\n                    if data is None:\n                        # No luck, the PEP302 loader cannot find the source\n                        # for this module.\n                        return []\n                    cache[filename] = (\n                        len(data), None,\n                        [line+'\\n' for line in data.splitlines()], fullname\n                    )\n                    return cache[filename][2]\n\n        # Try looking through the module search path, which is only useful\n        # when handling a relative filename.\n        if os.path.isabs(filename):\n            return []\n\n        for dirname in sys.path:\n            # When using imputil, sys.path may contain things other than\n            # strings; ignore them when it happens.\n            try:\n                fullname = os.path.join(dirname, basename)\n            except (TypeError, AttributeError):\n                # Not sufficiently string-like to do anything useful with.\n                continue\n            try:\n                stat = os.stat(fullname)\n                break\n            except os.error:\n                pass\n        else:\n            return []\n    try:\n        with open(fullname, 'rU') as fp:\n            lines = fp.readlines()\n    except IOError:\n        return []\n    if lines and not lines[-1].endswith('\\n'):\n        lines[-1] += '\\n'\n    size, mtime = stat.st_size, stat.st_mtime\n    cache[filename] = size, mtime, lines, fullname\n    return lines\n", 
    "locale": "\"\"\" Locale support.\n\n    The module provides low-level access to the C lib's locale APIs\n    and adds high level number formatting APIs as well as a locale\n    aliasing engine to complement these.\n\n    The aliasing engine includes support for many commonly used locale\n    names and maps them to values suitable for passing to the C lib's\n    setlocale() function. It also includes default encodings for all\n    supported locale names.\n\n\"\"\"\n\nimport sys\nimport encodings\nimport encodings.aliases\nimport re\nimport operator\nimport functools\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n# Try importing the _locale module.\n#\n# If this fails, fall back on a basic 'C' locale emulation.\n\n# Yuck:  LC_MESSAGES is non-standard:  can't tell whether it exists before\n# trying the import.  So __all__ is also fiddled at the end of the file.\n__all__ = [\"getlocale\", \"getdefaultlocale\", \"getpreferredencoding\", \"Error\",\n           \"setlocale\", \"resetlocale\", \"localeconv\", \"strcoll\", \"strxfrm\",\n           \"str\", \"atof\", \"atoi\", \"format\", \"format_string\", \"currency\",\n           \"normalize\", \"LC_CTYPE\", \"LC_COLLATE\", \"LC_TIME\", \"LC_MONETARY\",\n           \"LC_NUMERIC\", \"LC_ALL\", \"CHAR_MAX\"]\n\ntry:\n\n    from _locale import *\n\nexcept ImportError:\n\n    # Locale emulation\n\n    CHAR_MAX = 127\n    LC_ALL = 6\n    LC_COLLATE = 3\n    LC_CTYPE = 0\n    LC_MESSAGES = 5\n    LC_MONETARY = 4\n    LC_NUMERIC = 1\n    LC_TIME = 2\n    Error = ValueError\n\n    def localeconv():\n        \"\"\" localeconv() -> dict.\n            Returns numeric and monetary locale-specific parameters.\n        \"\"\"\n        # 'C' locale default values\n        return {'grouping': [127],\n                'currency_symbol': '',\n                'n_sign_posn': 127,\n                'p_cs_precedes': 127,\n                'n_cs_precedes': 127,\n                'mon_grouping': [],\n                'n_sep_by_space': 127,\n                'decimal_point': '.',\n                'negative_sign': '',\n                'positive_sign': '',\n                'p_sep_by_space': 127,\n                'int_curr_symbol': '',\n                'p_sign_posn': 127,\n                'thousands_sep': '',\n                'mon_thousands_sep': '',\n                'frac_digits': 127,\n                'mon_decimal_point': '',\n                'int_frac_digits': 127}\n\n    def setlocale(category, value=None):\n        \"\"\" setlocale(integer,string=None) -> string.\n            Activates/queries locale processing.\n        \"\"\"\n        if value not in (None, '', 'C'):\n            raise Error, '_locale emulation only supports \"C\" locale'\n        return 'C'\n\n    def strcoll(a,b):\n        \"\"\" strcoll(string,string) -> int.\n            Compares two strings according to the locale.\n        \"\"\"\n        return cmp(a,b)\n\n    def strxfrm(s):\n        \"\"\" strxfrm(string) -> string.\n            Returns a string that behaves for cmp locale-aware.\n        \"\"\"\n        return s\n\n\n_localeconv = localeconv\n\n# With this dict, you can override some items of localeconv's return value.\n# This is useful for testing purposes.\n_override_localeconv = {}\n\n@functools.wraps(_localeconv)\ndef localeconv():\n    d = _localeconv()\n    if _override_localeconv:\n        d.update(_override_localeconv)\n    return d\n\n\n### Number formatting APIs\n\n# Author: Martin von Loewis\n# improved by Georg Brandl\n\n# Iterate over grouping intervals\ndef _grouping_intervals(grouping):\n    last_interval = None\n    for interval in grouping:\n        # if grouping is -1, we are done\n        if interval == CHAR_MAX:\n            return\n        # 0: re-use last group ad infinitum\n        if interval == 0:\n            if last_interval is None:\n                raise ValueError(\"invalid grouping\")\n            while True:\n                yield last_interval\n        yield interval\n        last_interval = interval\n\n#perform the grouping from right to left\ndef _group(s, monetary=False):\n    conv = localeconv()\n    thousands_sep = conv[monetary and 'mon_thousands_sep' or 'thousands_sep']\n    grouping = conv[monetary and 'mon_grouping' or 'grouping']\n    if not grouping:\n        return (s, 0)\n    if s[-1] == ' ':\n        stripped = s.rstrip()\n        right_spaces = s[len(stripped):]\n        s = stripped\n    else:\n        right_spaces = ''\n    left_spaces = ''\n    groups = []\n    for interval in _grouping_intervals(grouping):\n        if not s or s[-1] not in \"0123456789\":\n            # only non-digit characters remain (sign, spaces)\n            left_spaces = s\n            s = ''\n            break\n        groups.append(s[-interval:])\n        s = s[:-interval]\n    if s:\n        groups.append(s)\n    groups.reverse()\n    return (\n        left_spaces + thousands_sep.join(groups) + right_spaces,\n        len(thousands_sep) * (len(groups) - 1)\n    )\n\n# Strip a given amount of excess padding from the given string\ndef _strip_padding(s, amount):\n    lpos = 0\n    while amount and s[lpos] == ' ':\n        lpos += 1\n        amount -= 1\n    rpos = len(s) - 1\n    while amount and s[rpos] == ' ':\n        rpos -= 1\n        amount -= 1\n    return s[lpos:rpos+1]\n\n_percent_re = re.compile(r'%(?:\\((?P<key>.*?)\\))?'\n                         r'(?P<modifiers>[-#0-9 +*.hlL]*?)[eEfFgGdiouxXcrs%]')\n\ndef format(percent, value, grouping=False, monetary=False, *additional):\n    \"\"\"Returns the locale-aware substitution of a %? specifier\n    (percent).\n\n    additional is for format strings which contain one or more\n    '*' modifiers.\"\"\"\n    # this is only for one-percent-specifier strings and this should be checked\n    match = _percent_re.match(percent)\n    if not match or len(match.group())!= len(percent):\n        raise ValueError((\"format() must be given exactly one %%char \"\n                         \"format specifier, %s not valid\") % repr(percent))\n    return _format(percent, value, grouping, monetary, *additional)\n\ndef _format(percent, value, grouping=False, monetary=False, *additional):\n    if additional:\n        formatted = percent % ((value,) + additional)\n    else:\n        formatted = percent % value\n    # floats and decimal ints need special action!\n    if percent[-1] in 'eEfFgG':\n        seps = 0\n        parts = formatted.split('.')\n        if grouping:\n            parts[0], seps = _group(parts[0], monetary=monetary)\n        decimal_point = localeconv()[monetary and 'mon_decimal_point'\n                                              or 'decimal_point']\n        formatted = decimal_point.join(parts)\n        if seps:\n            formatted = _strip_padding(formatted, seps)\n    elif percent[-1] in 'diu':\n        seps = 0\n        if grouping:\n            formatted, seps = _group(formatted, monetary=monetary)\n        if seps:\n            formatted = _strip_padding(formatted, seps)\n    return formatted\n\ndef format_string(f, val, grouping=False):\n    \"\"\"Formats a string in the same way that the % formatting would use,\n    but takes the current locale into account.\n    Grouping is applied if the third parameter is true.\"\"\"\n    percents = list(_percent_re.finditer(f))\n    new_f = _percent_re.sub('%s', f)\n\n    if operator.isMappingType(val):\n        new_val = []\n        for perc in percents:\n            if perc.group()[-1]=='%':\n                new_val.append('%')\n            else:\n                new_val.append(format(perc.group(), val, grouping))\n    else:\n        if not isinstance(val, tuple):\n            val = (val,)\n        new_val = []\n        i = 0\n        for perc in percents:\n            if perc.group()[-1]=='%':\n                new_val.append('%')\n            else:\n                starcount = perc.group('modifiers').count('*')\n                new_val.append(_format(perc.group(),\n                                      val[i],\n                                      grouping,\n                                      False,\n                                      *val[i+1:i+1+starcount]))\n                i += (1 + starcount)\n    val = tuple(new_val)\n\n    return new_f % val\n\ndef currency(val, symbol=True, grouping=False, international=False):\n    \"\"\"Formats val according to the currency settings\n    in the current locale.\"\"\"\n    conv = localeconv()\n\n    # check for illegal values\n    digits = conv[international and 'int_frac_digits' or 'frac_digits']\n    if digits == 127:\n        raise ValueError(\"Currency formatting is not possible using \"\n                         \"the 'C' locale.\")\n\n    s = format('%%.%if' % digits, abs(val), grouping, monetary=True)\n    # '<' and '>' are markers if the sign must be inserted between symbol and value\n    s = '<' + s + '>'\n\n    if symbol:\n        smb = conv[international and 'int_curr_symbol' or 'currency_symbol']\n        precedes = conv[val<0 and 'n_cs_precedes' or 'p_cs_precedes']\n        separated = conv[val<0 and 'n_sep_by_space' or 'p_sep_by_space']\n\n        if precedes:\n            s = smb + (separated and ' ' or '') + s\n        else:\n            s = s + (separated and ' ' or '') + smb\n\n    sign_pos = conv[val<0 and 'n_sign_posn' or 'p_sign_posn']\n    sign = conv[val<0 and 'negative_sign' or 'positive_sign']\n\n    if sign_pos == 0:\n        s = '(' + s + ')'\n    elif sign_pos == 1:\n        s = sign + s\n    elif sign_pos == 2:\n        s = s + sign\n    elif sign_pos == 3:\n        s = s.replace('<', sign)\n    elif sign_pos == 4:\n        s = s.replace('>', sign)\n    else:\n        # the default if nothing specified;\n        # this should be the most fitting sign position\n        s = sign + s\n\n    return s.replace('<', '').replace('>', '')\n\ndef str(val):\n    \"\"\"Convert float to integer, taking the locale into account.\"\"\"\n    return format(\"%.12g\", val)\n\ndef atof(string, func=float):\n    \"Parses a string as a float according to the locale settings.\"\n    #First, get rid of the grouping\n    ts = localeconv()['thousands_sep']\n    if ts:\n        string = string.replace(ts, '')\n    #next, replace the decimal point with a dot\n    dd = localeconv()['decimal_point']\n    if dd:\n        string = string.replace(dd, '.')\n    #finally, parse the string\n    return func(string)\n\ndef atoi(str):\n    \"Converts a string to an integer according to the locale settings.\"\n    return atof(str, int)\n\ndef _test():\n    setlocale(LC_ALL, \"\")\n    #do grouping\n    s1 = format(\"%d\", 123456789,1)\n    print s1, \"is\", atoi(s1)\n    #standard formatting\n    s1 = str(3.14)\n    print s1, \"is\", atof(s1)\n\n### Locale name aliasing engine\n\n# Author: Marc-Andre Lemburg, mal@lemburg.com\n# Various tweaks by Fredrik Lundh <fredrik@pythonware.com>\n\n# store away the low-level version of setlocale (it's\n# overridden below)\n_setlocale = setlocale\n\n# Avoid relying on the locale-dependent .lower() method\n# (see issue #1813).\n_ascii_lower_map = ''.join(\n    chr(x + 32 if x >= ord('A') and x <= ord('Z') else x)\n    for x in range(256)\n)\n\ndef _replace_encoding(code, encoding):\n    if '.' in code:\n        langname = code[:code.index('.')]\n    else:\n        langname = code\n    # Convert the encoding to a C lib compatible encoding string\n    norm_encoding = encodings.normalize_encoding(encoding)\n    #print('norm encoding: %r' % norm_encoding)\n    norm_encoding = encodings.aliases.aliases.get(norm_encoding,\n                                                  norm_encoding)\n    #print('aliased encoding: %r' % norm_encoding)\n    encoding = locale_encoding_alias.get(norm_encoding,\n                                         norm_encoding)\n    #print('found encoding %r' % encoding)\n    return langname + '.' + encoding\n\ndef normalize(localename):\n\n    \"\"\" Returns a normalized locale code for the given locale\n        name.\n\n        The returned locale code is formatted for use with\n        setlocale().\n\n        If normalization fails, the original name is returned\n        unchanged.\n\n        If the given encoding is not known, the function defaults to\n        the default encoding for the locale code just like setlocale()\n        does.\n\n    \"\"\"\n    # Normalize the locale name and extract the encoding and modifier\n    if isinstance(localename, _unicode):\n        localename = localename.encode('ascii')\n    code = localename.translate(_ascii_lower_map)\n    if ':' in code:\n        # ':' is sometimes used as encoding delimiter.\n        code = code.replace(':', '.')\n    if '@' in code:\n        code, modifier = code.split('@', 1)\n    else:\n        modifier = ''\n    if '.' in code:\n        langname, encoding = code.split('.')[:2]\n    else:\n        langname = code\n        encoding = ''\n\n    # First lookup: fullname (possibly with encoding and modifier)\n    lang_enc = langname\n    if encoding:\n        norm_encoding = encoding.replace('-', '')\n        norm_encoding = norm_encoding.replace('_', '')\n        lang_enc += '.' + norm_encoding\n    lookup_name = lang_enc\n    if modifier:\n        lookup_name += '@' + modifier\n    code = locale_alias.get(lookup_name, None)\n    if code is not None:\n        return code\n    #print('first lookup failed')\n\n    if modifier:\n        # Second try: fullname without modifier (possibly with encoding)\n        code = locale_alias.get(lang_enc, None)\n        if code is not None:\n            #print('lookup without modifier succeeded')\n            if '@' not in code:\n                return code + '@' + modifier\n            if code.split('@', 1)[1].translate(_ascii_lower_map) == modifier:\n                return code\n        #print('second lookup failed')\n\n    if encoding:\n        # Third try: langname (without encoding, possibly with modifier)\n        lookup_name = langname\n        if modifier:\n            lookup_name += '@' + modifier\n        code = locale_alias.get(lookup_name, None)\n        if code is not None:\n            #print('lookup without encoding succeeded')\n            if '@' not in code:\n                return _replace_encoding(code, encoding)\n            code, modifier = code.split('@', 1)\n            return _replace_encoding(code, encoding) + '@' + modifier\n\n        if modifier:\n            # Fourth try: langname (without encoding and modifier)\n            code = locale_alias.get(langname, None)\n            if code is not None:\n                #print('lookup without modifier and encoding succeeded')\n                if '@' not in code:\n                    return _replace_encoding(code, encoding) + '@' + modifier\n                code, defmod = code.split('@', 1)\n                if defmod.translate(_ascii_lower_map) == modifier:\n                    return _replace_encoding(code, encoding) + '@' + defmod\n\n    return localename\n\ndef _parse_localename(localename):\n\n    \"\"\" Parses the locale code for localename and returns the\n        result as tuple (language code, encoding).\n\n        The localename is normalized and passed through the locale\n        alias engine. A ValueError is raised in case the locale name\n        cannot be parsed.\n\n        The language code corresponds to RFC 1766.  code and encoding\n        can be None in case the values cannot be determined or are\n        unknown to this implementation.\n\n    \"\"\"\n    code = normalize(localename)\n    if '@' in code:\n        # Deal with locale modifiers\n        code, modifier = code.split('@', 1)\n        if modifier == 'euro' and '.' not in code:\n            # Assume Latin-9 for @euro locales. This is bogus,\n            # since some systems may use other encodings for these\n            # locales. Also, we ignore other modifiers.\n            return code, 'iso-8859-15'\n\n    if '.' in code:\n        return tuple(code.split('.')[:2])\n    elif code == 'C':\n        return None, None\n    raise ValueError, 'unknown locale: %s' % localename\n\ndef _build_localename(localetuple):\n\n    \"\"\" Builds a locale code from the given tuple (language code,\n        encoding).\n\n        No aliasing or normalizing takes place.\n\n    \"\"\"\n    language, encoding = localetuple\n    if language is None:\n        language = 'C'\n    if encoding is None:\n        return language\n    else:\n        return language + '.' + encoding\n\ndef getdefaultlocale(envvars=('LC_ALL', 'LC_CTYPE', 'LANG', 'LANGUAGE')):\n\n    \"\"\" Tries to determine the default locale settings and returns\n        them as tuple (language code, encoding).\n\n        According to POSIX, a program which has not called\n        setlocale(LC_ALL, \"\") runs using the portable 'C' locale.\n        Calling setlocale(LC_ALL, \"\") lets it use the default locale as\n        defined by the LANG variable. Since we don't want to interfere\n        with the current locale setting we thus emulate the behavior\n        in the way described above.\n\n        To maintain compatibility with other platforms, not only the\n        LANG variable is tested, but a list of variables given as\n        envvars parameter. The first found to be defined will be\n        used. envvars defaults to the search path used in GNU gettext;\n        it must always contain the variable name 'LANG'.\n\n        Except for the code 'C', the language code corresponds to RFC\n        1766.  code and encoding can be None in case the values cannot\n        be determined.\n\n    \"\"\"\n\n    try:\n        # check if it's supported by the _locale module\n        import _locale\n        code, encoding = _locale._getdefaultlocale()\n    except (ImportError, AttributeError):\n        pass\n    else:\n        # make sure the code/encoding values are valid\n        if sys.platform == \"win32\" and code and code[:2] == \"0x\":\n            # map windows language identifier to language name\n            code = windows_locale.get(int(code, 0))\n        # ...add other platform-specific processing here, if\n        # necessary...\n        return code, encoding\n\n    # fall back on POSIX behaviour\n    import os\n    lookup = os.environ.get\n    for variable in envvars:\n        localename = lookup(variable,None)\n        if localename:\n            if variable == 'LANGUAGE':\n                localename = localename.split(':')[0]\n            break\n    else:\n        localename = 'C'\n    return _parse_localename(localename)\n\n\ndef getlocale(category=LC_CTYPE):\n\n    \"\"\" Returns the current setting for the given locale category as\n        tuple (language code, encoding).\n\n        category may be one of the LC_* value except LC_ALL. It\n        defaults to LC_CTYPE.\n\n        Except for the code 'C', the language code corresponds to RFC\n        1766.  code and encoding can be None in case the values cannot\n        be determined.\n\n    \"\"\"\n    localename = _setlocale(category)\n    if category == LC_ALL and ';' in localename:\n        raise TypeError, 'category LC_ALL is not supported'\n    return _parse_localename(localename)\n\ndef setlocale(category, locale=None):\n\n    \"\"\" Set the locale for the given category.  The locale can be\n        a string, an iterable of two strings (language code and encoding),\n        or None.\n\n        Iterables are converted to strings using the locale aliasing\n        engine.  Locale strings are passed directly to the C lib.\n\n        category may be given as one of the LC_* values.\n\n    \"\"\"\n    if locale and type(locale) is not type(\"\"):\n        # convert to string\n        locale = normalize(_build_localename(locale))\n    return _setlocale(category, locale)\n\ndef resetlocale(category=LC_ALL):\n\n    \"\"\" Sets the locale for category to the default setting.\n\n        The default setting is determined by calling\n        getdefaultlocale(). category defaults to LC_ALL.\n\n    \"\"\"\n    _setlocale(category, _build_localename(getdefaultlocale()))\n\nif sys.platform.startswith(\"win\"):\n    # On Win32, this will return the ANSI code page\n    def getpreferredencoding(do_setlocale = True):\n        \"\"\"Return the charset that the user is likely using.\"\"\"\n        import _locale\n        return _locale._getdefaultlocale()[1]\nelse:\n    # On Unix, if CODESET is available, use that.\n    try:\n        CODESET\n    except NameError:\n        # Fall back to parsing environment variables :-(\n        def getpreferredencoding(do_setlocale = True):\n            \"\"\"Return the charset that the user is likely using,\n            by looking at environment variables.\"\"\"\n            return getdefaultlocale()[1]\n    else:\n        def getpreferredencoding(do_setlocale = True):\n            \"\"\"Return the charset that the user is likely using,\n            according to the system configuration.\"\"\"\n            if do_setlocale:\n                oldloc = setlocale(LC_CTYPE)\n                try:\n                    setlocale(LC_CTYPE, \"\")\n                except Error:\n                    pass\n                result = nl_langinfo(CODESET)\n                setlocale(LC_CTYPE, oldloc)\n                return result\n            else:\n                return nl_langinfo(CODESET)\n\n\n### Database\n#\n# The following data was extracted from the locale.alias file which\n# comes with X11 and then hand edited removing the explicit encoding\n# definitions and adding some more aliases. The file is usually\n# available as /usr/lib/X11/locale/locale.alias.\n#\n\n#\n# The local_encoding_alias table maps lowercase encoding alias names\n# to C locale encoding names (case-sensitive). Note that normalize()\n# first looks up the encoding in the encodings.aliases dictionary and\n# then applies this mapping to find the correct C lib name for the\n# encoding.\n#\nlocale_encoding_alias = {\n\n    # Mappings for non-standard encoding names used in locale names\n    '437':                          'C',\n    'c':                            'C',\n    'en':                           'ISO8859-1',\n    'jis':                          'JIS7',\n    'jis7':                         'JIS7',\n    'ajec':                         'eucJP',\n\n    # Mappings from Python codec names to C lib encoding names\n    'ascii':                        'ISO8859-1',\n    'latin_1':                      'ISO8859-1',\n    'iso8859_1':                    'ISO8859-1',\n    'iso8859_10':                   'ISO8859-10',\n    'iso8859_11':                   'ISO8859-11',\n    'iso8859_13':                   'ISO8859-13',\n    'iso8859_14':                   'ISO8859-14',\n    'iso8859_15':                   'ISO8859-15',\n    'iso8859_16':                   'ISO8859-16',\n    'iso8859_2':                    'ISO8859-2',\n    'iso8859_3':                    'ISO8859-3',\n    'iso8859_4':                    'ISO8859-4',\n    'iso8859_5':                    'ISO8859-5',\n    'iso8859_6':                    'ISO8859-6',\n    'iso8859_7':                    'ISO8859-7',\n    'iso8859_8':                    'ISO8859-8',\n    'iso8859_9':                    'ISO8859-9',\n    'iso2022_jp':                   'JIS7',\n    'shift_jis':                    'SJIS',\n    'tactis':                       'TACTIS',\n    'euc_jp':                       'eucJP',\n    'euc_kr':                       'eucKR',\n    'utf_8':                        'UTF-8',\n    'koi8_r':                       'KOI8-R',\n    'koi8_u':                       'KOI8-U',\n    # XXX This list is still incomplete. If you know more\n    # mappings, please file a bug report. Thanks.\n}\n\n#\n# The locale_alias table maps lowercase alias names to C locale names\n# (case-sensitive). Encodings are always separated from the locale\n# name using a dot ('.'); they should only be given in case the\n# language name is needed to interpret the given encoding alias\n# correctly (CJK codes often have this need).\n#\n# Note that the normalize() function which uses this tables\n# removes '_' and '-' characters from the encoding part of the\n# locale name before doing the lookup. This saves a lot of\n# space in the table.\n#\n# MAL 2004-12-10:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.4\n# and older):\n#\n#    updated 'bg' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'bg_bg' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'bulgarian' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'cz' -> 'cz_CZ.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'cz_cz' -> 'cz_CZ.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'czech' -> 'cs_CS.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'dutch' -> 'nl_BE.ISO8859-1' to 'nl_NL.ISO8859-1'\n#    updated 'et' -> 'et_EE.ISO8859-4' to 'et_EE.ISO8859-15'\n#    updated 'et_ee' -> 'et_EE.ISO8859-4' to 'et_EE.ISO8859-15'\n#    updated 'fi' -> 'fi_FI.ISO8859-1' to 'fi_FI.ISO8859-15'\n#    updated 'fi_fi' -> 'fi_FI.ISO8859-1' to 'fi_FI.ISO8859-15'\n#    updated 'iw' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'iw_il' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'japanese' -> 'ja_JP.SJIS' to 'ja_JP.eucJP'\n#    updated 'lt' -> 'lt_LT.ISO8859-4' to 'lt_LT.ISO8859-13'\n#    updated 'lv' -> 'lv_LV.ISO8859-4' to 'lv_LV.ISO8859-13'\n#    updated 'sl' -> 'sl_CS.ISO8859-2' to 'sl_SI.ISO8859-2'\n#    updated 'slovene' -> 'sl_CS.ISO8859-2' to 'sl_SI.ISO8859-2'\n#    updated 'th_th' -> 'th_TH.TACTIS' to 'th_TH.ISO8859-11'\n#    updated 'zh_cn' -> 'zh_CN.eucCN' to 'zh_CN.gb2312'\n#    updated 'zh_cn.big5' -> 'zh_TW.eucTW' to 'zh_TW.big5'\n#    updated 'zh_tw' -> 'zh_TW.eucTW' to 'zh_TW.big5'\n#\n# MAL 2008-05-30:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.5\n# and older):\n#\n#    updated 'cs_cs.iso88592' -> 'cs_CZ.ISO8859-2' to 'cs_CS.ISO8859-2'\n#    updated 'serbocroatian' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh_hr.iso88592' -> 'sh_HR.ISO8859-2' to 'hr_HR.ISO8859-2'\n#    updated 'sh_sp' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh_yu' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sp' -> 'sp_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sp_yu' -> 'sp_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_sp' -> 'sr_SP.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sr_yu' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.cp1251@cyrillic' -> 'sr_YU.CP1251' to 'sr_CS.CP1251'\n#    updated 'sr_yu.iso88592' -> 'sr_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sr_yu.iso88595' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.iso88595@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.microsoftcp1251@cyrillic' -> 'sr_YU.CP1251' to 'sr_CS.CP1251'\n#    updated 'sr_yu.utf8@cyrillic' -> 'sr_YU.UTF-8' to 'sr_CS.UTF-8'\n#    updated 'sr_yu@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#\n# AP 2010-04-12:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.6.5\n# and older):\n#\n#    updated 'ru' -> 'ru_RU.ISO8859-5' to 'ru_RU.UTF-8'\n#    updated 'ru_ru' -> 'ru_RU.ISO8859-5' to 'ru_RU.UTF-8'\n#    updated 'serbocroatian' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sh' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sh_yu' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#    updated 'sr@cyrillic' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#    updated 'sr@latn' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_cs.utf8@latn' -> 'sr_CS.UTF-8' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_cs@latn' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_yu' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_yu.utf8@cyrillic' -> 'sr_CS.UTF-8' to 'sr_RS.UTF-8'\n#    updated 'sr_yu@cyrillic' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#\n# SS 2013-12-20:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.7.6\n# and older):\n#\n#    updated 'a3' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'a3_az' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'a3_az.koi8c' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'cs_cs.iso88592' -> 'cs_CS.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'hebrew' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'hebrew.iso88598' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'sd' -> 'sd_IN@devanagari.UTF-8' to 'sd_IN.UTF-8'\n#    updated 'sr@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#    updated 'sr_cs' -> 'sr_RS.UTF-8' to 'sr_CS.UTF-8'\n#    updated 'sr_cs.utf8@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#    updated 'sr_cs@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n\nlocale_alias = {\n    'a3':                                   'az_AZ.KOI8-C',\n    'a3_az':                                'az_AZ.KOI8-C',\n    'a3_az.koi8c':                          'az_AZ.KOI8-C',\n    'a3_az.koic':                           'az_AZ.KOI8-C',\n    'af':                                   'af_ZA.ISO8859-1',\n    'af_za':                                'af_ZA.ISO8859-1',\n    'af_za.iso88591':                       'af_ZA.ISO8859-1',\n    'am':                                   'am_ET.UTF-8',\n    'am_et':                                'am_ET.UTF-8',\n    'american':                             'en_US.ISO8859-1',\n    'american.iso88591':                    'en_US.ISO8859-1',\n    'ar':                                   'ar_AA.ISO8859-6',\n    'ar_aa':                                'ar_AA.ISO8859-6',\n    'ar_aa.iso88596':                       'ar_AA.ISO8859-6',\n    'ar_ae':                                'ar_AE.ISO8859-6',\n    'ar_ae.iso88596':                       'ar_AE.ISO8859-6',\n    'ar_bh':                                'ar_BH.ISO8859-6',\n    'ar_bh.iso88596':                       'ar_BH.ISO8859-6',\n    'ar_dz':                                'ar_DZ.ISO8859-6',\n    'ar_dz.iso88596':                       'ar_DZ.ISO8859-6',\n    'ar_eg':                                'ar_EG.ISO8859-6',\n    'ar_eg.iso88596':                       'ar_EG.ISO8859-6',\n    'ar_in':                                'ar_IN.UTF-8',\n    'ar_iq':                                'ar_IQ.ISO8859-6',\n    'ar_iq.iso88596':                       'ar_IQ.ISO8859-6',\n    'ar_jo':                                'ar_JO.ISO8859-6',\n    'ar_jo.iso88596':                       'ar_JO.ISO8859-6',\n    'ar_kw':                                'ar_KW.ISO8859-6',\n    'ar_kw.iso88596':                       'ar_KW.ISO8859-6',\n    'ar_lb':                                'ar_LB.ISO8859-6',\n    'ar_lb.iso88596':                       'ar_LB.ISO8859-6',\n    'ar_ly':                                'ar_LY.ISO8859-6',\n    'ar_ly.iso88596':                       'ar_LY.ISO8859-6',\n    'ar_ma':                                'ar_MA.ISO8859-6',\n    'ar_ma.iso88596':                       'ar_MA.ISO8859-6',\n    'ar_om':                                'ar_OM.ISO8859-6',\n    'ar_om.iso88596':                       'ar_OM.ISO8859-6',\n    'ar_qa':                                'ar_QA.ISO8859-6',\n    'ar_qa.iso88596':                       'ar_QA.ISO8859-6',\n    'ar_sa':                                'ar_SA.ISO8859-6',\n    'ar_sa.iso88596':                       'ar_SA.ISO8859-6',\n    'ar_sd':                                'ar_SD.ISO8859-6',\n    'ar_sd.iso88596':                       'ar_SD.ISO8859-6',\n    'ar_sy':                                'ar_SY.ISO8859-6',\n    'ar_sy.iso88596':                       'ar_SY.ISO8859-6',\n    'ar_tn':                                'ar_TN.ISO8859-6',\n    'ar_tn.iso88596':                       'ar_TN.ISO8859-6',\n    'ar_ye':                                'ar_YE.ISO8859-6',\n    'ar_ye.iso88596':                       'ar_YE.ISO8859-6',\n    'arabic':                               'ar_AA.ISO8859-6',\n    'arabic.iso88596':                      'ar_AA.ISO8859-6',\n    'as':                                   'as_IN.UTF-8',\n    'as_in':                                'as_IN.UTF-8',\n    'az':                                   'az_AZ.ISO8859-9E',\n    'az_az':                                'az_AZ.ISO8859-9E',\n    'az_az.iso88599e':                      'az_AZ.ISO8859-9E',\n    'be':                                   'be_BY.CP1251',\n    'be@latin':                             'be_BY.UTF-8@latin',\n    'be_by':                                'be_BY.CP1251',\n    'be_by.cp1251':                         'be_BY.CP1251',\n    'be_by.microsoftcp1251':                'be_BY.CP1251',\n    'be_by.utf8@latin':                     'be_BY.UTF-8@latin',\n    'be_by@latin':                          'be_BY.UTF-8@latin',\n    'bg':                                   'bg_BG.CP1251',\n    'bg_bg':                                'bg_BG.CP1251',\n    'bg_bg.cp1251':                         'bg_BG.CP1251',\n    'bg_bg.iso88595':                       'bg_BG.ISO8859-5',\n    'bg_bg.koi8r':                          'bg_BG.KOI8-R',\n    'bg_bg.microsoftcp1251':                'bg_BG.CP1251',\n    'bn_in':                                'bn_IN.UTF-8',\n    'bo_in':                                'bo_IN.UTF-8',\n    'bokmal':                               'nb_NO.ISO8859-1',\n    'bokm\\xe5l':                            'nb_NO.ISO8859-1',\n    'br':                                   'br_FR.ISO8859-1',\n    'br_fr':                                'br_FR.ISO8859-1',\n    'br_fr.iso88591':                       'br_FR.ISO8859-1',\n    'br_fr.iso885914':                      'br_FR.ISO8859-14',\n    'br_fr.iso885915':                      'br_FR.ISO8859-15',\n    'br_fr.iso885915@euro':                 'br_FR.ISO8859-15',\n    'br_fr.utf8@euro':                      'br_FR.UTF-8',\n    'br_fr@euro':                           'br_FR.ISO8859-15',\n    'bs':                                   'bs_BA.ISO8859-2',\n    'bs_ba':                                'bs_BA.ISO8859-2',\n    'bs_ba.iso88592':                       'bs_BA.ISO8859-2',\n    'bulgarian':                            'bg_BG.CP1251',\n    'c':                                    'C',\n    'c-french':                             'fr_CA.ISO8859-1',\n    'c-french.iso88591':                    'fr_CA.ISO8859-1',\n    'c.ascii':                              'C',\n    'c.en':                                 'C',\n    'c.iso88591':                           'en_US.ISO8859-1',\n    'c_c':                                  'C',\n    'c_c.c':                                'C',\n    'ca':                                   'ca_ES.ISO8859-1',\n    'ca_ad':                                'ca_AD.ISO8859-1',\n    'ca_ad.iso88591':                       'ca_AD.ISO8859-1',\n    'ca_ad.iso885915':                      'ca_AD.ISO8859-15',\n    'ca_ad.iso885915@euro':                 'ca_AD.ISO8859-15',\n    'ca_ad.utf8@euro':                      'ca_AD.UTF-8',\n    'ca_ad@euro':                           'ca_AD.ISO8859-15',\n    'ca_es':                                'ca_ES.ISO8859-1',\n    'ca_es.iso88591':                       'ca_ES.ISO8859-1',\n    'ca_es.iso885915':                      'ca_ES.ISO8859-15',\n    'ca_es.iso885915@euro':                 'ca_ES.ISO8859-15',\n    'ca_es.utf8@euro':                      'ca_ES.UTF-8',\n    'ca_es@euro':                           'ca_ES.ISO8859-15',\n    'ca_fr':                                'ca_FR.ISO8859-1',\n    'ca_fr.iso88591':                       'ca_FR.ISO8859-1',\n    'ca_fr.iso885915':                      'ca_FR.ISO8859-15',\n    'ca_fr.iso885915@euro':                 'ca_FR.ISO8859-15',\n    'ca_fr.utf8@euro':                      'ca_FR.UTF-8',\n    'ca_fr@euro':                           'ca_FR.ISO8859-15',\n    'ca_it':                                'ca_IT.ISO8859-1',\n    'ca_it.iso88591':                       'ca_IT.ISO8859-1',\n    'ca_it.iso885915':                      'ca_IT.ISO8859-15',\n    'ca_it.iso885915@euro':                 'ca_IT.ISO8859-15',\n    'ca_it.utf8@euro':                      'ca_IT.UTF-8',\n    'ca_it@euro':                           'ca_IT.ISO8859-15',\n    'catalan':                              'ca_ES.ISO8859-1',\n    'cextend':                              'en_US.ISO8859-1',\n    'cextend.en':                           'en_US.ISO8859-1',\n    'chinese-s':                            'zh_CN.eucCN',\n    'chinese-t':                            'zh_TW.eucTW',\n    'croatian':                             'hr_HR.ISO8859-2',\n    'cs':                                   'cs_CZ.ISO8859-2',\n    'cs_cs':                                'cs_CZ.ISO8859-2',\n    'cs_cs.iso88592':                       'cs_CZ.ISO8859-2',\n    'cs_cz':                                'cs_CZ.ISO8859-2',\n    'cs_cz.iso88592':                       'cs_CZ.ISO8859-2',\n    'cy':                                   'cy_GB.ISO8859-1',\n    'cy_gb':                                'cy_GB.ISO8859-1',\n    'cy_gb.iso88591':                       'cy_GB.ISO8859-1',\n    'cy_gb.iso885914':                      'cy_GB.ISO8859-14',\n    'cy_gb.iso885915':                      'cy_GB.ISO8859-15',\n    'cy_gb@euro':                           'cy_GB.ISO8859-15',\n    'cz':                                   'cs_CZ.ISO8859-2',\n    'cz_cz':                                'cs_CZ.ISO8859-2',\n    'czech':                                'cs_CZ.ISO8859-2',\n    'da':                                   'da_DK.ISO8859-1',\n    'da.iso885915':                         'da_DK.ISO8859-15',\n    'da_dk':                                'da_DK.ISO8859-1',\n    'da_dk.88591':                          'da_DK.ISO8859-1',\n    'da_dk.885915':                         'da_DK.ISO8859-15',\n    'da_dk.iso88591':                       'da_DK.ISO8859-1',\n    'da_dk.iso885915':                      'da_DK.ISO8859-15',\n    'da_dk@euro':                           'da_DK.ISO8859-15',\n    'danish':                               'da_DK.ISO8859-1',\n    'danish.iso88591':                      'da_DK.ISO8859-1',\n    'dansk':                                'da_DK.ISO8859-1',\n    'de':                                   'de_DE.ISO8859-1',\n    'de.iso885915':                         'de_DE.ISO8859-15',\n    'de_at':                                'de_AT.ISO8859-1',\n    'de_at.iso88591':                       'de_AT.ISO8859-1',\n    'de_at.iso885915':                      'de_AT.ISO8859-15',\n    'de_at.iso885915@euro':                 'de_AT.ISO8859-15',\n    'de_at.utf8@euro':                      'de_AT.UTF-8',\n    'de_at@euro':                           'de_AT.ISO8859-15',\n    'de_be':                                'de_BE.ISO8859-1',\n    'de_be.iso88591':                       'de_BE.ISO8859-1',\n    'de_be.iso885915':                      'de_BE.ISO8859-15',\n    'de_be.iso885915@euro':                 'de_BE.ISO8859-15',\n    'de_be.utf8@euro':                      'de_BE.UTF-8',\n    'de_be@euro':                           'de_BE.ISO8859-15',\n    'de_ch':                                'de_CH.ISO8859-1',\n    'de_ch.iso88591':                       'de_CH.ISO8859-1',\n    'de_ch.iso885915':                      'de_CH.ISO8859-15',\n    'de_ch@euro':                           'de_CH.ISO8859-15',\n    'de_de':                                'de_DE.ISO8859-1',\n    'de_de.88591':                          'de_DE.ISO8859-1',\n    'de_de.885915':                         'de_DE.ISO8859-15',\n    'de_de.885915@euro':                    'de_DE.ISO8859-15',\n    'de_de.iso88591':                       'de_DE.ISO8859-1',\n    'de_de.iso885915':                      'de_DE.ISO8859-15',\n    'de_de.iso885915@euro':                 'de_DE.ISO8859-15',\n    'de_de.utf8@euro':                      'de_DE.UTF-8',\n    'de_de@euro':                           'de_DE.ISO8859-15',\n    'de_lu':                                'de_LU.ISO8859-1',\n    'de_lu.iso88591':                       'de_LU.ISO8859-1',\n    'de_lu.iso885915':                      'de_LU.ISO8859-15',\n    'de_lu.iso885915@euro':                 'de_LU.ISO8859-15',\n    'de_lu.utf8@euro':                      'de_LU.UTF-8',\n    'de_lu@euro':                           'de_LU.ISO8859-15',\n    'deutsch':                              'de_DE.ISO8859-1',\n    'dutch':                                'nl_NL.ISO8859-1',\n    'dutch.iso88591':                       'nl_BE.ISO8859-1',\n    'ee':                                   'ee_EE.ISO8859-4',\n    'ee_ee':                                'ee_EE.ISO8859-4',\n    'ee_ee.iso88594':                       'ee_EE.ISO8859-4',\n    'eesti':                                'et_EE.ISO8859-1',\n    'el':                                   'el_GR.ISO8859-7',\n    'el_gr':                                'el_GR.ISO8859-7',\n    'el_gr.iso88597':                       'el_GR.ISO8859-7',\n    'el_gr@euro':                           'el_GR.ISO8859-15',\n    'en':                                   'en_US.ISO8859-1',\n    'en.iso88591':                          'en_US.ISO8859-1',\n    'en_au':                                'en_AU.ISO8859-1',\n    'en_au.iso88591':                       'en_AU.ISO8859-1',\n    'en_be':                                'en_BE.ISO8859-1',\n    'en_be@euro':                           'en_BE.ISO8859-15',\n    'en_bw':                                'en_BW.ISO8859-1',\n    'en_bw.iso88591':                       'en_BW.ISO8859-1',\n    'en_ca':                                'en_CA.ISO8859-1',\n    'en_ca.iso88591':                       'en_CA.ISO8859-1',\n    'en_gb':                                'en_GB.ISO8859-1',\n    'en_gb.88591':                          'en_GB.ISO8859-1',\n    'en_gb.iso88591':                       'en_GB.ISO8859-1',\n    'en_gb.iso885915':                      'en_GB.ISO8859-15',\n    'en_gb@euro':                           'en_GB.ISO8859-15',\n    'en_hk':                                'en_HK.ISO8859-1',\n    'en_hk.iso88591':                       'en_HK.ISO8859-1',\n    'en_ie':                                'en_IE.ISO8859-1',\n    'en_ie.iso88591':                       'en_IE.ISO8859-1',\n    'en_ie.iso885915':                      'en_IE.ISO8859-15',\n    'en_ie.iso885915@euro':                 'en_IE.ISO8859-15',\n    'en_ie.utf8@euro':                      'en_IE.UTF-8',\n    'en_ie@euro':                           'en_IE.ISO8859-15',\n    'en_in':                                'en_IN.ISO8859-1',\n    'en_nz':                                'en_NZ.ISO8859-1',\n    'en_nz.iso88591':                       'en_NZ.ISO8859-1',\n    'en_ph':                                'en_PH.ISO8859-1',\n    'en_ph.iso88591':                       'en_PH.ISO8859-1',\n    'en_sg':                                'en_SG.ISO8859-1',\n    'en_sg.iso88591':                       'en_SG.ISO8859-1',\n    'en_uk':                                'en_GB.ISO8859-1',\n    'en_us':                                'en_US.ISO8859-1',\n    'en_us.88591':                          'en_US.ISO8859-1',\n    'en_us.885915':                         'en_US.ISO8859-15',\n    'en_us.iso88591':                       'en_US.ISO8859-1',\n    'en_us.iso885915':                      'en_US.ISO8859-15',\n    'en_us.iso885915@euro':                 'en_US.ISO8859-15',\n    'en_us@euro':                           'en_US.ISO8859-15',\n    'en_us@euro@euro':                      'en_US.ISO8859-15',\n    'en_za':                                'en_ZA.ISO8859-1',\n    'en_za.88591':                          'en_ZA.ISO8859-1',\n    'en_za.iso88591':                       'en_ZA.ISO8859-1',\n    'en_za.iso885915':                      'en_ZA.ISO8859-15',\n    'en_za@euro':                           'en_ZA.ISO8859-15',\n    'en_zw':                                'en_ZW.ISO8859-1',\n    'en_zw.iso88591':                       'en_ZW.ISO8859-1',\n    'eng_gb':                               'en_GB.ISO8859-1',\n    'eng_gb.8859':                          'en_GB.ISO8859-1',\n    'english':                              'en_EN.ISO8859-1',\n    'english.iso88591':                     'en_EN.ISO8859-1',\n    'english_uk':                           'en_GB.ISO8859-1',\n    'english_uk.8859':                      'en_GB.ISO8859-1',\n    'english_united-states':                'en_US.ISO8859-1',\n    'english_united-states.437':            'C',\n    'english_us':                           'en_US.ISO8859-1',\n    'english_us.8859':                      'en_US.ISO8859-1',\n    'english_us.ascii':                     'en_US.ISO8859-1',\n    'eo':                                   'eo_XX.ISO8859-3',\n    'eo_eo':                                'eo_EO.ISO8859-3',\n    'eo_eo.iso88593':                       'eo_EO.ISO8859-3',\n    'eo_xx':                                'eo_XX.ISO8859-3',\n    'eo_xx.iso88593':                       'eo_XX.ISO8859-3',\n    'es':                                   'es_ES.ISO8859-1',\n    'es_ar':                                'es_AR.ISO8859-1',\n    'es_ar.iso88591':                       'es_AR.ISO8859-1',\n    'es_bo':                                'es_BO.ISO8859-1',\n    'es_bo.iso88591':                       'es_BO.ISO8859-1',\n    'es_cl':                                'es_CL.ISO8859-1',\n    'es_cl.iso88591':                       'es_CL.ISO8859-1',\n    'es_co':                                'es_CO.ISO8859-1',\n    'es_co.iso88591':                       'es_CO.ISO8859-1',\n    'es_cr':                                'es_CR.ISO8859-1',\n    'es_cr.iso88591':                       'es_CR.ISO8859-1',\n    'es_do':                                'es_DO.ISO8859-1',\n    'es_do.iso88591':                       'es_DO.ISO8859-1',\n    'es_ec':                                'es_EC.ISO8859-1',\n    'es_ec.iso88591':                       'es_EC.ISO8859-1',\n    'es_es':                                'es_ES.ISO8859-1',\n    'es_es.88591':                          'es_ES.ISO8859-1',\n    'es_es.iso88591':                       'es_ES.ISO8859-1',\n    'es_es.iso885915':                      'es_ES.ISO8859-15',\n    'es_es.iso885915@euro':                 'es_ES.ISO8859-15',\n    'es_es.utf8@euro':                      'es_ES.UTF-8',\n    'es_es@euro':                           'es_ES.ISO8859-15',\n    'es_gt':                                'es_GT.ISO8859-1',\n    'es_gt.iso88591':                       'es_GT.ISO8859-1',\n    'es_hn':                                'es_HN.ISO8859-1',\n    'es_hn.iso88591':                       'es_HN.ISO8859-1',\n    'es_mx':                                'es_MX.ISO8859-1',\n    'es_mx.iso88591':                       'es_MX.ISO8859-1',\n    'es_ni':                                'es_NI.ISO8859-1',\n    'es_ni.iso88591':                       'es_NI.ISO8859-1',\n    'es_pa':                                'es_PA.ISO8859-1',\n    'es_pa.iso88591':                       'es_PA.ISO8859-1',\n    'es_pa.iso885915':                      'es_PA.ISO8859-15',\n    'es_pa@euro':                           'es_PA.ISO8859-15',\n    'es_pe':                                'es_PE.ISO8859-1',\n    'es_pe.iso88591':                       'es_PE.ISO8859-1',\n    'es_pe.iso885915':                      'es_PE.ISO8859-15',\n    'es_pe@euro':                           'es_PE.ISO8859-15',\n    'es_pr':                                'es_PR.ISO8859-1',\n    'es_pr.iso88591':                       'es_PR.ISO8859-1',\n    'es_py':                                'es_PY.ISO8859-1',\n    'es_py.iso88591':                       'es_PY.ISO8859-1',\n    'es_py.iso885915':                      'es_PY.ISO8859-15',\n    'es_py@euro':                           'es_PY.ISO8859-15',\n    'es_sv':                                'es_SV.ISO8859-1',\n    'es_sv.iso88591':                       'es_SV.ISO8859-1',\n    'es_sv.iso885915':                      'es_SV.ISO8859-15',\n    'es_sv@euro':                           'es_SV.ISO8859-15',\n    'es_us':                                'es_US.ISO8859-1',\n    'es_us.iso88591':                       'es_US.ISO8859-1',\n    'es_uy':                                'es_UY.ISO8859-1',\n    'es_uy.iso88591':                       'es_UY.ISO8859-1',\n    'es_uy.iso885915':                      'es_UY.ISO8859-15',\n    'es_uy@euro':                           'es_UY.ISO8859-15',\n    'es_ve':                                'es_VE.ISO8859-1',\n    'es_ve.iso88591':                       'es_VE.ISO8859-1',\n    'es_ve.iso885915':                      'es_VE.ISO8859-15',\n    'es_ve@euro':                           'es_VE.ISO8859-15',\n    'estonian':                             'et_EE.ISO8859-1',\n    'et':                                   'et_EE.ISO8859-15',\n    'et_ee':                                'et_EE.ISO8859-15',\n    'et_ee.iso88591':                       'et_EE.ISO8859-1',\n    'et_ee.iso885913':                      'et_EE.ISO8859-13',\n    'et_ee.iso885915':                      'et_EE.ISO8859-15',\n    'et_ee.iso88594':                       'et_EE.ISO8859-4',\n    'et_ee@euro':                           'et_EE.ISO8859-15',\n    'eu':                                   'eu_ES.ISO8859-1',\n    'eu_es':                                'eu_ES.ISO8859-1',\n    'eu_es.iso88591':                       'eu_ES.ISO8859-1',\n    'eu_es.iso885915':                      'eu_ES.ISO8859-15',\n    'eu_es.iso885915@euro':                 'eu_ES.ISO8859-15',\n    'eu_es.utf8@euro':                      'eu_ES.UTF-8',\n    'eu_es@euro':                           'eu_ES.ISO8859-15',\n    'fa':                                   'fa_IR.UTF-8',\n    'fa_ir':                                'fa_IR.UTF-8',\n    'fa_ir.isiri3342':                      'fa_IR.ISIRI-3342',\n    'fi':                                   'fi_FI.ISO8859-15',\n    'fi.iso885915':                         'fi_FI.ISO8859-15',\n    'fi_fi':                                'fi_FI.ISO8859-15',\n    'fi_fi.88591':                          'fi_FI.ISO8859-1',\n    'fi_fi.iso88591':                       'fi_FI.ISO8859-1',\n    'fi_fi.iso885915':                      'fi_FI.ISO8859-15',\n    'fi_fi.iso885915@euro':                 'fi_FI.ISO8859-15',\n    'fi_fi.utf8@euro':                      'fi_FI.UTF-8',\n    'fi_fi@euro':                           'fi_FI.ISO8859-15',\n    'finnish':                              'fi_FI.ISO8859-1',\n    'finnish.iso88591':                     'fi_FI.ISO8859-1',\n    'fo':                                   'fo_FO.ISO8859-1',\n    'fo_fo':                                'fo_FO.ISO8859-1',\n    'fo_fo.iso88591':                       'fo_FO.ISO8859-1',\n    'fo_fo.iso885915':                      'fo_FO.ISO8859-15',\n    'fo_fo@euro':                           'fo_FO.ISO8859-15',\n    'fr':                                   'fr_FR.ISO8859-1',\n    'fr.iso885915':                         'fr_FR.ISO8859-15',\n    'fr_be':                                'fr_BE.ISO8859-1',\n    'fr_be.88591':                          'fr_BE.ISO8859-1',\n    'fr_be.iso88591':                       'fr_BE.ISO8859-1',\n    'fr_be.iso885915':                      'fr_BE.ISO8859-15',\n    'fr_be.iso885915@euro':                 'fr_BE.ISO8859-15',\n    'fr_be.utf8@euro':                      'fr_BE.UTF-8',\n    'fr_be@euro':                           'fr_BE.ISO8859-15',\n    'fr_ca':                                'fr_CA.ISO8859-1',\n    'fr_ca.88591':                          'fr_CA.ISO8859-1',\n    'fr_ca.iso88591':                       'fr_CA.ISO8859-1',\n    'fr_ca.iso885915':                      'fr_CA.ISO8859-15',\n    'fr_ca@euro':                           'fr_CA.ISO8859-15',\n    'fr_ch':                                'fr_CH.ISO8859-1',\n    'fr_ch.88591':                          'fr_CH.ISO8859-1',\n    'fr_ch.iso88591':                       'fr_CH.ISO8859-1',\n    'fr_ch.iso885915':                      'fr_CH.ISO8859-15',\n    'fr_ch@euro':                           'fr_CH.ISO8859-15',\n    'fr_fr':                                'fr_FR.ISO8859-1',\n    'fr_fr.88591':                          'fr_FR.ISO8859-1',\n    'fr_fr.iso88591':                       'fr_FR.ISO8859-1',\n    'fr_fr.iso885915':                      'fr_FR.ISO8859-15',\n    'fr_fr.iso885915@euro':                 'fr_FR.ISO8859-15',\n    'fr_fr.utf8@euro':                      'fr_FR.UTF-8',\n    'fr_fr@euro':                           'fr_FR.ISO8859-15',\n    'fr_lu':                                'fr_LU.ISO8859-1',\n    'fr_lu.88591':                          'fr_LU.ISO8859-1',\n    'fr_lu.iso88591':                       'fr_LU.ISO8859-1',\n    'fr_lu.iso885915':                      'fr_LU.ISO8859-15',\n    'fr_lu.iso885915@euro':                 'fr_LU.ISO8859-15',\n    'fr_lu.utf8@euro':                      'fr_LU.UTF-8',\n    'fr_lu@euro':                           'fr_LU.ISO8859-15',\n    'fran\\xe7ais':                          'fr_FR.ISO8859-1',\n    'fre_fr':                               'fr_FR.ISO8859-1',\n    'fre_fr.8859':                          'fr_FR.ISO8859-1',\n    'french':                               'fr_FR.ISO8859-1',\n    'french.iso88591':                      'fr_CH.ISO8859-1',\n    'french_france':                        'fr_FR.ISO8859-1',\n    'french_france.8859':                   'fr_FR.ISO8859-1',\n    'ga':                                   'ga_IE.ISO8859-1',\n    'ga_ie':                                'ga_IE.ISO8859-1',\n    'ga_ie.iso88591':                       'ga_IE.ISO8859-1',\n    'ga_ie.iso885914':                      'ga_IE.ISO8859-14',\n    'ga_ie.iso885915':                      'ga_IE.ISO8859-15',\n    'ga_ie.iso885915@euro':                 'ga_IE.ISO8859-15',\n    'ga_ie.utf8@euro':                      'ga_IE.UTF-8',\n    'ga_ie@euro':                           'ga_IE.ISO8859-15',\n    'galego':                               'gl_ES.ISO8859-1',\n    'galician':                             'gl_ES.ISO8859-1',\n    'gd':                                   'gd_GB.ISO8859-1',\n    'gd_gb':                                'gd_GB.ISO8859-1',\n    'gd_gb.iso88591':                       'gd_GB.ISO8859-1',\n    'gd_gb.iso885914':                      'gd_GB.ISO8859-14',\n    'gd_gb.iso885915':                      'gd_GB.ISO8859-15',\n    'gd_gb@euro':                           'gd_GB.ISO8859-15',\n    'ger_de':                               'de_DE.ISO8859-1',\n    'ger_de.8859':                          'de_DE.ISO8859-1',\n    'german':                               'de_DE.ISO8859-1',\n    'german.iso88591':                      'de_CH.ISO8859-1',\n    'german_germany':                       'de_DE.ISO8859-1',\n    'german_germany.8859':                  'de_DE.ISO8859-1',\n    'gl':                                   'gl_ES.ISO8859-1',\n    'gl_es':                                'gl_ES.ISO8859-1',\n    'gl_es.iso88591':                       'gl_ES.ISO8859-1',\n    'gl_es.iso885915':                      'gl_ES.ISO8859-15',\n    'gl_es.iso885915@euro':                 'gl_ES.ISO8859-15',\n    'gl_es.utf8@euro':                      'gl_ES.UTF-8',\n    'gl_es@euro':                           'gl_ES.ISO8859-15',\n    'greek':                                'el_GR.ISO8859-7',\n    'greek.iso88597':                       'el_GR.ISO8859-7',\n    'gu_in':                                'gu_IN.UTF-8',\n    'gv':                                   'gv_GB.ISO8859-1',\n    'gv_gb':                                'gv_GB.ISO8859-1',\n    'gv_gb.iso88591':                       'gv_GB.ISO8859-1',\n    'gv_gb.iso885914':                      'gv_GB.ISO8859-14',\n    'gv_gb.iso885915':                      'gv_GB.ISO8859-15',\n    'gv_gb@euro':                           'gv_GB.ISO8859-15',\n    'he':                                   'he_IL.ISO8859-8',\n    'he_il':                                'he_IL.ISO8859-8',\n    'he_il.cp1255':                         'he_IL.CP1255',\n    'he_il.iso88598':                       'he_IL.ISO8859-8',\n    'he_il.microsoftcp1255':                'he_IL.CP1255',\n    'hebrew':                               'he_IL.ISO8859-8',\n    'hebrew.iso88598':                      'he_IL.ISO8859-8',\n    'hi':                                   'hi_IN.ISCII-DEV',\n    'hi_in':                                'hi_IN.ISCII-DEV',\n    'hi_in.isciidev':                       'hi_IN.ISCII-DEV',\n    'hne':                                  'hne_IN.UTF-8',\n    'hne_in':                               'hne_IN.UTF-8',\n    'hr':                                   'hr_HR.ISO8859-2',\n    'hr_hr':                                'hr_HR.ISO8859-2',\n    'hr_hr.iso88592':                       'hr_HR.ISO8859-2',\n    'hrvatski':                             'hr_HR.ISO8859-2',\n    'hu':                                   'hu_HU.ISO8859-2',\n    'hu_hu':                                'hu_HU.ISO8859-2',\n    'hu_hu.iso88592':                       'hu_HU.ISO8859-2',\n    'hungarian':                            'hu_HU.ISO8859-2',\n    'icelandic':                            'is_IS.ISO8859-1',\n    'icelandic.iso88591':                   'is_IS.ISO8859-1',\n    'id':                                   'id_ID.ISO8859-1',\n    'id_id':                                'id_ID.ISO8859-1',\n    'in':                                   'id_ID.ISO8859-1',\n    'in_id':                                'id_ID.ISO8859-1',\n    'is':                                   'is_IS.ISO8859-1',\n    'is_is':                                'is_IS.ISO8859-1',\n    'is_is.iso88591':                       'is_IS.ISO8859-1',\n    'is_is.iso885915':                      'is_IS.ISO8859-15',\n    'is_is@euro':                           'is_IS.ISO8859-15',\n    'iso-8859-1':                           'en_US.ISO8859-1',\n    'iso-8859-15':                          'en_US.ISO8859-15',\n    'iso8859-1':                            'en_US.ISO8859-1',\n    'iso8859-15':                           'en_US.ISO8859-15',\n    'iso_8859_1':                           'en_US.ISO8859-1',\n    'iso_8859_15':                          'en_US.ISO8859-15',\n    'it':                                   'it_IT.ISO8859-1',\n    'it.iso885915':                         'it_IT.ISO8859-15',\n    'it_ch':                                'it_CH.ISO8859-1',\n    'it_ch.iso88591':                       'it_CH.ISO8859-1',\n    'it_ch.iso885915':                      'it_CH.ISO8859-15',\n    'it_ch@euro':                           'it_CH.ISO8859-15',\n    'it_it':                                'it_IT.ISO8859-1',\n    'it_it.88591':                          'it_IT.ISO8859-1',\n    'it_it.iso88591':                       'it_IT.ISO8859-1',\n    'it_it.iso885915':                      'it_IT.ISO8859-15',\n    'it_it.iso885915@euro':                 'it_IT.ISO8859-15',\n    'it_it.utf8@euro':                      'it_IT.UTF-8',\n    'it_it@euro':                           'it_IT.ISO8859-15',\n    'italian':                              'it_IT.ISO8859-1',\n    'italian.iso88591':                     'it_IT.ISO8859-1',\n    'iu':                                   'iu_CA.NUNACOM-8',\n    'iu_ca':                                'iu_CA.NUNACOM-8',\n    'iu_ca.nunacom8':                       'iu_CA.NUNACOM-8',\n    'iw':                                   'he_IL.ISO8859-8',\n    'iw_il':                                'he_IL.ISO8859-8',\n    'iw_il.iso88598':                       'he_IL.ISO8859-8',\n    'ja':                                   'ja_JP.eucJP',\n    'ja.jis':                               'ja_JP.JIS7',\n    'ja.sjis':                              'ja_JP.SJIS',\n    'ja_jp':                                'ja_JP.eucJP',\n    'ja_jp.ajec':                           'ja_JP.eucJP',\n    'ja_jp.euc':                            'ja_JP.eucJP',\n    'ja_jp.eucjp':                          'ja_JP.eucJP',\n    'ja_jp.iso-2022-jp':                    'ja_JP.JIS7',\n    'ja_jp.iso2022jp':                      'ja_JP.JIS7',\n    'ja_jp.jis':                            'ja_JP.JIS7',\n    'ja_jp.jis7':                           'ja_JP.JIS7',\n    'ja_jp.mscode':                         'ja_JP.SJIS',\n    'ja_jp.pck':                            'ja_JP.SJIS',\n    'ja_jp.sjis':                           'ja_JP.SJIS',\n    'ja_jp.ujis':                           'ja_JP.eucJP',\n    'japan':                                'ja_JP.eucJP',\n    'japanese':                             'ja_JP.eucJP',\n    'japanese-euc':                         'ja_JP.eucJP',\n    'japanese.euc':                         'ja_JP.eucJP',\n    'japanese.sjis':                        'ja_JP.SJIS',\n    'jp_jp':                                'ja_JP.eucJP',\n    'ka':                                   'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge':                                'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge.georgianacademy':                'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge.georgianps':                     'ka_GE.GEORGIAN-PS',\n    'ka_ge.georgianrs':                     'ka_GE.GEORGIAN-ACADEMY',\n    'kl':                                   'kl_GL.ISO8859-1',\n    'kl_gl':                                'kl_GL.ISO8859-1',\n    'kl_gl.iso88591':                       'kl_GL.ISO8859-1',\n    'kl_gl.iso885915':                      'kl_GL.ISO8859-15',\n    'kl_gl@euro':                           'kl_GL.ISO8859-15',\n    'km_kh':                                'km_KH.UTF-8',\n    'kn':                                   'kn_IN.UTF-8',\n    'kn_in':                                'kn_IN.UTF-8',\n    'ko':                                   'ko_KR.eucKR',\n    'ko_kr':                                'ko_KR.eucKR',\n    'ko_kr.euc':                            'ko_KR.eucKR',\n    'ko_kr.euckr':                          'ko_KR.eucKR',\n    'korean':                               'ko_KR.eucKR',\n    'korean.euc':                           'ko_KR.eucKR',\n    'ks':                                   'ks_IN.UTF-8',\n    'ks_in':                                'ks_IN.UTF-8',\n    'ks_in@devanagari':                     'ks_IN.UTF-8@devanagari',\n    'kw':                                   'kw_GB.ISO8859-1',\n    'kw_gb':                                'kw_GB.ISO8859-1',\n    'kw_gb.iso88591':                       'kw_GB.ISO8859-1',\n    'kw_gb.iso885914':                      'kw_GB.ISO8859-14',\n    'kw_gb.iso885915':                      'kw_GB.ISO8859-15',\n    'kw_gb@euro':                           'kw_GB.ISO8859-15',\n    'ky':                                   'ky_KG.UTF-8',\n    'ky_kg':                                'ky_KG.UTF-8',\n    'lithuanian':                           'lt_LT.ISO8859-13',\n    'lo':                                   'lo_LA.MULELAO-1',\n    'lo_la':                                'lo_LA.MULELAO-1',\n    'lo_la.cp1133':                         'lo_LA.IBM-CP1133',\n    'lo_la.ibmcp1133':                      'lo_LA.IBM-CP1133',\n    'lo_la.mulelao1':                       'lo_LA.MULELAO-1',\n    'lt':                                   'lt_LT.ISO8859-13',\n    'lt_lt':                                'lt_LT.ISO8859-13',\n    'lt_lt.iso885913':                      'lt_LT.ISO8859-13',\n    'lt_lt.iso88594':                       'lt_LT.ISO8859-4',\n    'lv':                                   'lv_LV.ISO8859-13',\n    'lv_lv':                                'lv_LV.ISO8859-13',\n    'lv_lv.iso885913':                      'lv_LV.ISO8859-13',\n    'lv_lv.iso88594':                       'lv_LV.ISO8859-4',\n    'mai':                                  'mai_IN.UTF-8',\n    'mai_in':                               'mai_IN.UTF-8',\n    'mi':                                   'mi_NZ.ISO8859-1',\n    'mi_nz':                                'mi_NZ.ISO8859-1',\n    'mi_nz.iso88591':                       'mi_NZ.ISO8859-1',\n    'mk':                                   'mk_MK.ISO8859-5',\n    'mk_mk':                                'mk_MK.ISO8859-5',\n    'mk_mk.cp1251':                         'mk_MK.CP1251',\n    'mk_mk.iso88595':                       'mk_MK.ISO8859-5',\n    'mk_mk.microsoftcp1251':                'mk_MK.CP1251',\n    'ml':                                   'ml_IN.UTF-8',\n    'ml_in':                                'ml_IN.UTF-8',\n    'mr':                                   'mr_IN.UTF-8',\n    'mr_in':                                'mr_IN.UTF-8',\n    'ms':                                   'ms_MY.ISO8859-1',\n    'ms_my':                                'ms_MY.ISO8859-1',\n    'ms_my.iso88591':                       'ms_MY.ISO8859-1',\n    'mt':                                   'mt_MT.ISO8859-3',\n    'mt_mt':                                'mt_MT.ISO8859-3',\n    'mt_mt.iso88593':                       'mt_MT.ISO8859-3',\n    'nb':                                   'nb_NO.ISO8859-1',\n    'nb_no':                                'nb_NO.ISO8859-1',\n    'nb_no.88591':                          'nb_NO.ISO8859-1',\n    'nb_no.iso88591':                       'nb_NO.ISO8859-1',\n    'nb_no.iso885915':                      'nb_NO.ISO8859-15',\n    'nb_no@euro':                           'nb_NO.ISO8859-15',\n    'ne_np':                                'ne_NP.UTF-8',\n    'nl':                                   'nl_NL.ISO8859-1',\n    'nl.iso885915':                         'nl_NL.ISO8859-15',\n    'nl_be':                                'nl_BE.ISO8859-1',\n    'nl_be.88591':                          'nl_BE.ISO8859-1',\n    'nl_be.iso88591':                       'nl_BE.ISO8859-1',\n    'nl_be.iso885915':                      'nl_BE.ISO8859-15',\n    'nl_be.iso885915@euro':                 'nl_BE.ISO8859-15',\n    'nl_be.utf8@euro':                      'nl_BE.UTF-8',\n    'nl_be@euro':                           'nl_BE.ISO8859-15',\n    'nl_nl':                                'nl_NL.ISO8859-1',\n    'nl_nl.88591':                          'nl_NL.ISO8859-1',\n    'nl_nl.iso88591':                       'nl_NL.ISO8859-1',\n    'nl_nl.iso885915':                      'nl_NL.ISO8859-15',\n    'nl_nl.iso885915@euro':                 'nl_NL.ISO8859-15',\n    'nl_nl.utf8@euro':                      'nl_NL.UTF-8',\n    'nl_nl@euro':                           'nl_NL.ISO8859-15',\n    'nn':                                   'nn_NO.ISO8859-1',\n    'nn_no':                                'nn_NO.ISO8859-1',\n    'nn_no.88591':                          'nn_NO.ISO8859-1',\n    'nn_no.iso88591':                       'nn_NO.ISO8859-1',\n    'nn_no.iso885915':                      'nn_NO.ISO8859-15',\n    'nn_no@euro':                           'nn_NO.ISO8859-15',\n    'no':                                   'no_NO.ISO8859-1',\n    'no@nynorsk':                           'ny_NO.ISO8859-1',\n    'no_no':                                'no_NO.ISO8859-1',\n    'no_no.88591':                          'no_NO.ISO8859-1',\n    'no_no.iso88591':                       'no_NO.ISO8859-1',\n    'no_no.iso885915':                      'no_NO.ISO8859-15',\n    'no_no.iso88591@bokmal':                'no_NO.ISO8859-1',\n    'no_no.iso88591@nynorsk':               'no_NO.ISO8859-1',\n    'no_no@euro':                           'no_NO.ISO8859-15',\n    'norwegian':                            'no_NO.ISO8859-1',\n    'norwegian.iso88591':                   'no_NO.ISO8859-1',\n    'nr':                                   'nr_ZA.ISO8859-1',\n    'nr_za':                                'nr_ZA.ISO8859-1',\n    'nr_za.iso88591':                       'nr_ZA.ISO8859-1',\n    'nso':                                  'nso_ZA.ISO8859-15',\n    'nso_za':                               'nso_ZA.ISO8859-15',\n    'nso_za.iso885915':                     'nso_ZA.ISO8859-15',\n    'ny':                                   'ny_NO.ISO8859-1',\n    'ny_no':                                'ny_NO.ISO8859-1',\n    'ny_no.88591':                          'ny_NO.ISO8859-1',\n    'ny_no.iso88591':                       'ny_NO.ISO8859-1',\n    'ny_no.iso885915':                      'ny_NO.ISO8859-15',\n    'ny_no@euro':                           'ny_NO.ISO8859-15',\n    'nynorsk':                              'nn_NO.ISO8859-1',\n    'oc':                                   'oc_FR.ISO8859-1',\n    'oc_fr':                                'oc_FR.ISO8859-1',\n    'oc_fr.iso88591':                       'oc_FR.ISO8859-1',\n    'oc_fr.iso885915':                      'oc_FR.ISO8859-15',\n    'oc_fr@euro':                           'oc_FR.ISO8859-15',\n    'or':                                   'or_IN.UTF-8',\n    'or_in':                                'or_IN.UTF-8',\n    'pa':                                   'pa_IN.UTF-8',\n    'pa_in':                                'pa_IN.UTF-8',\n    'pd':                                   'pd_US.ISO8859-1',\n    'pd_de':                                'pd_DE.ISO8859-1',\n    'pd_de.iso88591':                       'pd_DE.ISO8859-1',\n    'pd_de.iso885915':                      'pd_DE.ISO8859-15',\n    'pd_de@euro':                           'pd_DE.ISO8859-15',\n    'pd_us':                                'pd_US.ISO8859-1',\n    'pd_us.iso88591':                       'pd_US.ISO8859-1',\n    'pd_us.iso885915':                      'pd_US.ISO8859-15',\n    'pd_us@euro':                           'pd_US.ISO8859-15',\n    'ph':                                   'ph_PH.ISO8859-1',\n    'ph_ph':                                'ph_PH.ISO8859-1',\n    'ph_ph.iso88591':                       'ph_PH.ISO8859-1',\n    'pl':                                   'pl_PL.ISO8859-2',\n    'pl_pl':                                'pl_PL.ISO8859-2',\n    'pl_pl.iso88592':                       'pl_PL.ISO8859-2',\n    'polish':                               'pl_PL.ISO8859-2',\n    'portuguese':                           'pt_PT.ISO8859-1',\n    'portuguese.iso88591':                  'pt_PT.ISO8859-1',\n    'portuguese_brazil':                    'pt_BR.ISO8859-1',\n    'portuguese_brazil.8859':               'pt_BR.ISO8859-1',\n    'posix':                                'C',\n    'posix-utf2':                           'C',\n    'pp':                                   'pp_AN.ISO8859-1',\n    'pp_an':                                'pp_AN.ISO8859-1',\n    'pp_an.iso88591':                       'pp_AN.ISO8859-1',\n    'pt':                                   'pt_PT.ISO8859-1',\n    'pt.iso885915':                         'pt_PT.ISO8859-15',\n    'pt_br':                                'pt_BR.ISO8859-1',\n    'pt_br.88591':                          'pt_BR.ISO8859-1',\n    'pt_br.iso88591':                       'pt_BR.ISO8859-1',\n    'pt_br.iso885915':                      'pt_BR.ISO8859-15',\n    'pt_br@euro':                           'pt_BR.ISO8859-15',\n    'pt_pt':                                'pt_PT.ISO8859-1',\n    'pt_pt.88591':                          'pt_PT.ISO8859-1',\n    'pt_pt.iso88591':                       'pt_PT.ISO8859-1',\n    'pt_pt.iso885915':                      'pt_PT.ISO8859-15',\n    'pt_pt.iso885915@euro':                 'pt_PT.ISO8859-15',\n    'pt_pt.utf8@euro':                      'pt_PT.UTF-8',\n    'pt_pt@euro':                           'pt_PT.ISO8859-15',\n    'ro':                                   'ro_RO.ISO8859-2',\n    'ro_ro':                                'ro_RO.ISO8859-2',\n    'ro_ro.iso88592':                       'ro_RO.ISO8859-2',\n    'romanian':                             'ro_RO.ISO8859-2',\n    'ru':                                   'ru_RU.UTF-8',\n    'ru.koi8r':                             'ru_RU.KOI8-R',\n    'ru_ru':                                'ru_RU.UTF-8',\n    'ru_ru.cp1251':                         'ru_RU.CP1251',\n    'ru_ru.iso88595':                       'ru_RU.ISO8859-5',\n    'ru_ru.koi8r':                          'ru_RU.KOI8-R',\n    'ru_ru.microsoftcp1251':                'ru_RU.CP1251',\n    'ru_ua':                                'ru_UA.KOI8-U',\n    'ru_ua.cp1251':                         'ru_UA.CP1251',\n    'ru_ua.koi8u':                          'ru_UA.KOI8-U',\n    'ru_ua.microsoftcp1251':                'ru_UA.CP1251',\n    'rumanian':                             'ro_RO.ISO8859-2',\n    'russian':                              'ru_RU.ISO8859-5',\n    'rw':                                   'rw_RW.ISO8859-1',\n    'rw_rw':                                'rw_RW.ISO8859-1',\n    'rw_rw.iso88591':                       'rw_RW.ISO8859-1',\n    'sd':                                   'sd_IN.UTF-8',\n    'sd@devanagari':                        'sd_IN.UTF-8@devanagari',\n    'sd_in':                                'sd_IN.UTF-8',\n    'sd_in@devanagari':                     'sd_IN.UTF-8@devanagari',\n    'se_no':                                'se_NO.UTF-8',\n    'serbocroatian':                        'sr_RS.UTF-8@latin',\n    'sh':                                   'sr_RS.UTF-8@latin',\n    'sh_ba.iso88592@bosnia':                'sr_CS.ISO8859-2',\n    'sh_hr':                                'sh_HR.ISO8859-2',\n    'sh_hr.iso88592':                       'hr_HR.ISO8859-2',\n    'sh_sp':                                'sr_CS.ISO8859-2',\n    'sh_yu':                                'sr_RS.UTF-8@latin',\n    'si':                                   'si_LK.UTF-8',\n    'si_lk':                                'si_LK.UTF-8',\n    'sinhala':                              'si_LK.UTF-8',\n    'sk':                                   'sk_SK.ISO8859-2',\n    'sk_sk':                                'sk_SK.ISO8859-2',\n    'sk_sk.iso88592':                       'sk_SK.ISO8859-2',\n    'sl':                                   'sl_SI.ISO8859-2',\n    'sl_cs':                                'sl_CS.ISO8859-2',\n    'sl_si':                                'sl_SI.ISO8859-2',\n    'sl_si.iso88592':                       'sl_SI.ISO8859-2',\n    'slovak':                               'sk_SK.ISO8859-2',\n    'slovene':                              'sl_SI.ISO8859-2',\n    'slovenian':                            'sl_SI.ISO8859-2',\n    'sp':                                   'sr_CS.ISO8859-5',\n    'sp_yu':                                'sr_CS.ISO8859-5',\n    'spanish':                              'es_ES.ISO8859-1',\n    'spanish.iso88591':                     'es_ES.ISO8859-1',\n    'spanish_spain':                        'es_ES.ISO8859-1',\n    'spanish_spain.8859':                   'es_ES.ISO8859-1',\n    'sq':                                   'sq_AL.ISO8859-2',\n    'sq_al':                                'sq_AL.ISO8859-2',\n    'sq_al.iso88592':                       'sq_AL.ISO8859-2',\n    'sr':                                   'sr_RS.UTF-8',\n    'sr@cyrillic':                          'sr_RS.UTF-8',\n    'sr@latin':                             'sr_RS.UTF-8@latin',\n    'sr@latn':                              'sr_CS.UTF-8@latin',\n    'sr_cs':                                'sr_CS.UTF-8',\n    'sr_cs.iso88592':                       'sr_CS.ISO8859-2',\n    'sr_cs.iso88592@latn':                  'sr_CS.ISO8859-2',\n    'sr_cs.iso88595':                       'sr_CS.ISO8859-5',\n    'sr_cs.utf8@latn':                      'sr_CS.UTF-8@latin',\n    'sr_cs@latn':                           'sr_CS.UTF-8@latin',\n    'sr_me':                                'sr_ME.UTF-8',\n    'sr_rs':                                'sr_RS.UTF-8',\n    'sr_rs.utf8@latn':                      'sr_RS.UTF-8@latin',\n    'sr_rs@latin':                          'sr_RS.UTF-8@latin',\n    'sr_rs@latn':                           'sr_RS.UTF-8@latin',\n    'sr_sp':                                'sr_CS.ISO8859-2',\n    'sr_yu':                                'sr_RS.UTF-8@latin',\n    'sr_yu.cp1251@cyrillic':                'sr_CS.CP1251',\n    'sr_yu.iso88592':                       'sr_CS.ISO8859-2',\n    'sr_yu.iso88595':                       'sr_CS.ISO8859-5',\n    'sr_yu.iso88595@cyrillic':              'sr_CS.ISO8859-5',\n    'sr_yu.microsoftcp1251@cyrillic':       'sr_CS.CP1251',\n    'sr_yu.utf8@cyrillic':                  'sr_RS.UTF-8',\n    'sr_yu@cyrillic':                       'sr_RS.UTF-8',\n    'ss':                                   'ss_ZA.ISO8859-1',\n    'ss_za':                                'ss_ZA.ISO8859-1',\n    'ss_za.iso88591':                       'ss_ZA.ISO8859-1',\n    'st':                                   'st_ZA.ISO8859-1',\n    'st_za':                                'st_ZA.ISO8859-1',\n    'st_za.iso88591':                       'st_ZA.ISO8859-1',\n    'sv':                                   'sv_SE.ISO8859-1',\n    'sv.iso885915':                         'sv_SE.ISO8859-15',\n    'sv_fi':                                'sv_FI.ISO8859-1',\n    'sv_fi.iso88591':                       'sv_FI.ISO8859-1',\n    'sv_fi.iso885915':                      'sv_FI.ISO8859-15',\n    'sv_fi.iso885915@euro':                 'sv_FI.ISO8859-15',\n    'sv_fi.utf8@euro':                      'sv_FI.UTF-8',\n    'sv_fi@euro':                           'sv_FI.ISO8859-15',\n    'sv_se':                                'sv_SE.ISO8859-1',\n    'sv_se.88591':                          'sv_SE.ISO8859-1',\n    'sv_se.iso88591':                       'sv_SE.ISO8859-1',\n    'sv_se.iso885915':                      'sv_SE.ISO8859-15',\n    'sv_se@euro':                           'sv_SE.ISO8859-15',\n    'swedish':                              'sv_SE.ISO8859-1',\n    'swedish.iso88591':                     'sv_SE.ISO8859-1',\n    'ta':                                   'ta_IN.TSCII-0',\n    'ta_in':                                'ta_IN.TSCII-0',\n    'ta_in.tscii':                          'ta_IN.TSCII-0',\n    'ta_in.tscii0':                         'ta_IN.TSCII-0',\n    'te':                                   'te_IN.UTF-8',\n    'tg':                                   'tg_TJ.KOI8-C',\n    'tg_tj':                                'tg_TJ.KOI8-C',\n    'tg_tj.koi8c':                          'tg_TJ.KOI8-C',\n    'th':                                   'th_TH.ISO8859-11',\n    'th_th':                                'th_TH.ISO8859-11',\n    'th_th.iso885911':                      'th_TH.ISO8859-11',\n    'th_th.tactis':                         'th_TH.TIS620',\n    'th_th.tis620':                         'th_TH.TIS620',\n    'thai':                                 'th_TH.ISO8859-11',\n    'tl':                                   'tl_PH.ISO8859-1',\n    'tl_ph':                                'tl_PH.ISO8859-1',\n    'tl_ph.iso88591':                       'tl_PH.ISO8859-1',\n    'tn':                                   'tn_ZA.ISO8859-15',\n    'tn_za':                                'tn_ZA.ISO8859-15',\n    'tn_za.iso885915':                      'tn_ZA.ISO8859-15',\n    'tr':                                   'tr_TR.ISO8859-9',\n    'tr_tr':                                'tr_TR.ISO8859-9',\n    'tr_tr.iso88599':                       'tr_TR.ISO8859-9',\n    'ts':                                   'ts_ZA.ISO8859-1',\n    'ts_za':                                'ts_ZA.ISO8859-1',\n    'ts_za.iso88591':                       'ts_ZA.ISO8859-1',\n    'tt':                                   'tt_RU.TATAR-CYR',\n    'tt_ru':                                'tt_RU.TATAR-CYR',\n    'tt_ru.koi8c':                          'tt_RU.KOI8-C',\n    'tt_ru.tatarcyr':                       'tt_RU.TATAR-CYR',\n    'turkish':                              'tr_TR.ISO8859-9',\n    'turkish.iso88599':                     'tr_TR.ISO8859-9',\n    'uk':                                   'uk_UA.KOI8-U',\n    'uk_ua':                                'uk_UA.KOI8-U',\n    'uk_ua.cp1251':                         'uk_UA.CP1251',\n    'uk_ua.iso88595':                       'uk_UA.ISO8859-5',\n    'uk_ua.koi8u':                          'uk_UA.KOI8-U',\n    'uk_ua.microsoftcp1251':                'uk_UA.CP1251',\n    'univ':                                 'en_US.utf',\n    'universal':                            'en_US.utf',\n    'universal.utf8@ucs4':                  'en_US.UTF-8',\n    'ur':                                   'ur_PK.CP1256',\n    'ur_in':                                'ur_IN.UTF-8',\n    'ur_pk':                                'ur_PK.CP1256',\n    'ur_pk.cp1256':                         'ur_PK.CP1256',\n    'ur_pk.microsoftcp1256':                'ur_PK.CP1256',\n    'uz':                                   'uz_UZ.UTF-8',\n    'uz_uz':                                'uz_UZ.UTF-8',\n    'uz_uz.iso88591':                       'uz_UZ.ISO8859-1',\n    'uz_uz.utf8@cyrillic':                  'uz_UZ.UTF-8',\n    'uz_uz@cyrillic':                       'uz_UZ.UTF-8',\n    've':                                   've_ZA.UTF-8',\n    've_za':                                've_ZA.UTF-8',\n    'vi':                                   'vi_VN.TCVN',\n    'vi_vn':                                'vi_VN.TCVN',\n    'vi_vn.tcvn':                           'vi_VN.TCVN',\n    'vi_vn.tcvn5712':                       'vi_VN.TCVN',\n    'vi_vn.viscii':                         'vi_VN.VISCII',\n    'vi_vn.viscii111':                      'vi_VN.VISCII',\n    'wa':                                   'wa_BE.ISO8859-1',\n    'wa_be':                                'wa_BE.ISO8859-1',\n    'wa_be.iso88591':                       'wa_BE.ISO8859-1',\n    'wa_be.iso885915':                      'wa_BE.ISO8859-15',\n    'wa_be.iso885915@euro':                 'wa_BE.ISO8859-15',\n    'wa_be@euro':                           'wa_BE.ISO8859-15',\n    'xh':                                   'xh_ZA.ISO8859-1',\n    'xh_za':                                'xh_ZA.ISO8859-1',\n    'xh_za.iso88591':                       'xh_ZA.ISO8859-1',\n    'yi':                                   'yi_US.CP1255',\n    'yi_us':                                'yi_US.CP1255',\n    'yi_us.cp1255':                         'yi_US.CP1255',\n    'yi_us.microsoftcp1255':                'yi_US.CP1255',\n    'zh':                                   'zh_CN.eucCN',\n    'zh_cn':                                'zh_CN.gb2312',\n    'zh_cn.big5':                           'zh_TW.big5',\n    'zh_cn.euc':                            'zh_CN.eucCN',\n    'zh_cn.gb18030':                        'zh_CN.gb18030',\n    'zh_cn.gb2312':                         'zh_CN.gb2312',\n    'zh_cn.gbk':                            'zh_CN.gbk',\n    'zh_hk':                                'zh_HK.big5hkscs',\n    'zh_hk.big5':                           'zh_HK.big5',\n    'zh_hk.big5hk':                         'zh_HK.big5hkscs',\n    'zh_hk.big5hkscs':                      'zh_HK.big5hkscs',\n    'zh_tw':                                'zh_TW.big5',\n    'zh_tw.big5':                           'zh_TW.big5',\n    'zh_tw.euc':                            'zh_TW.eucTW',\n    'zh_tw.euctw':                          'zh_TW.eucTW',\n    'zu':                                   'zu_ZA.ISO8859-1',\n    'zu_za':                                'zu_ZA.ISO8859-1',\n    'zu_za.iso88591':                       'zu_ZA.ISO8859-1',\n}\n\n#\n# This maps Windows language identifiers to locale strings.\n#\n# This list has been updated from\n# http://msdn.microsoft.com/library/default.asp?url=/library/en-us/intl/nls_238z.asp\n# to include every locale up to Windows Vista.\n#\n# NOTE: this mapping is incomplete.  If your language is missing, please\n# submit a bug report to the Python bug tracker at http://bugs.python.org/\n# Make sure you include the missing language identifier and the suggested\n# locale code.\n#\n\nwindows_locale = {\n    0x0436: \"af_ZA\", # Afrikaans\n    0x041c: \"sq_AL\", # Albanian\n    0x0484: \"gsw_FR\",# Alsatian - France\n    0x045e: \"am_ET\", # Amharic - Ethiopia\n    0x0401: \"ar_SA\", # Arabic - Saudi Arabia\n    0x0801: \"ar_IQ\", # Arabic - Iraq\n    0x0c01: \"ar_EG\", # Arabic - Egypt\n    0x1001: \"ar_LY\", # Arabic - Libya\n    0x1401: \"ar_DZ\", # Arabic - Algeria\n    0x1801: \"ar_MA\", # Arabic - Morocco\n    0x1c01: \"ar_TN\", # Arabic - Tunisia\n    0x2001: \"ar_OM\", # Arabic - Oman\n    0x2401: \"ar_YE\", # Arabic - Yemen\n    0x2801: \"ar_SY\", # Arabic - Syria\n    0x2c01: \"ar_JO\", # Arabic - Jordan\n    0x3001: \"ar_LB\", # Arabic - Lebanon\n    0x3401: \"ar_KW\", # Arabic - Kuwait\n    0x3801: \"ar_AE\", # Arabic - United Arab Emirates\n    0x3c01: \"ar_BH\", # Arabic - Bahrain\n    0x4001: \"ar_QA\", # Arabic - Qatar\n    0x042b: \"hy_AM\", # Armenian\n    0x044d: \"as_IN\", # Assamese - India\n    0x042c: \"az_AZ\", # Azeri - Latin\n    0x082c: \"az_AZ\", # Azeri - Cyrillic\n    0x046d: \"ba_RU\", # Bashkir\n    0x042d: \"eu_ES\", # Basque - Russia\n    0x0423: \"be_BY\", # Belarusian\n    0x0445: \"bn_IN\", # Begali\n    0x201a: \"bs_BA\", # Bosnian - Cyrillic\n    0x141a: \"bs_BA\", # Bosnian - Latin\n    0x047e: \"br_FR\", # Breton - France\n    0x0402: \"bg_BG\", # Bulgarian\n#    0x0455: \"my_MM\", # Burmese - Not supported\n    0x0403: \"ca_ES\", # Catalan\n    0x0004: \"zh_CHS\",# Chinese - Simplified\n    0x0404: \"zh_TW\", # Chinese - Taiwan\n    0x0804: \"zh_CN\", # Chinese - PRC\n    0x0c04: \"zh_HK\", # Chinese - Hong Kong S.A.R.\n    0x1004: \"zh_SG\", # Chinese - Singapore\n    0x1404: \"zh_MO\", # Chinese - Macao S.A.R.\n    0x7c04: \"zh_CHT\",# Chinese - Traditional\n    0x0483: \"co_FR\", # Corsican - France\n    0x041a: \"hr_HR\", # Croatian\n    0x101a: \"hr_BA\", # Croatian - Bosnia\n    0x0405: \"cs_CZ\", # Czech\n    0x0406: \"da_DK\", # Danish\n    0x048c: \"gbz_AF\",# Dari - Afghanistan\n    0x0465: \"div_MV\",# Divehi - Maldives\n    0x0413: \"nl_NL\", # Dutch - The Netherlands\n    0x0813: \"nl_BE\", # Dutch - Belgium\n    0x0409: \"en_US\", # English - United States\n    0x0809: \"en_GB\", # English - United Kingdom\n    0x0c09: \"en_AU\", # English - Australia\n    0x1009: \"en_CA\", # English - Canada\n    0x1409: \"en_NZ\", # English - New Zealand\n    0x1809: \"en_IE\", # English - Ireland\n    0x1c09: \"en_ZA\", # English - South Africa\n    0x2009: \"en_JA\", # English - Jamaica\n    0x2409: \"en_CB\", # English - Carribbean\n    0x2809: \"en_BZ\", # English - Belize\n    0x2c09: \"en_TT\", # English - Trinidad\n    0x3009: \"en_ZW\", # English - Zimbabwe\n    0x3409: \"en_PH\", # English - Philippines\n    0x4009: \"en_IN\", # English - India\n    0x4409: \"en_MY\", # English - Malaysia\n    0x4809: \"en_IN\", # English - Singapore\n    0x0425: \"et_EE\", # Estonian\n    0x0438: \"fo_FO\", # Faroese\n    0x0464: \"fil_PH\",# Filipino\n    0x040b: \"fi_FI\", # Finnish\n    0x040c: \"fr_FR\", # French - France\n    0x080c: \"fr_BE\", # French - Belgium\n    0x0c0c: \"fr_CA\", # French - Canada\n    0x100c: \"fr_CH\", # French - Switzerland\n    0x140c: \"fr_LU\", # French - Luxembourg\n    0x180c: \"fr_MC\", # French - Monaco\n    0x0462: \"fy_NL\", # Frisian - Netherlands\n    0x0456: \"gl_ES\", # Galician\n    0x0437: \"ka_GE\", # Georgian\n    0x0407: \"de_DE\", # German - Germany\n    0x0807: \"de_CH\", # German - Switzerland\n    0x0c07: \"de_AT\", # German - Austria\n    0x1007: \"de_LU\", # German - Luxembourg\n    0x1407: \"de_LI\", # German - Liechtenstein\n    0x0408: \"el_GR\", # Greek\n    0x046f: \"kl_GL\", # Greenlandic - Greenland\n    0x0447: \"gu_IN\", # Gujarati\n    0x0468: \"ha_NG\", # Hausa - Latin\n    0x040d: \"he_IL\", # Hebrew\n    0x0439: \"hi_IN\", # Hindi\n    0x040e: \"hu_HU\", # Hungarian\n    0x040f: \"is_IS\", # Icelandic\n    0x0421: \"id_ID\", # Indonesian\n    0x045d: \"iu_CA\", # Inuktitut - Syllabics\n    0x085d: \"iu_CA\", # Inuktitut - Latin\n    0x083c: \"ga_IE\", # Irish - Ireland\n    0x0410: \"it_IT\", # Italian - Italy\n    0x0810: \"it_CH\", # Italian - Switzerland\n    0x0411: \"ja_JP\", # Japanese\n    0x044b: \"kn_IN\", # Kannada - India\n    0x043f: \"kk_KZ\", # Kazakh\n    0x0453: \"kh_KH\", # Khmer - Cambodia\n    0x0486: \"qut_GT\",# K'iche - Guatemala\n    0x0487: \"rw_RW\", # Kinyarwanda - Rwanda\n    0x0457: \"kok_IN\",# Konkani\n    0x0412: \"ko_KR\", # Korean\n    0x0440: \"ky_KG\", # Kyrgyz\n    0x0454: \"lo_LA\", # Lao - Lao PDR\n    0x0426: \"lv_LV\", # Latvian\n    0x0427: \"lt_LT\", # Lithuanian\n    0x082e: \"dsb_DE\",# Lower Sorbian - Germany\n    0x046e: \"lb_LU\", # Luxembourgish\n    0x042f: \"mk_MK\", # FYROM Macedonian\n    0x043e: \"ms_MY\", # Malay - Malaysia\n    0x083e: \"ms_BN\", # Malay - Brunei Darussalam\n    0x044c: \"ml_IN\", # Malayalam - India\n    0x043a: \"mt_MT\", # Maltese\n    0x0481: \"mi_NZ\", # Maori\n    0x047a: \"arn_CL\",# Mapudungun\n    0x044e: \"mr_IN\", # Marathi\n    0x047c: \"moh_CA\",# Mohawk - Canada\n    0x0450: \"mn_MN\", # Mongolian - Cyrillic\n    0x0850: \"mn_CN\", # Mongolian - PRC\n    0x0461: \"ne_NP\", # Nepali\n    0x0414: \"nb_NO\", # Norwegian - Bokmal\n    0x0814: \"nn_NO\", # Norwegian - Nynorsk\n    0x0482: \"oc_FR\", # Occitan - France\n    0x0448: \"or_IN\", # Oriya - India\n    0x0463: \"ps_AF\", # Pashto - Afghanistan\n    0x0429: \"fa_IR\", # Persian\n    0x0415: \"pl_PL\", # Polish\n    0x0416: \"pt_BR\", # Portuguese - Brazil\n    0x0816: \"pt_PT\", # Portuguese - Portugal\n    0x0446: \"pa_IN\", # Punjabi\n    0x046b: \"quz_BO\",# Quechua (Bolivia)\n    0x086b: \"quz_EC\",# Quechua (Ecuador)\n    0x0c6b: \"quz_PE\",# Quechua (Peru)\n    0x0418: \"ro_RO\", # Romanian - Romania\n    0x0417: \"rm_CH\", # Romansh\n    0x0419: \"ru_RU\", # Russian\n    0x243b: \"smn_FI\",# Sami Finland\n    0x103b: \"smj_NO\",# Sami Norway\n    0x143b: \"smj_SE\",# Sami Sweden\n    0x043b: \"se_NO\", # Sami Northern Norway\n    0x083b: \"se_SE\", # Sami Northern Sweden\n    0x0c3b: \"se_FI\", # Sami Northern Finland\n    0x203b: \"sms_FI\",# Sami Skolt\n    0x183b: \"sma_NO\",# Sami Southern Norway\n    0x1c3b: \"sma_SE\",# Sami Southern Sweden\n    0x044f: \"sa_IN\", # Sanskrit\n    0x0c1a: \"sr_SP\", # Serbian - Cyrillic\n    0x1c1a: \"sr_BA\", # Serbian - Bosnia Cyrillic\n    0x081a: \"sr_SP\", # Serbian - Latin\n    0x181a: \"sr_BA\", # Serbian - Bosnia Latin\n    0x045b: \"si_LK\", # Sinhala - Sri Lanka\n    0x046c: \"ns_ZA\", # Northern Sotho\n    0x0432: \"tn_ZA\", # Setswana - Southern Africa\n    0x041b: \"sk_SK\", # Slovak\n    0x0424: \"sl_SI\", # Slovenian\n    0x040a: \"es_ES\", # Spanish - Spain\n    0x080a: \"es_MX\", # Spanish - Mexico\n    0x0c0a: \"es_ES\", # Spanish - Spain (Modern)\n    0x100a: \"es_GT\", # Spanish - Guatemala\n    0x140a: \"es_CR\", # Spanish - Costa Rica\n    0x180a: \"es_PA\", # Spanish - Panama\n    0x1c0a: \"es_DO\", # Spanish - Dominican Republic\n    0x200a: \"es_VE\", # Spanish - Venezuela\n    0x240a: \"es_CO\", # Spanish - Colombia\n    0x280a: \"es_PE\", # Spanish - Peru\n    0x2c0a: \"es_AR\", # Spanish - Argentina\n    0x300a: \"es_EC\", # Spanish - Ecuador\n    0x340a: \"es_CL\", # Spanish - Chile\n    0x380a: \"es_UR\", # Spanish - Uruguay\n    0x3c0a: \"es_PY\", # Spanish - Paraguay\n    0x400a: \"es_BO\", # Spanish - Bolivia\n    0x440a: \"es_SV\", # Spanish - El Salvador\n    0x480a: \"es_HN\", # Spanish - Honduras\n    0x4c0a: \"es_NI\", # Spanish - Nicaragua\n    0x500a: \"es_PR\", # Spanish - Puerto Rico\n    0x540a: \"es_US\", # Spanish - United States\n#    0x0430: \"\", # Sutu - Not supported\n    0x0441: \"sw_KE\", # Swahili\n    0x041d: \"sv_SE\", # Swedish - Sweden\n    0x081d: \"sv_FI\", # Swedish - Finland\n    0x045a: \"syr_SY\",# Syriac\n    0x0428: \"tg_TJ\", # Tajik - Cyrillic\n    0x085f: \"tmz_DZ\",# Tamazight - Latin\n    0x0449: \"ta_IN\", # Tamil\n    0x0444: \"tt_RU\", # Tatar\n    0x044a: \"te_IN\", # Telugu\n    0x041e: \"th_TH\", # Thai\n    0x0851: \"bo_BT\", # Tibetan - Bhutan\n    0x0451: \"bo_CN\", # Tibetan - PRC\n    0x041f: \"tr_TR\", # Turkish\n    0x0442: \"tk_TM\", # Turkmen - Cyrillic\n    0x0480: \"ug_CN\", # Uighur - Arabic\n    0x0422: \"uk_UA\", # Ukrainian\n    0x042e: \"wen_DE\",# Upper Sorbian - Germany\n    0x0420: \"ur_PK\", # Urdu\n    0x0820: \"ur_IN\", # Urdu - India\n    0x0443: \"uz_UZ\", # Uzbek - Latin\n    0x0843: \"uz_UZ\", # Uzbek - Cyrillic\n    0x042a: \"vi_VN\", # Vietnamese\n    0x0452: \"cy_GB\", # Welsh\n    0x0488: \"wo_SN\", # Wolof - Senegal\n    0x0434: \"xh_ZA\", # Xhosa - South Africa\n    0x0485: \"sah_RU\",# Yakut - Cyrillic\n    0x0478: \"ii_CN\", # Yi - PRC\n    0x046a: \"yo_NG\", # Yoruba - Nigeria\n    0x0435: \"zu_ZA\", # Zulu\n}\n\ndef _print_locale():\n\n    \"\"\" Test function.\n    \"\"\"\n    categories = {}\n    def _init_categories(categories=categories):\n        for k,v in globals().items():\n            if k[:3] == 'LC_':\n                categories[k] = v\n    _init_categories()\n    del categories['LC_ALL']\n\n    print 'Locale defaults as determined by getdefaultlocale():'\n    print '-'*72\n    lang, enc = getdefaultlocale()\n    print 'Language: ', lang or '(undefined)'\n    print 'Encoding: ', enc or '(undefined)'\n    print\n\n    print 'Locale settings on startup:'\n    print '-'*72\n    for name,category in categories.items():\n        print name, '...'\n        lang, enc = getlocale(category)\n        print '   Language: ', lang or '(undefined)'\n        print '   Encoding: ', enc or '(undefined)'\n        print\n\n    print\n    print 'Locale settings after calling resetlocale():'\n    print '-'*72\n    resetlocale()\n    for name,category in categories.items():\n        print name, '...'\n        lang, enc = getlocale(category)\n        print '   Language: ', lang or '(undefined)'\n        print '   Encoding: ', enc or '(undefined)'\n        print\n\n    try:\n        setlocale(LC_ALL, \"\")\n    except:\n        print 'NOTE:'\n        print 'setlocale(LC_ALL, \"\") does not support the default locale'\n        print 'given in the OS environment variables.'\n    else:\n        print\n        print 'Locale settings after calling setlocale(LC_ALL, \"\"):'\n        print '-'*72\n        for name,category in categories.items():\n            print name, '...'\n            lang, enc = getlocale(category)\n            print '   Language: ', lang or '(undefined)'\n            print '   Encoding: ', enc or '(undefined)'\n            print\n\n###\n\ntry:\n    LC_MESSAGES\nexcept NameError:\n    pass\nelse:\n    __all__.append(\"LC_MESSAGES\")\n\nif __name__=='__main__':\n    print 'Locale aliasing:'\n    print\n    _print_locale()\n    print\n    print 'Number formatting:'\n    print\n    _test()\n", 
    "logging.__init__": "# Copyright 2001-2014 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2014 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, cStringIO, traceback, warnings, weakref, collections\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'warn', 'warning']\n\ntry:\n    import codecs\nexcept ImportError:\n    codecs = None\n\ntry:\n    import thread\n    import threading\nexcept ImportError:\n    thread = None\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# Note: the attributes below are no longer maintained.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\ntry:\n    unicode\n    _unicode = True\nexcept NameError:\n    _unicode = False\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame.\n#\nif hasattr(sys, 'frozen'): #support for py2exe\n    _srcfile = \"logging%s__init__%s\" % (os.sep, __file__[-4:])\nelif __file__[-4:].lower() in ['.pyc', '.pyo']:\n    _srcfile = __file__[:-4] + '.py'\nelse:\n    _srcfile = __file__\n_srcfile = os.path.normcase(_srcfile)\n\n# next bit filched from 1.5.2's inspect.py\ndef currentframe():\n    \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n    try:\n        raise Exception\n    except:\n        return sys.exc_info()[2].tb_frame.f_back\n\nif hasattr(sys, '_getframe'): currentframe = lambda: sys._getframe(3)\n# done filching\n\n# _srcfile is only used in conjunction with sys._getframe().\n# To provide compatibility with older versions of Python, set _srcfile\n# to None if _getframe() is not available; this value will prevent\n# findCaller() from being called.\n#if not hasattr(sys, \"_getframe\"):\n#    _srcfile = None\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = 1\n\n#\n# If you don't want threading information in the log, set this to zero\n#\nlogThreads = 1\n\n#\n# If you don't want multiprocessing information in the log, set this to zero\n#\nlogMultiprocessing = 1\n\n#\n# If you don't want process information in the log, set this to zero\n#\nlogProcesses = 1\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n# NOTE(flaper87): This is different from\n# python's stdlib module since pypy's\n# dicts are much faster when their\n# keys are all of the same type.\n# Introduced in commit 9de7b40c586f\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n_levelNames = dict(_levelToName)\n_levelNames.update(_nameToLevel)   # backward compatibility\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    Otherwise, the string \"Level %s\" % level is returned.\n    \"\"\"\n\n    # NOTE(flaper87): Check also in _nameToLevel\n    # if value is None.\n    return (_levelToName.get(level) or\n            _nameToLevel.get(level, (\"Level %s\" % level)))\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    _acquireLock()\n    try:    #unlikely to cause an exception, but you never know...\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n    finally:\n        _releaseLock()\n\ndef _checkLevel(level):\n    if isinstance(level, (int, long)):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\" % level)\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\nif thread:\n    _lock = threading.RLock()\nelse:\n    _lock = None\n\ndef _acquireLock():\n    \"\"\"\n    Acquire the module-level lock for serializing access to shared data.\n\n    This should be released with _releaseLock().\n    \"\"\"\n    if _lock:\n        _lock.acquire()\n\ndef _releaseLock():\n    \"\"\"\n    Release the module-level lock acquired by calling _acquireLock().\n    \"\"\"\n    if _lock:\n        _lock.release()\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warn('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct\n        self.msecs = (ct - int(ct)) * 1000\n        self.relativeCreated = (self.created - _startTime) * 1000\n        if logThreads and thread:\n            self.thread = thread.get_ident()\n            self.threadName = threading.current_thread().name\n        else:\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing:\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except StandardError:\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n    def __str__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        if not _unicode: #if no unicode support...\n            msg = str(self.msg)\n        else:\n            msg = self.msg\n            if not isinstance(msg, basestring):\n                try:\n                    msg = str(self.msg)\n                except UnicodeError:\n                    msg = self.msg      #Defer encoding till later\n        if self.args:\n            msg = msg % self.args\n        return msg\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = LogRecord(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    default value of \"%s(message)\\\\n\" is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument (if omitted, you get the ISO8601 format).\n        \"\"\"\n        if fmt:\n            self._fmt = fmt\n        else:\n            self._fmt = \"%(message)s\"\n        self.datefmt = datefmt\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, the ISO8601 format is used. The resulting\n        string is returned. This function uses a user-configurable function\n        to convert the creation time to a tuple. By default, time.localtime()\n        is used; to change this for a particular formatter instance, set the\n        'converter' attribute to a function with the same signature as\n        time.localtime() or time.gmtime(). To change it for all formatters,\n        for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            t = time.strftime(\"%Y-%m-%d %H:%M:%S\", ct)\n            s = \"%s,%03d\" % (t, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = cStringIO.StringIO()\n        traceback.print_exception(ei[0], ei[1], ei[2], None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._fmt.find(\"%(asctime)\") >= 0\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self._fmt % record.__dict__\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            try:\n                s = s + record.exc_text\n            except UnicodeError:\n                # Sometimes filenames have non-ASCII chars, which can lead\n                # to errors when s is Unicode and record.exc_text is str\n                # See issue 8924.\n                # We also use replace for when there are multiple\n                # encodings, e.g. UTF-8 for the filesystem and latin-1\n                # for a script. See issue 13232.\n                s = s + record.exc_text.decode(sys.getfilesystemencoding(),\n                                               'replace')\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Is the specified record to be logged? Returns 0 for no, nonzero for\n        yes. If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return 1\n        elif self.name == record.name:\n            return 1\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return 0\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this and the record is then dropped. Returns a zero value if a record\n        is to be dropped, else non-zero.\n        \"\"\"\n        rv = 1\n        for f in self.filters:\n            if not f.filter(record):\n                rv = 0\n                break\n        return rv\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList\n    if acquire and release and handlers:\n        acquire()\n        try:\n            if wr in handlers:\n                handlers.remove(wr)\n        finally:\n            release()\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    _acquireLock()\n    try:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n    finally:\n        _releaseLock()\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        _acquireLock()\n        try:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n        finally:\n            _releaseLock()\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        if thread:\n            self.lock = threading.RLock()\n        else:\n            self.lock = None\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock. Returns whether the filter passed the record for\n        emission.\n        \"\"\"\n        rv = self.filter(record)\n        if rv:\n            self.acquire()\n            try:\n                self.emit(record)\n            finally:\n                self.release()\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        _acquireLock()\n        try:    #unlikely to raise an exception, but you never know...\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n        finally:\n            _releaseLock()\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            ei = sys.exc_info()\n            try:\n                traceback.print_exception(ei[0], ei[1], ei[2],\n                                          None, sys.stderr)\n                sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                 record.filename, record.lineno))\n            except IOError:\n                pass    # see issue 5971\n            finally:\n                del ei\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n        finally:\n            self.release()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            fs = \"%s\\n\"\n            if not _unicode: #if no unicode support...\n                stream.write(fs % msg)\n            else:\n                try:\n                    if (isinstance(msg, unicode) and\n                        getattr(stream, 'encoding', None)):\n                        ufs = u'%s\\n'\n                        try:\n                            stream.write(ufs % msg)\n                        except UnicodeEncodeError:\n                            #Printing to terminals sometimes fails. For example,\n                            #with an encoding of 'cp1251', the above write will\n                            #work if written to a stream opened or wrapped by\n                            #the codecs module, but fail when writing to a\n                            #terminal even when the codepage is set to cp1251.\n                            #An extra encoding step seems to be needed.\n                            stream.write((ufs % msg).encode(stream.encoding))\n                    else:\n                        stream.write(fs % msg)\n                except UnicodeError:\n                    stream.write(fs % msg.encode(\"UTF-8\"))\n            self.flush()\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=0):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        if codecs is None:\n            encoding = None\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        self.delay = delay\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream:\n                self.flush()\n                if hasattr(self.stream, \"close\"):\n                    self.stream.close()\n                self.stream = None\n            # Issue #19523: call unconditionally to\n            # prevent a handler leak when delay is set\n            StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        if self.encoding is None:\n            stream = open(self.baseFilename, self.mode)\n        else:\n            stream = codecs.open(self.baseFilename, self.mode, self.encoding)\n        return stream\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n        \"\"\"\n        if self.stream is None:\n            self.stream = self._open()\n        StreamHandler.emit(self, record)\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        #self.loggers = [alogger]\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        #if alogger not in self.loggers:\n        if alogger not in self.loggerMap:\n            #self.loggers.append(alogger)\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n_loggerClass = None\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = 0\n        self.loggerDict = {}\n        self.loggerClass = None\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, basestring):\n            raise TypeError('A logger name must be string or Unicode')\n        if isinstance(name, unicode):\n            name = name.encode('utf-8')\n        _acquireLock()\n        try:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        finally:\n            _releaseLock()\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = 1\n        self.handlers = []\n        self.disabled = 0\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    warn = warning\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        kwargs['exc_info'] = 1\n        self.error(msg, *args, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    fatal = critical\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=1)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is not None:\n            f = f.f_back\n        rv = \"(unknown file)\", 0, \"(unknown function)\"\n        while hasattr(f, \"f_code\"):\n            co = f.f_code\n            filename = os.path.normcase(co.co_filename)\n            if filename == _srcfile:\n                f = f.f_back\n                continue\n            rv = (co.co_filename, f.f_lineno, co.co_name)\n            break\n        return rv\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info, func=None, extra=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = LogRecord(name, level, fn, lno, msg, args, exc_info, func)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func = self.findCaller()\n            except ValueError:\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else:\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args, exc_info, func, extra)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if (not self.disabled) and self.filter(record):\n            self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n        finally:\n            _releaseLock()\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n        finally:\n            _releaseLock()\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0) and raiseExceptions and not self.manager.emittedNoHandlerWarning:\n            sys.stderr.write(\"No handlers could be found for logger\"\n                             \" \\\"%s\\\"\\n\" % self.name)\n            self.manager.emittedNoHandlerWarning = 1\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.manager.disable >= level:\n            return 0\n        return level >= self.getEffectiveLevel()\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.debug(msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.info(msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.error(msg, *args, **kwargs)\n\n    def exception(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        kwargs[\"exc_info\"] = 1\n        self.logger.error(msg, *args, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.critical(msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        See if the underlying logger is enabled for the specified level.\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured. It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    _acquireLock()\n    try:\n        if len(root.handlers) == 0:\n            filename = kwargs.get(\"filename\")\n            if filename:\n                mode = kwargs.get(\"filemode\", 'a')\n                hdlr = FileHandler(filename, mode)\n            else:\n                stream = kwargs.get(\"stream\")\n                hdlr = StreamHandler(stream)\n            fs = kwargs.get(\"format\", BASIC_FORMAT)\n            dfs = kwargs.get(\"datefmt\", None)\n            fmt = Formatter(fs, dfs)\n            hdlr.setFormatter(fmt)\n            root.addHandler(hdlr)\n            level = kwargs.get(\"level\")\n            if level is not None:\n                root.setLevel(level)\n    finally:\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if name:\n        return Logger.manager.getLogger(name)\n    else:\n        return root\n\n#def getRootLogger():\n#    \"\"\"\n#    Return the root logger.\n#\n#    Note that getLogger('') now does the same thing, so this function is\n#    deprecated and may disappear in the future.\n#    \"\"\"\n#    return root\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\nfatal = critical\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger,\n    with exception information.\n    \"\"\"\n    kwargs['exc_info'] = 1\n    error(msg, *args, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\nwarn = warning\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    h.flush()\n                    h.close()\n                except (IOError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except:\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        pass\n\n    def emit(self, record):\n        pass\n\n    def createLock(self):\n        self.lock = None\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        logger.warning(\"%s\", s)\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n", 
    "marshal": "# Note that PyPy contains also a built-in module 'marshal' which will\n# hide this one if compiled in.\n\nfrom _marshal import __doc__\nfrom _marshal import *\n", 
    "mimetools": "\"\"\"Various tools used by MIME-reading or MIME-writing programs.\"\"\"\n\n\nimport os\nimport sys\nimport tempfile\nfrom warnings import filterwarnings, catch_warnings\nwith catch_warnings():\n    if sys.py3kwarning:\n        filterwarnings(\"ignore\", \".*rfc822 has been removed\", DeprecationWarning)\n    import rfc822\n\nfrom warnings import warnpy3k\nwarnpy3k(\"in 3.x, mimetools has been removed in favor of the email package\",\n         stacklevel=2)\n\n__all__ = [\"Message\",\"choose_boundary\",\"encode\",\"decode\",\"copyliteral\",\n           \"copybinary\"]\n\nclass Message(rfc822.Message):\n    \"\"\"A derived class of rfc822.Message that knows about MIME headers and\n    contains some hooks for decoding encoded and multipart messages.\"\"\"\n\n    def __init__(self, fp, seekable = 1):\n        rfc822.Message.__init__(self, fp, seekable)\n        self.encodingheader = \\\n                self.getheader('content-transfer-encoding')\n        self.typeheader = \\\n                self.getheader('content-type')\n        self.parsetype()\n        self.parseplist()\n\n    def parsetype(self):\n        str = self.typeheader\n        if str is None:\n            str = 'text/plain'\n        if ';' in str:\n            i = str.index(';')\n            self.plisttext = str[i:]\n            str = str[:i]\n        else:\n            self.plisttext = ''\n        fields = str.split('/')\n        for i in range(len(fields)):\n            fields[i] = fields[i].strip().lower()\n        self.type = '/'.join(fields)\n        self.maintype = fields[0]\n        self.subtype = '/'.join(fields[1:])\n\n    def parseplist(self):\n        str = self.plisttext\n        self.plist = []\n        while str[:1] == ';':\n            str = str[1:]\n            if ';' in str:\n                # XXX Should parse quotes!\n                end = str.index(';')\n            else:\n                end = len(str)\n            f = str[:end]\n            if '=' in f:\n                i = f.index('=')\n                f = f[:i].strip().lower() + \\\n                        '=' + f[i+1:].strip()\n            self.plist.append(f.strip())\n            str = str[end:]\n\n    def getplist(self):\n        return self.plist\n\n    def getparam(self, name):\n        name = name.lower() + '='\n        n = len(name)\n        for p in self.plist:\n            if p[:n] == name:\n                return rfc822.unquote(p[n:])\n        return None\n\n    def getparamnames(self):\n        result = []\n        for p in self.plist:\n            i = p.find('=')\n            if i >= 0:\n                result.append(p[:i].lower())\n        return result\n\n    def getencoding(self):\n        if self.encodingheader is None:\n            return '7bit'\n        return self.encodingheader.lower()\n\n    def gettype(self):\n        return self.type\n\n    def getmaintype(self):\n        return self.maintype\n\n    def getsubtype(self):\n        return self.subtype\n\n\n\n\n# Utility functions\n# -----------------\n\ntry:\n    import thread\nexcept ImportError:\n    import dummy_thread as thread\n_counter_lock = thread.allocate_lock()\ndel thread\n\n_counter = 0\ndef _get_next_counter():\n    global _counter\n    _counter_lock.acquire()\n    _counter += 1\n    result = _counter\n    _counter_lock.release()\n    return result\n\n_prefix = None\n\ndef choose_boundary():\n    \"\"\"Return a string usable as a multipart boundary.\n\n    The string chosen is unique within a single program run, and\n    incorporates the user id (if available), process id (if available),\n    and current time.  So it's very unlikely the returned string appears\n    in message text, but there's no guarantee.\n\n    The boundary contains dots so you have to quote it in the header.\"\"\"\n\n    global _prefix\n    import time\n    if _prefix is None:\n        import socket\n        try:\n            hostid = socket.gethostbyname(socket.gethostname())\n        except socket.gaierror:\n            hostid = '127.0.0.1'\n        try:\n            uid = repr(os.getuid())\n        except AttributeError:\n            uid = '1'\n        try:\n            pid = repr(os.getpid())\n        except AttributeError:\n            pid = '1'\n        _prefix = hostid + '.' + uid + '.' + pid\n    return \"%s.%.3f.%d\" % (_prefix, time.time(), _get_next_counter())\n\n\n# Subroutines for decoding some common content-transfer-types\n\ndef decode(input, output, encoding):\n    \"\"\"Decode common content-transfer-encodings (base64, quopri, uuencode).\"\"\"\n    if encoding == 'base64':\n        import base64\n        return base64.decode(input, output)\n    if encoding == 'quoted-printable':\n        import quopri\n        return quopri.decode(input, output)\n    if encoding in ('uuencode', 'x-uuencode', 'uue', 'x-uue'):\n        import uu\n        return uu.decode(input, output)\n    if encoding in ('7bit', '8bit'):\n        return output.write(input.read())\n    if encoding in decodetab:\n        pipethrough(input, decodetab[encoding], output)\n    else:\n        raise ValueError, \\\n              'unknown Content-Transfer-Encoding: %s' % encoding\n\ndef encode(input, output, encoding):\n    \"\"\"Encode common content-transfer-encodings (base64, quopri, uuencode).\"\"\"\n    if encoding == 'base64':\n        import base64\n        return base64.encode(input, output)\n    if encoding == 'quoted-printable':\n        import quopri\n        return quopri.encode(input, output, 0)\n    if encoding in ('uuencode', 'x-uuencode', 'uue', 'x-uue'):\n        import uu\n        return uu.encode(input, output)\n    if encoding in ('7bit', '8bit'):\n        return output.write(input.read())\n    if encoding in encodetab:\n        pipethrough(input, encodetab[encoding], output)\n    else:\n        raise ValueError, \\\n              'unknown Content-Transfer-Encoding: %s' % encoding\n\n# The following is no longer used for standard encodings\n\n# XXX This requires that uudecode and mmencode are in $PATH\n\nuudecode_pipe = '''(\nTEMP=/tmp/@uu.$$\nsed \"s%^begin [0-7][0-7]* .*%begin 600 $TEMP%\" | uudecode\ncat $TEMP\nrm $TEMP\n)'''\n\ndecodetab = {\n        'uuencode':             uudecode_pipe,\n        'x-uuencode':           uudecode_pipe,\n        'uue':                  uudecode_pipe,\n        'x-uue':                uudecode_pipe,\n        'quoted-printable':     'mmencode -u -q',\n        'base64':               'mmencode -u -b',\n}\n\nencodetab = {\n        'x-uuencode':           'uuencode tempfile',\n        'uuencode':             'uuencode tempfile',\n        'x-uue':                'uuencode tempfile',\n        'uue':                  'uuencode tempfile',\n        'quoted-printable':     'mmencode -q',\n        'base64':               'mmencode -b',\n}\n\ndef pipeto(input, command):\n    pipe = os.popen(command, 'w')\n    copyliteral(input, pipe)\n    pipe.close()\n\ndef pipethrough(input, command, output):\n    (fd, tempname) = tempfile.mkstemp()\n    temp = os.fdopen(fd, 'w')\n    copyliteral(input, temp)\n    temp.close()\n    pipe = os.popen(command + ' <' + tempname, 'r')\n    copybinary(pipe, output)\n    pipe.close()\n    os.unlink(tempname)\n\ndef copyliteral(input, output):\n    while 1:\n        line = input.readline()\n        if not line: break\n        output.write(line)\n\ndef copybinary(input, output):\n    BUFSIZE = 8192\n    while 1:\n        line = input.read(BUFSIZE)\n        if not line: break\n        output.write(line)\n", 
    "mimetypes": "\"\"\"Guess the MIME type of a file.\n\nThis module defines two useful functions:\n\nguess_type(url, strict=1) -- guess the MIME type and encoding of a URL.\n\nguess_extension(type, strict=1) -- guess the extension for a given MIME type.\n\nIt also contains the following, for tuning the behavior:\n\nData:\n\nknownfiles -- list of files to parse\ninited -- flag set when init() has been called\nsuffix_map -- dictionary mapping suffixes to suffixes\nencodings_map -- dictionary mapping suffixes to encodings\ntypes_map -- dictionary mapping suffixes to types\n\nFunctions:\n\ninit([files]) -- parse a list of files, default knownfiles (on Windows, the\n  default values are taken from the registry)\nread_mime_types(file) -- parse one file, return a dictionary or None\n\"\"\"\n\nimport os\nimport sys\nimport posixpath\nimport urllib\ntry:\n    import _winreg\nexcept ImportError:\n    _winreg = None\n\n__all__ = [\n    \"guess_type\",\"guess_extension\",\"guess_all_extensions\",\n    \"add_type\",\"read_mime_types\",\"init\"\n]\n\nknownfiles = [\n    \"/etc/mime.types\",\n    \"/etc/httpd/mime.types\",                    # Mac OS X\n    \"/etc/httpd/conf/mime.types\",               # Apache\n    \"/etc/apache/mime.types\",                   # Apache 1\n    \"/etc/apache2/mime.types\",                  # Apache 2\n    \"/usr/local/etc/httpd/conf/mime.types\",\n    \"/usr/local/lib/netscape/mime.types\",\n    \"/usr/local/etc/httpd/conf/mime.types\",     # Apache 1.2\n    \"/usr/local/etc/mime.types\",                # Apache 1.3\n    ]\n\ninited = False\n_db = None\n\n\nclass MimeTypes:\n    \"\"\"MIME-types datastore.\n\n    This datastore can handle information from mime.types-style files\n    and supports basic determination of MIME type from a filename or\n    URL, and can guess a reasonable extension given a MIME type.\n    \"\"\"\n\n    def __init__(self, filenames=(), strict=True):\n        if not inited:\n            init()\n        self.encodings_map = encodings_map.copy()\n        self.suffix_map = suffix_map.copy()\n        self.types_map = ({}, {}) # dict for (non-strict, strict)\n        self.types_map_inv = ({}, {})\n        for (ext, type) in types_map.items():\n            self.add_type(type, ext, True)\n        for (ext, type) in common_types.items():\n            self.add_type(type, ext, False)\n        for name in filenames:\n            self.read(name, strict)\n\n    def add_type(self, type, ext, strict=True):\n        \"\"\"Add a mapping between a type and an extension.\n\n        When the extension is already known, the new\n        type will replace the old one. When the type\n        is already known the extension will be added\n        to the list of known extensions.\n\n        If strict is true, information will be added to\n        list of standard types, else to the list of non-standard\n        types.\n        \"\"\"\n        self.types_map[strict][ext] = type\n        exts = self.types_map_inv[strict].setdefault(type, [])\n        if ext not in exts:\n            exts.append(ext)\n\n    def guess_type(self, url, strict=True):\n        \"\"\"Guess the type of a file based on its URL.\n\n        Return value is a tuple (type, encoding) where type is None if\n        the type can't be guessed (no or unknown suffix) or a string\n        of the form type/subtype, usable for a MIME Content-type\n        header; and encoding is None for no encoding or the name of\n        the program used to encode (e.g. compress or gzip).  The\n        mappings are table driven.  Encoding suffixes are case\n        sensitive; type suffixes are first tried case sensitive, then\n        case insensitive.\n\n        The suffixes .tgz, .taz and .tz (case sensitive!) are all\n        mapped to '.tar.gz'.  (This is table-driven too, using the\n        dictionary suffix_map.)\n\n        Optional `strict' argument when False adds a bunch of commonly found,\n        but non-standard types.\n        \"\"\"\n        scheme, url = urllib.splittype(url)\n        if scheme == 'data':\n            # syntax of data URLs:\n            # dataurl   := \"data:\" [ mediatype ] [ \";base64\" ] \",\" data\n            # mediatype := [ type \"/\" subtype ] *( \";\" parameter )\n            # data      := *urlchar\n            # parameter := attribute \"=\" value\n            # type/subtype defaults to \"text/plain\"\n            comma = url.find(',')\n            if comma < 0:\n                # bad data URL\n                return None, None\n            semi = url.find(';', 0, comma)\n            if semi >= 0:\n                type = url[:semi]\n            else:\n                type = url[:comma]\n            if '=' in type or '/' not in type:\n                type = 'text/plain'\n            return type, None           # never compressed, so encoding is None\n        base, ext = posixpath.splitext(url)\n        while ext in self.suffix_map:\n            base, ext = posixpath.splitext(base + self.suffix_map[ext])\n        if ext in self.encodings_map:\n            encoding = self.encodings_map[ext]\n            base, ext = posixpath.splitext(base)\n        else:\n            encoding = None\n        types_map = self.types_map[True]\n        if ext in types_map:\n            return types_map[ext], encoding\n        elif ext.lower() in types_map:\n            return types_map[ext.lower()], encoding\n        elif strict:\n            return None, encoding\n        types_map = self.types_map[False]\n        if ext in types_map:\n            return types_map[ext], encoding\n        elif ext.lower() in types_map:\n            return types_map[ext.lower()], encoding\n        else:\n            return None, encoding\n\n    def guess_all_extensions(self, type, strict=True):\n        \"\"\"Guess the extensions for a file based on its MIME type.\n\n        Return value is a list of strings giving the possible filename\n        extensions, including the leading dot ('.').  The extension is not\n        guaranteed to have been associated with any particular data stream,\n        but would be mapped to the MIME type `type' by guess_type().\n\n        Optional `strict' argument when false adds a bunch of commonly found,\n        but non-standard types.\n        \"\"\"\n        type = type.lower()\n        extensions = self.types_map_inv[True].get(type, [])\n        if not strict:\n            for ext in self.types_map_inv[False].get(type, []):\n                if ext not in extensions:\n                    extensions.append(ext)\n        return extensions\n\n    def guess_extension(self, type, strict=True):\n        \"\"\"Guess the extension for a file based on its MIME type.\n\n        Return value is a string giving a filename extension,\n        including the leading dot ('.').  The extension is not\n        guaranteed to have been associated with any particular data\n        stream, but would be mapped to the MIME type `type' by\n        guess_type().  If no extension can be guessed for `type', None\n        is returned.\n\n        Optional `strict' argument when false adds a bunch of commonly found,\n        but non-standard types.\n        \"\"\"\n        extensions = self.guess_all_extensions(type, strict)\n        if not extensions:\n            return None\n        return extensions[0]\n\n    def read(self, filename, strict=True):\n        \"\"\"\n        Read a single mime.types-format file, specified by pathname.\n\n        If strict is true, information will be added to\n        list of standard types, else to the list of non-standard\n        types.\n        \"\"\"\n        with open(filename) as fp:\n            self.readfp(fp, strict)\n\n    def readfp(self, fp, strict=True):\n        \"\"\"\n        Read a single mime.types-format file.\n\n        If strict is true, information will be added to\n        list of standard types, else to the list of non-standard\n        types.\n        \"\"\"\n        while 1:\n            line = fp.readline()\n            if not line:\n                break\n            words = line.split()\n            for i in range(len(words)):\n                if words[i][0] == '#':\n                    del words[i:]\n                    break\n            if not words:\n                continue\n            type, suffixes = words[0], words[1:]\n            for suff in suffixes:\n                self.add_type(type, '.' + suff, strict)\n\n    def read_windows_registry(self, strict=True):\n        \"\"\"\n        Load the MIME types database from Windows registry.\n\n        If strict is true, information will be added to\n        list of standard types, else to the list of non-standard\n        types.\n        \"\"\"\n\n        # Windows only\n        if not _winreg:\n            return\n\n        def enum_types(mimedb):\n            i = 0\n            while True:\n                try:\n                    yield _winreg.EnumKey(mimedb, i)\n                except EnvironmentError:\n                    break\n                i += 1\n\n        default_encoding = sys.getdefaultencoding()\n        with _winreg.OpenKey(_winreg.HKEY_CLASSES_ROOT, '') as hkcr:\n            for subkeyname in enum_types(hkcr):\n                try:\n                    with _winreg.OpenKey(hkcr, subkeyname) as subkey:\n                        # Only check file extensions\n                        if not subkeyname.startswith(\".\"):\n                            continue\n                        # raises EnvironmentError if no 'Content Type' value\n                        mimetype, datatype = _winreg.QueryValueEx(\n                            subkey, 'Content Type')\n                        if datatype != _winreg.REG_SZ:\n                            continue\n                        try:\n                            mimetype = mimetype.encode(default_encoding)\n                        except UnicodeEncodeError:\n                            continue\n                        self.add_type(mimetype, subkeyname, strict)\n                except EnvironmentError:\n                    continue\n\ndef guess_type(url, strict=True):\n    \"\"\"Guess the type of a file based on its URL.\n\n    Return value is a tuple (type, encoding) where type is None if the\n    type can't be guessed (no or unknown suffix) or a string of the\n    form type/subtype, usable for a MIME Content-type header; and\n    encoding is None for no encoding or the name of the program used\n    to encode (e.g. compress or gzip).  The mappings are table\n    driven.  Encoding suffixes are case sensitive; type suffixes are\n    first tried case sensitive, then case insensitive.\n\n    The suffixes .tgz, .taz and .tz (case sensitive!) are all mapped\n    to \".tar.gz\".  (This is table-driven too, using the dictionary\n    suffix_map).\n\n    Optional `strict' argument when false adds a bunch of commonly found, but\n    non-standard types.\n    \"\"\"\n    if _db is None:\n        init()\n    return _db.guess_type(url, strict)\n\n\ndef guess_all_extensions(type, strict=True):\n    \"\"\"Guess the extensions for a file based on its MIME type.\n\n    Return value is a list of strings giving the possible filename\n    extensions, including the leading dot ('.').  The extension is not\n    guaranteed to have been associated with any particular data\n    stream, but would be mapped to the MIME type `type' by\n    guess_type().  If no extension can be guessed for `type', None\n    is returned.\n\n    Optional `strict' argument when false adds a bunch of commonly found,\n    but non-standard types.\n    \"\"\"\n    if _db is None:\n        init()\n    return _db.guess_all_extensions(type, strict)\n\ndef guess_extension(type, strict=True):\n    \"\"\"Guess the extension for a file based on its MIME type.\n\n    Return value is a string giving a filename extension, including the\n    leading dot ('.').  The extension is not guaranteed to have been\n    associated with any particular data stream, but would be mapped to the\n    MIME type `type' by guess_type().  If no extension can be guessed for\n    `type', None is returned.\n\n    Optional `strict' argument when false adds a bunch of commonly found,\n    but non-standard types.\n    \"\"\"\n    if _db is None:\n        init()\n    return _db.guess_extension(type, strict)\n\ndef add_type(type, ext, strict=True):\n    \"\"\"Add a mapping between a type and an extension.\n\n    When the extension is already known, the new\n    type will replace the old one. When the type\n    is already known the extension will be added\n    to the list of known extensions.\n\n    If strict is true, information will be added to\n    list of standard types, else to the list of non-standard\n    types.\n    \"\"\"\n    if _db is None:\n        init()\n    return _db.add_type(type, ext, strict)\n\n\ndef init(files=None):\n    global suffix_map, types_map, encodings_map, common_types\n    global inited, _db\n    inited = True    # so that MimeTypes.__init__() doesn't call us again\n    db = MimeTypes()\n    if files is None:\n        if _winreg:\n            db.read_windows_registry()\n        files = knownfiles\n    for file in files:\n        if os.path.isfile(file):\n            db.read(file)\n    encodings_map = db.encodings_map\n    suffix_map = db.suffix_map\n    types_map = db.types_map[True]\n    common_types = db.types_map[False]\n    # Make the DB a global variable now that it is fully initialized\n    _db = db\n\n\ndef read_mime_types(file):\n    try:\n        f = open(file)\n    except IOError:\n        return None\n    with f:\n        db = MimeTypes()\n        db.readfp(f, True)\n        return db.types_map[True]\n\n\ndef _default_mime_types():\n    global suffix_map\n    global encodings_map\n    global types_map\n    global common_types\n\n    suffix_map = {\n        '.tgz': '.tar.gz',\n        '.taz': '.tar.gz',\n        '.tz': '.tar.gz',\n        '.tbz2': '.tar.bz2',\n        '.txz': '.tar.xz',\n        }\n\n    encodings_map = {\n        '.gz': 'gzip',\n        '.Z': 'compress',\n        '.bz2': 'bzip2',\n        '.xz': 'xz',\n        }\n\n    # Before adding new types, make sure they are either registered with IANA,\n    # at http://www.isi.edu/in-notes/iana/assignments/media-types\n    # or extensions, i.e. using the x- prefix\n\n    # If you add to these, please keep them sorted!\n    types_map = {\n        '.a'      : 'application/octet-stream',\n        '.ai'     : 'application/postscript',\n        '.aif'    : 'audio/x-aiff',\n        '.aifc'   : 'audio/x-aiff',\n        '.aiff'   : 'audio/x-aiff',\n        '.au'     : 'audio/basic',\n        '.avi'    : 'video/x-msvideo',\n        '.bat'    : 'text/plain',\n        '.bcpio'  : 'application/x-bcpio',\n        '.bin'    : 'application/octet-stream',\n        '.bmp'    : 'image/x-ms-bmp',\n        '.c'      : 'text/plain',\n        # Duplicates :(\n        '.cdf'    : 'application/x-cdf',\n        '.cdf'    : 'application/x-netcdf',\n        '.cpio'   : 'application/x-cpio',\n        '.csh'    : 'application/x-csh',\n        '.css'    : 'text/css',\n        '.dll'    : 'application/octet-stream',\n        '.doc'    : 'application/msword',\n        '.dot'    : 'application/msword',\n        '.dvi'    : 'application/x-dvi',\n        '.eml'    : 'message/rfc822',\n        '.eps'    : 'application/postscript',\n        '.etx'    : 'text/x-setext',\n        '.exe'    : 'application/octet-stream',\n        '.gif'    : 'image/gif',\n        '.gtar'   : 'application/x-gtar',\n        '.h'      : 'text/plain',\n        '.hdf'    : 'application/x-hdf',\n        '.htm'    : 'text/html',\n        '.html'   : 'text/html',\n        '.ico'    : 'image/vnd.microsoft.icon',\n        '.ief'    : 'image/ief',\n        '.jpe'    : 'image/jpeg',\n        '.jpeg'   : 'image/jpeg',\n        '.jpg'    : 'image/jpeg',\n        '.js'     : 'application/javascript',\n        '.ksh'    : 'text/plain',\n        '.latex'  : 'application/x-latex',\n        '.m1v'    : 'video/mpeg',\n        '.man'    : 'application/x-troff-man',\n        '.me'     : 'application/x-troff-me',\n        '.mht'    : 'message/rfc822',\n        '.mhtml'  : 'message/rfc822',\n        '.mif'    : 'application/x-mif',\n        '.mov'    : 'video/quicktime',\n        '.movie'  : 'video/x-sgi-movie',\n        '.mp2'    : 'audio/mpeg',\n        '.mp3'    : 'audio/mpeg',\n        '.mp4'    : 'video/mp4',\n        '.mpa'    : 'video/mpeg',\n        '.mpe'    : 'video/mpeg',\n        '.mpeg'   : 'video/mpeg',\n        '.mpg'    : 'video/mpeg',\n        '.ms'     : 'application/x-troff-ms',\n        '.nc'     : 'application/x-netcdf',\n        '.nws'    : 'message/rfc822',\n        '.o'      : 'application/octet-stream',\n        '.obj'    : 'application/octet-stream',\n        '.oda'    : 'application/oda',\n        '.p12'    : 'application/x-pkcs12',\n        '.p7c'    : 'application/pkcs7-mime',\n        '.pbm'    : 'image/x-portable-bitmap',\n        '.pdf'    : 'application/pdf',\n        '.pfx'    : 'application/x-pkcs12',\n        '.pgm'    : 'image/x-portable-graymap',\n        '.pl'     : 'text/plain',\n        '.png'    : 'image/png',\n        '.pnm'    : 'image/x-portable-anymap',\n        '.pot'    : 'application/vnd.ms-powerpoint',\n        '.ppa'    : 'application/vnd.ms-powerpoint',\n        '.ppm'    : 'image/x-portable-pixmap',\n        '.pps'    : 'application/vnd.ms-powerpoint',\n        '.ppt'    : 'application/vnd.ms-powerpoint',\n        '.ps'     : 'application/postscript',\n        '.pwz'    : 'application/vnd.ms-powerpoint',\n        '.py'     : 'text/x-python',\n        '.pyc'    : 'application/x-python-code',\n        '.pyo'    : 'application/x-python-code',\n        '.qt'     : 'video/quicktime',\n        '.ra'     : 'audio/x-pn-realaudio',\n        '.ram'    : 'application/x-pn-realaudio',\n        '.ras'    : 'image/x-cmu-raster',\n        '.rdf'    : 'application/xml',\n        '.rgb'    : 'image/x-rgb',\n        '.roff'   : 'application/x-troff',\n        '.rtx'    : 'text/richtext',\n        '.sgm'    : 'text/x-sgml',\n        '.sgml'   : 'text/x-sgml',\n        '.sh'     : 'application/x-sh',\n        '.shar'   : 'application/x-shar',\n        '.snd'    : 'audio/basic',\n        '.so'     : 'application/octet-stream',\n        '.src'    : 'application/x-wais-source',\n        '.sv4cpio': 'application/x-sv4cpio',\n        '.sv4crc' : 'application/x-sv4crc',\n        '.swf'    : 'application/x-shockwave-flash',\n        '.t'      : 'application/x-troff',\n        '.tar'    : 'application/x-tar',\n        '.tcl'    : 'application/x-tcl',\n        '.tex'    : 'application/x-tex',\n        '.texi'   : 'application/x-texinfo',\n        '.texinfo': 'application/x-texinfo',\n        '.tif'    : 'image/tiff',\n        '.tiff'   : 'image/tiff',\n        '.tr'     : 'application/x-troff',\n        '.tsv'    : 'text/tab-separated-values',\n        '.txt'    : 'text/plain',\n        '.ustar'  : 'application/x-ustar',\n        '.vcf'    : 'text/x-vcard',\n        '.wav'    : 'audio/x-wav',\n        '.wiz'    : 'application/msword',\n        '.wsdl'   : 'application/xml',\n        '.xbm'    : 'image/x-xbitmap',\n        '.xlb'    : 'application/vnd.ms-excel',\n        # Duplicates :(\n        '.xls'    : 'application/excel',\n        '.xls'    : 'application/vnd.ms-excel',\n        '.xml'    : 'text/xml',\n        '.xpdl'   : 'application/xml',\n        '.xpm'    : 'image/x-xpixmap',\n        '.xsl'    : 'application/xml',\n        '.xwd'    : 'image/x-xwindowdump',\n        '.zip'    : 'application/zip',\n        }\n\n    # These are non-standard types, commonly found in the wild.  They will\n    # only match if strict=0 flag is given to the API methods.\n\n    # Please sort these too\n    common_types = {\n        '.jpg' : 'image/jpg',\n        '.mid' : 'audio/midi',\n        '.midi': 'audio/midi',\n        '.pct' : 'image/pict',\n        '.pic' : 'image/pict',\n        '.pict': 'image/pict',\n        '.rtf' : 'application/rtf',\n        '.xul' : 'text/xul'\n        }\n\n\n_default_mime_types()\n\n\nif __name__ == '__main__':\n    import getopt\n\n    USAGE = \"\"\"\\\nUsage: mimetypes.py [options] type\n\nOptions:\n    --help / -h       -- print this message and exit\n    --lenient / -l    -- additionally search of some common, but non-standard\n                         types.\n    --extension / -e  -- guess extension instead of type\n\nMore than one type argument may be given.\n\"\"\"\n\n    def usage(code, msg=''):\n        print USAGE\n        if msg: print msg\n        sys.exit(code)\n\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], 'hle',\n                                   ['help', 'lenient', 'extension'])\n    except getopt.error, msg:\n        usage(1, msg)\n\n    strict = 1\n    extension = 0\n    for opt, arg in opts:\n        if opt in ('-h', '--help'):\n            usage(0)\n        elif opt in ('-l', '--lenient'):\n            strict = 0\n        elif opt in ('-e', '--extension'):\n            extension = 1\n    for gtype in args:\n        if extension:\n            guess = guess_extension(gtype, strict)\n            if not guess: print \"I don't know anything about type\", gtype\n            else: print guess\n        else:\n            guess, encoding = guess_type(gtype, strict)\n            if not guess: print \"I don't know anything about type\", gtype\n            else: print 'type:', guess, 'encoding:', encoding\n", 
    "nturl2path": "\"\"\"Convert a NT pathname to a file URL and vice versa.\"\"\"\n\ndef url2pathname(url):\n    \"\"\"OS-specific conversion from a relative URL of the 'file' scheme\n    to a file system path; not recommended for general use.\"\"\"\n    # e.g.\n    # ///C|/foo/bar/spam.foo\n    # becomes\n    # C:\\foo\\bar\\spam.foo\n    import string, urllib\n    # Windows itself uses \":\" even in URLs.\n    url = url.replace(':', '|')\n    if not '|' in url:\n        # No drive specifier, just convert slashes\n        if url[:4] == '////':\n            # path is something like ////host/path/on/remote/host\n            # convert this to \\\\host\\path\\on\\remote\\host\n            # (notice halving of slashes at the start of the path)\n            url = url[2:]\n        components = url.split('/')\n        # make sure not to convert quoted slashes :-)\n        return urllib.unquote('\\\\'.join(components))\n    comp = url.split('|')\n    if len(comp) != 2 or comp[0][-1] not in string.ascii_letters:\n        error = 'Bad URL: ' + url\n        raise IOError, error\n    drive = comp[0][-1].upper()\n    path = drive + ':'\n    components = comp[1].split('/')\n    for comp in components:\n        if comp:\n            path = path + '\\\\' + urllib.unquote(comp)\n    # Issue #11474: url like '/C|/' should convert into 'C:\\\\'\n    if path.endswith(':') and url.endswith('/'):\n        path += '\\\\'\n    return path\n\ndef pathname2url(p):\n    \"\"\"OS-specific conversion from a file system path to a relative URL\n    of the 'file' scheme; not recommended for general use.\"\"\"\n    # e.g.\n    # C:\\foo\\bar\\spam.foo\n    # becomes\n    # ///C|/foo/bar/spam.foo\n    import urllib\n    if not ':' in p:\n        # No drive specifier, just convert slashes and quote the name\n        if p[:2] == '\\\\\\\\':\n        # path is something like \\\\host\\path\\on\\remote\\host\n        # convert this to ////host/path/on/remote/host\n        # (notice doubling of slashes at the start of the path)\n            p = '\\\\\\\\' + p\n        components = p.split('\\\\')\n        return urllib.quote('/'.join(components))\n    comp = p.split(':')\n    if len(comp) != 2 or len(comp[0]) > 1:\n        error = 'Bad path: ' + p\n        raise IOError, error\n\n    drive = urllib.quote(comp[0].upper())\n    components = comp[1].split('\\\\')\n    path = '///' + drive + ':'\n    for comp in components:\n        if comp:\n            path = path + '/' + urllib.quote(comp)\n    return path\n", 
    "numbers": "# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) for numbers, according to PEP 3141.\n\nTODO: Fill out more detailed documentation on the operators.\"\"\"\n\nfrom __future__ import division\nfrom abc import ABCMeta, abstractmethod, abstractproperty\n\n__all__ = [\"Number\", \"Complex\", \"Real\", \"Rational\", \"Integral\"]\n\nclass Number(object):\n    \"\"\"All numbers inherit from this class.\n\n    If you just want to check if an argument x is a number, without\n    caring what kind, use isinstance(x, Number).\n    \"\"\"\n    __metaclass__ = ABCMeta\n    __slots__ = ()\n\n    # Concrete numeric types must provide their own hash implementation\n    __hash__ = None\n\n\n## Notes on Decimal\n## ----------------\n## Decimal has all of the methods specified by the Real abc, but it should\n## not be registered as a Real because decimals do not interoperate with\n## binary floats (i.e.  Decimal('3.14') + 2.71828 is undefined).  But,\n## abstract reals are expected to interoperate (i.e. R1 + R2 should be\n## expected to work if R1 and R2 are both Reals).\n\nclass Complex(Number):\n    \"\"\"Complex defines the operations that work on the builtin complex type.\n\n    In short, those are: a conversion to complex, .real, .imag, +, -,\n    *, /, abs(), .conjugate, ==, and !=.\n\n    If it is given heterogenous arguments, and doesn't have special\n    knowledge about them, it should fall back to the builtin complex\n    type as described below.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __complex__(self):\n        \"\"\"Return a builtin complex instance. Called for complex(self).\"\"\"\n\n    # Will be __bool__ in 3.0.\n    def __nonzero__(self):\n        \"\"\"True if self != 0. Called for bool(self).\"\"\"\n        return self != 0\n\n    @abstractproperty\n    def real(self):\n        \"\"\"Retrieve the real component of this number.\n\n        This should subclass Real.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractproperty\n    def imag(self):\n        \"\"\"Retrieve the imaginary component of this number.\n\n        This should subclass Real.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __add__(self, other):\n        \"\"\"self + other\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __radd__(self, other):\n        \"\"\"other + self\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __neg__(self):\n        \"\"\"-self\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __pos__(self):\n        \"\"\"+self\"\"\"\n        raise NotImplementedError\n\n    def __sub__(self, other):\n        \"\"\"self - other\"\"\"\n        return self + -other\n\n    def __rsub__(self, other):\n        \"\"\"other - self\"\"\"\n        return -self + other\n\n    @abstractmethod\n    def __mul__(self, other):\n        \"\"\"self * other\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __rmul__(self, other):\n        \"\"\"other * self\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __div__(self, other):\n        \"\"\"self / other without __future__ division\n\n        May promote to float.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __rdiv__(self, other):\n        \"\"\"other / self without __future__ division\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __truediv__(self, other):\n        \"\"\"self / other with __future__ division.\n\n        Should promote to float when necessary.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __rtruediv__(self, other):\n        \"\"\"other / self with __future__ division\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __pow__(self, exponent):\n        \"\"\"self**exponent; should promote to float or complex when necessary.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __rpow__(self, base):\n        \"\"\"base ** self\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __abs__(self):\n        \"\"\"Returns the Real distance from 0. Called for abs(self).\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def conjugate(self):\n        \"\"\"(x+y*i).conjugate() returns (x-y*i).\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __eq__(self, other):\n        \"\"\"self == other\"\"\"\n        raise NotImplementedError\n\n    def __ne__(self, other):\n        \"\"\"self != other\"\"\"\n        # The default __ne__ doesn't negate __eq__ until 3.0.\n        return not (self == other)\n\nComplex.register(complex)\n\n\nclass Real(Complex):\n    \"\"\"To Complex, Real adds the operations that work on real numbers.\n\n    In short, those are: a conversion to float, trunc(), divmod,\n    %, <, <=, >, and >=.\n\n    Real also provides defaults for the derived operations.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __float__(self):\n        \"\"\"Any Real can be converted to a native float object.\n\n        Called for float(self).\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __trunc__(self):\n        \"\"\"trunc(self): Truncates self to an Integral.\n\n        Returns an Integral i such that:\n          * i>0 iff self>0;\n          * abs(i) <= abs(self);\n          * for any Integral j satisfying the first two conditions,\n            abs(i) >= abs(j) [i.e. i has \"maximal\" abs among those].\n        i.e. \"truncate towards 0\".\n        \"\"\"\n        raise NotImplementedError\n\n    def __divmod__(self, other):\n        \"\"\"divmod(self, other): The pair (self // other, self % other).\n\n        Sometimes this can be computed faster than the pair of\n        operations.\n        \"\"\"\n        return (self // other, self % other)\n\n    def __rdivmod__(self, other):\n        \"\"\"divmod(other, self): The pair (self // other, self % other).\n\n        Sometimes this can be computed faster than the pair of\n        operations.\n        \"\"\"\n        return (other // self, other % self)\n\n    @abstractmethod\n    def __floordiv__(self, other):\n        \"\"\"self // other: The floor() of self/other.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __rfloordiv__(self, other):\n        \"\"\"other // self: The floor() of other/self.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __mod__(self, other):\n        \"\"\"self % other\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __rmod__(self, other):\n        \"\"\"other % self\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __lt__(self, other):\n        \"\"\"self < other\n\n        < on Reals defines a total ordering, except perhaps for NaN.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __le__(self, other):\n        \"\"\"self <= other\"\"\"\n        raise NotImplementedError\n\n    # Concrete implementations of Complex abstract methods.\n    def __complex__(self):\n        \"\"\"complex(self) == complex(float(self), 0)\"\"\"\n        return complex(float(self))\n\n    @property\n    def real(self):\n        \"\"\"Real numbers are their real component.\"\"\"\n        return +self\n\n    @property\n    def imag(self):\n        \"\"\"Real numbers have no imaginary component.\"\"\"\n        return 0\n\n    def conjugate(self):\n        \"\"\"Conjugate is a no-op for Reals.\"\"\"\n        return +self\n\nReal.register(float)\n\n\nclass Rational(Real):\n    \"\"\".numerator and .denominator should be in lowest terms.\"\"\"\n\n    __slots__ = ()\n\n    @abstractproperty\n    def numerator(self):\n        raise NotImplementedError\n\n    @abstractproperty\n    def denominator(self):\n        raise NotImplementedError\n\n    # Concrete implementation of Real's conversion to float.\n    def __float__(self):\n        \"\"\"float(self) = self.numerator / self.denominator\n\n        It's important that this conversion use the integer's \"true\"\n        division rather than casting one side to float before dividing\n        so that ratios of huge integers convert without overflowing.\n\n        \"\"\"\n        return self.numerator / self.denominator\n\n\nclass Integral(Rational):\n    \"\"\"Integral adds a conversion to long and the bit-string operations.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __long__(self):\n        \"\"\"long(self)\"\"\"\n        raise NotImplementedError\n\n    def __index__(self):\n        \"\"\"Called whenever an index is needed, such as in slicing\"\"\"\n        return long(self)\n\n    @abstractmethod\n    def __pow__(self, exponent, modulus=None):\n        \"\"\"self ** exponent % modulus, but maybe faster.\n\n        Accept the modulus argument if you want to support the\n        3-argument version of pow(). Raise a TypeError if exponent < 0\n        or any argument isn't Integral. Otherwise, just implement the\n        2-argument version described in Complex.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __lshift__(self, other):\n        \"\"\"self << other\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __rlshift__(self, other):\n        \"\"\"other << self\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __rshift__(self, other):\n        \"\"\"self >> other\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __rrshift__(self, other):\n        \"\"\"other >> self\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __and__(self, other):\n        \"\"\"self & other\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __rand__(self, other):\n        \"\"\"other & self\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __xor__(self, other):\n        \"\"\"self ^ other\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __rxor__(self, other):\n        \"\"\"other ^ self\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __or__(self, other):\n        \"\"\"self | other\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __ror__(self, other):\n        \"\"\"other | self\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __invert__(self):\n        \"\"\"~self\"\"\"\n        raise NotImplementedError\n\n    # Concrete implementations of Rational and Real abstract methods.\n    def __float__(self):\n        \"\"\"float(self) == float(long(self))\"\"\"\n        return float(long(self))\n\n    @property\n    def numerator(self):\n        \"\"\"Integers are their own numerators.\"\"\"\n        return +self\n\n    @property\n    def denominator(self):\n        \"\"\"Integers have a denominator of 1.\"\"\"\n        return 1\n\nIntegral.register(int)\nIntegral.register(long)\n", 
    "opcode": "\"\"\"\nopcode module - potentially shared between dis and other modules which\noperate on bytecodes (e.g. peephole optimizers).\n\"\"\"\n\n__all__ = [\"cmp_op\", \"hasconst\", \"hasname\", \"hasjrel\", \"hasjabs\",\n           \"haslocal\", \"hascompare\", \"hasfree\", \"opname\", \"opmap\",\n           \"HAVE_ARGUMENT\", \"EXTENDED_ARG\"]\n\ncmp_op = ('<', '<=', '==', '!=', '>', '>=', 'in', 'not in', 'is',\n        'is not', 'exception match', 'BAD')\n\nhasconst = []\nhasname = []\nhasjrel = []\nhasjabs = []\nhaslocal = []\nhascompare = []\nhasfree = []\n\nopmap = {}\nopname = [''] * 256\nfor op in range(256): opname[op] = '<%r>' % (op,)\ndel op\n\ndef def_op(name, op):\n    opname[op] = name\n    opmap[name] = op\n\ndef name_op(name, op):\n    def_op(name, op)\n    hasname.append(op)\n\ndef jrel_op(name, op):\n    def_op(name, op)\n    hasjrel.append(op)\n\ndef jabs_op(name, op):\n    def_op(name, op)\n    hasjabs.append(op)\n\n# Instruction opcodes for compiled code\n# Blank lines correspond to available opcodes\n\ndef_op('STOP_CODE', 0)\ndef_op('POP_TOP', 1)\ndef_op('ROT_TWO', 2)\ndef_op('ROT_THREE', 3)\ndef_op('DUP_TOP', 4)\ndef_op('ROT_FOUR', 5)\n\ndef_op('NOP', 9)\ndef_op('UNARY_POSITIVE', 10)\ndef_op('UNARY_NEGATIVE', 11)\ndef_op('UNARY_NOT', 12)\ndef_op('UNARY_CONVERT', 13)\n\ndef_op('UNARY_INVERT', 15)\n\ndef_op('BINARY_POWER', 19)\ndef_op('BINARY_MULTIPLY', 20)\ndef_op('BINARY_DIVIDE', 21)\ndef_op('BINARY_MODULO', 22)\ndef_op('BINARY_ADD', 23)\ndef_op('BINARY_SUBTRACT', 24)\ndef_op('BINARY_SUBSCR', 25)\ndef_op('BINARY_FLOOR_DIVIDE', 26)\ndef_op('BINARY_TRUE_DIVIDE', 27)\ndef_op('INPLACE_FLOOR_DIVIDE', 28)\ndef_op('INPLACE_TRUE_DIVIDE', 29)\ndef_op('SLICE+0', 30)\ndef_op('SLICE+1', 31)\ndef_op('SLICE+2', 32)\ndef_op('SLICE+3', 33)\n\ndef_op('STORE_SLICE+0', 40)\ndef_op('STORE_SLICE+1', 41)\ndef_op('STORE_SLICE+2', 42)\ndef_op('STORE_SLICE+3', 43)\n\ndef_op('DELETE_SLICE+0', 50)\ndef_op('DELETE_SLICE+1', 51)\ndef_op('DELETE_SLICE+2', 52)\ndef_op('DELETE_SLICE+3', 53)\n\ndef_op('STORE_MAP', 54)\ndef_op('INPLACE_ADD', 55)\ndef_op('INPLACE_SUBTRACT', 56)\ndef_op('INPLACE_MULTIPLY', 57)\ndef_op('INPLACE_DIVIDE', 58)\ndef_op('INPLACE_MODULO', 59)\ndef_op('STORE_SUBSCR', 60)\ndef_op('DELETE_SUBSCR', 61)\ndef_op('BINARY_LSHIFT', 62)\ndef_op('BINARY_RSHIFT', 63)\ndef_op('BINARY_AND', 64)\ndef_op('BINARY_XOR', 65)\ndef_op('BINARY_OR', 66)\ndef_op('INPLACE_POWER', 67)\ndef_op('GET_ITER', 68)\n\ndef_op('PRINT_EXPR', 70)\ndef_op('PRINT_ITEM', 71)\ndef_op('PRINT_NEWLINE', 72)\ndef_op('PRINT_ITEM_TO', 73)\ndef_op('PRINT_NEWLINE_TO', 74)\ndef_op('INPLACE_LSHIFT', 75)\ndef_op('INPLACE_RSHIFT', 76)\ndef_op('INPLACE_AND', 77)\ndef_op('INPLACE_XOR', 78)\ndef_op('INPLACE_OR', 79)\ndef_op('BREAK_LOOP', 80)\ndef_op('WITH_CLEANUP', 81)\ndef_op('LOAD_LOCALS', 82)\ndef_op('RETURN_VALUE', 83)\ndef_op('IMPORT_STAR', 84)\ndef_op('EXEC_STMT', 85)\ndef_op('YIELD_VALUE', 86)\ndef_op('POP_BLOCK', 87)\ndef_op('END_FINALLY', 88)\ndef_op('BUILD_CLASS', 89)\n\nHAVE_ARGUMENT = 90              # Opcodes from here have an argument:\n\nname_op('STORE_NAME', 90)       # Index in name list\nname_op('DELETE_NAME', 91)      # \"\"\ndef_op('UNPACK_SEQUENCE', 92)   # Number of tuple items\njrel_op('FOR_ITER', 93)\ndef_op('LIST_APPEND', 94)\nname_op('STORE_ATTR', 95)       # Index in name list\nname_op('DELETE_ATTR', 96)      # \"\"\nname_op('STORE_GLOBAL', 97)     # \"\"\nname_op('DELETE_GLOBAL', 98)    # \"\"\ndef_op('DUP_TOPX', 99)          # number of items to duplicate\ndef_op('LOAD_CONST', 100)       # Index in const list\nhasconst.append(100)\nname_op('LOAD_NAME', 101)       # Index in name list\ndef_op('BUILD_TUPLE', 102)      # Number of tuple items\ndef_op('BUILD_LIST', 103)       # Number of list items\ndef_op('BUILD_SET', 104)        # Number of set items\ndef_op('BUILD_MAP', 105)        # Number of dict entries (upto 255)\nname_op('LOAD_ATTR', 106)       # Index in name list\ndef_op('COMPARE_OP', 107)       # Comparison operator\nhascompare.append(107)\nname_op('IMPORT_NAME', 108)     # Index in name list\nname_op('IMPORT_FROM', 109)     # Index in name list\njrel_op('JUMP_FORWARD', 110)    # Number of bytes to skip\njabs_op('JUMP_IF_FALSE_OR_POP', 111) # Target byte offset from beginning of code\njabs_op('JUMP_IF_TRUE_OR_POP', 112)  # \"\"\njabs_op('JUMP_ABSOLUTE', 113)        # \"\"\njabs_op('POP_JUMP_IF_FALSE', 114)    # \"\"\njabs_op('POP_JUMP_IF_TRUE', 115)     # \"\"\n\nname_op('LOAD_GLOBAL', 116)     # Index in name list\n\njabs_op('CONTINUE_LOOP', 119)   # Target address\njrel_op('SETUP_LOOP', 120)      # Distance to target address\njrel_op('SETUP_EXCEPT', 121)    # \"\"\njrel_op('SETUP_FINALLY', 122)   # \"\"\n\ndef_op('LOAD_FAST', 124)        # Local variable number\nhaslocal.append(124)\ndef_op('STORE_FAST', 125)       # Local variable number\nhaslocal.append(125)\ndef_op('DELETE_FAST', 126)      # Local variable number\nhaslocal.append(126)\n\ndef_op('RAISE_VARARGS', 130)    # Number of raise arguments (1, 2, or 3)\ndef_op('CALL_FUNCTION', 131)    # #args + (#kwargs << 8)\ndef_op('MAKE_FUNCTION', 132)    # Number of args with default values\ndef_op('BUILD_SLICE', 133)      # Number of items\ndef_op('MAKE_CLOSURE', 134)\ndef_op('LOAD_CLOSURE', 135)\nhasfree.append(135)\ndef_op('LOAD_DEREF', 136)\nhasfree.append(136)\ndef_op('STORE_DEREF', 137)\nhasfree.append(137)\n\ndef_op('CALL_FUNCTION_VAR', 140)     # #args + (#kwargs << 8)\ndef_op('CALL_FUNCTION_KW', 141)      # #args + (#kwargs << 8)\ndef_op('CALL_FUNCTION_VAR_KW', 142)  # #args + (#kwargs << 8)\n\njrel_op('SETUP_WITH', 143)\n\ndef_op('EXTENDED_ARG', 145)\nEXTENDED_ARG = 145\ndef_op('SET_ADD', 146)\ndef_op('MAP_ADD', 147)\n\n# pypy modification, experimental bytecode\ndef_op('LOOKUP_METHOD', 201)          # Index in name list\nhasname.append(201)\ndef_op('CALL_METHOD', 202)            # #args not including 'self'\ndef_op('BUILD_LIST_FROM_ARG', 203)\njrel_op('JUMP_IF_NOT_DEBUG', 204)     # jump over assert statements\n\ndel def_op, name_op, jrel_op, jabs_op\n", 
    "optparse": "\"\"\"A powerful, extensible, and easy-to-use option parser.\n\nBy Greg Ward <gward@python.net>\n\nOriginally distributed as Optik.\n\nFor support, use the optik-users@lists.sourceforge.net mailing list\n(http://lists.sourceforge.net/lists/listinfo/optik-users).\n\nSimple usage example:\n\n   from optparse import OptionParser\n\n   parser = OptionParser()\n   parser.add_option(\"-f\", \"--file\", dest=\"filename\",\n                     help=\"write report to FILE\", metavar=\"FILE\")\n   parser.add_option(\"-q\", \"--quiet\",\n                     action=\"store_false\", dest=\"verbose\", default=True,\n                     help=\"don't print status messages to stdout\")\n\n   (options, args) = parser.parse_args()\n\"\"\"\n\n__version__ = \"1.5.3\"\n\n__all__ = ['Option',\n           'make_option',\n           'SUPPRESS_HELP',\n           'SUPPRESS_USAGE',\n           'Values',\n           'OptionContainer',\n           'OptionGroup',\n           'OptionParser',\n           'HelpFormatter',\n           'IndentedHelpFormatter',\n           'TitledHelpFormatter',\n           'OptParseError',\n           'OptionError',\n           'OptionConflictError',\n           'OptionValueError',\n           'BadOptionError']\n\n__copyright__ = \"\"\"\nCopyright (c) 2001-2006 Gregory P. Ward.  All rights reserved.\nCopyright (c) 2002-2006 Python Software Foundation.  All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n  * Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n\n  * Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in the\n    documentation and/or other materials provided with the distribution.\n\n  * Neither the name of the author nor the names of its\n    contributors may be used to endorse or promote products derived from\n    this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\nIS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\nTO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\n\nimport sys, os\nimport types\nimport textwrap\n\ndef _repr(self):\n    return \"<%s at 0x%x: %s>\" % (self.__class__.__name__, id(self), self)\n\n\n# This file was generated from:\n#   Id: option_parser.py 527 2006-07-23 15:21:30Z greg\n#   Id: option.py 522 2006-06-11 16:22:03Z gward\n#   Id: help.py 527 2006-07-23 15:21:30Z greg\n#   Id: errors.py 509 2006-04-20 00:58:24Z gward\n\ntry:\n    from gettext import gettext\nexcept ImportError:\n    def gettext(message):\n        return message\n_ = gettext\n\n\nclass OptParseError (Exception):\n    def __init__(self, msg):\n        self.msg = msg\n\n    def __str__(self):\n        return self.msg\n\n\nclass OptionError (OptParseError):\n    \"\"\"\n    Raised if an Option instance is created with invalid or\n    inconsistent arguments.\n    \"\"\"\n\n    def __init__(self, msg, option):\n        self.msg = msg\n        self.option_id = str(option)\n\n    def __str__(self):\n        if self.option_id:\n            return \"option %s: %s\" % (self.option_id, self.msg)\n        else:\n            return self.msg\n\nclass OptionConflictError (OptionError):\n    \"\"\"\n    Raised if conflicting options are added to an OptionParser.\n    \"\"\"\n\nclass OptionValueError (OptParseError):\n    \"\"\"\n    Raised if an invalid option value is encountered on the command\n    line.\n    \"\"\"\n\nclass BadOptionError (OptParseError):\n    \"\"\"\n    Raised if an invalid option is seen on the command line.\n    \"\"\"\n    def __init__(self, opt_str):\n        self.opt_str = opt_str\n\n    def __str__(self):\n        return _(\"no such option: %s\") % self.opt_str\n\nclass AmbiguousOptionError (BadOptionError):\n    \"\"\"\n    Raised if an ambiguous option is seen on the command line.\n    \"\"\"\n    def __init__(self, opt_str, possibilities):\n        BadOptionError.__init__(self, opt_str)\n        self.possibilities = possibilities\n\n    def __str__(self):\n        return (_(\"ambiguous option: %s (%s?)\")\n                % (self.opt_str, \", \".join(self.possibilities)))\n\n\nclass HelpFormatter:\n\n    \"\"\"\n    Abstract base class for formatting option help.  OptionParser\n    instances should use one of the HelpFormatter subclasses for\n    formatting help; by default IndentedHelpFormatter is used.\n\n    Instance attributes:\n      parser : OptionParser\n        the controlling OptionParser instance\n      indent_increment : int\n        the number of columns to indent per nesting level\n      max_help_position : int\n        the maximum starting column for option help text\n      help_position : int\n        the calculated starting column for option help text;\n        initially the same as the maximum\n      width : int\n        total number of columns for output (pass None to constructor for\n        this value to be taken from the $COLUMNS environment variable)\n      level : int\n        current indentation level\n      current_indent : int\n        current indentation level (in columns)\n      help_width : int\n        number of columns available for option help text (calculated)\n      default_tag : str\n        text to replace with each option's default value, \"%default\"\n        by default.  Set to false value to disable default value expansion.\n      option_strings : { Option : str }\n        maps Option instances to the snippet of help text explaining\n        the syntax of that option, e.g. \"-h, --help\" or\n        \"-fFILE, --file=FILE\"\n      _short_opt_fmt : str\n        format string controlling how short options with values are\n        printed in help text.  Must be either \"%s%s\" (\"-fFILE\") or\n        \"%s %s\" (\"-f FILE\"), because those are the two syntaxes that\n        Optik supports.\n      _long_opt_fmt : str\n        similar but for long options; must be either \"%s %s\" (\"--file FILE\")\n        or \"%s=%s\" (\"--file=FILE\").\n    \"\"\"\n\n    NO_DEFAULT_VALUE = \"none\"\n\n    def __init__(self,\n                 indent_increment,\n                 max_help_position,\n                 width,\n                 short_first):\n        self.parser = None\n        self.indent_increment = indent_increment\n        if width is None:\n            try:\n                width = int(os.environ['COLUMNS'])\n            except (KeyError, ValueError):\n                width = 80\n            width -= 2\n        self.width = width\n        self.help_position = self.max_help_position = \\\n                min(max_help_position, max(width - 20, indent_increment * 2))\n        self.current_indent = 0\n        self.level = 0\n        self.help_width = None          # computed later\n        self.short_first = short_first\n        self.default_tag = \"%default\"\n        self.option_strings = {}\n        self._short_opt_fmt = \"%s %s\"\n        self._long_opt_fmt = \"%s=%s\"\n\n    def set_parser(self, parser):\n        self.parser = parser\n\n    def set_short_opt_delimiter(self, delim):\n        if delim not in (\"\", \" \"):\n            raise ValueError(\n                \"invalid metavar delimiter for short options: %r\" % delim)\n        self._short_opt_fmt = \"%s\" + delim + \"%s\"\n\n    def set_long_opt_delimiter(self, delim):\n        if delim not in (\"=\", \" \"):\n            raise ValueError(\n                \"invalid metavar delimiter for long options: %r\" % delim)\n        self._long_opt_fmt = \"%s\" + delim + \"%s\"\n\n    def indent(self):\n        self.current_indent += self.indent_increment\n        self.level += 1\n\n    def dedent(self):\n        self.current_indent -= self.indent_increment\n        assert self.current_indent >= 0, \"Indent decreased below 0.\"\n        self.level -= 1\n\n    def format_usage(self, usage):\n        raise NotImplementedError, \"subclasses must implement\"\n\n    def format_heading(self, heading):\n        raise NotImplementedError, \"subclasses must implement\"\n\n    def _format_text(self, text):\n        \"\"\"\n        Format a paragraph of free-form text for inclusion in the\n        help output at the current indentation level.\n        \"\"\"\n        text_width = max(self.width - self.current_indent, 11)\n        indent = \" \"*self.current_indent\n        return textwrap.fill(text,\n                             text_width,\n                             initial_indent=indent,\n                             subsequent_indent=indent)\n\n    def format_description(self, description):\n        if description:\n            return self._format_text(description) + \"\\n\"\n        else:\n            return \"\"\n\n    def format_epilog(self, epilog):\n        if epilog:\n            return \"\\n\" + self._format_text(epilog) + \"\\n\"\n        else:\n            return \"\"\n\n\n    def expand_default(self, option):\n        if self.parser is None or not self.default_tag:\n            return option.help\n\n        default_value = self.parser.defaults.get(option.dest)\n        if default_value is NO_DEFAULT or default_value is None:\n            default_value = self.NO_DEFAULT_VALUE\n\n        return option.help.replace(self.default_tag, str(default_value))\n\n    def format_option(self, option):\n        # The help for each option consists of two parts:\n        #   * the opt strings and metavars\n        #     eg. (\"-x\", or \"-fFILENAME, --file=FILENAME\")\n        #   * the user-supplied help string\n        #     eg. (\"turn on expert mode\", \"read data from FILENAME\")\n        #\n        # If possible, we write both of these on the same line:\n        #   -x      turn on expert mode\n        #\n        # But if the opt string list is too long, we put the help\n        # string on a second line, indented to the same column it would\n        # start in if it fit on the first line.\n        #   -fFILENAME, --file=FILENAME\n        #           read data from FILENAME\n        result = []\n        opts = self.option_strings[option]\n        opt_width = self.help_position - self.current_indent - 2\n        if len(opts) > opt_width:\n            opts = \"%*s%s\\n\" % (self.current_indent, \"\", opts)\n            indent_first = self.help_position\n        else:                       # start help on same line as opts\n            opts = \"%*s%-*s  \" % (self.current_indent, \"\", opt_width, opts)\n            indent_first = 0\n        result.append(opts)\n        if option.help:\n            help_text = self.expand_default(option)\n            help_lines = textwrap.wrap(help_text, self.help_width)\n            result.append(\"%*s%s\\n\" % (indent_first, \"\", help_lines[0]))\n            result.extend([\"%*s%s\\n\" % (self.help_position, \"\", line)\n                           for line in help_lines[1:]])\n        elif opts[-1] != \"\\n\":\n            result.append(\"\\n\")\n        return \"\".join(result)\n\n    def store_option_strings(self, parser):\n        self.indent()\n        max_len = 0\n        for opt in parser.option_list:\n            strings = self.format_option_strings(opt)\n            self.option_strings[opt] = strings\n            max_len = max(max_len, len(strings) + self.current_indent)\n        self.indent()\n        for group in parser.option_groups:\n            for opt in group.option_list:\n                strings = self.format_option_strings(opt)\n                self.option_strings[opt] = strings\n                max_len = max(max_len, len(strings) + self.current_indent)\n        self.dedent()\n        self.dedent()\n        self.help_position = min(max_len + 2, self.max_help_position)\n        self.help_width = max(self.width - self.help_position, 11)\n\n    def format_option_strings(self, option):\n        \"\"\"Return a comma-separated list of option strings & metavariables.\"\"\"\n        if option.takes_value():\n            metavar = option.metavar or option.dest.upper()\n            short_opts = [self._short_opt_fmt % (sopt, metavar)\n                          for sopt in option._short_opts]\n            long_opts = [self._long_opt_fmt % (lopt, metavar)\n                         for lopt in option._long_opts]\n        else:\n            short_opts = option._short_opts\n            long_opts = option._long_opts\n\n        if self.short_first:\n            opts = short_opts + long_opts\n        else:\n            opts = long_opts + short_opts\n\n        return \", \".join(opts)\n\nclass IndentedHelpFormatter (HelpFormatter):\n    \"\"\"Format help with indented section bodies.\n    \"\"\"\n\n    def __init__(self,\n                 indent_increment=2,\n                 max_help_position=24,\n                 width=None,\n                 short_first=1):\n        HelpFormatter.__init__(\n            self, indent_increment, max_help_position, width, short_first)\n\n    def format_usage(self, usage):\n        return _(\"Usage: %s\\n\") % usage\n\n    def format_heading(self, heading):\n        return \"%*s%s:\\n\" % (self.current_indent, \"\", heading)\n\n\nclass TitledHelpFormatter (HelpFormatter):\n    \"\"\"Format help with underlined section headers.\n    \"\"\"\n\n    def __init__(self,\n                 indent_increment=0,\n                 max_help_position=24,\n                 width=None,\n                 short_first=0):\n        HelpFormatter.__init__ (\n            self, indent_increment, max_help_position, width, short_first)\n\n    def format_usage(self, usage):\n        return \"%s  %s\\n\" % (self.format_heading(_(\"Usage\")), usage)\n\n    def format_heading(self, heading):\n        return \"%s\\n%s\\n\" % (heading, \"=-\"[self.level] * len(heading))\n\n\ndef _parse_num(val, type):\n    if val[:2].lower() == \"0x\":         # hexadecimal\n        radix = 16\n    elif val[:2].lower() == \"0b\":       # binary\n        radix = 2\n        val = val[2:] or \"0\"            # have to remove \"0b\" prefix\n    elif val[:1] == \"0\":                # octal\n        radix = 8\n    else:                               # decimal\n        radix = 10\n\n    return type(val, radix)\n\ndef _parse_int(val):\n    return _parse_num(val, int)\n\ndef _parse_long(val):\n    return _parse_num(val, long)\n\n_builtin_cvt = { \"int\" : (_parse_int, _(\"integer\")),\n                 \"long\" : (_parse_long, _(\"long integer\")),\n                 \"float\" : (float, _(\"floating-point\")),\n                 \"complex\" : (complex, _(\"complex\")) }\n\ndef check_builtin(option, opt, value):\n    (cvt, what) = _builtin_cvt[option.type]\n    try:\n        return cvt(value)\n    except ValueError:\n        raise OptionValueError(\n            _(\"option %s: invalid %s value: %r\") % (opt, what, value))\n\ndef check_choice(option, opt, value):\n    if value in option.choices:\n        return value\n    else:\n        choices = \", \".join(map(repr, option.choices))\n        raise OptionValueError(\n            _(\"option %s: invalid choice: %r (choose from %s)\")\n            % (opt, value, choices))\n\n# Not supplying a default is different from a default of None,\n# so we need an explicit \"not supplied\" value.\nNO_DEFAULT = (\"NO\", \"DEFAULT\")\n\n\nclass Option:\n    \"\"\"\n    Instance attributes:\n      _short_opts : [string]\n      _long_opts : [string]\n\n      action : string\n      type : string\n      dest : string\n      default : any\n      nargs : int\n      const : any\n      choices : [string]\n      callback : function\n      callback_args : (any*)\n      callback_kwargs : { string : any }\n      help : string\n      metavar : string\n    \"\"\"\n\n    # The list of instance attributes that may be set through\n    # keyword args to the constructor.\n    ATTRS = ['action',\n             'type',\n             'dest',\n             'default',\n             'nargs',\n             'const',\n             'choices',\n             'callback',\n             'callback_args',\n             'callback_kwargs',\n             'help',\n             'metavar']\n\n    # The set of actions allowed by option parsers.  Explicitly listed\n    # here so the constructor can validate its arguments.\n    ACTIONS = (\"store\",\n               \"store_const\",\n               \"store_true\",\n               \"store_false\",\n               \"append\",\n               \"append_const\",\n               \"count\",\n               \"callback\",\n               \"help\",\n               \"version\")\n\n    # The set of actions that involve storing a value somewhere;\n    # also listed just for constructor argument validation.  (If\n    # the action is one of these, there must be a destination.)\n    STORE_ACTIONS = (\"store\",\n                     \"store_const\",\n                     \"store_true\",\n                     \"store_false\",\n                     \"append\",\n                     \"append_const\",\n                     \"count\")\n\n    # The set of actions for which it makes sense to supply a value\n    # type, ie. which may consume an argument from the command line.\n    TYPED_ACTIONS = (\"store\",\n                     \"append\",\n                     \"callback\")\n\n    # The set of actions which *require* a value type, ie. that\n    # always consume an argument from the command line.\n    ALWAYS_TYPED_ACTIONS = (\"store\",\n                            \"append\")\n\n    # The set of actions which take a 'const' attribute.\n    CONST_ACTIONS = (\"store_const\",\n                     \"append_const\")\n\n    # The set of known types for option parsers.  Again, listed here for\n    # constructor argument validation.\n    TYPES = (\"string\", \"int\", \"long\", \"float\", \"complex\", \"choice\")\n\n    # Dictionary of argument checking functions, which convert and\n    # validate option arguments according to the option type.\n    #\n    # Signature of checking functions is:\n    #   check(option : Option, opt : string, value : string) -> any\n    # where\n    #   option is the Option instance calling the checker\n    #   opt is the actual option seen on the command-line\n    #     (eg. \"-a\", \"--file\")\n    #   value is the option argument seen on the command-line\n    #\n    # The return value should be in the appropriate Python type\n    # for option.type -- eg. an integer if option.type == \"int\".\n    #\n    # If no checker is defined for a type, arguments will be\n    # unchecked and remain strings.\n    TYPE_CHECKER = { \"int\"    : check_builtin,\n                     \"long\"   : check_builtin,\n                     \"float\"  : check_builtin,\n                     \"complex\": check_builtin,\n                     \"choice\" : check_choice,\n                   }\n\n\n    # CHECK_METHODS is a list of unbound method objects; they are called\n    # by the constructor, in order, after all attributes are\n    # initialized.  The list is created and filled in later, after all\n    # the methods are actually defined.  (I just put it here because I\n    # like to define and document all class attributes in the same\n    # place.)  Subclasses that add another _check_*() method should\n    # define their own CHECK_METHODS list that adds their check method\n    # to those from this class.\n    CHECK_METHODS = None\n\n\n    # -- Constructor/initialization methods ----------------------------\n\n    def __init__(self, *opts, **attrs):\n        # Set _short_opts, _long_opts attrs from 'opts' tuple.\n        # Have to be set now, in case no option strings are supplied.\n        self._short_opts = []\n        self._long_opts = []\n        opts = self._check_opt_strings(opts)\n        self._set_opt_strings(opts)\n\n        # Set all other attrs (action, type, etc.) from 'attrs' dict\n        self._set_attrs(attrs)\n\n        # Check all the attributes we just set.  There are lots of\n        # complicated interdependencies, but luckily they can be farmed\n        # out to the _check_*() methods listed in CHECK_METHODS -- which\n        # could be handy for subclasses!  The one thing these all share\n        # is that they raise OptionError if they discover a problem.\n        for checker in self.CHECK_METHODS:\n            checker(self)\n\n    def _check_opt_strings(self, opts):\n        # Filter out None because early versions of Optik had exactly\n        # one short option and one long option, either of which\n        # could be None.\n        opts = filter(None, opts)\n        if not opts:\n            raise TypeError(\"at least one option string must be supplied\")\n        return opts\n\n    def _set_opt_strings(self, opts):\n        for opt in opts:\n            if len(opt) < 2:\n                raise OptionError(\n                    \"invalid option string %r: \"\n                    \"must be at least two characters long\" % opt, self)\n            elif len(opt) == 2:\n                if not (opt[0] == \"-\" and opt[1] != \"-\"):\n                    raise OptionError(\n                        \"invalid short option string %r: \"\n                        \"must be of the form -x, (x any non-dash char)\" % opt,\n                        self)\n                self._short_opts.append(opt)\n            else:\n                if not (opt[0:2] == \"--\" and opt[2] != \"-\"):\n                    raise OptionError(\n                        \"invalid long option string %r: \"\n                        \"must start with --, followed by non-dash\" % opt,\n                        self)\n                self._long_opts.append(opt)\n\n    def _set_attrs(self, attrs):\n        for attr in self.ATTRS:\n            if attr in attrs:\n                setattr(self, attr, attrs[attr])\n                del attrs[attr]\n            else:\n                if attr == 'default':\n                    setattr(self, attr, NO_DEFAULT)\n                else:\n                    setattr(self, attr, None)\n        if attrs:\n            attrs = attrs.keys()\n            attrs.sort()\n            raise OptionError(\n                \"invalid keyword arguments: %s\" % \", \".join(attrs),\n                self)\n\n\n    # -- Constructor validation methods --------------------------------\n\n    def _check_action(self):\n        if self.action is None:\n            self.action = \"store\"\n        elif self.action not in self.ACTIONS:\n            raise OptionError(\"invalid action: %r\" % self.action, self)\n\n    def _check_type(self):\n        if self.type is None:\n            if self.action in self.ALWAYS_TYPED_ACTIONS:\n                if self.choices is not None:\n                    # The \"choices\" attribute implies \"choice\" type.\n                    self.type = \"choice\"\n                else:\n                    # No type given?  \"string\" is the most sensible default.\n                    self.type = \"string\"\n        else:\n            # Allow type objects or builtin type conversion functions\n            # (int, str, etc.) as an alternative to their names.  (The\n            # complicated check of __builtin__ is only necessary for\n            # Python 2.1 and earlier, and is short-circuited by the\n            # first check on modern Pythons.)\n            import __builtin__\n            if ( type(self.type) is types.TypeType or\n                 (hasattr(self.type, \"__name__\") and\n                  getattr(__builtin__, self.type.__name__, None) is self.type) ):\n                self.type = self.type.__name__\n\n            if self.type == \"str\":\n                self.type = \"string\"\n\n            if self.type not in self.TYPES:\n                raise OptionError(\"invalid option type: %r\" % self.type, self)\n            if self.action not in self.TYPED_ACTIONS:\n                raise OptionError(\n                    \"must not supply a type for action %r\" % self.action, self)\n\n    def _check_choice(self):\n        if self.type == \"choice\":\n            if self.choices is None:\n                raise OptionError(\n                    \"must supply a list of choices for type 'choice'\", self)\n            elif type(self.choices) not in (types.TupleType, types.ListType):\n                raise OptionError(\n                    \"choices must be a list of strings ('%s' supplied)\"\n                    % str(type(self.choices)).split(\"'\")[1], self)\n        elif self.choices is not None:\n            raise OptionError(\n                \"must not supply choices for type %r\" % self.type, self)\n\n    def _check_dest(self):\n        # No destination given, and we need one for this action.  The\n        # self.type check is for callbacks that take a value.\n        takes_value = (self.action in self.STORE_ACTIONS or\n                       self.type is not None)\n        if self.dest is None and takes_value:\n\n            # Glean a destination from the first long option string,\n            # or from the first short option string if no long options.\n            if self._long_opts:\n                # eg. \"--foo-bar\" -> \"foo_bar\"\n                self.dest = self._long_opts[0][2:].replace('-', '_')\n            else:\n                self.dest = self._short_opts[0][1]\n\n    def _check_const(self):\n        if self.action not in self.CONST_ACTIONS and self.const is not None:\n            raise OptionError(\n                \"'const' must not be supplied for action %r\" % self.action,\n                self)\n\n    def _check_nargs(self):\n        if self.action in self.TYPED_ACTIONS:\n            if self.nargs is None:\n                self.nargs = 1\n        elif self.nargs is not None:\n            raise OptionError(\n                \"'nargs' must not be supplied for action %r\" % self.action,\n                self)\n\n    def _check_callback(self):\n        if self.action == \"callback\":\n            if not hasattr(self.callback, '__call__'):\n                raise OptionError(\n                    \"callback not callable: %r\" % self.callback, self)\n            if (self.callback_args is not None and\n                type(self.callback_args) is not types.TupleType):\n                raise OptionError(\n                    \"callback_args, if supplied, must be a tuple: not %r\"\n                    % self.callback_args, self)\n            if (self.callback_kwargs is not None and\n                type(self.callback_kwargs) is not types.DictType):\n                raise OptionError(\n                    \"callback_kwargs, if supplied, must be a dict: not %r\"\n                    % self.callback_kwargs, self)\n        else:\n            if self.callback is not None:\n                raise OptionError(\n                    \"callback supplied (%r) for non-callback option\"\n                    % self.callback, self)\n            if self.callback_args is not None:\n                raise OptionError(\n                    \"callback_args supplied for non-callback option\", self)\n            if self.callback_kwargs is not None:\n                raise OptionError(\n                    \"callback_kwargs supplied for non-callback option\", self)\n\n\n    CHECK_METHODS = [_check_action,\n                     _check_type,\n                     _check_choice,\n                     _check_dest,\n                     _check_const,\n                     _check_nargs,\n                     _check_callback]\n\n\n    # -- Miscellaneous methods -----------------------------------------\n\n    def __str__(self):\n        return \"/\".join(self._short_opts + self._long_opts)\n\n    __repr__ = _repr\n\n    def takes_value(self):\n        return self.type is not None\n\n    def get_opt_string(self):\n        if self._long_opts:\n            return self._long_opts[0]\n        else:\n            return self._short_opts[0]\n\n\n    # -- Processing methods --------------------------------------------\n\n    def check_value(self, opt, value):\n        checker = self.TYPE_CHECKER.get(self.type)\n        if checker is None:\n            return value\n        else:\n            return checker(self, opt, value)\n\n    def convert_value(self, opt, value):\n        if value is not None:\n            if self.nargs == 1:\n                return self.check_value(opt, value)\n            else:\n                return tuple([self.check_value(opt, v) for v in value])\n\n    def process(self, opt, value, values, parser):\n\n        # First, convert the value(s) to the right type.  Howl if any\n        # value(s) are bogus.\n        value = self.convert_value(opt, value)\n\n        # And then take whatever action is expected of us.\n        # This is a separate method to make life easier for\n        # subclasses to add new actions.\n        return self.take_action(\n            self.action, self.dest, opt, value, values, parser)\n\n    def take_action(self, action, dest, opt, value, values, parser):\n        if action == \"store\":\n            setattr(values, dest, value)\n        elif action == \"store_const\":\n            setattr(values, dest, self.const)\n        elif action == \"store_true\":\n            setattr(values, dest, True)\n        elif action == \"store_false\":\n            setattr(values, dest, False)\n        elif action == \"append\":\n            values.ensure_value(dest, []).append(value)\n        elif action == \"append_const\":\n            values.ensure_value(dest, []).append(self.const)\n        elif action == \"count\":\n            setattr(values, dest, values.ensure_value(dest, 0) + 1)\n        elif action == \"callback\":\n            args = self.callback_args or ()\n            kwargs = self.callback_kwargs or {}\n            self.callback(self, opt, value, parser, *args, **kwargs)\n        elif action == \"help\":\n            parser.print_help()\n            parser.exit()\n        elif action == \"version\":\n            parser.print_version()\n            parser.exit()\n        else:\n            raise ValueError(\"unknown action %r\" % self.action)\n\n        return 1\n\n# class Option\n\n\nSUPPRESS_HELP = \"SUPPRESS\"+\"HELP\"\nSUPPRESS_USAGE = \"SUPPRESS\"+\"USAGE\"\n\ntry:\n    basestring\nexcept NameError:\n    def isbasestring(x):\n        return isinstance(x, (types.StringType, types.UnicodeType))\nelse:\n    def isbasestring(x):\n        return isinstance(x, basestring)\n\nclass Values:\n\n    def __init__(self, defaults=None):\n        if defaults:\n            for (attr, val) in defaults.items():\n                setattr(self, attr, val)\n\n    def __str__(self):\n        return str(self.__dict__)\n\n    __repr__ = _repr\n\n    def __cmp__(self, other):\n        if isinstance(other, Values):\n            return cmp(self.__dict__, other.__dict__)\n        elif isinstance(other, types.DictType):\n            return cmp(self.__dict__, other)\n        else:\n            return -1\n\n    def _update_careful(self, dict):\n        \"\"\"\n        Update the option values from an arbitrary dictionary, but only\n        use keys from dict that already have a corresponding attribute\n        in self.  Any keys in dict without a corresponding attribute\n        are silently ignored.\n        \"\"\"\n        for attr in dir(self):\n            if attr in dict:\n                dval = dict[attr]\n                if dval is not None:\n                    setattr(self, attr, dval)\n\n    def _update_loose(self, dict):\n        \"\"\"\n        Update the option values from an arbitrary dictionary,\n        using all keys from the dictionary regardless of whether\n        they have a corresponding attribute in self or not.\n        \"\"\"\n        self.__dict__.update(dict)\n\n    def _update(self, dict, mode):\n        if mode == \"careful\":\n            self._update_careful(dict)\n        elif mode == \"loose\":\n            self._update_loose(dict)\n        else:\n            raise ValueError, \"invalid update mode: %r\" % mode\n\n    def read_module(self, modname, mode=\"careful\"):\n        __import__(modname)\n        mod = sys.modules[modname]\n        self._update(vars(mod), mode)\n\n    def read_file(self, filename, mode=\"careful\"):\n        vars = {}\n        execfile(filename, vars)\n        self._update(vars, mode)\n\n    def ensure_value(self, attr, value):\n        if not hasattr(self, attr) or getattr(self, attr) is None:\n            setattr(self, attr, value)\n        return getattr(self, attr)\n\n\nclass OptionContainer:\n\n    \"\"\"\n    Abstract base class.\n\n    Class attributes:\n      standard_option_list : [Option]\n        list of standard options that will be accepted by all instances\n        of this parser class (intended to be overridden by subclasses).\n\n    Instance attributes:\n      option_list : [Option]\n        the list of Option objects contained by this OptionContainer\n      _short_opt : { string : Option }\n        dictionary mapping short option strings, eg. \"-f\" or \"-X\",\n        to the Option instances that implement them.  If an Option\n        has multiple short option strings, it will appears in this\n        dictionary multiple times. [1]\n      _long_opt : { string : Option }\n        dictionary mapping long option strings, eg. \"--file\" or\n        \"--exclude\", to the Option instances that implement them.\n        Again, a given Option can occur multiple times in this\n        dictionary. [1]\n      defaults : { string : any }\n        dictionary mapping option destination names to default\n        values for each destination [1]\n\n    [1] These mappings are common to (shared by) all components of the\n        controlling OptionParser, where they are initially created.\n\n    \"\"\"\n\n    def __init__(self, option_class, conflict_handler, description):\n        # Initialize the option list and related data structures.\n        # This method must be provided by subclasses, and it must\n        # initialize at least the following instance attributes:\n        # option_list, _short_opt, _long_opt, defaults.\n        self._create_option_list()\n\n        self.option_class = option_class\n        self.set_conflict_handler(conflict_handler)\n        self.set_description(description)\n\n    def _create_option_mappings(self):\n        # For use by OptionParser constructor -- create the master\n        # option mappings used by this OptionParser and all\n        # OptionGroups that it owns.\n        self._short_opt = {}            # single letter -> Option instance\n        self._long_opt = {}             # long option -> Option instance\n        self.defaults = {}              # maps option dest -> default value\n\n\n    def _share_option_mappings(self, parser):\n        # For use by OptionGroup constructor -- use shared option\n        # mappings from the OptionParser that owns this OptionGroup.\n        self._short_opt = parser._short_opt\n        self._long_opt = parser._long_opt\n        self.defaults = parser.defaults\n\n    def set_conflict_handler(self, handler):\n        if handler not in (\"error\", \"resolve\"):\n            raise ValueError, \"invalid conflict_resolution value %r\" % handler\n        self.conflict_handler = handler\n\n    def set_description(self, description):\n        self.description = description\n\n    def get_description(self):\n        return self.description\n\n\n    def destroy(self):\n        \"\"\"see OptionParser.destroy().\"\"\"\n        del self._short_opt\n        del self._long_opt\n        del self.defaults\n\n\n    # -- Option-adding methods -----------------------------------------\n\n    def _check_conflict(self, option):\n        conflict_opts = []\n        for opt in option._short_opts:\n            if opt in self._short_opt:\n                conflict_opts.append((opt, self._short_opt[opt]))\n        for opt in option._long_opts:\n            if opt in self._long_opt:\n                conflict_opts.append((opt, self._long_opt[opt]))\n\n        if conflict_opts:\n            handler = self.conflict_handler\n            if handler == \"error\":\n                raise OptionConflictError(\n                    \"conflicting option string(s): %s\"\n                    % \", \".join([co[0] for co in conflict_opts]),\n                    option)\n            elif handler == \"resolve\":\n                for (opt, c_option) in conflict_opts:\n                    if opt.startswith(\"--\"):\n                        c_option._long_opts.remove(opt)\n                        del self._long_opt[opt]\n                    else:\n                        c_option._short_opts.remove(opt)\n                        del self._short_opt[opt]\n                    if not (c_option._short_opts or c_option._long_opts):\n                        c_option.container.option_list.remove(c_option)\n\n    def add_option(self, *args, **kwargs):\n        \"\"\"add_option(Option)\n           add_option(opt_str, ..., kwarg=val, ...)\n        \"\"\"\n        if type(args[0]) in types.StringTypes:\n            option = self.option_class(*args, **kwargs)\n        elif len(args) == 1 and not kwargs:\n            option = args[0]\n            if not isinstance(option, Option):\n                raise TypeError, \"not an Option instance: %r\" % option\n        else:\n            raise TypeError, \"invalid arguments\"\n\n        self._check_conflict(option)\n\n        self.option_list.append(option)\n        option.container = self\n        for opt in option._short_opts:\n            self._short_opt[opt] = option\n        for opt in option._long_opts:\n            self._long_opt[opt] = option\n\n        if option.dest is not None:     # option has a dest, we need a default\n            if option.default is not NO_DEFAULT:\n                self.defaults[option.dest] = option.default\n            elif option.dest not in self.defaults:\n                self.defaults[option.dest] = None\n\n        return option\n\n    def add_options(self, option_list):\n        for option in option_list:\n            self.add_option(option)\n\n    # -- Option query/removal methods ----------------------------------\n\n    def get_option(self, opt_str):\n        return (self._short_opt.get(opt_str) or\n                self._long_opt.get(opt_str))\n\n    def has_option(self, opt_str):\n        return (opt_str in self._short_opt or\n                opt_str in self._long_opt)\n\n    def remove_option(self, opt_str):\n        option = self._short_opt.get(opt_str)\n        if option is None:\n            option = self._long_opt.get(opt_str)\n        if option is None:\n            raise ValueError(\"no such option %r\" % opt_str)\n\n        for opt in option._short_opts:\n            del self._short_opt[opt]\n        for opt in option._long_opts:\n            del self._long_opt[opt]\n        option.container.option_list.remove(option)\n\n\n    # -- Help-formatting methods ---------------------------------------\n\n    def format_option_help(self, formatter):\n        if not self.option_list:\n            return \"\"\n        result = []\n        for option in self.option_list:\n            if not option.help is SUPPRESS_HELP:\n                result.append(formatter.format_option(option))\n        return \"\".join(result)\n\n    def format_description(self, formatter):\n        return formatter.format_description(self.get_description())\n\n    def format_help(self, formatter):\n        result = []\n        if self.description:\n            result.append(self.format_description(formatter))\n        if self.option_list:\n            result.append(self.format_option_help(formatter))\n        return \"\\n\".join(result)\n\n\nclass OptionGroup (OptionContainer):\n\n    def __init__(self, parser, title, description=None):\n        self.parser = parser\n        OptionContainer.__init__(\n            self, parser.option_class, parser.conflict_handler, description)\n        self.title = title\n\n    def _create_option_list(self):\n        self.option_list = []\n        self._share_option_mappings(self.parser)\n\n    def set_title(self, title):\n        self.title = title\n\n    def destroy(self):\n        \"\"\"see OptionParser.destroy().\"\"\"\n        OptionContainer.destroy(self)\n        del self.option_list\n\n    # -- Help-formatting methods ---------------------------------------\n\n    def format_help(self, formatter):\n        result = formatter.format_heading(self.title)\n        formatter.indent()\n        result += OptionContainer.format_help(self, formatter)\n        formatter.dedent()\n        return result\n\n\nclass OptionParser (OptionContainer):\n\n    \"\"\"\n    Class attributes:\n      standard_option_list : [Option]\n        list of standard options that will be accepted by all instances\n        of this parser class (intended to be overridden by subclasses).\n\n    Instance attributes:\n      usage : string\n        a usage string for your program.  Before it is displayed\n        to the user, \"%prog\" will be expanded to the name of\n        your program (self.prog or os.path.basename(sys.argv[0])).\n      prog : string\n        the name of the current program (to override\n        os.path.basename(sys.argv[0])).\n      description : string\n        A paragraph of text giving a brief overview of your program.\n        optparse reformats this paragraph to fit the current terminal\n        width and prints it when the user requests help (after usage,\n        but before the list of options).\n      epilog : string\n        paragraph of help text to print after option help\n\n      option_groups : [OptionGroup]\n        list of option groups in this parser (option groups are\n        irrelevant for parsing the command-line, but very useful\n        for generating help)\n\n      allow_interspersed_args : bool = true\n        if true, positional arguments may be interspersed with options.\n        Assuming -a and -b each take a single argument, the command-line\n          -ablah foo bar -bboo baz\n        will be interpreted the same as\n          -ablah -bboo -- foo bar baz\n        If this flag were false, that command line would be interpreted as\n          -ablah -- foo bar -bboo baz\n        -- ie. we stop processing options as soon as we see the first\n        non-option argument.  (This is the tradition followed by\n        Python's getopt module, Perl's Getopt::Std, and other argument-\n        parsing libraries, but it is generally annoying to users.)\n\n      process_default_values : bool = true\n        if true, option default values are processed similarly to option\n        values from the command line: that is, they are passed to the\n        type-checking function for the option's type (as long as the\n        default value is a string).  (This really only matters if you\n        have defined custom types; see SF bug #955889.)  Set it to false\n        to restore the behaviour of Optik 1.4.1 and earlier.\n\n      rargs : [string]\n        the argument list currently being parsed.  Only set when\n        parse_args() is active, and continually trimmed down as\n        we consume arguments.  Mainly there for the benefit of\n        callback options.\n      largs : [string]\n        the list of leftover arguments that we have skipped while\n        parsing options.  If allow_interspersed_args is false, this\n        list is always empty.\n      values : Values\n        the set of option values currently being accumulated.  Only\n        set when parse_args() is active.  Also mainly for callbacks.\n\n    Because of the 'rargs', 'largs', and 'values' attributes,\n    OptionParser is not thread-safe.  If, for some perverse reason, you\n    need to parse command-line arguments simultaneously in different\n    threads, use different OptionParser instances.\n\n    \"\"\"\n\n    standard_option_list = []\n\n    def __init__(self,\n                 usage=None,\n                 option_list=None,\n                 option_class=Option,\n                 version=None,\n                 conflict_handler=\"error\",\n                 description=None,\n                 formatter=None,\n                 add_help_option=True,\n                 prog=None,\n                 epilog=None):\n        OptionContainer.__init__(\n            self, option_class, conflict_handler, description)\n        self.set_usage(usage)\n        self.prog = prog\n        self.version = version\n        self.allow_interspersed_args = True\n        self.process_default_values = True\n        if formatter is None:\n            formatter = IndentedHelpFormatter()\n        self.formatter = formatter\n        self.formatter.set_parser(self)\n        self.epilog = epilog\n\n        # Populate the option list; initial sources are the\n        # standard_option_list class attribute, the 'option_list'\n        # argument, and (if applicable) the _add_version_option() and\n        # _add_help_option() methods.\n        self._populate_option_list(option_list,\n                                   add_help=add_help_option)\n\n        self._init_parsing_state()\n\n\n    def destroy(self):\n        \"\"\"\n        Declare that you are done with this OptionParser.  This cleans up\n        reference cycles so the OptionParser (and all objects referenced by\n        it) can be garbage-collected promptly.  After calling destroy(), the\n        OptionParser is unusable.\n        \"\"\"\n        OptionContainer.destroy(self)\n        for group in self.option_groups:\n            group.destroy()\n        del self.option_list\n        del self.option_groups\n        del self.formatter\n\n\n    # -- Private methods -----------------------------------------------\n    # (used by our or OptionContainer's constructor)\n\n    def _create_option_list(self):\n        self.option_list = []\n        self.option_groups = []\n        self._create_option_mappings()\n\n    def _add_help_option(self):\n        self.add_option(\"-h\", \"--help\",\n                        action=\"help\",\n                        help=_(\"show this help message and exit\"))\n\n    def _add_version_option(self):\n        self.add_option(\"--version\",\n                        action=\"version\",\n                        help=_(\"show program's version number and exit\"))\n\n    def _populate_option_list(self, option_list, add_help=True):\n        if self.standard_option_list:\n            self.add_options(self.standard_option_list)\n        if option_list:\n            self.add_options(option_list)\n        if self.version:\n            self._add_version_option()\n        if add_help:\n            self._add_help_option()\n\n    def _init_parsing_state(self):\n        # These are set in parse_args() for the convenience of callbacks.\n        self.rargs = None\n        self.largs = None\n        self.values = None\n\n\n    # -- Simple modifier methods ---------------------------------------\n\n    def set_usage(self, usage):\n        if usage is None:\n            self.usage = _(\"%prog [options]\")\n        elif usage is SUPPRESS_USAGE:\n            self.usage = None\n        # For backwards compatibility with Optik 1.3 and earlier.\n        elif usage.lower().startswith(\"usage: \"):\n            self.usage = usage[7:]\n        else:\n            self.usage = usage\n\n    def enable_interspersed_args(self):\n        \"\"\"Set parsing to not stop on the first non-option, allowing\n        interspersing switches with command arguments. This is the\n        default behavior. See also disable_interspersed_args() and the\n        class documentation description of the attribute\n        allow_interspersed_args.\"\"\"\n        self.allow_interspersed_args = True\n\n    def disable_interspersed_args(self):\n        \"\"\"Set parsing to stop on the first non-option. Use this if\n        you have a command processor which runs another command that\n        has options of its own and you want to make sure these options\n        don't get confused.\n        \"\"\"\n        self.allow_interspersed_args = False\n\n    def set_process_default_values(self, process):\n        self.process_default_values = process\n\n    def set_default(self, dest, value):\n        self.defaults[dest] = value\n\n    def set_defaults(self, **kwargs):\n        self.defaults.update(kwargs)\n\n    def _get_all_options(self):\n        options = self.option_list[:]\n        for group in self.option_groups:\n            options.extend(group.option_list)\n        return options\n\n    def get_default_values(self):\n        if not self.process_default_values:\n            # Old, pre-Optik 1.5 behaviour.\n            return Values(self.defaults)\n\n        defaults = self.defaults.copy()\n        for option in self._get_all_options():\n            default = defaults.get(option.dest)\n            if isbasestring(default):\n                opt_str = option.get_opt_string()\n                defaults[option.dest] = option.check_value(opt_str, default)\n\n        return Values(defaults)\n\n\n    # -- OptionGroup methods -------------------------------------------\n\n    def add_option_group(self, *args, **kwargs):\n        # XXX lots of overlap with OptionContainer.add_option()\n        if type(args[0]) is types.StringType:\n            group = OptionGroup(self, *args, **kwargs)\n        elif len(args) == 1 and not kwargs:\n            group = args[0]\n            if not isinstance(group, OptionGroup):\n                raise TypeError, \"not an OptionGroup instance: %r\" % group\n            if group.parser is not self:\n                raise ValueError, \"invalid OptionGroup (wrong parser)\"\n        else:\n            raise TypeError, \"invalid arguments\"\n\n        self.option_groups.append(group)\n        return group\n\n    def get_option_group(self, opt_str):\n        option = (self._short_opt.get(opt_str) or\n                  self._long_opt.get(opt_str))\n        if option and option.container is not self:\n            return option.container\n        return None\n\n\n    # -- Option-parsing methods ----------------------------------------\n\n    def _get_args(self, args):\n        if args is None:\n            return sys.argv[1:]\n        else:\n            return args[:]              # don't modify caller's list\n\n    def parse_args(self, args=None, values=None):\n        \"\"\"\n        parse_args(args : [string] = sys.argv[1:],\n                   values : Values = None)\n        -> (values : Values, args : [string])\n\n        Parse the command-line options found in 'args' (default:\n        sys.argv[1:]).  Any errors result in a call to 'error()', which\n        by default prints the usage message to stderr and calls\n        sys.exit() with an error message.  On success returns a pair\n        (values, args) where 'values' is an Values instance (with all\n        your option values) and 'args' is the list of arguments left\n        over after parsing options.\n        \"\"\"\n        rargs = self._get_args(args)\n        if values is None:\n            values = self.get_default_values()\n\n        # Store the halves of the argument list as attributes for the\n        # convenience of callbacks:\n        #   rargs\n        #     the rest of the command-line (the \"r\" stands for\n        #     \"remaining\" or \"right-hand\")\n        #   largs\n        #     the leftover arguments -- ie. what's left after removing\n        #     options and their arguments (the \"l\" stands for \"leftover\"\n        #     or \"left-hand\")\n        self.rargs = rargs\n        self.largs = largs = []\n        self.values = values\n\n        try:\n            stop = self._process_args(largs, rargs, values)\n        except (BadOptionError, OptionValueError), err:\n            self.error(str(err))\n\n        args = largs + rargs\n        return self.check_values(values, args)\n\n    def check_values(self, values, args):\n        \"\"\"\n        check_values(values : Values, args : [string])\n        -> (values : Values, args : [string])\n\n        Check that the supplied option values and leftover arguments are\n        valid.  Returns the option values and leftover arguments\n        (possibly adjusted, possibly completely new -- whatever you\n        like).  Default implementation just returns the passed-in\n        values; subclasses may override as desired.\n        \"\"\"\n        return (values, args)\n\n    def _process_args(self, largs, rargs, values):\n        \"\"\"_process_args(largs : [string],\n                         rargs : [string],\n                         values : Values)\n\n        Process command-line arguments and populate 'values', consuming\n        options and arguments from 'rargs'.  If 'allow_interspersed_args' is\n        false, stop at the first non-option argument.  If true, accumulate any\n        interspersed non-option arguments in 'largs'.\n        \"\"\"\n        while rargs:\n            arg = rargs[0]\n            # We handle bare \"--\" explicitly, and bare \"-\" is handled by the\n            # standard arg handler since the short arg case ensures that the\n            # len of the opt string is greater than 1.\n            if arg == \"--\":\n                del rargs[0]\n                return\n            elif arg[0:2] == \"--\":\n                # process a single long option (possibly with value(s))\n                self._process_long_opt(rargs, values)\n            elif arg[:1] == \"-\" and len(arg) > 1:\n                # process a cluster of short options (possibly with\n                # value(s) for the last one only)\n                self._process_short_opts(rargs, values)\n            elif self.allow_interspersed_args:\n                largs.append(arg)\n                del rargs[0]\n            else:\n                return                  # stop now, leave this arg in rargs\n\n        # Say this is the original argument list:\n        # [arg0, arg1, ..., arg(i-1), arg(i), arg(i+1), ..., arg(N-1)]\n        #                            ^\n        # (we are about to process arg(i)).\n        #\n        # Then rargs is [arg(i), ..., arg(N-1)] and largs is a *subset* of\n        # [arg0, ..., arg(i-1)] (any options and their arguments will have\n        # been removed from largs).\n        #\n        # The while loop will usually consume 1 or more arguments per pass.\n        # If it consumes 1 (eg. arg is an option that takes no arguments),\n        # then after _process_arg() is done the situation is:\n        #\n        #   largs = subset of [arg0, ..., arg(i)]\n        #   rargs = [arg(i+1), ..., arg(N-1)]\n        #\n        # If allow_interspersed_args is false, largs will always be\n        # *empty* -- still a subset of [arg0, ..., arg(i-1)], but\n        # not a very interesting subset!\n\n    def _match_long_opt(self, opt):\n        \"\"\"_match_long_opt(opt : string) -> string\n\n        Determine which long option string 'opt' matches, ie. which one\n        it is an unambiguous abbreviation for.  Raises BadOptionError if\n        'opt' doesn't unambiguously match any long option string.\n        \"\"\"\n        return _match_abbrev(opt, self._long_opt)\n\n    def _process_long_opt(self, rargs, values):\n        arg = rargs.pop(0)\n\n        # Value explicitly attached to arg?  Pretend it's the next\n        # argument.\n        if \"=\" in arg:\n            (opt, next_arg) = arg.split(\"=\", 1)\n            rargs.insert(0, next_arg)\n            had_explicit_value = True\n        else:\n            opt = arg\n            had_explicit_value = False\n\n        opt = self._match_long_opt(opt)\n        option = self._long_opt[opt]\n        if option.takes_value():\n            nargs = option.nargs\n            if len(rargs) < nargs:\n                if nargs == 1:\n                    self.error(_(\"%s option requires an argument\") % opt)\n                else:\n                    self.error(_(\"%s option requires %d arguments\")\n                               % (opt, nargs))\n            elif nargs == 1:\n                value = rargs.pop(0)\n            else:\n                value = tuple(rargs[0:nargs])\n                del rargs[0:nargs]\n\n        elif had_explicit_value:\n            self.error(_(\"%s option does not take a value\") % opt)\n\n        else:\n            value = None\n\n        option.process(opt, value, values, self)\n\n    def _process_short_opts(self, rargs, values):\n        arg = rargs.pop(0)\n        stop = False\n        i = 1\n        for ch in arg[1:]:\n            opt = \"-\" + ch\n            option = self._short_opt.get(opt)\n            i += 1                      # we have consumed a character\n\n            if not option:\n                raise BadOptionError(opt)\n            if option.takes_value():\n                # Any characters left in arg?  Pretend they're the\n                # next arg, and stop consuming characters of arg.\n                if i < len(arg):\n                    rargs.insert(0, arg[i:])\n                    stop = True\n\n                nargs = option.nargs\n                if len(rargs) < nargs:\n                    if nargs == 1:\n                        self.error(_(\"%s option requires an argument\") % opt)\n                    else:\n                        self.error(_(\"%s option requires %d arguments\")\n                                   % (opt, nargs))\n                elif nargs == 1:\n                    value = rargs.pop(0)\n                else:\n                    value = tuple(rargs[0:nargs])\n                    del rargs[0:nargs]\n\n            else:                       # option doesn't take a value\n                value = None\n\n            option.process(opt, value, values, self)\n\n            if stop:\n                break\n\n\n    # -- Feedback methods ----------------------------------------------\n\n    def get_prog_name(self):\n        if self.prog is None:\n            return os.path.basename(sys.argv[0])\n        else:\n            return self.prog\n\n    def expand_prog_name(self, s):\n        return s.replace(\"%prog\", self.get_prog_name())\n\n    def get_description(self):\n        return self.expand_prog_name(self.description)\n\n    def exit(self, status=0, msg=None):\n        if msg:\n            sys.stderr.write(msg)\n        sys.exit(status)\n\n    def error(self, msg):\n        \"\"\"error(msg : string)\n\n        Print a usage message incorporating 'msg' to stderr and exit.\n        If you override this in a subclass, it should not return -- it\n        should either exit or raise an exception.\n        \"\"\"\n        self.print_usage(sys.stderr)\n        self.exit(2, \"%s: error: %s\\n\" % (self.get_prog_name(), msg))\n\n    def get_usage(self):\n        if self.usage:\n            return self.formatter.format_usage(\n                self.expand_prog_name(self.usage))\n        else:\n            return \"\"\n\n    def print_usage(self, file=None):\n        \"\"\"print_usage(file : file = stdout)\n\n        Print the usage message for the current program (self.usage) to\n        'file' (default stdout).  Any occurrence of the string \"%prog\" in\n        self.usage is replaced with the name of the current program\n        (basename of sys.argv[0]).  Does nothing if self.usage is empty\n        or not defined.\n        \"\"\"\n        if self.usage:\n            print >>file, self.get_usage()\n\n    def get_version(self):\n        if self.version:\n            return self.expand_prog_name(self.version)\n        else:\n            return \"\"\n\n    def print_version(self, file=None):\n        \"\"\"print_version(file : file = stdout)\n\n        Print the version message for this program (self.version) to\n        'file' (default stdout).  As with print_usage(), any occurrence\n        of \"%prog\" in self.version is replaced by the current program's\n        name.  Does nothing if self.version is empty or undefined.\n        \"\"\"\n        if self.version:\n            print >>file, self.get_version()\n\n    def format_option_help(self, formatter=None):\n        if formatter is None:\n            formatter = self.formatter\n        formatter.store_option_strings(self)\n        result = []\n        result.append(formatter.format_heading(_(\"Options\")))\n        formatter.indent()\n        if self.option_list:\n            result.append(OptionContainer.format_option_help(self, formatter))\n            result.append(\"\\n\")\n        for group in self.option_groups:\n            result.append(group.format_help(formatter))\n            result.append(\"\\n\")\n        formatter.dedent()\n        # Drop the last \"\\n\", or the header if no options or option groups:\n        return \"\".join(result[:-1])\n\n    def format_epilog(self, formatter):\n        return formatter.format_epilog(self.epilog)\n\n    def format_help(self, formatter=None):\n        if formatter is None:\n            formatter = self.formatter\n        result = []\n        if self.usage:\n            result.append(self.get_usage() + \"\\n\")\n        if self.description:\n            result.append(self.format_description(formatter) + \"\\n\")\n        result.append(self.format_option_help(formatter))\n        result.append(self.format_epilog(formatter))\n        return \"\".join(result)\n\n    # used by test suite\n    def _get_encoding(self, file):\n        encoding = getattr(file, \"encoding\", None)\n        if not encoding:\n            encoding = sys.getdefaultencoding()\n        return encoding\n\n    def print_help(self, file=None):\n        \"\"\"print_help(file : file = stdout)\n\n        Print an extended help message, listing all options and any\n        help text provided with them, to 'file' (default stdout).\n        \"\"\"\n        if file is None:\n            file = sys.stdout\n        encoding = self._get_encoding(file)\n        file.write(self.format_help().encode(encoding, \"replace\"))\n\n# class OptionParser\n\n\ndef _match_abbrev(s, wordmap):\n    \"\"\"_match_abbrev(s : string, wordmap : {string : Option}) -> string\n\n    Return the string key in 'wordmap' for which 's' is an unambiguous\n    abbreviation.  If 's' is found to be ambiguous or doesn't match any of\n    'words', raise BadOptionError.\n    \"\"\"\n    # Is there an exact match?\n    if s in wordmap:\n        return s\n    else:\n        # Isolate all words with s as a prefix.\n        possibilities = [word for word in wordmap.keys()\n                         if word.startswith(s)]\n        # No exact match, so there had better be just one possibility.\n        if len(possibilities) == 1:\n            return possibilities[0]\n        elif not possibilities:\n            raise BadOptionError(s)\n        else:\n            # More than one possible completion: ambiguous prefix.\n            possibilities.sort()\n            raise AmbiguousOptionError(s, possibilities)\n\n\n# Some day, there might be many Option classes.  As of Optik 1.3, the\n# preferred way to instantiate Options is indirectly, via make_option(),\n# which will become a factory function when there are many Option\n# classes.\nmake_option = Option\n", 
    "os": "r\"\"\"OS routines for NT or Posix depending on what system we're on.\n\nThis exports:\n  - all functions from posix, nt, os2, or ce, e.g. unlink, stat, etc.\n  - os.path is one of the modules posixpath, or ntpath\n  - os.name is 'posix', 'nt', 'os2', 'ce' or 'riscos'\n  - os.curdir is a string representing the current directory ('.' or ':')\n  - os.pardir is a string representing the parent directory ('..' or '::')\n  - os.sep is the (or a most common) pathname separator ('/' or ':' or '\\\\')\n  - os.extsep is the extension separator ('.' or '/')\n  - os.altsep is the alternate pathname separator (None or '/')\n  - os.pathsep is the component separator used in $PATH etc\n  - os.linesep is the line separator in text files ('\\r' or '\\n' or '\\r\\n')\n  - os.defpath is the default search path for executables\n  - os.devnull is the file path of the null device ('/dev/null', etc.)\n\nPrograms that import and use 'os' stand a better chance of being\nportable between different platforms.  Of course, they must then\nonly use functions that are defined by all platforms (e.g., unlink\nand opendir), and leave all pathname manipulation to os.path\n(e.g., split and join).\n\"\"\"\n\n#'\n\nimport sys, errno\n\n_names = sys.builtin_module_names\n\n# Note:  more names are added to __all__ later.\n__all__ = [\"altsep\", \"curdir\", \"pardir\", \"sep\", \"extsep\", \"pathsep\", \"linesep\",\n           \"defpath\", \"name\", \"path\", \"devnull\",\n           \"SEEK_SET\", \"SEEK_CUR\", \"SEEK_END\"]\n\ndef _get_exports_list(module):\n    try:\n        return list(module.__all__)\n    except AttributeError:\n        return [n for n in dir(module) if n[0] != '_']\n\nif 'posix' in _names:\n    name = 'posix'\n    linesep = '\\n'\n    from posix import *\n    try:\n        from posix import _exit\n    except ImportError:\n        pass\n    import posixpath as path\n\n    import posix\n    __all__.extend(_get_exports_list(posix))\n    del posix\n\nelif 'nt' in _names:\n    name = 'nt'\n    linesep = '\\r\\n'\n    from nt import *\n    try:\n        from nt import _exit\n    except ImportError:\n        pass\n    import ntpath as path\n\n    import nt\n    __all__.extend(_get_exports_list(nt))\n    del nt\n\nelif 'os2' in _names:\n    name = 'os2'\n    linesep = '\\r\\n'\n    from os2 import *\n    try:\n        from os2 import _exit\n    except ImportError:\n        pass\n    if sys.version.find('EMX GCC') == -1:\n        import ntpath as path\n    else:\n        import os2emxpath as path\n        from _emx_link import link\n\n    import os2\n    __all__.extend(_get_exports_list(os2))\n    del os2\n\nelif 'ce' in _names:\n    name = 'ce'\n    linesep = '\\r\\n'\n    from ce import *\n    try:\n        from ce import _exit\n    except ImportError:\n        pass\n    # We can use the standard Windows path.\n    import ntpath as path\n\n    import ce\n    __all__.extend(_get_exports_list(ce))\n    del ce\n\nelif 'riscos' in _names:\n    name = 'riscos'\n    linesep = '\\n'\n    from riscos import *\n    try:\n        from riscos import _exit\n    except ImportError:\n        pass\n    import riscospath as path\n\n    import riscos\n    __all__.extend(_get_exports_list(riscos))\n    del riscos\n\nelse:\n    raise ImportError, 'no os specific module found'\n\nsys.modules['os.path'] = path\nfrom os.path import (curdir, pardir, sep, pathsep, defpath, extsep, altsep,\n    devnull)\n\ndel _names\n\n# Python uses fixed values for the SEEK_ constants; they are mapped\n# to native constants if necessary in posixmodule.c\nSEEK_SET = 0\nSEEK_CUR = 1\nSEEK_END = 2\n\n#'\n\n# Super directory utilities.\n# (Inspired by Eric Raymond; the doc strings are mostly his)\n\ndef makedirs(name, mode=0777):\n    \"\"\"makedirs(path [, mode=0777])\n\n    Super-mkdir; create a leaf directory and all intermediate ones.\n    Works like mkdir, except that any intermediate path segment (not\n    just the rightmost) will be created if it does not exist.  This is\n    recursive.\n\n    \"\"\"\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    if head and tail and not path.exists(head):\n        try:\n            makedirs(head, mode)\n        except OSError, e:\n            # be happy if someone already created the path\n            if e.errno != errno.EEXIST:\n                raise\n        if tail == curdir:           # xxx/newdir/. exists if xxx/newdir exists\n            return\n    mkdir(name, mode)\n\ndef removedirs(name):\n    \"\"\"removedirs(path)\n\n    Super-rmdir; remove a leaf directory and all empty intermediate\n    ones.  Works like rmdir except that, if the leaf directory is\n    successfully removed, directories corresponding to rightmost path\n    segments will be pruned away until either the whole path is\n    consumed or an error occurs.  Errors during this latter phase are\n    ignored -- they generally mean that a directory was not empty.\n\n    \"\"\"\n    rmdir(name)\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    while head and tail:\n        try:\n            rmdir(head)\n        except error:\n            break\n        head, tail = path.split(head)\n\ndef renames(old, new):\n    \"\"\"renames(old, new)\n\n    Super-rename; create directories as necessary and delete any left\n    empty.  Works like rename, except creation of any intermediate\n    directories needed to make the new pathname good is attempted\n    first.  After the rename, directories corresponding to rightmost\n    path segments of the old name will be pruned way until either the\n    whole path is consumed or a nonempty directory is found.\n\n    Note: this function can fail with the new directory structure made\n    if you lack permissions needed to unlink the leaf directory or\n    file.\n\n    \"\"\"\n    head, tail = path.split(new)\n    if head and tail and not path.exists(head):\n        makedirs(head)\n    rename(old, new)\n    head, tail = path.split(old)\n    if head and tail:\n        try:\n            removedirs(head)\n        except error:\n            pass\n\n__all__.extend([\"makedirs\", \"removedirs\", \"renames\"])\n\ndef walk(top, topdown=True, onerror=None, followlinks=False):\n    \"\"\"Directory tree generator.\n\n    For each directory in the directory tree rooted at top (including top\n    itself, but excluding '.' and '..'), yields a 3-tuple\n\n        dirpath, dirnames, filenames\n\n    dirpath is a string, the path to the directory.  dirnames is a list of\n    the names of the subdirectories in dirpath (excluding '.' and '..').\n    filenames is a list of the names of the non-directory files in dirpath.\n    Note that the names in the lists are just names, with no path components.\n    To get a full path (which begins with top) to a file or directory in\n    dirpath, do os.path.join(dirpath, name).\n\n    If optional arg 'topdown' is true or not specified, the triple for a\n    directory is generated before the triples for any of its subdirectories\n    (directories are generated top down).  If topdown is false, the triple\n    for a directory is generated after the triples for all of its\n    subdirectories (directories are generated bottom up).\n\n    When topdown is true, the caller can modify the dirnames list in-place\n    (e.g., via del or slice assignment), and walk will only recurse into the\n    subdirectories whose names remain in dirnames; this can be used to prune the\n    search, or to impose a specific order of visiting.  Modifying dirnames when\n    topdown is false is ineffective, since the directories in dirnames have\n    already been generated by the time dirnames itself is generated. No matter\n    the value of topdown, the list of subdirectories is retrieved before the\n    tuples for the directory and its subdirectories are generated.\n\n    By default errors from the os.listdir() call are ignored.  If\n    optional arg 'onerror' is specified, it should be a function; it\n    will be called with one argument, an os.error instance.  It can\n    report the error to continue with the walk, or raise the exception\n    to abort the walk.  Note that the filename is available as the\n    filename attribute of the exception object.\n\n    By default, os.walk does not follow symbolic links to subdirectories on\n    systems that support them.  In order to get this functionality, set the\n    optional argument 'followlinks' to true.\n\n    Caution:  if you pass a relative pathname for top, don't change the\n    current working directory between resumptions of walk.  walk never\n    changes the current directory, and assumes that the client doesn't\n    either.\n\n    Example:\n\n    import os\n    from os.path import join, getsize\n    for root, dirs, files in os.walk('python/Lib/email'):\n        print root, \"consumes\",\n        print sum([getsize(join(root, name)) for name in files]),\n        print \"bytes in\", len(files), \"non-directory files\"\n        if 'CVS' in dirs:\n            dirs.remove('CVS')  # don't visit CVS directories\n\n    \"\"\"\n\n    islink, join, isdir = path.islink, path.join, path.isdir\n\n    # We may not have read permission for top, in which case we can't\n    # get a list of the files the directory contains.  os.path.walk\n    # always suppressed the exception then, rather than blow up for a\n    # minor reason when (say) a thousand readable directories are still\n    # left to visit.  That logic is copied here.\n    try:\n        # Note that listdir and error are globals in this module due\n        # to earlier import-*.\n        names = listdir(top)\n    except error, err:\n        if onerror is not None:\n            onerror(err)\n        return\n\n    dirs, nondirs = [], []\n    for name in names:\n        if isdir(join(top, name)):\n            dirs.append(name)\n        else:\n            nondirs.append(name)\n\n    if topdown:\n        yield top, dirs, nondirs\n    for name in dirs:\n        new_path = join(top, name)\n        if followlinks or not islink(new_path):\n            for x in walk(new_path, topdown, onerror, followlinks):\n                yield x\n    if not topdown:\n        yield top, dirs, nondirs\n\n__all__.append(\"walk\")\n\n# Make sure os.environ exists, at least\ntry:\n    environ\nexcept NameError:\n    environ = {}\n\ndef execl(file, *args):\n    \"\"\"execl(file, *args)\n\n    Execute the executable file with argument list args, replacing the\n    current process. \"\"\"\n    execv(file, args)\n\ndef execle(file, *args):\n    \"\"\"execle(file, *args, env)\n\n    Execute the executable file with argument list args and\n    environment env, replacing the current process. \"\"\"\n    env = args[-1]\n    execve(file, args[:-1], env)\n\ndef execlp(file, *args):\n    \"\"\"execlp(file, *args)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args, replacing the current process. \"\"\"\n    execvp(file, args)\n\ndef execlpe(file, *args):\n    \"\"\"execlpe(file, *args, env)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args and environment env, replacing the current\n    process. \"\"\"\n    env = args[-1]\n    execvpe(file, args[:-1], env)\n\ndef execvp(file, args):\n    \"\"\"execvp(file, args)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args, replacing the current process.\n    args may be a list or tuple of strings. \"\"\"\n    _execvpe(file, args)\n\ndef execvpe(file, args, env):\n    \"\"\"execvpe(file, args, env)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args and environment env , replacing the\n    current process.\n    args may be a list or tuple of strings. \"\"\"\n    _execvpe(file, args, env)\n\n__all__.extend([\"execl\",\"execle\",\"execlp\",\"execlpe\",\"execvp\",\"execvpe\"])\n\ndef _execvpe(file, args, env=None):\n    if env is not None:\n        func = execve\n        argrest = (args, env)\n    else:\n        func = execv\n        argrest = (args,)\n        env = environ\n\n    head, tail = path.split(file)\n    if head:\n        func(file, *argrest)\n        return\n    if 'PATH' in env:\n        envpath = env['PATH']\n    else:\n        envpath = defpath\n    PATH = envpath.split(pathsep)\n    saved_exc = None\n    saved_tb = None\n    for dir in PATH:\n        fullname = path.join(dir, file)\n        try:\n            func(fullname, *argrest)\n        except error, e:\n            tb = sys.exc_info()[2]\n            if (e.errno != errno.ENOENT and e.errno != errno.ENOTDIR\n                and saved_exc is None):\n                saved_exc = e\n                saved_tb = tb\n    if saved_exc:\n        raise error, saved_exc, saved_tb\n    raise error, e, tb\n\n# Change environ to automatically call putenv() if it exists\ntry:\n    # This will fail if there's no putenv\n    putenv\nexcept NameError:\n    pass\nelse:\n    import UserDict\n\n    # Fake unsetenv() for Windows\n    # not sure about os2 here but\n    # I'm guessing they are the same.\n\n    if name in ('os2', 'nt'):\n        def unsetenv(key):\n            putenv(key, \"\")\n\n    if name == \"riscos\":\n        # On RISC OS, all env access goes through getenv and putenv\n        from riscosenviron import _Environ\n    elif name in ('os2', 'nt'):  # Where Env Var Names Must Be UPPERCASE\n        # But we store them as upper case\n        class _Environ(UserDict.IterableUserDict):\n            def __init__(self, environ):\n                UserDict.UserDict.__init__(self)\n                data = self.data\n                for k, v in environ.items():\n                    data[k.upper()] = v\n            def __setitem__(self, key, item):\n                putenv(key, item)\n                self.data[key.upper()] = item\n            def __getitem__(self, key):\n                return self.data[key.upper()]\n            try:\n                unsetenv\n            except NameError:\n                def __delitem__(self, key):\n                    del self.data[key.upper()]\n            else:\n                def __delitem__(self, key):\n                    unsetenv(key)\n                    del self.data[key.upper()]\n                def clear(self):\n                    for key in self.data.keys():\n                        unsetenv(key)\n                        del self.data[key]\n                def pop(self, key, *args):\n                    unsetenv(key)\n                    return self.data.pop(key.upper(), *args)\n            def has_key(self, key):\n                return key.upper() in self.data\n            def __contains__(self, key):\n                return key.upper() in self.data\n            def get(self, key, failobj=None):\n                return self.data.get(key.upper(), failobj)\n            def update(self, dict=None, **kwargs):\n                if dict:\n                    try:\n                        keys = dict.keys()\n                    except AttributeError:\n                        # List of (key, value)\n                        for k, v in dict:\n                            self[k] = v\n                    else:\n                        # got keys\n                        # cannot use items(), since mappings\n                        # may not have them.\n                        for k in keys:\n                            self[k] = dict[k]\n                if kwargs:\n                    self.update(kwargs)\n            def copy(self):\n                return dict(self)\n\n    else:  # Where Env Var Names Can Be Mixed Case\n        class _Environ(UserDict.IterableUserDict):\n            def __init__(self, environ):\n                UserDict.UserDict.__init__(self)\n                self.data = environ\n            def __setitem__(self, key, item):\n                putenv(key, item)\n                self.data[key] = item\n            def update(self,  dict=None, **kwargs):\n                if dict:\n                    try:\n                        keys = dict.keys()\n                    except AttributeError:\n                        # List of (key, value)\n                        for k, v in dict:\n                            self[k] = v\n                    else:\n                        # got keys\n                        # cannot use items(), since mappings\n                        # may not have them.\n                        for k in keys:\n                            self[k] = dict[k]\n                if kwargs:\n                    self.update(kwargs)\n            try:\n                unsetenv\n            except NameError:\n                pass\n            else:\n                def __delitem__(self, key):\n                    unsetenv(key)\n                    del self.data[key]\n                def clear(self):\n                    for key in self.data.keys():\n                        unsetenv(key)\n                        del self.data[key]\n                def pop(self, key, *args):\n                    unsetenv(key)\n                    return self.data.pop(key, *args)\n            def copy(self):\n                return dict(self)\n\n\n    environ = _Environ(environ)\n\ndef getenv(key, default=None):\n    \"\"\"Get an environment variable, return None if it doesn't exist.\n    The optional second argument can specify an alternate default.\"\"\"\n    return environ.get(key, default)\n__all__.append(\"getenv\")\n\ndef _exists(name):\n    return name in globals()\n\n# Supply spawn*() (probably only for Unix)\nif _exists(\"fork\") and not _exists(\"spawnv\") and _exists(\"execv\"):\n\n    P_WAIT = 0\n    P_NOWAIT = P_NOWAITO = 1\n\n    # XXX Should we support P_DETACH?  I suppose it could fork()**2\n    # and close the std I/O streams.  Also, P_OVERLAY is the same\n    # as execv*()?\n\n    def _spawnvef(mode, file, args, env, func):\n        # Internal helper; func is the exec*() function to use\n        pid = fork()\n        if not pid:\n            # Child\n            try:\n                if env is None:\n                    func(file, args)\n                else:\n                    func(file, args, env)\n            except:\n                _exit(127)\n        else:\n            # Parent\n            if mode == P_NOWAIT:\n                return pid # Caller is responsible for waiting!\n            while 1:\n                wpid, sts = waitpid(pid, 0)\n                if WIFSTOPPED(sts):\n                    continue\n                elif WIFSIGNALED(sts):\n                    return -WTERMSIG(sts)\n                elif WIFEXITED(sts):\n                    return WEXITSTATUS(sts)\n                else:\n                    raise error, \"Not stopped, signaled or exited???\"\n\n    def spawnv(mode, file, args):\n        \"\"\"spawnv(mode, file, args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, None, execv)\n\n    def spawnve(mode, file, args, env):\n        \"\"\"spawnve(mode, file, args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nspecified environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, env, execve)\n\n    # Note: spawnvp[e] is't currently supported on Windows\n\n    def spawnvp(mode, file, args):\n        \"\"\"spawnvp(mode, file, args) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, None, execvp)\n\n    def spawnvpe(mode, file, args, env):\n        \"\"\"spawnvpe(mode, file, args, env) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, env, execvpe)\n\nif _exists(\"spawnv\"):\n    # These aren't supplied by the basic Windows code\n    # but can be easily implemented in Python\n\n    def spawnl(mode, file, *args):\n        \"\"\"spawnl(mode, file, *args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return spawnv(mode, file, args)\n\n    def spawnle(mode, file, *args):\n        \"\"\"spawnle(mode, file, *args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nsupplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        env = args[-1]\n        return spawnve(mode, file, args[:-1], env)\n\n\n    __all__.extend([\"spawnv\", \"spawnve\", \"spawnl\", \"spawnle\",])\n\n\nif _exists(\"spawnvp\"):\n    # At the moment, Windows doesn't implement spawnvp[e],\n    # so it won't have spawnlp[e] either.\n    def spawnlp(mode, file, *args):\n        \"\"\"spawnlp(mode, file, *args) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return spawnvp(mode, file, args)\n\n    def spawnlpe(mode, file, *args):\n        \"\"\"spawnlpe(mode, file, *args, env) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        env = args[-1]\n        return spawnvpe(mode, file, args[:-1], env)\n\n\n    __all__.extend([\"spawnvp\", \"spawnvpe\", \"spawnlp\", \"spawnlpe\",])\n\n\n# Supply popen2 etc. (for Unix)\nif _exists(\"fork\"):\n    if not _exists(\"popen2\"):\n        def popen2(cmd, mode=\"t\", bufsize=-1):\n            \"\"\"Execute the shell command 'cmd' in a sub-process.  On UNIX, 'cmd'\n            may be a sequence, in which case arguments will be passed directly to\n            the program without shell intervention (as with os.spawnv()).  If 'cmd'\n            is a string it will be passed to the shell (as with os.system()). If\n            'bufsize' is specified, it sets the buffer size for the I/O pipes.  The\n            file objects (child_stdin, child_stdout) are returned.\"\"\"\n            import warnings\n            msg = \"os.popen2 is deprecated.  Use the subprocess module.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n            import subprocess\n            PIPE = subprocess.PIPE\n            p = subprocess.Popen(cmd, shell=isinstance(cmd, basestring),\n                                 bufsize=bufsize, stdin=PIPE, stdout=PIPE,\n                                 close_fds=True)\n            return p.stdin, p.stdout\n        __all__.append(\"popen2\")\n\n    if not _exists(\"popen3\"):\n        def popen3(cmd, mode=\"t\", bufsize=-1):\n            \"\"\"Execute the shell command 'cmd' in a sub-process.  On UNIX, 'cmd'\n            may be a sequence, in which case arguments will be passed directly to\n            the program without shell intervention (as with os.spawnv()).  If 'cmd'\n            is a string it will be passed to the shell (as with os.system()). If\n            'bufsize' is specified, it sets the buffer size for the I/O pipes.  The\n            file objects (child_stdin, child_stdout, child_stderr) are returned.\"\"\"\n            import warnings\n            msg = \"os.popen3 is deprecated.  Use the subprocess module.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n            import subprocess\n            PIPE = subprocess.PIPE\n            p = subprocess.Popen(cmd, shell=isinstance(cmd, basestring),\n                                 bufsize=bufsize, stdin=PIPE, stdout=PIPE,\n                                 stderr=PIPE, close_fds=True)\n            return p.stdin, p.stdout, p.stderr\n        __all__.append(\"popen3\")\n\n    if not _exists(\"popen4\"):\n        def popen4(cmd, mode=\"t\", bufsize=-1):\n            \"\"\"Execute the shell command 'cmd' in a sub-process.  On UNIX, 'cmd'\n            may be a sequence, in which case arguments will be passed directly to\n            the program without shell intervention (as with os.spawnv()).  If 'cmd'\n            is a string it will be passed to the shell (as with os.system()). If\n            'bufsize' is specified, it sets the buffer size for the I/O pipes.  The\n            file objects (child_stdin, child_stdout_stderr) are returned.\"\"\"\n            import warnings\n            msg = \"os.popen4 is deprecated.  Use the subprocess module.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n            import subprocess\n            PIPE = subprocess.PIPE\n            p = subprocess.Popen(cmd, shell=isinstance(cmd, basestring),\n                                 bufsize=bufsize, stdin=PIPE, stdout=PIPE,\n                                 stderr=subprocess.STDOUT, close_fds=True)\n            return p.stdin, p.stdout\n        __all__.append(\"popen4\")\n\nimport copy_reg as _copy_reg\n\ndef _make_stat_result(tup, dict):\n    return stat_result(tup, dict)\n\ndef _pickle_stat_result(sr):\n    (type, args) = sr.__reduce__()\n    return (_make_stat_result, args)\n\ntry:\n    _copy_reg.pickle(stat_result, _pickle_stat_result, _make_stat_result)\nexcept NameError: # stat_result may not exist\n    pass\n\ndef _make_statvfs_result(tup, dict):\n    return statvfs_result(tup, dict)\n\ndef _pickle_statvfs_result(sr):\n    (type, args) = sr.__reduce__()\n    return (_make_statvfs_result, args)\n\ntry:\n    _copy_reg.pickle(statvfs_result, _pickle_statvfs_result,\n                     _make_statvfs_result)\nexcept NameError: # statvfs_result may not exist\n    pass\n", 
    "pdb": "#! /usr/bin/env python\n\n\"\"\"A Python debugger.\"\"\"\n\n# (See pdb.doc for documentation.)\n\nimport sys\nimport linecache\nimport cmd\nimport bdb\nfrom repr import Repr\nimport os\nimport re\nimport pprint\nimport traceback\n\n\nclass Restart(Exception):\n    \"\"\"Causes a debugger to be restarted for the debugged python program.\"\"\"\n    pass\n\n# Create a custom safe Repr instance and increase its maxstring.\n# The default of 30 truncates error messages too easily.\n_repr = Repr()\n_repr.maxstring = 200\n_saferepr = _repr.repr\n\n__all__ = [\"run\", \"pm\", \"Pdb\", \"runeval\", \"runctx\", \"runcall\", \"set_trace\",\n           \"post_mortem\", \"help\"]\n\ndef find_function(funcname, filename):\n    cre = re.compile(r'def\\s+%s\\s*[(]' % re.escape(funcname))\n    try:\n        fp = open(filename)\n    except IOError:\n        return None\n    # consumer of this info expects the first line to be 1\n    lineno = 1\n    answer = None\n    while 1:\n        line = fp.readline()\n        if line == '':\n            break\n        if cre.match(line):\n            answer = funcname, filename, lineno\n            break\n        lineno = lineno + 1\n    fp.close()\n    return answer\n\n\n# Interaction prompt line will separate file and call info from code\n# text using value of line_prefix string.  A newline and arrow may\n# be to your liking.  You can set it once pdb is imported using the\n# command \"pdb.line_prefix = '\\n% '\".\n# line_prefix = ': '    # Use this to get the old situation back\nline_prefix = '\\n-> '   # Probably a better default\n\nclass Pdb(bdb.Bdb, cmd.Cmd):\n\n    def __init__(self, completekey='tab', stdin=None, stdout=None, skip=None):\n        bdb.Bdb.__init__(self, skip=skip)\n        cmd.Cmd.__init__(self, completekey, stdin, stdout)\n        if stdout:\n            self.use_rawinput = 0\n        self.prompt = '(Pdb) '\n        self.aliases = {}\n        self.mainpyfile = ''\n        self._wait_for_mainpyfile = 0\n        # Try to load readline if it exists\n        try:\n            import readline\n        except ImportError:\n            pass\n\n        # Read $HOME/.pdbrc and ./.pdbrc\n        self.rcLines = []\n        if 'HOME' in os.environ:\n            envHome = os.environ['HOME']\n            try:\n                rcFile = open(os.path.join(envHome, \".pdbrc\"))\n            except IOError:\n                pass\n            else:\n                for line in rcFile.readlines():\n                    self.rcLines.append(line)\n                rcFile.close()\n        try:\n            rcFile = open(\".pdbrc\")\n        except IOError:\n            pass\n        else:\n            for line in rcFile.readlines():\n                self.rcLines.append(line)\n            rcFile.close()\n\n        self.commands = {} # associates a command list to breakpoint numbers\n        self.commands_doprompt = {} # for each bp num, tells if the prompt\n                                    # must be disp. after execing the cmd list\n        self.commands_silent = {} # for each bp num, tells if the stack trace\n                                  # must be disp. after execing the cmd list\n        self.commands_defining = False # True while in the process of defining\n                                       # a command list\n        self.commands_bnum = None # The breakpoint number for which we are\n                                  # defining a list\n\n    def reset(self):\n        bdb.Bdb.reset(self)\n        self.forget()\n\n    def forget(self):\n        self.lineno = None\n        self.stack = []\n        self.curindex = 0\n        self.curframe = None\n\n    def setup(self, f, t):\n        self.forget()\n        self.stack, self.curindex = self.get_stack(f, t)\n        self.curframe = self.stack[self.curindex][0]\n        # The f_locals dictionary is updated from the actual frame\n        # locals whenever the .f_locals accessor is called, so we\n        # cache it here to ensure that modifications are not overwritten.\n        self.curframe_locals = self.curframe.f_locals\n        self.execRcLines()\n\n    # Can be executed earlier than 'setup' if desired\n    def execRcLines(self):\n        if self.rcLines:\n            # Make local copy because of recursion\n            rcLines = self.rcLines\n            # executed only once\n            self.rcLines = []\n            for line in rcLines:\n                line = line[:-1]\n                if len(line) > 0 and line[0] != '#':\n                    self.onecmd(line)\n\n    # Override Bdb methods\n\n    def user_call(self, frame, argument_list):\n        \"\"\"This method is called when there is the remote possibility\n        that we ever need to stop in this function.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        if self.stop_here(frame):\n            print >>self.stdout, '--Call--'\n            self.interaction(frame, None)\n\n    def user_line(self, frame):\n        \"\"\"This function is called when we stop or break at this line.\"\"\"\n        if self._wait_for_mainpyfile:\n            if (self.mainpyfile != self.canonic(frame.f_code.co_filename)\n                or frame.f_lineno<= 0):\n                return\n            self._wait_for_mainpyfile = 0\n        if self.bp_commands(frame):\n            self.interaction(frame, None)\n\n    def bp_commands(self,frame):\n        \"\"\"Call every command that was set for the current active breakpoint\n        (if there is one).\n\n        Returns True if the normal interaction function must be called,\n        False otherwise.\"\"\"\n        # self.currentbp is set in bdb in Bdb.break_here if a breakpoint was hit\n        if getattr(self, \"currentbp\", False) and \\\n               self.currentbp in self.commands:\n            currentbp = self.currentbp\n            self.currentbp = 0\n            lastcmd_back = self.lastcmd\n            self.setup(frame, None)\n            for line in self.commands[currentbp]:\n                self.onecmd(line)\n            self.lastcmd = lastcmd_back\n            if not self.commands_silent[currentbp]:\n                self.print_stack_entry(self.stack[self.curindex])\n            if self.commands_doprompt[currentbp]:\n                self.cmdloop()\n            self.forget()\n            return\n        return 1\n\n    def user_return(self, frame, return_value):\n        \"\"\"This function is called when a return trap is set here.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        frame.f_locals['__return__'] = return_value\n        print >>self.stdout, '--Return--'\n        self.interaction(frame, None)\n\n    def user_exception(self, frame, exc_info):\n        \"\"\"This function is called if an exception occurs,\n        but only if we are to stop at or just below this level.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        exc_type, exc_value, exc_traceback = exc_info\n        frame.f_locals['__exception__'] = exc_type, exc_value\n        if type(exc_type) == type(''):\n            exc_type_name = exc_type\n        else: exc_type_name = exc_type.__name__\n        print >>self.stdout, exc_type_name + ':', _saferepr(exc_value)\n        self.interaction(frame, exc_traceback)\n\n    # General interaction function\n\n    def interaction(self, frame, traceback):\n        self.setup(frame, traceback)\n        self.print_stack_entry(self.stack[self.curindex])\n        self.cmdloop()\n        self.forget()\n\n    def displayhook(self, obj):\n        \"\"\"Custom displayhook for the exec in default(), which prevents\n        assignment of the _ variable in the builtins.\n        \"\"\"\n        # reproduce the behavior of the standard displayhook, not printing None\n        if obj is not None:\n            print repr(obj)\n\n    def default(self, line):\n        if line[:1] == '!': line = line[1:]\n        locals = self.curframe_locals\n        globals = self.curframe.f_globals\n        try:\n            code = compile(line + '\\n', '<stdin>', 'single')\n            save_stdout = sys.stdout\n            save_stdin = sys.stdin\n            save_displayhook = sys.displayhook\n            try:\n                sys.stdin = self.stdin\n                sys.stdout = self.stdout\n                sys.displayhook = self.displayhook\n                exec code in globals, locals\n            finally:\n                sys.stdout = save_stdout\n                sys.stdin = save_stdin\n                sys.displayhook = save_displayhook\n        except:\n            t, v = sys.exc_info()[:2]\n            if type(t) == type(''):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', v\n\n    def precmd(self, line):\n        \"\"\"Handle alias expansion and ';;' separator.\"\"\"\n        if not line.strip():\n            return line\n        args = line.split()\n        while args[0] in self.aliases:\n            line = self.aliases[args[0]]\n            ii = 1\n            for tmpArg in args[1:]:\n                line = line.replace(\"%\" + str(ii),\n                                      tmpArg)\n                ii = ii + 1\n            line = line.replace(\"%*\", ' '.join(args[1:]))\n            args = line.split()\n        # split into ';;' separated commands\n        # unless it's an alias command\n        if args[0] != 'alias':\n            marker = line.find(';;')\n            if marker >= 0:\n                # queue up everything after marker\n                next = line[marker+2:].lstrip()\n                self.cmdqueue.append(next)\n                line = line[:marker].rstrip()\n        return line\n\n    def onecmd(self, line):\n        \"\"\"Interpret the argument as though it had been typed in response\n        to the prompt.\n\n        Checks whether this line is typed at the normal prompt or in\n        a breakpoint command list definition.\n        \"\"\"\n        if not self.commands_defining:\n            return cmd.Cmd.onecmd(self, line)\n        else:\n            return self.handle_command_def(line)\n\n    def handle_command_def(self,line):\n        \"\"\"Handles one command line during command list definition.\"\"\"\n        cmd, arg, line = self.parseline(line)\n        if not cmd:\n            return\n        if cmd == 'silent':\n            self.commands_silent[self.commands_bnum] = True\n            return # continue to handle other cmd def in the cmd list\n        elif cmd == 'end':\n            self.cmdqueue = []\n            return 1 # end of cmd list\n        cmdlist = self.commands[self.commands_bnum]\n        if arg:\n            cmdlist.append(cmd+' '+arg)\n        else:\n            cmdlist.append(cmd)\n        # Determine if we must stop\n        try:\n            func = getattr(self, 'do_' + cmd)\n        except AttributeError:\n            func = self.default\n        # one of the resuming commands\n        if func.func_name in self.commands_resuming:\n            self.commands_doprompt[self.commands_bnum] = False\n            self.cmdqueue = []\n            return 1\n        return\n\n    # Command definitions, called by cmdloop()\n    # The argument is the remaining string on the command line\n    # Return true to exit from the command loop\n\n    do_h = cmd.Cmd.do_help\n\n    def do_commands(self, arg):\n        \"\"\"Defines a list of commands associated to a breakpoint.\n\n        Those commands will be executed whenever the breakpoint causes\n        the program to stop execution.\"\"\"\n        if not arg:\n            bnum = len(bdb.Breakpoint.bpbynumber)-1\n        else:\n            try:\n                bnum = int(arg)\n            except:\n                print >>self.stdout, \"Usage : commands [bnum]\\n        ...\" \\\n                                     \"\\n        end\"\n                return\n        self.commands_bnum = bnum\n        self.commands[bnum] = []\n        self.commands_doprompt[bnum] = True\n        self.commands_silent[bnum] = False\n        prompt_back = self.prompt\n        self.prompt = '(com) '\n        self.commands_defining = True\n        try:\n            self.cmdloop()\n        finally:\n            self.commands_defining = False\n            self.prompt = prompt_back\n\n    def do_break(self, arg, temporary = 0):\n        # break [ ([filename:]lineno | function) [, \"condition\"] ]\n        if not arg:\n            if self.breaks:  # There's at least one\n                print >>self.stdout, \"Num Type         Disp Enb   Where\"\n                for bp in bdb.Breakpoint.bpbynumber:\n                    if bp:\n                        bp.bpprint(self.stdout)\n            return\n        # parse arguments; comma has lowest precedence\n        # and cannot occur in filename\n        filename = None\n        lineno = None\n        cond = None\n        comma = arg.find(',')\n        if comma > 0:\n            # parse stuff after comma: \"condition\"\n            cond = arg[comma+1:].lstrip()\n            arg = arg[:comma].rstrip()\n        # parse stuff before comma: [filename:]lineno | function\n        colon = arg.rfind(':')\n        funcname = None\n        if colon >= 0:\n            filename = arg[:colon].rstrip()\n            f = self.lookupmodule(filename)\n            if not f:\n                print >>self.stdout, '*** ', repr(filename),\n                print >>self.stdout, 'not found from sys.path'\n                return\n            else:\n                filename = f\n            arg = arg[colon+1:].lstrip()\n            try:\n                lineno = int(arg)\n            except ValueError, msg:\n                print >>self.stdout, '*** Bad lineno:', arg\n                return\n        else:\n            # no colon; can be lineno or function\n            try:\n                lineno = int(arg)\n            except ValueError:\n                try:\n                    func = eval(arg,\n                                self.curframe.f_globals,\n                                self.curframe_locals)\n                except:\n                    func = arg\n                try:\n                    if hasattr(func, 'im_func'):\n                        func = func.im_func\n                    code = func.func_code\n                    #use co_name to identify the bkpt (function names\n                    #could be aliased, but co_name is invariant)\n                    funcname = code.co_name\n                    lineno = code.co_firstlineno\n                    filename = code.co_filename\n                except:\n                    # last thing to try\n                    (ok, filename, ln) = self.lineinfo(arg)\n                    if not ok:\n                        print >>self.stdout, '*** The specified object',\n                        print >>self.stdout, repr(arg),\n                        print >>self.stdout, 'is not a function'\n                        print >>self.stdout, 'or was not found along sys.path.'\n                        return\n                    funcname = ok # ok contains a function name\n                    lineno = int(ln)\n        if not filename:\n            filename = self.defaultFile()\n        # Check for reasonable breakpoint\n        line = self.checkline(filename, lineno)\n        if line:\n            # now set the break point\n            err = self.set_break(filename, line, temporary, cond, funcname)\n            if err: print >>self.stdout, '***', err\n            else:\n                bp = self.get_breaks(filename, line)[-1]\n                print >>self.stdout, \"Breakpoint %d at %s:%d\" % (bp.number,\n                                                                 bp.file,\n                                                                 bp.line)\n\n    # To be overridden in derived debuggers\n    def defaultFile(self):\n        \"\"\"Produce a reasonable default.\"\"\"\n        filename = self.curframe.f_code.co_filename\n        if filename == '<string>' and self.mainpyfile:\n            filename = self.mainpyfile\n        return filename\n\n    do_b = do_break\n\n    def do_tbreak(self, arg):\n        self.do_break(arg, 1)\n\n    def lineinfo(self, identifier):\n        failed = (None, None, None)\n        # Input is identifier, may be in single quotes\n        idstring = identifier.split(\"'\")\n        if len(idstring) == 1:\n            # not in single quotes\n            id = idstring[0].strip()\n        elif len(idstring) == 3:\n            # quoted\n            id = idstring[1].strip()\n        else:\n            return failed\n        if id == '': return failed\n        parts = id.split('.')\n        # Protection for derived debuggers\n        if parts[0] == 'self':\n            del parts[0]\n            if len(parts) == 0:\n                return failed\n        # Best first guess at file to look at\n        fname = self.defaultFile()\n        if len(parts) == 1:\n            item = parts[0]\n        else:\n            # More than one part.\n            # First is module, second is method/class\n            f = self.lookupmodule(parts[0])\n            if f:\n                fname = f\n            item = parts[1]\n        answer = find_function(item, fname)\n        return answer or failed\n\n    def checkline(self, filename, lineno):\n        \"\"\"Check whether specified line seems to be executable.\n\n        Return `lineno` if it is, 0 if not (e.g. a docstring, comment, blank\n        line or EOF). Warning: testing is not comprehensive.\n        \"\"\"\n        # this method should be callable before starting debugging, so default\n        # to \"no globals\" if there is no current frame\n        globs = self.curframe.f_globals if hasattr(self, 'curframe') else None\n        line = linecache.getline(filename, lineno, globs)\n        if not line:\n            print >>self.stdout, 'End of file'\n            return 0\n        line = line.strip()\n        # Don't allow setting breakpoint at a blank line\n        if (not line or (line[0] == '#') or\n             (line[:3] == '\"\"\"') or line[:3] == \"'''\"):\n            print >>self.stdout, '*** Blank or comment'\n            return 0\n        return lineno\n\n    def do_enable(self, arg):\n        args = arg.split()\n        for i in args:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n\n            bp = bdb.Breakpoint.bpbynumber[i]\n            if bp:\n                bp.enable()\n\n    def do_disable(self, arg):\n        args = arg.split()\n        for i in args:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n\n            bp = bdb.Breakpoint.bpbynumber[i]\n            if bp:\n                bp.disable()\n\n    def do_condition(self, arg):\n        # arg is breakpoint number and condition\n        args = arg.split(' ', 1)\n        try:\n            bpnum = int(args[0].strip())\n        except ValueError:\n            # something went wrong\n            print >>self.stdout, \\\n                'Breakpoint index %r is not a number' % args[0]\n            return\n        try:\n            cond = args[1]\n        except:\n            cond = None\n        try:\n            bp = bdb.Breakpoint.bpbynumber[bpnum]\n        except IndexError:\n            print >>self.stdout, 'Breakpoint index %r is not valid' % args[0]\n            return\n        if bp:\n            bp.cond = cond\n            if not cond:\n                print >>self.stdout, 'Breakpoint', bpnum,\n                print >>self.stdout, 'is now unconditional.'\n\n    def do_ignore(self,arg):\n        \"\"\"arg is bp number followed by ignore count.\"\"\"\n        args = arg.split()\n        try:\n            bpnum = int(args[0].strip())\n        except ValueError:\n            # something went wrong\n            print >>self.stdout, \\\n                'Breakpoint index %r is not a number' % args[0]\n            return\n        try:\n            count = int(args[1].strip())\n        except:\n            count = 0\n        try:\n            bp = bdb.Breakpoint.bpbynumber[bpnum]\n        except IndexError:\n            print >>self.stdout, 'Breakpoint index %r is not valid' % args[0]\n            return\n        if bp:\n            bp.ignore = count\n            if count > 0:\n                reply = 'Will ignore next '\n                if count > 1:\n                    reply = reply + '%d crossings' % count\n                else:\n                    reply = reply + '1 crossing'\n                print >>self.stdout, reply + ' of breakpoint %d.' % bpnum\n            else:\n                print >>self.stdout, 'Will stop next time breakpoint',\n                print >>self.stdout, bpnum, 'is reached.'\n\n    def do_clear(self, arg):\n        \"\"\"Three possibilities, tried in this order:\n        clear -> clear all breaks, ask for confirmation\n        clear file:lineno -> clear all breaks at file:lineno\n        clear bpno bpno ... -> clear breakpoints by number\"\"\"\n        if not arg:\n            try:\n                reply = raw_input('Clear all breaks? ')\n            except EOFError:\n                reply = 'no'\n            reply = reply.strip().lower()\n            if reply in ('y', 'yes'):\n                self.clear_all_breaks()\n            return\n        if ':' in arg:\n            # Make sure it works for \"clear C:\\foo\\bar.py:12\"\n            i = arg.rfind(':')\n            filename = arg[:i]\n            arg = arg[i+1:]\n            try:\n                lineno = int(arg)\n            except ValueError:\n                err = \"Invalid line number (%s)\" % arg\n            else:\n                err = self.clear_break(filename, lineno)\n            if err: print >>self.stdout, '***', err\n            return\n        numberlist = arg.split()\n        for i in numberlist:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n            err = self.clear_bpbynumber(i)\n            if err:\n                print >>self.stdout, '***', err\n            else:\n                print >>self.stdout, 'Deleted breakpoint', i\n    do_cl = do_clear # 'c' is already an abbreviation for 'continue'\n\n    def do_where(self, arg):\n        self.print_stack_trace()\n    do_w = do_where\n    do_bt = do_where\n\n    def do_up(self, arg):\n        if self.curindex == 0:\n            print >>self.stdout, '*** Oldest frame'\n        else:\n            self.curindex = self.curindex - 1\n            self.curframe = self.stack[self.curindex][0]\n            self.curframe_locals = self.curframe.f_locals\n            self.print_stack_entry(self.stack[self.curindex])\n            self.lineno = None\n    do_u = do_up\n\n    def do_down(self, arg):\n        if self.curindex + 1 == len(self.stack):\n            print >>self.stdout, '*** Newest frame'\n        else:\n            self.curindex = self.curindex + 1\n            self.curframe = self.stack[self.curindex][0]\n            self.curframe_locals = self.curframe.f_locals\n            self.print_stack_entry(self.stack[self.curindex])\n            self.lineno = None\n    do_d = do_down\n\n    def do_until(self, arg):\n        self.set_until(self.curframe)\n        return 1\n    do_unt = do_until\n\n    def do_step(self, arg):\n        self.set_step()\n        return 1\n    do_s = do_step\n\n    def do_next(self, arg):\n        self.set_next(self.curframe)\n        return 1\n    do_n = do_next\n\n    def do_run(self, arg):\n        \"\"\"Restart program by raising an exception to be caught in the main\n        debugger loop.  If arguments were given, set them in sys.argv.\"\"\"\n        if arg:\n            import shlex\n            argv0 = sys.argv[0:1]\n            sys.argv = shlex.split(arg)\n            sys.argv[:0] = argv0\n        raise Restart\n\n    do_restart = do_run\n\n    def do_return(self, arg):\n        self.set_return(self.curframe)\n        return 1\n    do_r = do_return\n\n    def do_continue(self, arg):\n        self.set_continue()\n        return 1\n    do_c = do_cont = do_continue\n\n    def do_jump(self, arg):\n        if self.curindex + 1 != len(self.stack):\n            print >>self.stdout, \"*** You can only jump within the bottom frame\"\n            return\n        try:\n            arg = int(arg)\n        except ValueError:\n            print >>self.stdout, \"*** The 'jump' command requires a line number.\"\n        else:\n            try:\n                # Do the jump, fix up our copy of the stack, and display the\n                # new position\n                self.curframe.f_lineno = arg\n                self.stack[self.curindex] = self.stack[self.curindex][0], arg\n                self.print_stack_entry(self.stack[self.curindex])\n            except ValueError, e:\n                print >>self.stdout, '*** Jump failed:', e\n    do_j = do_jump\n\n    def do_debug(self, arg):\n        sys.settrace(None)\n        globals = self.curframe.f_globals\n        locals = self.curframe_locals\n        p = Pdb(self.completekey, self.stdin, self.stdout)\n        p.prompt = \"(%s) \" % self.prompt.strip()\n        print >>self.stdout, \"ENTERING RECURSIVE DEBUGGER\"\n        sys.call_tracing(p.run, (arg, globals, locals))\n        print >>self.stdout, \"LEAVING RECURSIVE DEBUGGER\"\n        sys.settrace(self.trace_dispatch)\n        self.lastcmd = p.lastcmd\n\n    def do_quit(self, arg):\n        self._user_requested_quit = 1\n        self.set_quit()\n        return 1\n\n    do_q = do_quit\n    do_exit = do_quit\n\n    def do_EOF(self, arg):\n        print >>self.stdout\n        self._user_requested_quit = 1\n        self.set_quit()\n        return 1\n\n    def do_args(self, arg):\n        co = self.curframe.f_code\n        dict = self.curframe_locals\n        n = co.co_argcount\n        if co.co_flags & 4: n = n+1\n        if co.co_flags & 8: n = n+1\n        for i in range(n):\n            name = co.co_varnames[i]\n            print >>self.stdout, name, '=',\n            if name in dict: print >>self.stdout, dict[name]\n            else: print >>self.stdout, \"*** undefined ***\"\n    do_a = do_args\n\n    def do_retval(self, arg):\n        if '__return__' in self.curframe_locals:\n            print >>self.stdout, self.curframe_locals['__return__']\n        else:\n            print >>self.stdout, '*** Not yet returned!'\n    do_rv = do_retval\n\n    def _getval(self, arg):\n        try:\n            return eval(arg, self.curframe.f_globals,\n                        self.curframe_locals)\n        except:\n            t, v = sys.exc_info()[:2]\n            if isinstance(t, str):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', repr(v)\n            raise\n\n    def do_p(self, arg):\n        try:\n            print >>self.stdout, repr(self._getval(arg))\n        except:\n            pass\n\n    def do_pp(self, arg):\n        try:\n            pprint.pprint(self._getval(arg), self.stdout)\n        except:\n            pass\n\n    def do_list(self, arg):\n        self.lastcmd = 'list'\n        last = None\n        if arg:\n            try:\n                x = eval(arg, {}, {})\n                if type(x) == type(()):\n                    first, last = x\n                    first = int(first)\n                    last = int(last)\n                    if last < first:\n                        # Assume it's a count\n                        last = first + last\n                else:\n                    first = max(1, int(x) - 5)\n            except:\n                print >>self.stdout, '*** Error in argument:', repr(arg)\n                return\n        elif self.lineno is None:\n            first = max(1, self.curframe.f_lineno - 5)\n        else:\n            first = self.lineno + 1\n        if last is None:\n            last = first + 10\n        filename = self.curframe.f_code.co_filename\n        breaklist = self.get_file_breaks(filename)\n        try:\n            for lineno in range(first, last+1):\n                line = linecache.getline(filename, lineno,\n                                         self.curframe.f_globals)\n                if not line:\n                    print >>self.stdout, '[EOF]'\n                    break\n                else:\n                    s = repr(lineno).rjust(3)\n                    if len(s) < 4: s = s + ' '\n                    if lineno in breaklist: s = s + 'B'\n                    else: s = s + ' '\n                    if lineno == self.curframe.f_lineno:\n                        s = s + '->'\n                    print >>self.stdout, s + '\\t' + line,\n                    self.lineno = lineno\n        except KeyboardInterrupt:\n            pass\n    do_l = do_list\n\n    def do_whatis(self, arg):\n        try:\n            value = eval(arg, self.curframe.f_globals,\n                            self.curframe_locals)\n        except:\n            t, v = sys.exc_info()[:2]\n            if type(t) == type(''):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', repr(v)\n            return\n        code = None\n        # Is it a function?\n        try: code = value.func_code\n        except: pass\n        if code:\n            print >>self.stdout, 'Function', code.co_name\n            return\n        # Is it an instance method?\n        try: code = value.im_func.func_code\n        except: pass\n        if code:\n            print >>self.stdout, 'Method', code.co_name\n            return\n        # None of the above...\n        print >>self.stdout, type(value)\n\n    def do_alias(self, arg):\n        args = arg.split()\n        if len(args) == 0:\n            keys = self.aliases.keys()\n            keys.sort()\n            for alias in keys:\n                print >>self.stdout, \"%s = %s\" % (alias, self.aliases[alias])\n            return\n        if args[0] in self.aliases and len(args) == 1:\n            print >>self.stdout, \"%s = %s\" % (args[0], self.aliases[args[0]])\n        else:\n            self.aliases[args[0]] = ' '.join(args[1:])\n\n    def do_unalias(self, arg):\n        args = arg.split()\n        if len(args) == 0: return\n        if args[0] in self.aliases:\n            del self.aliases[args[0]]\n\n    #list of all the commands making the program resume execution.\n    commands_resuming = ['do_continue', 'do_step', 'do_next', 'do_return',\n                         'do_quit', 'do_jump']\n\n    # Print a traceback starting at the top stack frame.\n    # The most recently entered frame is printed last;\n    # this is different from dbx and gdb, but consistent with\n    # the Python interpreter's stack trace.\n    # It is also consistent with the up/down commands (which are\n    # compatible with dbx and gdb: up moves towards 'main()'\n    # and down moves towards the most recent stack frame).\n\n    def print_stack_trace(self):\n        try:\n            for frame_lineno in self.stack:\n                self.print_stack_entry(frame_lineno)\n        except KeyboardInterrupt:\n            pass\n\n    def print_stack_entry(self, frame_lineno, prompt_prefix=line_prefix):\n        frame, lineno = frame_lineno\n        if frame is self.curframe:\n            print >>self.stdout, '>',\n        else:\n            print >>self.stdout, ' ',\n        print >>self.stdout, self.format_stack_entry(frame_lineno,\n                                                     prompt_prefix)\n\n\n    # Help methods (derived from pdb.doc)\n\n    def help_help(self):\n        self.help_h()\n\n    def help_h(self):\n        print >>self.stdout, \"\"\"h(elp)\nWithout argument, print the list of available commands.\nWith a command name as argument, print help about that command\n\"help pdb\" pipes the full documentation file to the $PAGER\n\"help exec\" gives help on the ! command\"\"\"\n\n    def help_where(self):\n        self.help_w()\n\n    def help_w(self):\n        print >>self.stdout, \"\"\"w(here)\nPrint a stack trace, with the most recent frame at the bottom.\nAn arrow indicates the \"current frame\", which determines the\ncontext of most commands.  'bt' is an alias for this command.\"\"\"\n\n    help_bt = help_w\n\n    def help_down(self):\n        self.help_d()\n\n    def help_d(self):\n        print >>self.stdout, \"\"\"d(own)\nMove the current frame one level down in the stack trace\n(to a newer frame).\"\"\"\n\n    def help_up(self):\n        self.help_u()\n\n    def help_u(self):\n        print >>self.stdout, \"\"\"u(p)\nMove the current frame one level up in the stack trace\n(to an older frame).\"\"\"\n\n    def help_break(self):\n        self.help_b()\n\n    def help_b(self):\n        print >>self.stdout, \"\"\"b(reak) ([file:]lineno | function) [, condition]\nWith a line number argument, set a break there in the current\nfile.  With a function name, set a break at first executable line\nof that function.  Without argument, list all breaks.  If a second\nargument is present, it is a string specifying an expression\nwhich must evaluate to true before the breakpoint is honored.\n\nThe line number may be prefixed with a filename and a colon,\nto specify a breakpoint in another file (probably one that\nhasn't been loaded yet).  The file is searched for on sys.path;\nthe .py suffix may be omitted.\"\"\"\n\n    def help_clear(self):\n        self.help_cl()\n\n    def help_cl(self):\n        print >>self.stdout, \"cl(ear) filename:lineno\"\n        print >>self.stdout, \"\"\"cl(ear) [bpnumber [bpnumber...]]\nWith a space separated list of breakpoint numbers, clear\nthose breakpoints.  Without argument, clear all breaks (but\nfirst ask confirmation).  With a filename:lineno argument,\nclear all breaks at that line in that file.\n\nNote that the argument is different from previous versions of\nthe debugger (in python distributions 1.5.1 and before) where\na linenumber was used instead of either filename:lineno or\nbreakpoint numbers.\"\"\"\n\n    def help_tbreak(self):\n        print >>self.stdout, \"\"\"tbreak  same arguments as break, but breakpoint\nis removed when first hit.\"\"\"\n\n    def help_enable(self):\n        print >>self.stdout, \"\"\"enable bpnumber [bpnumber ...]\nEnables the breakpoints given as a space separated list of\nbp numbers.\"\"\"\n\n    def help_disable(self):\n        print >>self.stdout, \"\"\"disable bpnumber [bpnumber ...]\nDisables the breakpoints given as a space separated list of\nbp numbers.\"\"\"\n\n    def help_ignore(self):\n        print >>self.stdout, \"\"\"ignore bpnumber count\nSets the ignore count for the given breakpoint number.  A breakpoint\nbecomes active when the ignore count is zero.  When non-zero, the\ncount is decremented each time the breakpoint is reached and the\nbreakpoint is not disabled and any associated condition evaluates\nto true.\"\"\"\n\n    def help_condition(self):\n        print >>self.stdout, \"\"\"condition bpnumber str_condition\nstr_condition is a string specifying an expression which\nmust evaluate to true before the breakpoint is honored.\nIf str_condition is absent, any existing condition is removed;\ni.e., the breakpoint is made unconditional.\"\"\"\n\n    def help_step(self):\n        self.help_s()\n\n    def help_s(self):\n        print >>self.stdout, \"\"\"s(tep)\nExecute the current line, stop at the first possible occasion\n(either in a function that is called or in the current function).\"\"\"\n\n    def help_until(self):\n        self.help_unt()\n\n    def help_unt(self):\n        print \"\"\"unt(il)\nContinue execution until the line with a number greater than the current\none is reached or until the current frame returns\"\"\"\n\n    def help_next(self):\n        self.help_n()\n\n    def help_n(self):\n        print >>self.stdout, \"\"\"n(ext)\nContinue execution until the next line in the current function\nis reached or it returns.\"\"\"\n\n    def help_return(self):\n        self.help_r()\n\n    def help_r(self):\n        print >>self.stdout, \"\"\"r(eturn)\nContinue execution until the current function returns.\"\"\"\n\n    def help_continue(self):\n        self.help_c()\n\n    def help_cont(self):\n        self.help_c()\n\n    def help_c(self):\n        print >>self.stdout, \"\"\"c(ont(inue))\nContinue execution, only stop when a breakpoint is encountered.\"\"\"\n\n    def help_jump(self):\n        self.help_j()\n\n    def help_j(self):\n        print >>self.stdout, \"\"\"j(ump) lineno\nSet the next line that will be executed.\"\"\"\n\n    def help_debug(self):\n        print >>self.stdout, \"\"\"debug code\nEnter a recursive debugger that steps through the code argument\n(which is an arbitrary expression or statement to be executed\nin the current environment).\"\"\"\n\n    def help_list(self):\n        self.help_l()\n\n    def help_l(self):\n        print >>self.stdout, \"\"\"l(ist) [first [,last]]\nList source code for the current file.\nWithout arguments, list 11 lines around the current line\nor continue the previous listing.\nWith one argument, list 11 lines starting at that line.\nWith two arguments, list the given range;\nif the second argument is less than the first, it is a count.\"\"\"\n\n    def help_args(self):\n        self.help_a()\n\n    def help_a(self):\n        print >>self.stdout, \"\"\"a(rgs)\nPrint the arguments of the current function.\"\"\"\n\n    def help_p(self):\n        print >>self.stdout, \"\"\"p expression\nPrint the value of the expression.\"\"\"\n\n    def help_pp(self):\n        print >>self.stdout, \"\"\"pp expression\nPretty-print the value of the expression.\"\"\"\n\n    def help_exec(self):\n        print >>self.stdout, \"\"\"(!) statement\nExecute the (one-line) statement in the context of\nthe current stack frame.\nThe exclamation point can be omitted unless the first word\nof the statement resembles a debugger command.\nTo assign to a global variable you must always prefix the\ncommand with a 'global' command, e.g.:\n(Pdb) global list_options; list_options = ['-l']\n(Pdb)\"\"\"\n\n    def help_run(self):\n        print \"\"\"run [args...]\nRestart the debugged python program. If a string is supplied, it is\nsplit with \"shlex\" and the result is used as the new sys.argv.\nHistory, breakpoints, actions and debugger options are preserved.\n\"restart\" is an alias for \"run\".\"\"\"\n\n    help_restart = help_run\n\n    def help_quit(self):\n        self.help_q()\n\n    def help_q(self):\n        print >>self.stdout, \"\"\"q(uit) or exit - Quit from the debugger.\nThe program being executed is aborted.\"\"\"\n\n    help_exit = help_q\n\n    def help_whatis(self):\n        print >>self.stdout, \"\"\"whatis arg\nPrints the type of the argument.\"\"\"\n\n    def help_EOF(self):\n        print >>self.stdout, \"\"\"EOF\nHandles the receipt of EOF as a command.\"\"\"\n\n    def help_alias(self):\n        print >>self.stdout, \"\"\"alias [name [command [parameter parameter ...]]]\nCreates an alias called 'name' the executes 'command'.  The command\nmust *not* be enclosed in quotes.  Replaceable parameters are\nindicated by %1, %2, and so on, while %* is replaced by all the\nparameters.  If no command is given, the current alias for name\nis shown. If no name is given, all aliases are listed.\n\nAliases may be nested and can contain anything that can be\nlegally typed at the pdb prompt.  Note!  You *can* override\ninternal pdb commands with aliases!  Those internal commands\nare then hidden until the alias is removed.  Aliasing is recursively\napplied to the first word of the command line; all other words\nin the line are left alone.\n\nSome useful aliases (especially when placed in the .pdbrc file) are:\n\n#Print instance variables (usage \"pi classInst\")\nalias pi for k in %1.__dict__.keys(): print \"%1.\",k,\"=\",%1.__dict__[k]\n\n#Print instance variables in self\nalias ps pi self\n\"\"\"\n\n    def help_unalias(self):\n        print >>self.stdout, \"\"\"unalias name\nDeletes the specified alias.\"\"\"\n\n    def help_commands(self):\n        print >>self.stdout, \"\"\"commands [bpnumber]\n(com) ...\n(com) end\n(Pdb)\n\nSpecify a list of commands for breakpoint number bpnumber.  The\ncommands themselves appear on the following lines.  Type a line\ncontaining just 'end' to terminate the commands.\n\nTo remove all commands from a breakpoint, type commands and\nfollow it immediately with  end; that is, give no commands.\n\nWith no bpnumber argument, commands refers to the last\nbreakpoint set.\n\nYou can use breakpoint commands to start your program up again.\nSimply use the continue command, or step, or any other\ncommand that resumes execution.\n\nSpecifying any command resuming execution (currently continue,\nstep, next, return, jump, quit and their abbreviations) terminates\nthe command list (as if that command was immediately followed by end).\nThis is because any time you resume execution\n(even with a simple next or step), you may encounter\nanother breakpoint--which could have its own command list, leading to\nambiguities about which list to execute.\n\n   If you use the 'silent' command in the command list, the\nusual message about stopping at a breakpoint is not printed.  This may\nbe desirable for breakpoints that are to print a specific message and\nthen continue.  If none of the other commands print anything, you\nsee no sign that the breakpoint was reached.\n\"\"\"\n\n    def help_pdb(self):\n        help()\n\n    def lookupmodule(self, filename):\n        \"\"\"Helper function for break/clear parsing -- may be overridden.\n\n        lookupmodule() translates (possibly incomplete) file or module name\n        into an absolute file name.\n        \"\"\"\n        if os.path.isabs(filename) and  os.path.exists(filename):\n            return filename\n        f = os.path.join(sys.path[0], filename)\n        if  os.path.exists(f) and self.canonic(f) == self.mainpyfile:\n            return f\n        root, ext = os.path.splitext(filename)\n        if ext == '':\n            filename = filename + '.py'\n        if os.path.isabs(filename):\n            return filename\n        for dirname in sys.path:\n            while os.path.islink(dirname):\n                dirname = os.readlink(dirname)\n            fullname = os.path.join(dirname, filename)\n            if os.path.exists(fullname):\n                return fullname\n        return None\n\n    def _runscript(self, filename):\n        # The script has to run in __main__ namespace (or imports from\n        # __main__ will break).\n        #\n        # So we clear up the __main__ and set several special variables\n        # (this gets rid of pdb's globals and cleans old variables on restarts).\n        import __main__\n        __main__.__dict__.clear()\n        __main__.__dict__.update({\"__name__\"    : \"__main__\",\n                                  \"__file__\"    : filename,\n                                  \"__builtins__\": __builtins__,\n                                 })\n\n        # When bdb sets tracing, a number of call and line events happens\n        # BEFORE debugger even reaches user's code (and the exact sequence of\n        # events depends on python version). So we take special measures to\n        # avoid stopping before we reach the main script (see user_line and\n        # user_call for details).\n        self._wait_for_mainpyfile = 1\n        self.mainpyfile = self.canonic(filename)\n        self._user_requested_quit = 0\n        statement = 'execfile(%r)' % filename\n        self.run(statement)\n\n# Simplified interface\n\ndef run(statement, globals=None, locals=None):\n    Pdb().run(statement, globals, locals)\n\ndef runeval(expression, globals=None, locals=None):\n    return Pdb().runeval(expression, globals, locals)\n\ndef runctx(statement, globals, locals):\n    # B/W compatibility\n    run(statement, globals, locals)\n\ndef runcall(*args, **kwds):\n    return Pdb().runcall(*args, **kwds)\n\ndef set_trace():\n    Pdb().set_trace(sys._getframe().f_back)\n\n# Post-Mortem interface\n\ndef post_mortem(t=None):\n    # handling the default\n    if t is None:\n        # sys.exc_info() returns (type, value, traceback) if an exception is\n        # being handled, otherwise it returns None\n        t = sys.exc_info()[2]\n        if t is None:\n            raise ValueError(\"A valid traceback must be passed if no \"\n                                               \"exception is being handled\")\n\n    p = Pdb()\n    p.reset()\n    p.interaction(None, t)\n\ndef pm():\n    post_mortem(sys.last_traceback)\n\n\n# Main program for testing\n\nTESTCMD = 'import x; x.main()'\n\ndef test():\n    run(TESTCMD)\n\n# print help\ndef help():\n    for dirname in sys.path:\n        fullname = os.path.join(dirname, 'pdb.doc')\n        if os.path.exists(fullname):\n            sts = os.system('${PAGER-more} '+fullname)\n            if sts: print '*** Pager exit status:', sts\n            break\n    else:\n        print 'Sorry, can\\'t find the help file \"pdb.doc\"',\n        print 'along the Python search path'\n\ndef main():\n    if not sys.argv[1:] or sys.argv[1] in (\"--help\", \"-h\"):\n        print \"usage: pdb.py scriptfile [arg] ...\"\n        sys.exit(2)\n\n    mainpyfile =  sys.argv[1]     # Get script filename\n    if not os.path.exists(mainpyfile):\n        print 'Error:', mainpyfile, 'does not exist'\n        sys.exit(1)\n\n    del sys.argv[0]         # Hide \"pdb.py\" from argument list\n\n    # Replace pdb's dir with script's dir in front of module search path.\n    sys.path[0] = os.path.dirname(mainpyfile)\n\n    # Note on saving/restoring sys.argv: it's a good idea when sys.argv was\n    # modified by the script being debugged. It's a bad idea when it was\n    # changed by the user from the command line. There is a \"restart\" command\n    # which allows explicit specification of command line arguments.\n    pdb = Pdb()\n    while True:\n        try:\n            pdb._runscript(mainpyfile)\n            if pdb._user_requested_quit:\n                break\n            print \"The program finished and will be restarted\"\n        except Restart:\n            print \"Restarting\", mainpyfile, \"with arguments:\"\n            print \"\\t\" + \" \".join(sys.argv[1:])\n        except SystemExit:\n            # In most cases SystemExit does not warrant a post-mortem session.\n            print \"The program exited via sys.exit(). Exit status: \",\n            print sys.exc_info()[1]\n        except:\n            traceback.print_exc()\n            print \"Uncaught exception. Entering post mortem debugging\"\n            print \"Running 'cont' or 'step' will restart the program\"\n            t = sys.exc_info()[2]\n            pdb.interaction(None, t)\n            print \"Post mortem debugger finished. The \" + mainpyfile + \\\n                  \" will be restarted\"\n\n\n# When invoked as main program, invoke the debugger on a script\nif __name__ == '__main__':\n    import pdb\n    pdb.main()\n", 
    "pickle": "\"\"\"Create portable serialized representations of Python objects.\n\nSee module cPickle for a (much) faster implementation.\nSee module copy_reg for a mechanism for registering custom picklers.\nSee module pickletools source for extensive comments.\n\nClasses:\n\n    Pickler\n    Unpickler\n\nFunctions:\n\n    dump(object, file)\n    dumps(object) -> string\n    load(file) -> object\n    loads(string) -> object\n\nMisc variables:\n\n    __version__\n    format_version\n    compatible_formats\n\n\"\"\"\n\n__version__ = \"$Revision: 72223 $\"       # Code version\n\nfrom types import *\nfrom copy_reg import dispatch_table\nfrom copy_reg import _extension_registry, _inverted_registry, _extension_cache\nimport marshal\nimport sys\nimport struct\nimport re\n\n__all__ = [\"PickleError\", \"PicklingError\", \"UnpicklingError\", \"Pickler\",\n           \"Unpickler\", \"dump\", \"dumps\", \"load\", \"loads\"]\n\n# These are purely informational; no code uses these.\nformat_version = \"2.0\"                  # File format version we write\ncompatible_formats = [\"1.0\",            # Original protocol 0\n                      \"1.1\",            # Protocol 0 with INST added\n                      \"1.2\",            # Original protocol 1\n                      \"1.3\",            # Protocol 1 with BINFLOAT added\n                      \"2.0\",            # Protocol 2\n                      ]                 # Old format versions we can read\n\n# Keep in synch with cPickle.  This is the highest protocol number we\n# know how to read.\nHIGHEST_PROTOCOL = 2\n\n# Why use struct.pack() for pickling but marshal.loads() for\n# unpickling?  struct.pack() is 40% faster than marshal.dumps(), but\n# marshal.loads() is twice as fast as struct.unpack()!\nmloads = marshal.loads\n\nclass PickleError(Exception):\n    \"\"\"A common base class for the other pickling exceptions.\"\"\"\n    pass\n\nclass PicklingError(PickleError):\n    \"\"\"This exception is raised when an unpicklable object is passed to the\n    dump() method.\n\n    \"\"\"\n    pass\n\nclass UnpicklingError(PickleError):\n    \"\"\"This exception is raised when there is a problem unpickling an object,\n    such as a security violation.\n\n    Note that other exceptions may also be raised during unpickling, including\n    (but not necessarily limited to) AttributeError, EOFError, ImportError,\n    and IndexError.\n\n    \"\"\"\n    pass\n\n# An instance of _Stop is raised by Unpickler.load_stop() in response to\n# the STOP opcode, passing the object that is the result of unpickling.\nclass _Stop(Exception):\n    def __init__(self, value):\n        self.value = value\n\n# Jython has PyStringMap; it's a dict subclass with string keys\ntry:\n    from org.python.core import PyStringMap\nexcept ImportError:\n    PyStringMap = None\n\n# UnicodeType may or may not be exported (normally imported from types)\ntry:\n    UnicodeType\nexcept NameError:\n    UnicodeType = None\n\n# Pickle opcodes.  See pickletools.py for extensive docs.  The listing\n# here is in kind-of alphabetical order of 1-character pickle code.\n# pickletools groups them by purpose.\n\nMARK            = '('   # push special markobject on stack\nSTOP            = '.'   # every pickle ends with STOP\nPOP             = '0'   # discard topmost stack item\nPOP_MARK        = '1'   # discard stack top through topmost markobject\nDUP             = '2'   # duplicate top stack item\nFLOAT           = 'F'   # push float object; decimal string argument\nINT             = 'I'   # push integer or bool; decimal string argument\nBININT          = 'J'   # push four-byte signed int\nBININT1         = 'K'   # push 1-byte unsigned int\nLONG            = 'L'   # push long; decimal string argument\nBININT2         = 'M'   # push 2-byte unsigned int\nNONE            = 'N'   # push None\nPERSID          = 'P'   # push persistent object; id is taken from string arg\nBINPERSID       = 'Q'   #  \"       \"         \"  ;  \"  \"   \"     \"  stack\nREDUCE          = 'R'   # apply callable to argtuple, both on stack\nSTRING          = 'S'   # push string; NL-terminated string argument\nBINSTRING       = 'T'   # push string; counted binary string argument\nSHORT_BINSTRING = 'U'   #  \"     \"   ;    \"      \"       \"      \" < 256 bytes\nUNICODE         = 'V'   # push Unicode string; raw-unicode-escaped'd argument\nBINUNICODE      = 'X'   #   \"     \"       \"  ; counted UTF-8 string argument\nAPPEND          = 'a'   # append stack top to list below it\nBUILD           = 'b'   # call __setstate__ or __dict__.update()\nGLOBAL          = 'c'   # push self.find_class(modname, name); 2 string args\nDICT            = 'd'   # build a dict from stack items\nEMPTY_DICT      = '}'   # push empty dict\nAPPENDS         = 'e'   # extend list on stack by topmost stack slice\nGET             = 'g'   # push item from memo on stack; index is string arg\nBINGET          = 'h'   #   \"    \"    \"    \"   \"   \"  ;   \"    \" 1-byte arg\nINST            = 'i'   # build & push class instance\nLONG_BINGET     = 'j'   # push item from memo on stack; index is 4-byte arg\nLIST            = 'l'   # build list from topmost stack items\nEMPTY_LIST      = ']'   # push empty list\nOBJ             = 'o'   # build & push class instance\nPUT             = 'p'   # store stack top in memo; index is string arg\nBINPUT          = 'q'   #   \"     \"    \"   \"   \" ;   \"    \" 1-byte arg\nLONG_BINPUT     = 'r'   #   \"     \"    \"   \"   \" ;   \"    \" 4-byte arg\nSETITEM         = 's'   # add key+value pair to dict\nTUPLE           = 't'   # build tuple from topmost stack items\nEMPTY_TUPLE     = ')'   # push empty tuple\nSETITEMS        = 'u'   # modify dict by adding topmost key+value pairs\nBINFLOAT        = 'G'   # push float; arg is 8-byte float encoding\n\nTRUE            = 'I01\\n'  # not an opcode; see INT docs in pickletools.py\nFALSE           = 'I00\\n'  # not an opcode; see INT docs in pickletools.py\n\n# Protocol 2\n\nPROTO           = '\\x80'  # identify pickle protocol\nNEWOBJ          = '\\x81'  # build object by applying cls.__new__ to argtuple\nEXT1            = '\\x82'  # push object from extension registry; 1-byte index\nEXT2            = '\\x83'  # ditto, but 2-byte index\nEXT4            = '\\x84'  # ditto, but 4-byte index\nTUPLE1          = '\\x85'  # build 1-tuple from stack top\nTUPLE2          = '\\x86'  # build 2-tuple from two topmost stack items\nTUPLE3          = '\\x87'  # build 3-tuple from three topmost stack items\nNEWTRUE         = '\\x88'  # push True\nNEWFALSE        = '\\x89'  # push False\nLONG1           = '\\x8a'  # push long from < 256 bytes\nLONG4           = '\\x8b'  # push really big long\n\n_tuplesize2code = [EMPTY_TUPLE, TUPLE1, TUPLE2, TUPLE3]\n\n\n__all__.extend([x for x in dir() if re.match(\"[A-Z][A-Z0-9_]+$\",x)])\ndel x\n\n\n# Pickling machinery\n\nclass Pickler(object):\n\n    def __init__(self, file, protocol=None):\n        \"\"\"This takes a file-like object for writing a pickle data stream.\n\n        The optional protocol argument tells the pickler to use the\n        given protocol; supported protocols are 0, 1, 2.  The default\n        protocol is 0, to be backwards compatible.  (Protocol 0 is the\n        only protocol that can be written to a file opened in text\n        mode and read back successfully.  When using a protocol higher\n        than 0, make sure the file is opened in binary mode, both when\n        pickling and unpickling.)\n\n        Protocol 1 is more efficient than protocol 0; protocol 2 is\n        more efficient than protocol 1.\n\n        Specifying a negative protocol version selects the highest\n        protocol version supported.  The higher the protocol used, the\n        more recent the version of Python needed to read the pickle\n        produced.\n\n        The file parameter must have a write() method that accepts a single\n        string argument.  It can thus be an open file object, a StringIO\n        object, or any other custom object that meets this interface.\n\n        \"\"\"\n        if protocol is None:\n            protocol = 0\n        if protocol < 0:\n            protocol = HIGHEST_PROTOCOL\n        elif not 0 <= protocol <= HIGHEST_PROTOCOL:\n            raise ValueError(\"pickle protocol must be <= %d\" % HIGHEST_PROTOCOL)\n        self.write = file.write\n        self.memo = {}\n        self.proto = int(protocol)\n        self.bin = protocol >= 1\n        self.fast = 0\n\n    def clear_memo(self):\n        \"\"\"Clears the pickler's \"memo\".\n\n        The memo is the data structure that remembers which objects the\n        pickler has already seen, so that shared or recursive objects are\n        pickled by reference and not by value.  This method is useful when\n        re-using picklers.\n\n        \"\"\"\n        self.memo.clear()\n\n    def dump(self, obj):\n        \"\"\"Write a pickled representation of obj to the open file.\"\"\"\n        if self.proto >= 2:\n            self.write(PROTO + chr(self.proto))\n        self.save(obj)\n        self.write(STOP)\n\n    def memoize(self, obj):\n        \"\"\"Store an object in the memo.\"\"\"\n\n        # The Pickler memo is a dictionary mapping object ids to 2-tuples\n        # that contain the Unpickler memo key and the object being memoized.\n        # The memo key is written to the pickle and will become\n        # the key in the Unpickler's memo.  The object is stored in the\n        # Pickler memo so that transient objects are kept alive during\n        # pickling.\n\n        # The use of the Unpickler memo length as the memo key is just a\n        # convention.  The only requirement is that the memo values be unique.\n        # But there appears no advantage to any other scheme, and this\n        # scheme allows the Unpickler memo to be implemented as a plain (but\n        # growable) array, indexed by memo key.\n        if self.fast:\n            return\n        assert id(obj) not in self.memo\n        memo_len = len(self.memo)\n        self.write(self.put(memo_len))\n        self.memo[id(obj)] = memo_len, obj\n\n    # Return a PUT (BINPUT, LONG_BINPUT) opcode string, with argument i.\n    def put(self, i, pack=struct.pack):\n        if self.bin:\n            if i < 256:\n                return BINPUT + chr(i)\n            else:\n                return LONG_BINPUT + pack(\"<i\", i)\n\n        return PUT + repr(i) + '\\n'\n\n    # Return a GET (BINGET, LONG_BINGET) opcode string, with argument i.\n    def get(self, i, pack=struct.pack):\n        if self.bin:\n            if i < 256:\n                return BINGET + chr(i)\n            else:\n                return LONG_BINGET + pack(\"<i\", i)\n\n        return GET + repr(i) + '\\n'\n\n    def save(self, obj):\n        # Check for persistent id (defined by a subclass)\n        pid = self.persistent_id(obj)\n        if pid is not None:\n            self.save_pers(pid)\n            return\n\n        # Check the memo\n        x = self.memo.get(id(obj))\n        if x:\n            self.write(self.get(x[0]))\n            return\n\n        # Check the type dispatch table\n        t = type(obj)\n        f = self.dispatch.get(t)\n        if f:\n            f(self, obj) # Call unbound method with explicit self\n            return\n\n        # Check copy_reg.dispatch_table\n        reduce = dispatch_table.get(t)\n        if reduce:\n            rv = reduce(obj)\n        else:\n            # Check for a class with a custom metaclass; treat as regular class\n            try:\n                issc = issubclass(t, TypeType)\n            except TypeError: # t is not a class (old Boost; see SF #502085)\n                issc = 0\n            if issc:\n                self.save_global(obj)\n                return\n\n            # Check for a __reduce_ex__ method, fall back to __reduce__\n            reduce = getattr(obj, \"__reduce_ex__\", None)\n            if reduce:\n                rv = reduce(self.proto)\n            else:\n                reduce = getattr(obj, \"__reduce__\", None)\n                if reduce:\n                    rv = reduce()\n                else:\n                    raise PicklingError(\"Can't pickle %r object: %r\" %\n                                        (t.__name__, obj))\n\n        # Check for string returned by reduce(), meaning \"save as global\"\n        if type(rv) is StringType:\n            self.save_global(obj, rv)\n            return\n\n        # Assert that reduce() returned a tuple\n        if type(rv) is not TupleType:\n            raise PicklingError(\"%s must return string or tuple\" % reduce)\n\n        # Assert that it returned an appropriately sized tuple\n        l = len(rv)\n        if not (2 <= l <= 5):\n            raise PicklingError(\"Tuple returned by %s must have \"\n                                \"two to five elements\" % reduce)\n\n        # Save the reduce() output and finally memoize the object\n        self.save_reduce(obj=obj, *rv)\n\n    def persistent_id(self, obj):\n        # This exists so a subclass can override it\n        return None\n\n    def save_pers(self, pid):\n        # Save a persistent id reference\n        if self.bin:\n            self.save(pid)\n            self.write(BINPERSID)\n        else:\n            self.write(PERSID + str(pid) + '\\n')\n\n    def save_reduce(self, func, args, state=None,\n                    listitems=None, dictitems=None, obj=None):\n        # This API is called by some subclasses\n\n        # Assert that args is a tuple or None\n        if not isinstance(args, TupleType):\n            raise PicklingError(\"args from reduce() should be a tuple\")\n\n        # Assert that func is callable\n        if not hasattr(func, '__call__'):\n            raise PicklingError(\"func from reduce should be callable\")\n\n        save = self.save\n        write = self.write\n\n        # Protocol 2 special case: if func's name is __newobj__, use NEWOBJ\n        if self.proto >= 2 and getattr(func, \"__name__\", \"\") == \"__newobj__\":\n            # A __reduce__ implementation can direct protocol 2 to\n            # use the more efficient NEWOBJ opcode, while still\n            # allowing protocol 0 and 1 to work normally.  For this to\n            # work, the function returned by __reduce__ should be\n            # called __newobj__, and its first argument should be a\n            # new-style class.  The implementation for __newobj__\n            # should be as follows, although pickle has no way to\n            # verify this:\n            #\n            # def __newobj__(cls, *args):\n            #     return cls.__new__(cls, *args)\n            #\n            # Protocols 0 and 1 will pickle a reference to __newobj__,\n            # while protocol 2 (and above) will pickle a reference to\n            # cls, the remaining args tuple, and the NEWOBJ code,\n            # which calls cls.__new__(cls, *args) at unpickling time\n            # (see load_newobj below).  If __reduce__ returns a\n            # three-tuple, the state from the third tuple item will be\n            # pickled regardless of the protocol, calling __setstate__\n            # at unpickling time (see load_build below).\n            #\n            # Note that no standard __newobj__ implementation exists;\n            # you have to provide your own.  This is to enforce\n            # compatibility with Python 2.2 (pickles written using\n            # protocol 0 or 1 in Python 2.3 should be unpicklable by\n            # Python 2.2).\n            cls = args[0]\n            if not hasattr(cls, \"__new__\"):\n                raise PicklingError(\n                    \"args[0] from __newobj__ args has no __new__\")\n            if obj is not None and cls is not obj.__class__:\n                raise PicklingError(\n                    \"args[0] from __newobj__ args has the wrong class\")\n            args = args[1:]\n            save(cls)\n            save(args)\n            write(NEWOBJ)\n        else:\n            save(func)\n            save(args)\n            write(REDUCE)\n\n        if obj is not None:\n            self.memoize(obj)\n\n        # More new special cases (that work with older protocols as\n        # well): when __reduce__ returns a tuple with 4 or 5 items,\n        # the 4th and 5th item should be iterators that provide list\n        # items and dict items (as (key, value) tuples), or None.\n\n        if listitems is not None:\n            self._batch_appends(listitems)\n\n        if dictitems is not None:\n            self._batch_setitems(dictitems)\n\n        if state is not None:\n            save(state)\n            write(BUILD)\n\n    # Methods below this point are dispatched through the dispatch table\n\n    dispatch = {}\n\n    def save_none(self, obj):\n        self.write(NONE)\n    dispatch[NoneType] = save_none\n\n    def save_bool(self, obj):\n        if self.proto >= 2:\n            self.write(obj and NEWTRUE or NEWFALSE)\n        else:\n            self.write(obj and TRUE or FALSE)\n    dispatch[bool] = save_bool\n\n    def save_int(self, obj, pack=struct.pack):\n        if self.bin:\n            # If the int is small enough to fit in a signed 4-byte 2's-comp\n            # format, we can store it more efficiently than the general\n            # case.\n            # First one- and two-byte unsigned ints:\n            if obj >= 0:\n                if obj <= 0xff:\n                    self.write(BININT1 + chr(obj))\n                    return\n                if obj <= 0xffff:\n                    self.write(\"%c%c%c\" % (BININT2, obj&0xff, obj>>8))\n                    return\n            # Next check for 4-byte signed ints:\n            high_bits = obj >> 31  # note that Python shift sign-extends\n            if high_bits == 0 or high_bits == -1:\n                # All high bits are copies of bit 2**31, so the value\n                # fits in a 4-byte signed int.\n                self.write(BININT + pack(\"<i\", obj))\n                return\n        # Text pickle, or int too big to fit in signed 4-byte format.\n        self.write(INT + repr(obj) + '\\n')\n    dispatch[IntType] = save_int\n\n    def save_long(self, obj, pack=struct.pack):\n        if self.proto >= 2:\n            bytes = encode_long(obj)\n            n = len(bytes)\n            if n < 256:\n                self.write(LONG1 + chr(n) + bytes)\n            else:\n                self.write(LONG4 + pack(\"<i\", n) + bytes)\n            return\n        self.write(LONG + repr(obj) + '\\n')\n    dispatch[LongType] = save_long\n\n    def save_float(self, obj, pack=struct.pack):\n        if self.bin:\n            self.write(BINFLOAT + pack('>d', obj))\n        else:\n            self.write(FLOAT + repr(obj) + '\\n')\n    dispatch[FloatType] = save_float\n\n    def save_string(self, obj, pack=struct.pack):\n        if self.bin:\n            n = len(obj)\n            if n < 256:\n                self.write(SHORT_BINSTRING + chr(n) + obj)\n            else:\n                self.write(BINSTRING + pack(\"<i\", n) + obj)\n        else:\n            self.write(STRING + repr(obj) + '\\n')\n        self.memoize(obj)\n    dispatch[StringType] = save_string\n\n    def save_unicode(self, obj, pack=struct.pack):\n        if self.bin:\n            encoding = obj.encode('utf-8')\n            n = len(encoding)\n            self.write(BINUNICODE + pack(\"<i\", n) + encoding)\n        else:\n            obj = obj.replace(\"\\\\\", \"\\\\u005c\")\n            obj = obj.replace(\"\\n\", \"\\\\u000a\")\n            self.write(UNICODE + obj.encode('raw-unicode-escape') + '\\n')\n        self.memoize(obj)\n    dispatch[UnicodeType] = save_unicode\n\n    if StringType is UnicodeType:\n        # This is true for Jython\n        def save_string(self, obj, pack=struct.pack):\n            unicode = obj.isunicode()\n\n            if self.bin:\n                if unicode:\n                    obj = obj.encode(\"utf-8\")\n                l = len(obj)\n                if l < 256 and not unicode:\n                    self.write(SHORT_BINSTRING + chr(l) + obj)\n                else:\n                    s = pack(\"<i\", l)\n                    if unicode:\n                        self.write(BINUNICODE + s + obj)\n                    else:\n                        self.write(BINSTRING + s + obj)\n            else:\n                if unicode:\n                    obj = obj.replace(\"\\\\\", \"\\\\u005c\")\n                    obj = obj.replace(\"\\n\", \"\\\\u000a\")\n                    obj = obj.encode('raw-unicode-escape')\n                    self.write(UNICODE + obj + '\\n')\n                else:\n                    self.write(STRING + repr(obj) + '\\n')\n            self.memoize(obj)\n        dispatch[StringType] = save_string\n\n    def save_tuple(self, obj):\n        write = self.write\n        proto = self.proto\n\n        n = len(obj)\n        if n == 0:\n            if proto:\n                write(EMPTY_TUPLE)\n            else:\n                write(MARK + TUPLE)\n            return\n\n        save = self.save\n        memo = self.memo\n        if n <= 3 and proto >= 2:\n            for element in obj:\n                save(element)\n            # Subtle.  Same as in the big comment below.\n            if id(obj) in memo:\n                get = self.get(memo[id(obj)][0])\n                write(POP * n + get)\n            else:\n                write(_tuplesize2code[n])\n                self.memoize(obj)\n            return\n\n        # proto 0 or proto 1 and tuple isn't empty, or proto > 1 and tuple\n        # has more than 3 elements.\n        write(MARK)\n        for element in obj:\n            save(element)\n\n        if id(obj) in memo:\n            # Subtle.  d was not in memo when we entered save_tuple(), so\n            # the process of saving the tuple's elements must have saved\n            # the tuple itself:  the tuple is recursive.  The proper action\n            # now is to throw away everything we put on the stack, and\n            # simply GET the tuple (it's already constructed).  This check\n            # could have been done in the \"for element\" loop instead, but\n            # recursive tuples are a rare thing.\n            get = self.get(memo[id(obj)][0])\n            if proto:\n                write(POP_MARK + get)\n            else:   # proto 0 -- POP_MARK not available\n                write(POP * (n+1) + get)\n            return\n\n        # No recursion.\n        self.write(TUPLE)\n        self.memoize(obj)\n\n    dispatch[TupleType] = save_tuple\n\n    # save_empty_tuple() isn't used by anything in Python 2.3.  However, I\n    # found a Pickler subclass in Zope3 that calls it, so it's not harmless\n    # to remove it.\n    def save_empty_tuple(self, obj):\n        self.write(EMPTY_TUPLE)\n\n    def save_list(self, obj):\n        write = self.write\n\n        if self.bin:\n            write(EMPTY_LIST)\n        else:   # proto 0 -- can't use EMPTY_LIST\n            write(MARK + LIST)\n\n        self.memoize(obj)\n        self._batch_appends(iter(obj))\n\n    dispatch[ListType] = save_list\n\n    # Keep in synch with cPickle's BATCHSIZE.  Nothing will break if it gets\n    # out of synch, though.\n    _BATCHSIZE = 1000\n\n    def _batch_appends(self, items):\n        # Helper to batch up APPENDS sequences\n        save = self.save\n        write = self.write\n\n        if not self.bin:\n            for x in items:\n                save(x)\n                write(APPEND)\n            return\n\n        r = xrange(self._BATCHSIZE)\n        while items is not None:\n            tmp = []\n            for i in r:\n                try:\n                    x = items.next()\n                    tmp.append(x)\n                except StopIteration:\n                    items = None\n                    break\n            n = len(tmp)\n            if n > 1:\n                write(MARK)\n                for x in tmp:\n                    save(x)\n                write(APPENDS)\n            elif n:\n                save(tmp[0])\n                write(APPEND)\n            # else tmp is empty, and we're done\n\n    def save_dict(self, obj):\n        modict_saver = self._pickle_maybe_moduledict(obj)\n        if modict_saver is not None:\n            return self.save_reduce(*modict_saver)\n\n        write = self.write\n\n        if self.bin:\n            write(EMPTY_DICT)\n        else:   # proto 0 -- can't use EMPTY_DICT\n            write(MARK + DICT)\n\n        self.memoize(obj)\n        self._batch_setitems(obj.iteritems())\n\n    dispatch[DictionaryType] = save_dict\n    if not PyStringMap is None:\n        dispatch[PyStringMap] = save_dict\n\n    def _batch_setitems(self, items):\n        # Helper to batch up SETITEMS sequences; proto >= 1 only\n        save = self.save\n        write = self.write\n\n        if not self.bin:\n            for k, v in items:\n                save(k)\n                save(v)\n                write(SETITEM)\n            return\n\n        r = xrange(self._BATCHSIZE)\n        while items is not None:\n            tmp = []\n            for i in r:\n                try:\n                    tmp.append(items.next())\n                except StopIteration:\n                    items = None\n                    break\n            n = len(tmp)\n            if n > 1:\n                write(MARK)\n                for k, v in tmp:\n                    save(k)\n                    save(v)\n                write(SETITEMS)\n            elif n:\n                k, v = tmp[0]\n                save(k)\n                save(v)\n                write(SETITEM)\n            # else tmp is empty, and we're done\n\n    def _pickle_maybe_moduledict(self, obj):\n        # save module dictionary as \"getattr(module, '__dict__')\"\n        try:\n            name = obj['__name__']\n            if type(name) is not str:\n                return None\n            themodule = sys.modules[name]\n            if type(themodule) is not ModuleType:\n                return None\n            if themodule.__dict__ is not obj:\n                return None\n        except (AttributeError, KeyError, TypeError):\n            return None\n\n        return getattr, (themodule, '__dict__')\n\n\n    def save_inst(self, obj):\n        cls = obj.__class__\n\n        memo  = self.memo\n        write = self.write\n        save  = self.save\n\n        if hasattr(obj, '__getinitargs__'):\n            args = obj.__getinitargs__()\n            len(args) # XXX Assert it's a sequence\n            _keep_alive(args, memo)\n        else:\n            args = ()\n\n        write(MARK)\n\n        if self.bin:\n            save(cls)\n            for arg in args:\n                save(arg)\n            write(OBJ)\n        else:\n            for arg in args:\n                save(arg)\n            write(INST + cls.__module__ + '\\n' + cls.__name__ + '\\n')\n\n        self.memoize(obj)\n\n        try:\n            getstate = obj.__getstate__\n        except AttributeError:\n            stuff = obj.__dict__\n        else:\n            stuff = getstate()\n            _keep_alive(stuff, memo)\n        save(stuff)\n        write(BUILD)\n\n    dispatch[InstanceType] = save_inst\n\n    def save_function(self, obj):\n        try:\n            return self.save_global(obj)\n        except PicklingError, e:\n            pass\n        # Check copy_reg.dispatch_table\n        reduce = dispatch_table.get(type(obj))\n        if reduce:\n            rv = reduce(obj)\n        else:\n            # Check for a __reduce_ex__ method, fall back to __reduce__\n            reduce = getattr(obj, \"__reduce_ex__\", None)\n            if reduce:\n                rv = reduce(self.proto)\n            else:\n                reduce = getattr(obj, \"__reduce__\", None)\n                if reduce:\n                    rv = reduce()\n                else:\n                    raise e\n        return self.save_reduce(obj=obj, *rv)\n    dispatch[FunctionType] = save_function\n\n    def save_global(self, obj, name=None, pack=struct.pack):\n        write = self.write\n        memo = self.memo\n\n        if name is None:\n            name = obj.__name__\n\n        module = getattr(obj, \"__module__\", None)\n        if module is None:\n            module = whichmodule(obj, name)\n\n        try:\n            __import__(module)\n            mod = sys.modules[module]\n            klass = getattr(mod, name)\n        except (ImportError, KeyError, AttributeError):\n            raise PicklingError(\n                \"Can't pickle %r: it's not found as %s.%s\" %\n                (obj, module, name))\n        else:\n            if klass is not obj:\n                raise PicklingError(\n                    \"Can't pickle %r: it's not the same object as %s.%s\" %\n                    (obj, module, name))\n\n        if self.proto >= 2:\n            code = _extension_registry.get((module, name))\n            if code:\n                assert code > 0\n                if code <= 0xff:\n                    write(EXT1 + chr(code))\n                elif code <= 0xffff:\n                    write(\"%c%c%c\" % (EXT2, code&0xff, code>>8))\n                else:\n                    write(EXT4 + pack(\"<i\", code))\n                return\n\n        write(GLOBAL + module + '\\n' + name + '\\n')\n        self.memoize(obj)\n\n    dispatch[ClassType] = save_global\n    dispatch[BuiltinFunctionType] = save_global\n    dispatch[TypeType] = save_global\n\n# Pickling helpers\n\ndef _keep_alive(x, memo):\n    \"\"\"Keeps a reference to the object x in the memo.\n\n    Because we remember objects by their id, we have\n    to assure that possibly temporary objects are kept\n    alive by referencing them.\n    We store a reference at the id of the memo, which should\n    normally not be used unless someone tries to deepcopy\n    the memo itself...\n    \"\"\"\n    try:\n        memo[id(memo)].append(x)\n    except KeyError:\n        # aha, this is the first one :-)\n        memo[id(memo)]=[x]\n\n\n# A cache for whichmodule(), mapping a function object to the name of\n# the module in which the function was found.\n\nclassmap = {} # called classmap for backwards compatibility\n\ndef whichmodule(func, funcname):\n    \"\"\"Figure out the module in which a function occurs.\n\n    Search sys.modules for the module.\n    Cache in classmap.\n    Return a module name.\n    If the function cannot be found, return \"__main__\".\n    \"\"\"\n    # Python functions should always get an __module__ from their globals.\n    mod = getattr(func, \"__module__\", None)\n    if mod is not None:\n        return mod\n    if func in classmap:\n        return classmap[func]\n\n    for name, module in sys.modules.items():\n        if module is None:\n            continue # skip dummy package entries\n        if name != '__main__' and getattr(module, funcname, None) is func:\n            break\n    else:\n        name = '__main__'\n    classmap[func] = name\n    return name\n\n\n# Unpickling machinery\n\nclass Unpickler(object):\n\n    def __init__(self, file):\n        \"\"\"This takes a file-like object for reading a pickle data stream.\n\n        The protocol version of the pickle is detected automatically, so no\n        proto argument is needed.\n\n        The file-like object must have two methods, a read() method that\n        takes an integer argument, and a readline() method that requires no\n        arguments.  Both methods should return a string.  Thus file-like\n        object can be a file object opened for reading, a StringIO object,\n        or any other custom object that meets this interface.\n        \"\"\"\n        self.readline = file.readline\n        self.read = file.read\n        self.memo = {}\n\n    def load(self):\n        \"\"\"Read a pickled object representation from the open file.\n\n        Return the reconstituted object hierarchy specified in the file.\n        \"\"\"\n        self.mark = object() # any new unique object\n        self.stack = []\n        self.append = self.stack.append\n        read = self.read\n        dispatch = self.dispatch\n        try:\n            while 1:\n                key = read(1)\n                dispatch[key](self)\n        except _Stop, stopinst:\n            return stopinst.value\n\n    # Return largest index k such that self.stack[k] is self.mark.\n    # If the stack doesn't contain a mark, eventually raises IndexError.\n    # This could be sped by maintaining another stack, of indices at which\n    # the mark appears.  For that matter, the latter stack would suffice,\n    # and we wouldn't need to push mark objects on self.stack at all.\n    # Doing so is probably a good thing, though, since if the pickle is\n    # corrupt (or hostile) we may get a clue from finding self.mark embedded\n    # in unpickled objects.\n    def marker(self):\n        stack = self.stack\n        mark = self.mark\n        k = len(stack)-1\n        while stack[k] is not mark: k = k-1\n        return k\n\n    dispatch = {}\n\n    def load_eof(self):\n        raise EOFError\n    dispatch[''] = load_eof\n\n    def load_proto(self):\n        proto = ord(self.read(1))\n        if not 0 <= proto <= 2:\n            raise ValueError, \"unsupported pickle protocol: %d\" % proto\n    dispatch[PROTO] = load_proto\n\n    def load_persid(self):\n        pid = self.readline()[:-1]\n        self.append(self.persistent_load(pid))\n    dispatch[PERSID] = load_persid\n\n    def load_binpersid(self):\n        pid = self.stack.pop()\n        self.append(self.persistent_load(pid))\n    dispatch[BINPERSID] = load_binpersid\n\n    def load_none(self):\n        self.append(None)\n    dispatch[NONE] = load_none\n\n    def load_false(self):\n        self.append(False)\n    dispatch[NEWFALSE] = load_false\n\n    def load_true(self):\n        self.append(True)\n    dispatch[NEWTRUE] = load_true\n\n    def load_int(self):\n        data = self.readline()\n        if data == FALSE[1:]:\n            val = False\n        elif data == TRUE[1:]:\n            val = True\n        else:\n            try:\n                val = int(data)\n            except ValueError:\n                val = long(data)\n        self.append(val)\n    dispatch[INT] = load_int\n\n    def load_binint(self):\n        self.append(mloads('i' + self.read(4)))\n    dispatch[BININT] = load_binint\n\n    def load_binint1(self):\n        self.append(ord(self.read(1)))\n    dispatch[BININT1] = load_binint1\n\n    def load_binint2(self):\n        self.append(mloads('i' + self.read(2) + '\\000\\000'))\n    dispatch[BININT2] = load_binint2\n\n    def load_long(self):\n        self.append(long(self.readline()[:-1], 0))\n    dispatch[LONG] = load_long\n\n    def load_long1(self):\n        n = ord(self.read(1))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG1] = load_long1\n\n    def load_long4(self):\n        n = mloads('i' + self.read(4))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG4] = load_long4\n\n    def load_float(self):\n        self.append(float(self.readline()[:-1]))\n    dispatch[FLOAT] = load_float\n\n    def load_binfloat(self, unpack=struct.unpack):\n        self.append(unpack('>d', self.read(8))[0])\n    dispatch[BINFLOAT] = load_binfloat\n\n    def load_string(self):\n        rep = self.readline()[:-1]\n        for q in \"\\\"'\": # double or single quote\n            if rep.startswith(q):\n                if len(rep) < 2 or not rep.endswith(q):\n                    raise ValueError, \"insecure string pickle\"\n                rep = rep[len(q):-len(q)]\n                break\n        else:\n            raise ValueError, \"insecure string pickle\"\n        self.append(rep.decode(\"string-escape\"))\n    dispatch[STRING] = load_string\n\n    def load_binstring(self):\n        len = mloads('i' + self.read(4))\n        self.append(self.read(len))\n    dispatch[BINSTRING] = load_binstring\n\n    def load_unicode(self):\n        self.append(unicode(self.readline()[:-1],'raw-unicode-escape'))\n    dispatch[UNICODE] = load_unicode\n\n    def load_binunicode(self):\n        len = mloads('i' + self.read(4))\n        self.append(unicode(self.read(len),'utf-8'))\n    dispatch[BINUNICODE] = load_binunicode\n\n    def load_short_binstring(self):\n        len = ord(self.read(1))\n        self.append(self.read(len))\n    dispatch[SHORT_BINSTRING] = load_short_binstring\n\n    def load_tuple(self):\n        k = self.marker()\n        self.stack[k:] = [tuple(self.stack[k+1:])]\n    dispatch[TUPLE] = load_tuple\n\n    def load_empty_tuple(self):\n        self.stack.append(())\n    dispatch[EMPTY_TUPLE] = load_empty_tuple\n\n    def load_tuple1(self):\n        self.stack[-1] = (self.stack[-1],)\n    dispatch[TUPLE1] = load_tuple1\n\n    def load_tuple2(self):\n        self.stack[-2:] = [(self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE2] = load_tuple2\n\n    def load_tuple3(self):\n        self.stack[-3:] = [(self.stack[-3], self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE3] = load_tuple3\n\n    def load_empty_list(self):\n        self.stack.append([])\n    dispatch[EMPTY_LIST] = load_empty_list\n\n    def load_empty_dictionary(self):\n        self.stack.append({})\n    dispatch[EMPTY_DICT] = load_empty_dictionary\n\n    def load_list(self):\n        k = self.marker()\n        self.stack[k:] = [self.stack[k+1:]]\n    dispatch[LIST] = load_list\n\n    def load_dict(self):\n        k = self.marker()\n        d = {}\n        items = self.stack[k+1:]\n        for i in range(0, len(items), 2):\n            key = items[i]\n            value = items[i+1]\n            d[key] = value\n        self.stack[k:] = [d]\n    dispatch[DICT] = load_dict\n\n    # INST and OBJ differ only in how they get a class object.  It's not\n    # only sensible to do the rest in a common routine, the two routines\n    # previously diverged and grew different bugs.\n    # klass is the class to instantiate, and k points to the topmost mark\n    # object, following which are the arguments for klass.__init__.\n    def _instantiate(self, klass, k):\n        args = tuple(self.stack[k+1:])\n        del self.stack[k:]\n        instantiated = 0\n        if (not args and\n                type(klass) is ClassType and\n                not hasattr(klass, \"__getinitargs__\")):\n            try:\n                value = _EmptyClass()\n                value.__class__ = klass\n                instantiated = 1\n            except RuntimeError:\n                # In restricted execution, assignment to inst.__class__ is\n                # prohibited\n                pass\n        if not instantiated:\n            try:\n                value = klass(*args)\n            except TypeError, err:\n                raise TypeError, \"in constructor for %s: %s\" % (\n                    klass.__name__, str(err)), sys.exc_info()[2]\n        self.append(value)\n\n    def load_inst(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self._instantiate(klass, self.marker())\n    dispatch[INST] = load_inst\n\n    def load_obj(self):\n        # Stack is ... markobject classobject arg1 arg2 ...\n        k = self.marker()\n        klass = self.stack.pop(k+1)\n        self._instantiate(klass, k)\n    dispatch[OBJ] = load_obj\n\n    def load_newobj(self):\n        args = self.stack.pop()\n        cls = self.stack[-1]\n        obj = cls.__new__(cls, *args)\n        self.stack[-1] = obj\n    dispatch[NEWOBJ] = load_newobj\n\n    def load_global(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self.append(klass)\n    dispatch[GLOBAL] = load_global\n\n    def load_ext1(self):\n        code = ord(self.read(1))\n        self.get_extension(code)\n    dispatch[EXT1] = load_ext1\n\n    def load_ext2(self):\n        code = mloads('i' + self.read(2) + '\\000\\000')\n        self.get_extension(code)\n    dispatch[EXT2] = load_ext2\n\n    def load_ext4(self):\n        code = mloads('i' + self.read(4))\n        self.get_extension(code)\n    dispatch[EXT4] = load_ext4\n\n    def get_extension(self, code):\n        nil = []\n        obj = _extension_cache.get(code, nil)\n        if obj is not nil:\n            self.append(obj)\n            return\n        key = _inverted_registry.get(code)\n        if not key:\n            raise ValueError(\"unregistered extension code %d\" % code)\n        obj = self.find_class(*key)\n        _extension_cache[code] = obj\n        self.append(obj)\n\n    def find_class(self, module, name):\n        # Subclasses may override this\n        __import__(module)\n        mod = sys.modules[module]\n        klass = getattr(mod, name)\n        return klass\n\n    def load_reduce(self):\n        stack = self.stack\n        args = stack.pop()\n        func = stack[-1]\n        value = func(*args)\n        stack[-1] = value\n    dispatch[REDUCE] = load_reduce\n\n    def load_pop(self):\n        del self.stack[-1]\n    dispatch[POP] = load_pop\n\n    def load_pop_mark(self):\n        k = self.marker()\n        del self.stack[k:]\n    dispatch[POP_MARK] = load_pop_mark\n\n    def load_dup(self):\n        self.append(self.stack[-1])\n    dispatch[DUP] = load_dup\n\n    def load_get(self):\n        self.append(self.memo[self.readline()[:-1]])\n    dispatch[GET] = load_get\n\n    def load_binget(self):\n        i = ord(self.read(1))\n        self.append(self.memo[repr(i)])\n    dispatch[BINGET] = load_binget\n\n    def load_long_binget(self):\n        i = mloads('i' + self.read(4))\n        self.append(self.memo[repr(i)])\n    dispatch[LONG_BINGET] = load_long_binget\n\n    def load_put(self):\n        self.memo[self.readline()[:-1]] = self.stack[-1]\n    dispatch[PUT] = load_put\n\n    def load_binput(self):\n        i = ord(self.read(1))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[BINPUT] = load_binput\n\n    def load_long_binput(self):\n        i = mloads('i' + self.read(4))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[LONG_BINPUT] = load_long_binput\n\n    def load_append(self):\n        stack = self.stack\n        value = stack.pop()\n        list = stack[-1]\n        list.append(value)\n    dispatch[APPEND] = load_append\n\n    def load_appends(self):\n        stack = self.stack\n        mark = self.marker()\n        list = stack[mark - 1]\n        list.extend(stack[mark + 1:])\n        del stack[mark:]\n    dispatch[APPENDS] = load_appends\n\n    def load_setitem(self):\n        stack = self.stack\n        value = stack.pop()\n        key = stack.pop()\n        dict = stack[-1]\n        dict[key] = value\n    dispatch[SETITEM] = load_setitem\n\n    def load_setitems(self):\n        stack = self.stack\n        mark = self.marker()\n        dict = stack[mark - 1]\n        for i in range(mark + 1, len(stack), 2):\n            dict[stack[i]] = stack[i + 1]\n\n        del stack[mark:]\n    dispatch[SETITEMS] = load_setitems\n\n    def load_build(self):\n        stack = self.stack\n        state = stack.pop()\n        inst = stack[-1]\n        setstate = getattr(inst, \"__setstate__\", None)\n        if setstate:\n            setstate(state)\n            return\n        slotstate = None\n        if isinstance(state, tuple) and len(state) == 2:\n            state, slotstate = state\n        if state:\n            try:\n                d = inst.__dict__\n                try:\n                    for k, v in state.iteritems():\n                        d[intern(k)] = v\n                # keys in state don't have to be strings\n                # don't blow up, but don't go out of our way\n                except TypeError:\n                    d.update(state)\n\n            except RuntimeError:\n                # XXX In restricted execution, the instance's __dict__\n                # is not accessible.  Use the old way of unpickling\n                # the instance variables.  This is a semantic\n                # difference when unpickling in restricted\n                # vs. unrestricted modes.\n                # Note, however, that cPickle has never tried to do the\n                # .update() business, and always uses\n                #     PyObject_SetItem(inst.__dict__, key, value) in a\n                # loop over state.items().\n                for k, v in state.items():\n                    setattr(inst, k, v)\n        if slotstate:\n            for k, v in slotstate.items():\n                setattr(inst, k, v)\n    dispatch[BUILD] = load_build\n\n    def load_mark(self):\n        self.append(self.mark)\n    dispatch[MARK] = load_mark\n\n    def load_stop(self):\n        value = self.stack.pop()\n        raise _Stop(value)\n    dispatch[STOP] = load_stop\n\n# Helper class for load_inst/load_obj\n\nclass _EmptyClass:\n    pass\n\n# Encode/decode longs in linear time.\n\nimport binascii as _binascii\n\ndef encode_long(x):\n    r\"\"\"Encode a long to a two's complement little-endian binary string.\n    Note that 0L is a special case, returning an empty string, to save a\n    byte in the LONG1 pickling context.\n\n    >>> encode_long(0L)\n    ''\n    >>> encode_long(255L)\n    '\\xff\\x00'\n    >>> encode_long(32767L)\n    '\\xff\\x7f'\n    >>> encode_long(-256L)\n    '\\x00\\xff'\n    >>> encode_long(-32768L)\n    '\\x00\\x80'\n    >>> encode_long(-128L)\n    '\\x80'\n    >>> encode_long(127L)\n    '\\x7f'\n    >>>\n    \"\"\"\n\n    if x == 0:\n        return ''\n    if x > 0:\n        ashex = hex(x)\n        assert ashex.startswith(\"0x\")\n        njunkchars = 2 + ashex.endswith('L')\n        nibbles = len(ashex) - njunkchars\n        if nibbles & 1:\n            # need an even # of nibbles for unhexlify\n            ashex = \"0x0\" + ashex[2:]\n        elif int(ashex[2], 16) >= 8:\n            # \"looks negative\", so need a byte of sign bits\n            ashex = \"0x00\" + ashex[2:]\n    else:\n        # Build the 256's-complement:  (1L << nbytes) + x.  The trick is\n        # to find the number of bytes in linear time (although that should\n        # really be a constant-time task).\n        ashex = hex(-x)\n        assert ashex.startswith(\"0x\")\n        njunkchars = 2 + ashex.endswith('L')\n        nibbles = len(ashex) - njunkchars\n        if nibbles & 1:\n            # Extend to a full byte.\n            nibbles += 1\n        nbits = nibbles * 4\n        x += 1L << nbits\n        assert x > 0\n        ashex = hex(x)\n        njunkchars = 2 + ashex.endswith('L')\n        newnibbles = len(ashex) - njunkchars\n        if newnibbles < nibbles:\n            ashex = \"0x\" + \"0\" * (nibbles - newnibbles) + ashex[2:]\n        if int(ashex[2], 16) < 8:\n            # \"looks positive\", so need a byte of sign bits\n            ashex = \"0xff\" + ashex[2:]\n\n    if ashex.endswith('L'):\n        ashex = ashex[2:-1]\n    else:\n        ashex = ashex[2:]\n    assert len(ashex) & 1 == 0, (x, ashex)\n    binary = _binascii.unhexlify(ashex)\n    return binary[::-1]\n\ndef decode_long(data):\n    r\"\"\"Decode a long from a two's complement little-endian binary string.\n\n    >>> decode_long('')\n    0L\n    >>> decode_long(\"\\xff\\x00\")\n    255L\n    >>> decode_long(\"\\xff\\x7f\")\n    32767L\n    >>> decode_long(\"\\x00\\xff\")\n    -256L\n    >>> decode_long(\"\\x00\\x80\")\n    -32768L\n    >>> decode_long(\"\\x80\")\n    -128L\n    >>> decode_long(\"\\x7f\")\n    127L\n    \"\"\"\n\n    nbytes = len(data)\n    if nbytes == 0:\n        return 0L\n    ashex = _binascii.hexlify(data[::-1])\n    n = long(ashex, 16) # quadratic time before Python 2.3; linear now\n    if data[-1] >= '\\x80':\n        n -= 1L << (nbytes * 8)\n    return n\n\n# Shorthands\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\ndef dump(obj, file, protocol=None):\n    Pickler(file, protocol).dump(obj)\n\ndef dumps(obj, protocol=None):\n    file = StringIO()\n    Pickler(file, protocol).dump(obj)\n    return file.getvalue()\n\ndef load(file):\n    return Unpickler(file).load()\n\ndef loads(str):\n    file = StringIO(str)\n    return Unpickler(file).load()\n\n# Doctest\n\ndef _test():\n    import doctest\n    return doctest.testmod()\n\nif __name__ == \"__main__\":\n    _test()\n", 
    "pkgutil": "\"\"\"Utilities to support packages.\"\"\"\n\n# NOTE: This module must remain compatible with Python 2.3, as it is shared\n# by setuptools for distribution with Python 2.3 and up.\n\nimport os\nimport sys\nimport imp\nimport os.path\nfrom types import ModuleType\n\n__all__ = [\n    'get_importer', 'iter_importers', 'get_loader', 'find_loader',\n    'walk_packages', 'iter_modules', 'get_data',\n    'ImpImporter', 'ImpLoader', 'read_code', 'extend_path',\n]\n\ndef read_code(stream):\n    # This helper is needed in order for the PEP 302 emulation to\n    # correctly handle compiled files\n    import marshal\n\n    magic = stream.read(4)\n    if magic != imp.get_magic():\n        return None\n\n    stream.read(4) # Skip timestamp\n    return marshal.load(stream)\n\n\ndef simplegeneric(func):\n    \"\"\"Make a trivial single-dispatch generic function\"\"\"\n    registry = {}\n    def wrapper(*args, **kw):\n        ob = args[0]\n        try:\n            cls = ob.__class__\n        except AttributeError:\n            cls = type(ob)\n        try:\n            mro = cls.__mro__\n        except AttributeError:\n            try:\n                class cls(cls, object):\n                    pass\n                mro = cls.__mro__[1:]\n            except TypeError:\n                mro = object,   # must be an ExtensionClass or some such  :(\n        for t in mro:\n            if t in registry:\n                return registry[t](*args, **kw)\n        else:\n            return func(*args, **kw)\n    try:\n        wrapper.__name__ = func.__name__\n    except (TypeError, AttributeError):\n        pass    # Python 2.3 doesn't allow functions to be renamed\n\n    def register(typ, func=None):\n        if func is None:\n            return lambda f: register(typ, f)\n        registry[typ] = func\n        return func\n\n    wrapper.__dict__ = func.__dict__\n    wrapper.__doc__ = func.__doc__\n    wrapper.register = register\n    return wrapper\n\n\ndef walk_packages(path=None, prefix='', onerror=None):\n    \"\"\"Yields (module_loader, name, ispkg) for all modules recursively\n    on path, or, if path is None, all accessible modules.\n\n    'path' should be either None or a list of paths to look for\n    modules in.\n\n    'prefix' is a string to output on the front of every module name\n    on output.\n\n    Note that this function must import all *packages* (NOT all\n    modules!) on the given path, in order to access the __path__\n    attribute to find submodules.\n\n    'onerror' is a function which gets called with one argument (the\n    name of the package which was being imported) if any exception\n    occurs while trying to import a package.  If no onerror function is\n    supplied, ImportErrors are caught and ignored, while all other\n    exceptions are propagated, terminating the search.\n\n    Examples:\n\n    # list all modules python can access\n    walk_packages()\n\n    # list all submodules of ctypes\n    walk_packages(ctypes.__path__, ctypes.__name__+'.')\n    \"\"\"\n\n    def seen(p, m={}):\n        if p in m:\n            return True\n        m[p] = True\n\n    for importer, name, ispkg in iter_modules(path, prefix):\n        yield importer, name, ispkg\n\n        if ispkg:\n            try:\n                __import__(name)\n            except ImportError:\n                if onerror is not None:\n                    onerror(name)\n            except Exception:\n                if onerror is not None:\n                    onerror(name)\n                else:\n                    raise\n            else:\n                path = getattr(sys.modules[name], '__path__', None) or []\n\n                # don't traverse path items we've seen before\n                path = [p for p in path if not seen(p)]\n\n                for item in walk_packages(path, name+'.', onerror):\n                    yield item\n\n\ndef iter_modules(path=None, prefix=''):\n    \"\"\"Yields (module_loader, name, ispkg) for all submodules on path,\n    or, if path is None, all top-level modules on sys.path.\n\n    'path' should be either None or a list of paths to look for\n    modules in.\n\n    'prefix' is a string to output on the front of every module name\n    on output.\n    \"\"\"\n\n    if path is None:\n        importers = iter_importers()\n    else:\n        importers = map(get_importer, path)\n\n    yielded = {}\n    for i in importers:\n        for name, ispkg in iter_importer_modules(i, prefix):\n            if name not in yielded:\n                yielded[name] = 1\n                yield i, name, ispkg\n\n\n#@simplegeneric\ndef iter_importer_modules(importer, prefix=''):\n    if not hasattr(importer, 'iter_modules'):\n        return []\n    return importer.iter_modules(prefix)\n\niter_importer_modules = simplegeneric(iter_importer_modules)\n\n\nclass ImpImporter:\n    \"\"\"PEP 302 Importer that wraps Python's \"classic\" import algorithm\n\n    ImpImporter(dirname) produces a PEP 302 importer that searches that\n    directory.  ImpImporter(None) produces a PEP 302 importer that searches\n    the current sys.path, plus any modules that are frozen or built-in.\n\n    Note that ImpImporter does not currently support being used by placement\n    on sys.meta_path.\n    \"\"\"\n\n    def __init__(self, path=None):\n        self.path = path\n\n    def find_module(self, fullname, path=None):\n        # Note: we ignore 'path' argument since it is only used via meta_path\n        subname = fullname.split(\".\")[-1]\n        if subname != fullname and self.path is None:\n            return None\n        if self.path is None:\n            path = None\n        else:\n            path = [os.path.realpath(self.path)]\n        try:\n            file, filename, etc = imp.find_module(subname, path)\n        except ImportError:\n            return None\n        return ImpLoader(fullname, file, filename, etc)\n\n    def iter_modules(self, prefix=''):\n        if self.path is None or not os.path.isdir(self.path):\n            return\n\n        yielded = {}\n        import inspect\n        try:\n            filenames = os.listdir(self.path)\n        except OSError:\n            # ignore unreadable directories like import does\n            filenames = []\n        filenames.sort()  # handle packages before same-named modules\n\n        for fn in filenames:\n            modname = inspect.getmodulename(fn)\n            if modname=='__init__' or modname in yielded:\n                continue\n\n            path = os.path.join(self.path, fn)\n            ispkg = False\n\n            if not modname and os.path.isdir(path) and '.' not in fn:\n                modname = fn\n                try:\n                    dircontents = os.listdir(path)\n                except OSError:\n                    # ignore unreadable directories like import does\n                    dircontents = []\n                for fn in dircontents:\n                    subname = inspect.getmodulename(fn)\n                    if subname=='__init__':\n                        ispkg = True\n                        break\n                else:\n                    continue    # not a package\n\n            if modname and '.' not in modname:\n                yielded[modname] = 1\n                yield prefix + modname, ispkg\n\n\nclass ImpLoader:\n    \"\"\"PEP 302 Loader that wraps Python's \"classic\" import algorithm\n    \"\"\"\n    code = source = None\n\n    def __init__(self, fullname, file, filename, etc):\n        self.file = file\n        self.filename = filename\n        self.fullname = fullname\n        self.etc = etc\n\n    def load_module(self, fullname):\n        self._reopen()\n        try:\n            mod = imp.load_module(fullname, self.file, self.filename, self.etc)\n        finally:\n            if self.file:\n                self.file.close()\n        # Note: we don't set __loader__ because we want the module to look\n        # normal; i.e. this is just a wrapper for standard import machinery\n        return mod\n\n    def get_data(self, pathname):\n        with open(pathname, \"rb\") as f:\n            return f.read()\n\n    def _reopen(self):\n        if self.file and self.file.closed:\n            mod_type = self.etc[2]\n            if mod_type==imp.PY_SOURCE:\n                self.file = open(self.filename, 'rU')\n            elif mod_type in (imp.PY_COMPILED, imp.C_EXTENSION):\n                self.file = open(self.filename, 'rb')\n\n    def _fix_name(self, fullname):\n        if fullname is None:\n            fullname = self.fullname\n        elif fullname != self.fullname:\n            raise ImportError(\"Loader for module %s cannot handle \"\n                              \"module %s\" % (self.fullname, fullname))\n        return fullname\n\n    def is_package(self, fullname):\n        fullname = self._fix_name(fullname)\n        return self.etc[2]==imp.PKG_DIRECTORY\n\n    def get_code(self, fullname=None):\n        fullname = self._fix_name(fullname)\n        if self.code is None:\n            mod_type = self.etc[2]\n            if mod_type==imp.PY_SOURCE:\n                source = self.get_source(fullname)\n                self.code = compile(source, self.filename, 'exec')\n            elif mod_type==imp.PY_COMPILED:\n                self._reopen()\n                try:\n                    self.code = read_code(self.file)\n                finally:\n                    self.file.close()\n            elif mod_type==imp.PKG_DIRECTORY:\n                self.code = self._get_delegate().get_code()\n        return self.code\n\n    def get_source(self, fullname=None):\n        fullname = self._fix_name(fullname)\n        if self.source is None:\n            mod_type = self.etc[2]\n            if mod_type==imp.PY_SOURCE:\n                self._reopen()\n                try:\n                    self.source = self.file.read()\n                finally:\n                    self.file.close()\n            elif mod_type==imp.PY_COMPILED:\n                if os.path.exists(self.filename[:-1]):\n                    f = open(self.filename[:-1], 'rU')\n                    self.source = f.read()\n                    f.close()\n            elif mod_type==imp.PKG_DIRECTORY:\n                self.source = self._get_delegate().get_source()\n        return self.source\n\n\n    def _get_delegate(self):\n        return ImpImporter(self.filename).find_module('__init__')\n\n    def get_filename(self, fullname=None):\n        fullname = self._fix_name(fullname)\n        mod_type = self.etc[2]\n        if self.etc[2]==imp.PKG_DIRECTORY:\n            return self._get_delegate().get_filename()\n        elif self.etc[2] in (imp.PY_SOURCE, imp.PY_COMPILED, imp.C_EXTENSION):\n            return self.filename\n        return None\n\n\ntry:\n    import zipimport\n    from zipimport import zipimporter\n\n    def iter_zipimport_modules(importer, prefix=''):\n        dirlist = zipimport._zip_directory_cache[importer.archive].keys()\n        dirlist.sort()\n        _prefix = importer.prefix\n        plen = len(_prefix)\n        yielded = {}\n        import inspect\n        for fn in dirlist:\n            if not fn.startswith(_prefix):\n                continue\n\n            fn = fn[plen:].split(os.sep)\n\n            if len(fn)==2 and fn[1].startswith('__init__.py'):\n                if fn[0] not in yielded:\n                    yielded[fn[0]] = 1\n                    yield fn[0], True\n\n            if len(fn)!=1:\n                continue\n\n            modname = inspect.getmodulename(fn[0])\n            if modname=='__init__':\n                continue\n\n            if modname and '.' not in modname and modname not in yielded:\n                yielded[modname] = 1\n                yield prefix + modname, False\n\n    iter_importer_modules.register(zipimporter, iter_zipimport_modules)\n\nexcept ImportError:\n    pass\n\n\ndef get_importer(path_item):\n    \"\"\"Retrieve a PEP 302 importer for the given path item\n\n    The returned importer is cached in sys.path_importer_cache\n    if it was newly created by a path hook.\n\n    If there is no importer, a wrapper around the basic import\n    machinery is returned. This wrapper is never inserted into\n    the importer cache (None is inserted instead).\n\n    The cache (or part of it) can be cleared manually if a\n    rescan of sys.path_hooks is necessary.\n    \"\"\"\n    try:\n        importer = sys.path_importer_cache[path_item]\n    except KeyError:\n        for path_hook in sys.path_hooks:\n            try:\n                importer = path_hook(path_item)\n                break\n            except ImportError:\n                pass\n        else:\n            importer = None\n        sys.path_importer_cache.setdefault(path_item, importer)\n\n    if importer is None:\n        try:\n            importer = ImpImporter(path_item)\n        except ImportError:\n            importer = None\n    return importer\n\n\ndef iter_importers(fullname=\"\"):\n    \"\"\"Yield PEP 302 importers for the given module name\n\n    If fullname contains a '.', the importers will be for the package\n    containing fullname, otherwise they will be importers for sys.meta_path,\n    sys.path, and Python's \"classic\" import machinery, in that order.  If\n    the named module is in a package, that package is imported as a side\n    effect of invoking this function.\n\n    Non PEP 302 mechanisms (e.g. the Windows registry) used by the\n    standard import machinery to find files in alternative locations\n    are partially supported, but are searched AFTER sys.path. Normally,\n    these locations are searched BEFORE sys.path, preventing sys.path\n    entries from shadowing them.\n\n    For this to cause a visible difference in behaviour, there must\n    be a module or package name that is accessible via both sys.path\n    and one of the non PEP 302 file system mechanisms. In this case,\n    the emulation will find the former version, while the builtin\n    import mechanism will find the latter.\n\n    Items of the following types can be affected by this discrepancy:\n        imp.C_EXTENSION, imp.PY_SOURCE, imp.PY_COMPILED, imp.PKG_DIRECTORY\n    \"\"\"\n    if fullname.startswith('.'):\n        raise ImportError(\"Relative module names not supported\")\n    if '.' in fullname:\n        # Get the containing package's __path__\n        pkg = '.'.join(fullname.split('.')[:-1])\n        if pkg not in sys.modules:\n            __import__(pkg)\n        path = getattr(sys.modules[pkg], '__path__', None) or []\n    else:\n        for importer in sys.meta_path:\n            yield importer\n        path = sys.path\n    for item in path:\n        yield get_importer(item)\n    if '.' not in fullname:\n        yield ImpImporter()\n\ndef get_loader(module_or_name):\n    \"\"\"Get a PEP 302 \"loader\" object for module_or_name\n\n    If the module or package is accessible via the normal import\n    mechanism, a wrapper around the relevant part of that machinery\n    is returned.  Returns None if the module cannot be found or imported.\n    If the named module is not already imported, its containing package\n    (if any) is imported, in order to establish the package __path__.\n\n    This function uses iter_importers(), and is thus subject to the same\n    limitations regarding platform-specific special import locations such\n    as the Windows registry.\n    \"\"\"\n    if module_or_name in sys.modules:\n        module_or_name = sys.modules[module_or_name]\n    if isinstance(module_or_name, ModuleType):\n        module = module_or_name\n        loader = getattr(module, '__loader__', None)\n        if loader is not None:\n            return loader\n        fullname = module.__name__\n    else:\n        fullname = module_or_name\n    return find_loader(fullname)\n\ndef find_loader(fullname):\n    \"\"\"Find a PEP 302 \"loader\" object for fullname\n\n    If fullname contains dots, path must be the containing package's __path__.\n    Returns None if the module cannot be found or imported. This function uses\n    iter_importers(), and is thus subject to the same limitations regarding\n    platform-specific special import locations such as the Windows registry.\n    \"\"\"\n    for importer in iter_importers(fullname):\n        loader = importer.find_module(fullname)\n        if loader is not None:\n            return loader\n\n    return None\n\n\ndef extend_path(path, name):\n    \"\"\"Extend a package's path.\n\n    Intended use is to place the following code in a package's __init__.py:\n\n        from pkgutil import extend_path\n        __path__ = extend_path(__path__, __name__)\n\n    This will add to the package's __path__ all subdirectories of\n    directories on sys.path named after the package.  This is useful\n    if one wants to distribute different parts of a single logical\n    package as multiple directories.\n\n    It also looks for *.pkg files beginning where * matches the name\n    argument.  This feature is similar to *.pth files (see site.py),\n    except that it doesn't special-case lines starting with 'import'.\n    A *.pkg file is trusted at face value: apart from checking for\n    duplicates, all entries found in a *.pkg file are added to the\n    path, regardless of whether they are exist the filesystem.  (This\n    is a feature.)\n\n    If the input path is not a list (as is the case for frozen\n    packages) it is returned unchanged.  The input path is not\n    modified; an extended copy is returned.  Items are only appended\n    to the copy at the end.\n\n    It is assumed that sys.path is a sequence.  Items of sys.path that\n    are not (unicode or 8-bit) strings referring to existing\n    directories are ignored.  Unicode items of sys.path that cause\n    errors when used as filenames may cause this function to raise an\n    exception (in line with os.path.isdir() behavior).\n    \"\"\"\n\n    if not isinstance(path, list):\n        # This could happen e.g. when this is called from inside a\n        # frozen package.  Return the path unchanged in that case.\n        return path\n\n    pname = os.path.join(*name.split('.')) # Reconstitute as relative path\n    # Just in case os.extsep != '.'\n    sname = os.extsep.join(name.split('.'))\n    sname_pkg = sname + os.extsep + \"pkg\"\n    init_py = \"__init__\" + os.extsep + \"py\"\n\n    path = path[:] # Start with a copy of the existing path\n\n    for dir in sys.path:\n        if not isinstance(dir, basestring) or not os.path.isdir(dir):\n            continue\n        subdir = os.path.join(dir, pname)\n        # XXX This may still add duplicate entries to path on\n        # case-insensitive filesystems\n        initfile = os.path.join(subdir, init_py)\n        if subdir not in path and os.path.isfile(initfile):\n            path.append(subdir)\n        # XXX Is this the right thing for subpackages like zope.app?\n        # It looks for a file named \"zope.app.pkg\"\n        pkgfile = os.path.join(dir, sname_pkg)\n        if os.path.isfile(pkgfile):\n            try:\n                f = open(pkgfile)\n            except IOError, msg:\n                sys.stderr.write(\"Can't open %s: %s\\n\" %\n                                 (pkgfile, msg))\n            else:\n                for line in f:\n                    line = line.rstrip('\\n')\n                    if not line or line.startswith('#'):\n                        continue\n                    path.append(line) # Don't check for existence!\n                f.close()\n\n    return path\n\ndef get_data(package, resource):\n    \"\"\"Get a resource from a package.\n\n    This is a wrapper round the PEP 302 loader get_data API. The package\n    argument should be the name of a package, in standard module format\n    (foo.bar). The resource argument should be in the form of a relative\n    filename, using '/' as the path separator. The parent directory name '..'\n    is not allowed, and nor is a rooted name (starting with a '/').\n\n    The function returns a binary string, which is the contents of the\n    specified resource.\n\n    For packages located in the filesystem, which have already been imported,\n    this is the rough equivalent of\n\n        d = os.path.dirname(sys.modules[package].__file__)\n        data = open(os.path.join(d, resource), 'rb').read()\n\n    If the package cannot be located or loaded, or it uses a PEP 302 loader\n    which does not support get_data(), then None is returned.\n    \"\"\"\n\n    loader = get_loader(package)\n    if loader is None or not hasattr(loader, 'get_data'):\n        return None\n    mod = sys.modules.get(package) or loader.load_module(package)\n    if mod is None or not hasattr(mod, '__file__'):\n        return None\n\n    # Modify the resource name to be compatible with the loader.get_data\n    # signature - an os.path format \"filename\" starting with the dirname of\n    # the package's __file__\n    parts = resource.split('/')\n    parts.insert(0, os.path.dirname(mod.__file__))\n    resource_name = os.path.join(*parts)\n    return loader.get_data(resource_name)\n", 
    "posixpath": "\"\"\"Common operations on Posix pathnames.\n\nInstead of importing this module directly, import os and refer to\nthis module as os.path.  The \"os.path\" name is an alias for this\nmodule on Posix systems; on other systems (e.g. Mac, Windows),\nos.path provides the same operations in a manner specific to that\nplatform, and is an alias to another module (e.g. macpath, ntpath).\n\nSome of this can actually be useful on non-Posix systems too, e.g.\nfor manipulation of the pathname component of URLs.\n\"\"\"\n\nimport os\nimport sys\nimport stat\nimport genericpath\nimport warnings\nfrom genericpath import *\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n__all__ = [\"normcase\",\"isabs\",\"join\",\"splitdrive\",\"split\",\"splitext\",\n           \"basename\",\"dirname\",\"commonprefix\",\"getsize\",\"getmtime\",\n           \"getatime\",\"getctime\",\"islink\",\"exists\",\"lexists\",\"isdir\",\"isfile\",\n           \"ismount\",\"walk\",\"expanduser\",\"expandvars\",\"normpath\",\"abspath\",\n           \"samefile\",\"sameopenfile\",\"samestat\",\n           \"curdir\",\"pardir\",\"sep\",\"pathsep\",\"defpath\",\"altsep\",\"extsep\",\n           \"devnull\",\"realpath\",\"supports_unicode_filenames\",\"relpath\"]\n\n# strings representing various path-related bits and pieces\ncurdir = '.'\npardir = '..'\nextsep = '.'\nsep = '/'\npathsep = ':'\ndefpath = ':/bin:/usr/bin'\naltsep = None\ndevnull = '/dev/null'\n\n# Normalize the case of a pathname.  Trivial in Posix, string.lower on Mac.\n# On MS-DOS this may also turn slashes into backslashes; however, other\n# normalizations (such as optimizing '../' away) are not allowed\n# (another function should be defined to do that).\n\ndef normcase(s):\n    \"\"\"Normalize case of pathname.  Has no effect under Posix\"\"\"\n    return s\n\n\n# Return whether a path is absolute.\n# Trivial in Posix, harder on the Mac or MS-DOS.\n\ndef isabs(s):\n    \"\"\"Test whether a path is absolute\"\"\"\n    return s.startswith('/')\n\n\n# Join pathnames.\n# Ignore the previous parts if a part is absolute.\n# Insert a '/' unless the first part is empty or already ends in '/'.\n\ndef join(a, *p):\n    \"\"\"Join two or more pathname components, inserting '/' as needed.\n    If any component is an absolute path, all previous path components\n    will be discarded.  An empty last part will result in a path that\n    ends with a separator.\"\"\"\n    path = a\n    for b in p:\n        if b.startswith('/'):\n            path = b\n        elif path == '' or path.endswith('/'):\n            path +=  b\n        else:\n            path += '/' + b\n    return path\n\n\n# Split a path in head (everything up to the last '/') and tail (the\n# rest).  If the path ends in '/', tail will be empty.  If there is no\n# '/' in the path, head  will be empty.\n# Trailing '/'es are stripped from head unless it is the root.\n\ndef split(p):\n    \"\"\"Split a pathname.  Returns tuple \"(head, tail)\" where \"tail\" is\n    everything after the final slash.  Either part may be empty.\"\"\"\n    i = p.rfind('/') + 1\n    head, tail = p[:i], p[i:]\n    if head and head != '/'*len(head):\n        head = head.rstrip('/')\n    return head, tail\n\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\ndef splitext(p):\n    return genericpath._splitext(p, sep, altsep, extsep)\nsplitext.__doc__ = genericpath._splitext.__doc__\n\n# Split a pathname into a drive specification and the rest of the\n# path.  Useful on DOS/Windows/NT; on Unix, the drive is always empty.\n\ndef splitdrive(p):\n    \"\"\"Split a pathname into drive and path. On Posix, drive is always\n    empty.\"\"\"\n    return '', p\n\n\n# Return the tail (basename) part of a path, same as split(path)[1].\n\ndef basename(p):\n    \"\"\"Returns the final component of a pathname\"\"\"\n    i = p.rfind('/') + 1\n    return p[i:]\n\n\n# Return the head (dirname) part of a path, same as split(path)[0].\n\ndef dirname(p):\n    \"\"\"Returns the directory component of a pathname\"\"\"\n    i = p.rfind('/') + 1\n    head = p[:i]\n    if head and head != '/'*len(head):\n        head = head.rstrip('/')\n    return head\n\n\n# Is a path a symbolic link?\n# This will always return false on systems where os.lstat doesn't exist.\n\ndef islink(path):\n    \"\"\"Test whether a path is a symbolic link\"\"\"\n    try:\n        st = os.lstat(path)\n    except (os.error, AttributeError):\n        return False\n    return stat.S_ISLNK(st.st_mode)\n\n# Being true for dangling symbolic links is also useful.\n\ndef lexists(path):\n    \"\"\"Test whether a path exists.  Returns True for broken symbolic links\"\"\"\n    try:\n        os.lstat(path)\n    except os.error:\n        return False\n    return True\n\n\n# Are two filenames really pointing to the same file?\n\ndef samefile(f1, f2):\n    \"\"\"Test whether two pathnames reference the same actual file\"\"\"\n    s1 = os.stat(f1)\n    s2 = os.stat(f2)\n    return samestat(s1, s2)\n\n\n# Are two open files really referencing the same file?\n# (Not necessarily the same file descriptor!)\n\ndef sameopenfile(fp1, fp2):\n    \"\"\"Test whether two open file objects reference the same file\"\"\"\n    s1 = os.fstat(fp1)\n    s2 = os.fstat(fp2)\n    return samestat(s1, s2)\n\n\n# Are two stat buffers (obtained from stat, fstat or lstat)\n# describing the same file?\n\ndef samestat(s1, s2):\n    \"\"\"Test whether two stat buffers reference the same file\"\"\"\n    return s1.st_ino == s2.st_ino and \\\n           s1.st_dev == s2.st_dev\n\n\n# Is a path a mount point?\n# (Does this work for all UNIXes?  Is it even guaranteed to work by Posix?)\n\ndef ismount(path):\n    \"\"\"Test whether a path is a mount point\"\"\"\n    if islink(path):\n        # A symlink can never be a mount point\n        return False\n    try:\n        s1 = os.lstat(path)\n        s2 = os.lstat(join(path, '..'))\n    except os.error:\n        return False # It doesn't exist -- so not a mount point :-)\n    dev1 = s1.st_dev\n    dev2 = s2.st_dev\n    if dev1 != dev2:\n        return True     # path/.. on a different device as path\n    ino1 = s1.st_ino\n    ino2 = s2.st_ino\n    if ino1 == ino2:\n        return True     # path/.. is the same i-node as path\n    return False\n\n\n# Directory tree walk.\n# For each directory under top (including top itself, but excluding\n# '.' and '..'), func(arg, dirname, filenames) is called, where\n# dirname is the name of the directory and filenames is the list\n# of files (and subdirectories etc.) in the directory.\n# The func may modify the filenames list, to implement a filter,\n# or to impose a different order of visiting.\n\ndef walk(top, func, arg):\n    \"\"\"Directory tree walk with callback function.\n\n    For each directory in the directory tree rooted at top (including top\n    itself, but excluding '.' and '..'), call func(arg, dirname, fnames).\n    dirname is the name of the directory, and fnames a list of the names of\n    the files and subdirectories in dirname (excluding '.' and '..').  func\n    may modify the fnames list in-place (e.g. via del or slice assignment),\n    and walk will only recurse into the subdirectories whose names remain in\n    fnames; this can be used to implement a filter, or to impose a specific\n    order of visiting.  No semantics are defined for, or required of, arg,\n    beyond that arg is always passed to func.  It can be used, e.g., to pass\n    a filename pattern, or a mutable object designed to accumulate\n    statistics.  Passing None for arg is common.\"\"\"\n    warnings.warnpy3k(\"In 3.x, os.path.walk is removed in favor of os.walk.\",\n                      stacklevel=2)\n    try:\n        names = os.listdir(top)\n    except os.error:\n        return\n    func(arg, top, names)\n    for name in names:\n        name = join(top, name)\n        try:\n            st = os.lstat(name)\n        except os.error:\n            continue\n        if stat.S_ISDIR(st.st_mode):\n            walk(name, func, arg)\n\n\n# Expand paths beginning with '~' or '~user'.\n# '~' means $HOME; '~user' means that user's home directory.\n# If the path doesn't begin with '~', or if the user or $HOME is unknown,\n# the path is returned unchanged (leaving error reporting to whatever\n# function is called with the expanded path as argument).\n# See also module 'glob' for expansion of *, ? and [...] in pathnames.\n# (A function should also be defined to do full *sh-style environment\n# variable expansion.)\n\ndef expanduser(path):\n    \"\"\"Expand ~ and ~user constructions.  If user or $HOME is unknown,\n    do nothing.\"\"\"\n    if not path.startswith('~'):\n        return path\n    i = path.find('/', 1)\n    if i < 0:\n        i = len(path)\n    if i == 1:\n        if 'HOME' not in os.environ:\n            import pwd\n            userhome = pwd.getpwuid(os.getuid()).pw_dir\n        else:\n            userhome = os.environ['HOME']\n    else:\n        import pwd\n        try:\n            pwent = pwd.getpwnam(path[1:i])\n        except KeyError:\n            return path\n        userhome = pwent.pw_dir\n    userhome = userhome.rstrip('/')\n    return (userhome + path[i:]) or '/'\n\n\n# Expand paths containing shell variable substitutions.\n# This expands the forms $variable and ${variable} only.\n# Non-existent variables are left unchanged.\n\n_varprog = None\n_uvarprog = None\n\ndef expandvars(path):\n    \"\"\"Expand shell variables of form $var and ${var}.  Unknown variables\n    are left unchanged.\"\"\"\n    global _varprog, _uvarprog\n    if '$' not in path:\n        return path\n    if isinstance(path, _unicode):\n        if not _varprog:\n            import re\n            _varprog = re.compile(r'\\$(\\w+|\\{[^}]*\\})')\n        varprog = _varprog\n        encoding = sys.getfilesystemencoding()\n    else:\n        if not _uvarprog:\n            import re\n            _uvarprog = re.compile(_unicode(r'\\$(\\w+|\\{[^}]*\\})'), re.UNICODE)\n        varprog = _uvarprog\n        encoding = None\n    i = 0\n    while True:\n        m = varprog.search(path, i)\n        if not m:\n            break\n        i, j = m.span(0)\n        name = m.group(1)\n        if name.startswith('{') and name.endswith('}'):\n            name = name[1:-1]\n        if encoding:\n            name = name.encode(encoding)\n        if name in os.environ:\n            tail = path[j:]\n            value = os.environ[name]\n            if encoding:\n                value = value.decode(encoding)\n            path = path[:i] + value\n            i = len(path)\n            path += tail\n        else:\n            i = j\n    return path\n\n\n# Normalize a path, e.g. A//B, A/./B and A/foo/../B all become A/B.\n# It should be understood that this may change the meaning of the path\n# if it contains symbolic links!\n\ndef normpath(path):\n    \"\"\"Normalize path, eliminating double slashes, etc.\"\"\"\n    # Preserve unicode (if path is unicode)\n    slash, dot = (u'/', u'.') if isinstance(path, _unicode) else ('/', '.')\n    if path == '':\n        return dot\n    initial_slashes = path.startswith('/')\n    # POSIX allows one or two initial slashes, but treats three or more\n    # as single slash.\n    if (initial_slashes and\n        path.startswith('//') and not path.startswith('///')):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if comp in ('', '.'):\n            continue\n        if (comp != '..' or (not initial_slashes and not new_comps) or\n             (new_comps and new_comps[-1] == '..')):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = slash.join(comps)\n    if initial_slashes:\n        path = slash*initial_slashes + path\n    return path or dot\n\n\ndef abspath(path):\n    \"\"\"Return an absolute path.\"\"\"\n    if not isabs(path):\n        if isinstance(path, _unicode):\n            cwd = os.getcwdu()\n        else:\n            cwd = os.getcwd()\n        path = join(cwd, path)\n    return normpath(path)\n\n\n# Return a canonical path (i.e. the absolute location of a file on the\n# filesystem).\n\ndef realpath(filename):\n    \"\"\"Return the canonical path of the specified filename, eliminating any\nsymbolic links encountered in the path.\"\"\"\n    path, ok = _joinrealpath('', filename, {})\n    return abspath(path)\n\n# Join two paths, normalizing ang eliminating any symbolic links\n# encountered in the second path.\ndef _joinrealpath(path, rest, seen):\n    if isabs(rest):\n        rest = rest[1:]\n        path = sep\n\n    while rest:\n        name, _, rest = rest.partition(sep)\n        if not name or name == curdir:\n            # current dir\n            continue\n        if name == pardir:\n            # parent dir\n            if path:\n                path, name = split(path)\n                if name == pardir:\n                    path = join(path, pardir, pardir)\n            else:\n                path = pardir\n            continue\n        newpath = join(path, name)\n        if not islink(newpath):\n            path = newpath\n            continue\n        # Resolve the symbolic link\n        if newpath in seen:\n            # Already seen this path\n            path = seen[newpath]\n            if path is not None:\n                # use cached value\n                continue\n            # The symlink is not resolved, so we must have a symlink loop.\n            # Return already resolved part + rest of the path unchanged.\n            return join(newpath, rest), False\n        seen[newpath] = None # not resolved symlink\n        path, ok = _joinrealpath(path, os.readlink(newpath), seen)\n        if not ok:\n            return join(path, rest), False\n        seen[newpath] = path # resolved symlink\n\n    return path, True\n\n\nsupports_unicode_filenames = (sys.platform == 'darwin')\n\ndef relpath(path, start=curdir):\n    \"\"\"Return a relative version of a path\"\"\"\n\n    if not path:\n        raise ValueError(\"no path specified\")\n\n    start_list = [x for x in abspath(start).split(sep) if x]\n    path_list = [x for x in abspath(path).split(sep) if x]\n\n    # Work out how much of the filepath is shared by start and path.\n    i = len(commonprefix([start_list, path_list]))\n\n    rel_list = [pardir] * (len(start_list)-i) + path_list[i:]\n    if not rel_list:\n        return curdir\n    return join(*rel_list)\n", 
    "pprint": "#  Author:      Fred L. Drake, Jr.\n#               fdrake@acm.org\n#\n#  This is a simple little module I wrote to make life easier.  I didn't\n#  see anything quite like it in the library, though I may have overlooked\n#  something.  I wrote this when I was trying to read some heavily nested\n#  tuples with fairly non-descriptive content.  This is modeled very much\n#  after Lisp/Scheme - style pretty-printing of lists.  If you find it\n#  useful, thank small children who sleep at night.\n\n\"\"\"Support to pretty-print lists, tuples, & dictionaries recursively.\n\nVery simple, but useful, especially in debugging data structures.\n\nClasses\n-------\n\nPrettyPrinter()\n    Handle pretty-printing operations onto a stream using a configured\n    set of formatting parameters.\n\nFunctions\n---------\n\npformat()\n    Format a Python object into a pretty-printed representation.\n\npprint()\n    Pretty-print a Python object to a stream [default is sys.stdout].\n\nsaferepr()\n    Generate a 'standard' repr()-like value, but protect against recursive\n    data structures.\n\n\"\"\"\n\nimport sys as _sys\nimport warnings\n\ntry:\n    from cStringIO import StringIO as _StringIO\nexcept ImportError:\n    from StringIO import StringIO as _StringIO\n\n__all__ = [\"pprint\",\"pformat\",\"isreadable\",\"isrecursive\",\"saferepr\",\n           \"PrettyPrinter\"]\n\n# cache these for faster access:\n_commajoin = \", \".join\n_id = id\n_len = len\n_type = type\n\n\ndef pprint(object, stream=None, indent=1, width=80, depth=None):\n    \"\"\"Pretty-print a Python object to a stream [default is sys.stdout].\"\"\"\n    printer = PrettyPrinter(\n        stream=stream, indent=indent, width=width, depth=depth)\n    printer.pprint(object)\n\ndef pformat(object, indent=1, width=80, depth=None):\n    \"\"\"Format a Python object into a pretty-printed representation.\"\"\"\n    return PrettyPrinter(indent=indent, width=width, depth=depth).pformat(object)\n\ndef saferepr(object):\n    \"\"\"Version of repr() which can handle recursive data structures.\"\"\"\n    return _safe_repr(object, {}, None, 0)[0]\n\ndef isreadable(object):\n    \"\"\"Determine if saferepr(object) is readable by eval().\"\"\"\n    return _safe_repr(object, {}, None, 0)[1]\n\ndef isrecursive(object):\n    \"\"\"Determine if object requires a recursive representation.\"\"\"\n    return _safe_repr(object, {}, None, 0)[2]\n\ndef _sorted(iterable):\n    with warnings.catch_warnings():\n        if _sys.py3kwarning:\n            warnings.filterwarnings(\"ignore\", \"comparing unequal types \"\n                                    \"not supported\", DeprecationWarning)\n        return sorted(iterable)\n\nclass PrettyPrinter:\n    def __init__(self, indent=1, width=80, depth=None, stream=None):\n        \"\"\"Handle pretty printing operations onto a stream using a set of\n        configured parameters.\n\n        indent\n            Number of spaces to indent for each level of nesting.\n\n        width\n            Attempted maximum number of columns in the output.\n\n        depth\n            The maximum depth to print out nested structures.\n\n        stream\n            The desired output stream.  If omitted (or false), the standard\n            output stream available at construction will be used.\n\n        \"\"\"\n        indent = int(indent)\n        width = int(width)\n        assert indent >= 0, \"indent must be >= 0\"\n        assert depth is None or depth > 0, \"depth must be > 0\"\n        assert width, \"width must be != 0\"\n        self._depth = depth\n        self._indent_per_level = indent\n        self._width = width\n        if stream is not None:\n            self._stream = stream\n        else:\n            self._stream = _sys.stdout\n\n    def pprint(self, object):\n        self._format(object, self._stream, 0, 0, {}, 0)\n        self._stream.write(\"\\n\")\n\n    def pformat(self, object):\n        sio = _StringIO()\n        self._format(object, sio, 0, 0, {}, 0)\n        return sio.getvalue()\n\n    def isrecursive(self, object):\n        return self.format(object, {}, 0, 0)[2]\n\n    def isreadable(self, object):\n        s, readable, recursive = self.format(object, {}, 0, 0)\n        return readable and not recursive\n\n    def _format(self, object, stream, indent, allowance, context, level):\n        level = level + 1\n        objid = _id(object)\n        if objid in context:\n            stream.write(_recursion(object))\n            self._recursive = True\n            self._readable = False\n            return\n        rep = self._repr(object, context, level - 1)\n        typ = _type(object)\n        sepLines = _len(rep) > (self._width - 1 - indent - allowance)\n        write = stream.write\n\n        if self._depth and level > self._depth:\n            write(rep)\n            return\n\n        r = getattr(typ, \"__repr__\", None)\n        if issubclass(typ, dict) and r == dict.__repr__:\n            write('{')\n            if self._indent_per_level > 1:\n                write((self._indent_per_level - 1) * ' ')\n            length = _len(object)\n            if length:\n                context[objid] = 1\n                indent = indent + self._indent_per_level\n                items = _sorted(object.items())\n                key, ent = items[0]\n                rep = self._repr(key, context, level)\n                write(rep)\n                write(': ')\n                self._format(ent, stream, indent + _len(rep) + 2,\n                              allowance + 1, context, level)\n                if length > 1:\n                    for key, ent in items[1:]:\n                        rep = self._repr(key, context, level)\n                        if sepLines:\n                            write(',\\n%s%s: ' % (' '*indent, rep))\n                        else:\n                            write(', %s: ' % rep)\n                        self._format(ent, stream, indent + _len(rep) + 2,\n                                      allowance + 1, context, level)\n                indent = indent - self._indent_per_level\n                del context[objid]\n            write('}')\n            return\n\n        if ((issubclass(typ, list) and r == list.__repr__) or\n            (issubclass(typ, tuple) and r == tuple.__repr__) or\n            (issubclass(typ, set) and r == set.__repr__) or\n            (issubclass(typ, frozenset) and r == frozenset.__repr__)\n           ):\n            length = _len(object)\n            if issubclass(typ, list):\n                write('[')\n                endchar = ']'\n            elif issubclass(typ, tuple):\n                write('(')\n                endchar = ')'\n            else:\n                if not length:\n                    write(rep)\n                    return\n                write(typ.__name__)\n                write('([')\n                endchar = '])'\n                indent += len(typ.__name__) + 1\n                object = _sorted(object)\n            if self._indent_per_level > 1 and sepLines:\n                write((self._indent_per_level - 1) * ' ')\n            if length:\n                context[objid] = 1\n                indent = indent + self._indent_per_level\n                self._format(object[0], stream, indent, allowance + 1,\n                             context, level)\n                if length > 1:\n                    for ent in object[1:]:\n                        if sepLines:\n                            write(',\\n' + ' '*indent)\n                        else:\n                            write(', ')\n                        self._format(ent, stream, indent,\n                                      allowance + 1, context, level)\n                indent = indent - self._indent_per_level\n                del context[objid]\n            if issubclass(typ, tuple) and length == 1:\n                write(',')\n            write(endchar)\n            return\n\n        write(rep)\n\n    def _repr(self, object, context, level):\n        repr, readable, recursive = self.format(object, context.copy(),\n                                                self._depth, level)\n        if not readable:\n            self._readable = False\n        if recursive:\n            self._recursive = True\n        return repr\n\n    def format(self, object, context, maxlevels, level):\n        \"\"\"Format object for a specific context, returning a string\n        and flags indicating whether the representation is 'readable'\n        and whether the object represents a recursive construct.\n        \"\"\"\n        return _safe_repr(object, context, maxlevels, level)\n\n\n# Return triple (repr_string, isreadable, isrecursive).\n\ndef _safe_repr(object, context, maxlevels, level):\n    typ = _type(object)\n    if typ is str:\n        if 'locale' not in _sys.modules:\n            return repr(object), True, False\n        if \"'\" in object and '\"' not in object:\n            closure = '\"'\n            quotes = {'\"': '\\\\\"'}\n        else:\n            closure = \"'\"\n            quotes = {\"'\": \"\\\\'\"}\n        qget = quotes.get\n        sio = _StringIO()\n        write = sio.write\n        for char in object:\n            if char.isalpha():\n                write(char)\n            else:\n                write(qget(char, repr(char)[1:-1]))\n        return (\"%s%s%s\" % (closure, sio.getvalue(), closure)), True, False\n\n    r = getattr(typ, \"__repr__\", None)\n    if issubclass(typ, dict) and r == dict.__repr__:\n        if not object:\n            return \"{}\", True, False\n        objid = _id(object)\n        if maxlevels and level >= maxlevels:\n            return \"{...}\", False, objid in context\n        if objid in context:\n            return _recursion(object), False, True\n        context[objid] = 1\n        readable = True\n        recursive = False\n        components = []\n        append = components.append\n        level += 1\n        saferepr = _safe_repr\n        for k, v in _sorted(object.items()):\n            krepr, kreadable, krecur = saferepr(k, context, maxlevels, level)\n            vrepr, vreadable, vrecur = saferepr(v, context, maxlevels, level)\n            append(\"%s: %s\" % (krepr, vrepr))\n            readable = readable and kreadable and vreadable\n            if krecur or vrecur:\n                recursive = True\n        del context[objid]\n        return \"{%s}\" % _commajoin(components), readable, recursive\n\n    if (issubclass(typ, list) and r == list.__repr__) or \\\n       (issubclass(typ, tuple) and r == tuple.__repr__):\n        if issubclass(typ, list):\n            if not object:\n                return \"[]\", True, False\n            format = \"[%s]\"\n        elif _len(object) == 1:\n            format = \"(%s,)\"\n        else:\n            if not object:\n                return \"()\", True, False\n            format = \"(%s)\"\n        objid = _id(object)\n        if maxlevels and level >= maxlevels:\n            return format % \"...\", False, objid in context\n        if objid in context:\n            return _recursion(object), False, True\n        context[objid] = 1\n        readable = True\n        recursive = False\n        components = []\n        append = components.append\n        level += 1\n        for o in object:\n            orepr, oreadable, orecur = _safe_repr(o, context, maxlevels, level)\n            append(orepr)\n            if not oreadable:\n                readable = False\n            if orecur:\n                recursive = True\n        del context[objid]\n        return format % _commajoin(components), readable, recursive\n\n    rep = repr(object)\n    return rep, (rep and not rep.startswith('<')), False\n\n\ndef _recursion(object):\n    return (\"<Recursion on %s with id=%s>\"\n            % (_type(object).__name__, _id(object)))\n\n\ndef _perfcheck(object=None):\n    import time\n    if object is None:\n        object = [(\"string\", (1, 2), [3, 4], {5: 6, 7: 8})] * 100000\n    p = PrettyPrinter()\n    t1 = time.time()\n    _safe_repr(object, {}, None, 0)\n    t2 = time.time()\n    p.pformat(object)\n    t3 = time.time()\n    print \"_safe_repr:\", t2 - t1\n    print \"pformat:\", t3 - t2\n\nif __name__ == \"__main__\":\n    _perfcheck()\n", 
    "pwd": "# ctypes implementation: Victor Stinner, 2008-05-08\n\"\"\"\nThis module provides access to the Unix password database.\nIt is available on all Unix versions.\n\nPassword database entries are reported as 7-tuples containing the following\nitems from the password database (see `<pwd.h>'), in order:\npw_name, pw_passwd, pw_uid, pw_gid, pw_gecos, pw_dir, pw_shell.\nThe uid and gid items are integers, all others are strings. An\nexception is raised if the entry asked for cannot be found.\n\"\"\"\n\nimport sys\nif sys.platform == 'win32':\n    raise ImportError(\"No pwd module on Windows\")\n\nfrom ctypes_support import standard_c_lib as libc\nfrom ctypes import Structure, POINTER, c_int, c_char_p, c_long\nfrom _structseq import structseqtype, structseqfield\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n\nuid_t = c_int\ngid_t = c_int\ntime_t = c_long\n\nif sys.platform == 'darwin':\n    class passwd(Structure):\n        _fields_ = (\n            (\"pw_name\", c_char_p),\n            (\"pw_passwd\", c_char_p),\n            (\"pw_uid\", uid_t),\n            (\"pw_gid\", gid_t),\n            (\"pw_change\", time_t),\n            (\"pw_class\", c_char_p),\n            (\"pw_gecos\", c_char_p),\n            (\"pw_dir\", c_char_p),\n            (\"pw_shell\", c_char_p),\n            (\"pw_expire\", time_t),\n            (\"pw_fields\", c_int),\n        )\n        def __iter__(self):\n            yield self.pw_name\n            yield self.pw_passwd\n            yield self.pw_uid\n            yield self.pw_gid\n            yield self.pw_gecos\n            yield self.pw_dir\n            yield self.pw_shell\nelse:\n    class passwd(Structure):\n        _fields_ = (\n            (\"pw_name\", c_char_p),\n            (\"pw_passwd\", c_char_p),\n            (\"pw_uid\", uid_t),\n            (\"pw_gid\", gid_t),\n            (\"pw_gecos\", c_char_p),\n            (\"pw_dir\", c_char_p),\n            (\"pw_shell\", c_char_p),\n        )\n        def __iter__(self):\n            yield self.pw_name\n            yield self.pw_passwd\n            yield self.pw_uid\n            yield self.pw_gid\n            yield self.pw_gecos\n            yield self.pw_dir\n            yield self.pw_shell\n\nclass struct_passwd:\n    \"\"\"\n    pwd.struct_passwd: Results from getpw*() routines.\n\n    This object may be accessed either as a tuple of\n      (pw_name,pw_passwd,pw_uid,pw_gid,pw_gecos,pw_dir,pw_shell)\n    or via the object attributes as named in the above tuple.\n    \"\"\"\n    __metaclass__ = structseqtype\n    name = \"pwd.struct_passwd\"\n    pw_name = structseqfield(0)\n    pw_passwd = structseqfield(1)\n    pw_uid = structseqfield(2)\n    pw_gid = structseqfield(3)\n    pw_gecos = structseqfield(4)\n    pw_dir = structseqfield(5)\n    pw_shell = structseqfield(6)\n\npasswd_p = POINTER(passwd)\n\n_getpwuid = libc.getpwuid\n_getpwuid.argtypes = (uid_t,)\n_getpwuid.restype = passwd_p\n\n_getpwnam = libc.getpwnam\n_getpwnam.argtypes = (c_char_p,)\n_getpwnam.restype = passwd_p\n\n_setpwent = libc.setpwent\n_setpwent.argtypes = None\n_setpwent.restype = None\n\n_getpwent = libc.getpwent\n_getpwent.argtypes = None\n_getpwent.restype = passwd_p\n\n_endpwent = libc.endpwent\n_endpwent.argtypes = None\n_endpwent.restype = None\n\n@builtinify\ndef mkpwent(pw):\n    pw = pw.contents\n    return struct_passwd(pw)\n\n@builtinify\ndef getpwuid(uid):\n    \"\"\"\n    getpwuid(uid) -> (pw_name,pw_passwd,pw_uid,\n                      pw_gid,pw_gecos,pw_dir,pw_shell)\n    Return the password database entry for the given numeric user ID.\n    See pwd.__doc__ for more on password database entries.\n    \"\"\"\n    pw = _getpwuid(uid)\n    if not pw:\n        raise KeyError(\"getpwuid(): uid not found: %s\" % uid)\n    return mkpwent(pw)\n\n@builtinify\ndef getpwnam(name):\n    \"\"\"\n    getpwnam(name) -> (pw_name,pw_passwd,pw_uid,\n                        pw_gid,pw_gecos,pw_dir,pw_shell)\n    Return the password database entry for the given user name.\n    See pwd.__doc__ for more on password database entries.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"expected string\")\n    pw = _getpwnam(name)\n    if not pw:\n        raise KeyError(\"getpwname(): name not found: %s\" % name)\n    return mkpwent(pw)\n\n@builtinify\ndef getpwall():\n    \"\"\"\n    getpwall() -> list_of_entries\n    Return a list of all available password database entries, in arbitrary order.\n    See pwd.__doc__ for more on password database entries.\n    \"\"\"\n    users = []\n    _setpwent()\n    while True:\n        pw = _getpwent()\n        if not pw:\n            break\n        users.append(mkpwent(pw))\n    _endpwent()\n    return users\n\n__all__ = ('struct_passwd', 'getpwuid', 'getpwnam', 'getpwall')\n\nif __name__ == \"__main__\":\n# Uncomment next line to test CPython implementation\n#    from pwd import getpwuid, getpwnam, getpwall\n    from os import getuid\n    uid = getuid()\n    pw = getpwuid(uid)\n    print(\"uid %s: %s\" % (pw.pw_uid, pw))\n    name = pw.pw_name\n    print(\"name %r: %s\" % (name, getpwnam(name)))\n    print(\"All:\")\n    for pw in getpwall():\n        print(pw)\n\n", 
    "quopri": "#! /usr/bin/env python\n\n\"\"\"Conversions to/from quoted-printable transport encoding as per RFC 1521.\"\"\"\n\n# (Dec 1991 version).\n\n__all__ = [\"encode\", \"decode\", \"encodestring\", \"decodestring\"]\n\nESCAPE = '='\nMAXLINESIZE = 76\nHEX = '0123456789ABCDEF'\nEMPTYSTRING = ''\n\ntry:\n    from binascii import a2b_qp, b2a_qp\nexcept ImportError:\n    a2b_qp = None\n    b2a_qp = None\n\n\ndef needsquoting(c, quotetabs, header):\n    \"\"\"Decide whether a particular character needs to be quoted.\n\n    The 'quotetabs' flag indicates whether embedded tabs and spaces should be\n    quoted.  Note that line-ending tabs and spaces are always encoded, as per\n    RFC 1521.\n    \"\"\"\n    if c in ' \\t':\n        return quotetabs\n    # if header, we have to escape _ because _ is used to escape space\n    if c == '_':\n        return header\n    return c == ESCAPE or not (' ' <= c <= '~')\n\ndef quote(c):\n    \"\"\"Quote a single character.\"\"\"\n    i = ord(c)\n    return ESCAPE + HEX[i//16] + HEX[i%16]\n\n\n\ndef encode(input, output, quotetabs, header = 0):\n    \"\"\"Read 'input', apply quoted-printable encoding, and write to 'output'.\n\n    'input' and 'output' are files with readline() and write() methods.\n    The 'quotetabs' flag indicates whether embedded tabs and spaces should be\n    quoted.  Note that line-ending tabs and spaces are always encoded, as per\n    RFC 1521.\n    The 'header' flag indicates whether we are encoding spaces as _ as per\n    RFC 1522.\n    \"\"\"\n\n    if b2a_qp is not None:\n        data = input.read()\n        odata = b2a_qp(data, quotetabs = quotetabs, header = header)\n        output.write(odata)\n        return\n\n    def write(s, output=output, lineEnd='\\n'):\n        # RFC 1521 requires that the line ending in a space or tab must have\n        # that trailing character encoded.\n        if s and s[-1:] in ' \\t':\n            output.write(s[:-1] + quote(s[-1]) + lineEnd)\n        elif s == '.':\n            output.write(quote(s) + lineEnd)\n        else:\n            output.write(s + lineEnd)\n\n    prevline = None\n    while 1:\n        line = input.readline()\n        if not line:\n            break\n        outline = []\n        # Strip off any readline induced trailing newline\n        stripped = ''\n        if line[-1:] == '\\n':\n            line = line[:-1]\n            stripped = '\\n'\n        # Calculate the un-length-limited encoded line\n        for c in line:\n            if needsquoting(c, quotetabs, header):\n                c = quote(c)\n            if header and c == ' ':\n                outline.append('_')\n            else:\n                outline.append(c)\n        # First, write out the previous line\n        if prevline is not None:\n            write(prevline)\n        # Now see if we need any soft line breaks because of RFC-imposed\n        # length limitations.  Then do the thisline->prevline dance.\n        thisline = EMPTYSTRING.join(outline)\n        while len(thisline) > MAXLINESIZE:\n            # Don't forget to include the soft line break `=' sign in the\n            # length calculation!\n            write(thisline[:MAXLINESIZE-1], lineEnd='=\\n')\n            thisline = thisline[MAXLINESIZE-1:]\n        # Write out the current line\n        prevline = thisline\n    # Write out the last line, without a trailing newline\n    if prevline is not None:\n        write(prevline, lineEnd=stripped)\n\ndef encodestring(s, quotetabs = 0, header = 0):\n    if b2a_qp is not None:\n        return b2a_qp(s, quotetabs = quotetabs, header = header)\n    from cStringIO import StringIO\n    infp = StringIO(s)\n    outfp = StringIO()\n    encode(infp, outfp, quotetabs, header)\n    return outfp.getvalue()\n\n\n\ndef decode(input, output, header = 0):\n    \"\"\"Read 'input', apply quoted-printable decoding, and write to 'output'.\n    'input' and 'output' are files with readline() and write() methods.\n    If 'header' is true, decode underscore as space (per RFC 1522).\"\"\"\n\n    if a2b_qp is not None:\n        data = input.read()\n        odata = a2b_qp(data, header = header)\n        output.write(odata)\n        return\n\n    new = ''\n    while 1:\n        line = input.readline()\n        if not line: break\n        i, n = 0, len(line)\n        if n > 0 and line[n-1] == '\\n':\n            partial = 0; n = n-1\n            # Strip trailing whitespace\n            while n > 0 and line[n-1] in \" \\t\\r\":\n                n = n-1\n        else:\n            partial = 1\n        while i < n:\n            c = line[i]\n            if c == '_' and header:\n                new = new + ' '; i = i+1\n            elif c != ESCAPE:\n                new = new + c; i = i+1\n            elif i+1 == n and not partial:\n                partial = 1; break\n            elif i+1 < n and line[i+1] == ESCAPE:\n                new = new + ESCAPE; i = i+2\n            elif i+2 < n and ishex(line[i+1]) and ishex(line[i+2]):\n                new = new + chr(unhex(line[i+1:i+3])); i = i+3\n            else: # Bad escape sequence -- leave it in\n                new = new + c; i = i+1\n        if not partial:\n            output.write(new + '\\n')\n            new = ''\n    if new:\n        output.write(new)\n\ndef decodestring(s, header = 0):\n    if a2b_qp is not None:\n        return a2b_qp(s, header = header)\n    from cStringIO import StringIO\n    infp = StringIO(s)\n    outfp = StringIO()\n    decode(infp, outfp, header = header)\n    return outfp.getvalue()\n\n\n\n# Other helper functions\ndef ishex(c):\n    \"\"\"Return true if the character 'c' is a hexadecimal digit.\"\"\"\n    return '0' <= c <= '9' or 'a' <= c <= 'f' or 'A' <= c <= 'F'\n\ndef unhex(s):\n    \"\"\"Get the integer value of a hexadecimal number.\"\"\"\n    bits = 0\n    for c in s:\n        if '0' <= c <= '9':\n            i = ord('0')\n        elif 'a' <= c <= 'f':\n            i = ord('a')-10\n        elif 'A' <= c <= 'F':\n            i = ord('A')-10\n        else:\n            break\n        bits = bits*16 + (ord(c) - i)\n    return bits\n\n\n\ndef main():\n    import sys\n    import getopt\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], 'td')\n    except getopt.error, msg:\n        sys.stdout = sys.stderr\n        print msg\n        print \"usage: quopri [-t | -d] [file] ...\"\n        print \"-t: quote tabs\"\n        print \"-d: decode; default encode\"\n        sys.exit(2)\n    deco = 0\n    tabs = 0\n    for o, a in opts:\n        if o == '-t': tabs = 1\n        if o == '-d': deco = 1\n    if tabs and deco:\n        sys.stdout = sys.stderr\n        print \"-t and -d are mutually exclusive\"\n        sys.exit(2)\n    if not args: args = ['-']\n    sts = 0\n    for file in args:\n        if file == '-':\n            fp = sys.stdin\n        else:\n            try:\n                fp = open(file)\n            except IOError, msg:\n                sys.stderr.write(\"%s: can't open (%s)\\n\" % (file, msg))\n                sts = 1\n                continue\n        if deco:\n            decode(fp, sys.stdout)\n        else:\n            encode(fp, sys.stdout, tabs)\n        if fp is not sys.stdin:\n            fp.close()\n    if sts:\n        sys.exit(sts)\n\n\n\nif __name__ == '__main__':\n    main()\n", 
    "random": "\"\"\"Random variable generators.\n\n    integers\n    --------\n           uniform within range\n\n    sequences\n    ---------\n           pick random element\n           pick random sample\n           generate random permutation\n\n    distributions on the real line:\n    ------------------------------\n           uniform\n           triangular\n           normal (Gaussian)\n           lognormal\n           negative exponential\n           gamma\n           beta\n           pareto\n           Weibull\n\n    distributions on the circle (angles 0 to 2pi)\n    ---------------------------------------------\n           circular uniform\n           von Mises\n\nGeneral notes on the underlying Mersenne Twister core generator:\n\n* The period is 2**19937-1.\n* It is one of the most extensively tested generators in existence.\n* Without a direct way to compute N steps forward, the semantics of\n  jumpahead(n) are weakened to simply jump to another distant state and rely\n  on the large period to avoid overlapping sequences.\n* The random() method is implemented in C, executes in a single Python step,\n  and is, therefore, threadsafe.\n\n\"\"\"\n\nfrom __future__ import division\nfrom warnings import warn as _warn\nfrom math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil\nfrom math import sqrt as _sqrt, acos as _acos, cos as _cos, sin as _sin\nfrom os import urandom as _urandom\nfrom binascii import hexlify as _hexlify\nimport hashlib as _hashlib\n\n__all__ = [\"Random\",\"seed\",\"random\",\"uniform\",\"randint\",\"choice\",\"sample\",\n           \"randrange\",\"shuffle\",\"normalvariate\",\"lognormvariate\",\n           \"expovariate\",\"vonmisesvariate\",\"gammavariate\",\"triangular\",\n           \"gauss\",\"betavariate\",\"paretovariate\",\"weibullvariate\",\n           \"getstate\",\"setstate\",\"jumpahead\", \"WichmannHill\", \"getrandbits\",\n           \"SystemRandom\"]\n\nNV_MAGICCONST = 4 * _exp(-0.5)/_sqrt(2.0)\nTWOPI = 2.0*_pi\nLOG4 = _log(4.0)\nSG_MAGICCONST = 1.0 + _log(4.5)\nBPF = 53        # Number of bits in a float\nRECIP_BPF = 2**-BPF\n\n\n# Translated by Guido van Rossum from C source provided by\n# Adrian Baddeley.  Adapted by Raymond Hettinger for use with\n# the Mersenne Twister  and os.urandom() core generators.\n\nimport _random\n\nclass Random(_random.Random):\n    \"\"\"Random number generator base class used by bound module functions.\n\n    Used to instantiate instances of Random to get generators that don't\n    share state.  Especially useful for multi-threaded programs, creating\n    a different instance of Random for each thread, and using the jumpahead()\n    method to ensure that the generated sequences seen by each thread don't\n    overlap.\n\n    Class Random can also be subclassed if you want to use a different basic\n    generator of your own devising: in that case, override the following\n    methods: random(), seed(), getstate(), setstate() and jumpahead().\n    Optionally, implement a getrandbits() method so that randrange() can cover\n    arbitrarily large ranges.\n\n    \"\"\"\n\n    VERSION = 3     # used by getstate/setstate\n\n    def __init__(self, x=None):\n        \"\"\"Initialize an instance.\n\n        Optional argument x controls seeding, as for Random.seed().\n        \"\"\"\n\n        self.seed(x)\n        self.gauss_next = None\n\n    def seed(self, a=None):\n        \"\"\"Initialize internal state from hashable object.\n\n        None or no argument seeds from current time or from an operating\n        system specific randomness source if available.\n\n        If a is not None or an int or long, hash(a) is used instead.\n        \"\"\"\n\n        if a is None:\n            try:\n                # Seed with enough bytes to span the 19937 bit\n                # state space for the Mersenne Twister\n                a = long(_hexlify(_urandom(2500)), 16)\n            except NotImplementedError:\n                import time\n                a = long(time.time() * 256) # use fractional seconds\n\n        super(Random, self).seed(a)\n        self.gauss_next = None\n\n    def getstate(self):\n        \"\"\"Return internal state; can be passed to setstate() later.\"\"\"\n        return self.VERSION, super(Random, self).getstate(), self.gauss_next\n\n    def setstate(self, state):\n        \"\"\"Restore internal state from object returned by getstate().\"\"\"\n        version = state[0]\n        if version == 3:\n            version, internalstate, self.gauss_next = state\n            super(Random, self).setstate(internalstate)\n        elif version == 2:\n            version, internalstate, self.gauss_next = state\n            # In version 2, the state was saved as signed ints, which causes\n            #   inconsistencies between 32/64-bit systems. The state is\n            #   really unsigned 32-bit ints, so we convert negative ints from\n            #   version 2 to positive longs for version 3.\n            try:\n                internalstate = tuple( long(x) % (2**32) for x in internalstate )\n            except ValueError, e:\n                raise TypeError, e\n            super(Random, self).setstate(internalstate)\n        else:\n            raise ValueError(\"state with version %s passed to \"\n                             \"Random.setstate() of version %s\" %\n                             (version, self.VERSION))\n\n    def jumpahead(self, n):\n        \"\"\"Change the internal state to one that is likely far away\n        from the current state.  This method will not be in Py3.x,\n        so it is better to simply reseed.\n        \"\"\"\n        # The super.jumpahead() method uses shuffling to change state,\n        # so it needs a large and \"interesting\" n to work with.  Here,\n        # we use hashing to create a large n for the shuffle.\n        s = repr(n) + repr(self.getstate())\n        n = int(_hashlib.new('sha512', s).hexdigest(), 16)\n        super(Random, self).jumpahead(n)\n\n## ---- Methods below this point do not need to be overridden when\n## ---- subclassing for the purpose of using a different core generator.\n\n## -------------------- pickle support  -------------------\n\n    def __getstate__(self): # for pickle\n        return self.getstate()\n\n    def __setstate__(self, state):  # for pickle\n        self.setstate(state)\n\n    def __reduce__(self):\n        return self.__class__, (), self.getstate()\n\n## -------------------- integer methods  -------------------\n\n    def randrange(self, start, stop=None, step=1, _int=int, _maxwidth=1L<<BPF):\n        \"\"\"Choose a random item from range(start, stop[, step]).\n\n        This fixes the problem with randint() which includes the\n        endpoint; in Python this is usually not what you want.\n\n        \"\"\"\n\n        # This code is a bit messy to make it fast for the\n        # common case while still doing adequate error checking.\n        istart = _int(start)\n        if istart != start:\n            raise ValueError, \"non-integer arg 1 for randrange()\"\n        if stop is None:\n            if istart > 0:\n                if istart >= _maxwidth:\n                    return self._randbelow(istart)\n                return _int(self.random() * istart)\n            raise ValueError, \"empty range for randrange()\"\n\n        # stop argument supplied.\n        istop = _int(stop)\n        if istop != stop:\n            raise ValueError, \"non-integer stop for randrange()\"\n        width = istop - istart\n        if step == 1 and width > 0:\n            # Note that\n            #     int(istart + self.random()*width)\n            # instead would be incorrect.  For example, consider istart\n            # = -2 and istop = 0.  Then the guts would be in\n            # -2.0 to 0.0 exclusive on both ends (ignoring that random()\n            # might return 0.0), and because int() truncates toward 0, the\n            # final result would be -1 or 0 (instead of -2 or -1).\n            #     istart + int(self.random()*width)\n            # would also be incorrect, for a subtler reason:  the RHS\n            # can return a long, and then randrange() would also return\n            # a long, but we're supposed to return an int (for backward\n            # compatibility).\n\n            if width >= _maxwidth:\n                return _int(istart + self._randbelow(width))\n            return _int(istart + _int(self.random()*width))\n        if step == 1:\n            raise ValueError, \"empty range for randrange() (%d,%d, %d)\" % (istart, istop, width)\n\n        # Non-unit step argument supplied.\n        istep = _int(step)\n        if istep != step:\n            raise ValueError, \"non-integer step for randrange()\"\n        if istep > 0:\n            n = (width + istep - 1) // istep\n        elif istep < 0:\n            n = (width + istep + 1) // istep\n        else:\n            raise ValueError, \"zero step for randrange()\"\n\n        if n <= 0:\n            raise ValueError, \"empty range for randrange()\"\n\n        if n >= _maxwidth:\n            return istart + istep*self._randbelow(n)\n        return istart + istep*_int(self.random() * n)\n\n    def randint(self, a, b):\n        \"\"\"Return random integer in range [a, b], including both end points.\n        \"\"\"\n\n        return self.randrange(a, b+1)\n\n    def _randbelow(self, n, _log=_log, _int=int, _maxwidth=1L<<BPF):\n        \"\"\"Return a random int in the range [0,n)\n\n        Handles the case where n has more bits than returned\n        by a single call to the underlying generator.\n        \"\"\"\n\n        try:\n            getrandbits = self.getrandbits\n        except AttributeError:\n            pass\n        else:\n            # Only call self.getrandbits if the original random() builtin method\n            # has not been overridden or if a new getrandbits() was supplied.\n            # This assures that the two methods correspond.\n            if (self.random == super(Random, self).random or\n                    getrandbits != super(Random, self).getrandbits):\n                k = _int(1.00001 + _log(n-1, 2.0))   # 2**k > n-1 > 2**(k-2)\n                r = getrandbits(k)\n                while r >= n:\n                    r = getrandbits(k)\n                return r\n        if n >= _maxwidth:\n            _warn(\"Underlying random() generator does not supply \\n\"\n                \"enough bits to choose from a population range this large\")\n        return _int(self.random() * n)\n\n## -------------------- sequence methods  -------------------\n\n    def choice(self, seq):\n        \"\"\"Choose a random element from a non-empty sequence.\"\"\"\n        return seq[int(self.random() * len(seq))]  # raises IndexError if seq is empty\n\n    def shuffle(self, x, random=None):\n        \"\"\"x, random=random.random -> shuffle list x in place; return None.\n\n        Optional arg random is a 0-argument function returning a random\n        float in [0.0, 1.0); by default, the standard random.random.\n\n        \"\"\"\n\n        if random is None:\n            random = self.random\n        _int = int\n        for i in reversed(xrange(1, len(x))):\n            # pick an element in x[:i+1] with which to exchange x[i]\n            j = _int(random() * (i+1))\n            x[i], x[j] = x[j], x[i]\n\n    def sample(self, population, k):\n        \"\"\"Chooses k unique random elements from a population sequence.\n\n        Returns a new list containing elements from the population while\n        leaving the original population unchanged.  The resulting list is\n        in selection order so that all sub-slices will also be valid random\n        samples.  This allows raffle winners (the sample) to be partitioned\n        into grand prize and second place winners (the subslices).\n\n        Members of the population need not be hashable or unique.  If the\n        population contains repeats, then each occurrence is a possible\n        selection in the sample.\n\n        To choose a sample in a range of integers, use xrange as an argument.\n        This is especially fast and space efficient for sampling from a\n        large population:   sample(xrange(10000000), 60)\n        \"\"\"\n\n        # Sampling without replacement entails tracking either potential\n        # selections (the pool) in a list or previous selections in a set.\n\n        # When the number of selections is small compared to the\n        # population, then tracking selections is efficient, requiring\n        # only a small set and an occasional reselection.  For\n        # a larger number of selections, the pool tracking method is\n        # preferred since the list takes less space than the\n        # set and it doesn't suffer from frequent reselections.\n\n        n = len(population)\n        if not 0 <= k <= n:\n            raise ValueError(\"sample larger than population\")\n        random = self.random\n        _int = int\n        result = [None] * k\n        setsize = 21        # size of a small set minus size of an empty list\n        if k > 5:\n            setsize += 4 ** _ceil(_log(k * 3, 4)) # table size for big sets\n        if n <= setsize or hasattr(population, \"keys\"):\n            # An n-length list is smaller than a k-length set, or this is a\n            # mapping type so the other algorithm wouldn't work.\n            pool = list(population)\n            for i in xrange(k):         # invariant:  non-selected at [0,n-i)\n                j = _int(random() * (n-i))\n                result[i] = pool[j]\n                pool[j] = pool[n-i-1]   # move non-selected item into vacancy\n        else:\n            try:\n                selected = set()\n                selected_add = selected.add\n                for i in xrange(k):\n                    j = _int(random() * n)\n                    while j in selected:\n                        j = _int(random() * n)\n                    selected_add(j)\n                    result[i] = population[j]\n            except (TypeError, KeyError):   # handle (at least) sets\n                if isinstance(population, list):\n                    raise\n                return self.sample(tuple(population), k)\n        return result\n\n## -------------------- real-valued distributions  -------------------\n\n## -------------------- uniform distribution -------------------\n\n    def uniform(self, a, b):\n        \"Get a random number in the range [a, b) or [a, b] depending on rounding.\"\n        return a + (b-a) * self.random()\n\n## -------------------- triangular --------------------\n\n    def triangular(self, low=0.0, high=1.0, mode=None):\n        \"\"\"Triangular distribution.\n\n        Continuous distribution bounded by given lower and upper limits,\n        and having a given mode value in-between.\n\n        http://en.wikipedia.org/wiki/Triangular_distribution\n\n        \"\"\"\n        u = self.random()\n        try:\n            c = 0.5 if mode is None else (mode - low) / (high - low)\n        except ZeroDivisionError:\n            return low\n        if u > c:\n            u = 1.0 - u\n            c = 1.0 - c\n            low, high = high, low\n        return low + (high - low) * (u * c) ** 0.5\n\n## -------------------- normal distribution --------------------\n\n    def normalvariate(self, mu, sigma):\n        \"\"\"Normal distribution.\n\n        mu is the mean, and sigma is the standard deviation.\n\n        \"\"\"\n        # mu = mean, sigma = standard deviation\n\n        # Uses Kinderman and Monahan method. Reference: Kinderman,\n        # A.J. and Monahan, J.F., \"Computer generation of random\n        # variables using the ratio of uniform deviates\", ACM Trans\n        # Math Software, 3, (1977), pp257-260.\n\n        random = self.random\n        while 1:\n            u1 = random()\n            u2 = 1.0 - random()\n            z = NV_MAGICCONST*(u1-0.5)/u2\n            zz = z*z/4.0\n            if zz <= -_log(u2):\n                break\n        return mu + z*sigma\n\n## -------------------- lognormal distribution --------------------\n\n    def lognormvariate(self, mu, sigma):\n        \"\"\"Log normal distribution.\n\n        If you take the natural logarithm of this distribution, you'll get a\n        normal distribution with mean mu and standard deviation sigma.\n        mu can have any value, and sigma must be greater than zero.\n\n        \"\"\"\n        return _exp(self.normalvariate(mu, sigma))\n\n## -------------------- exponential distribution --------------------\n\n    def expovariate(self, lambd):\n        \"\"\"Exponential distribution.\n\n        lambd is 1.0 divided by the desired mean.  It should be\n        nonzero.  (The parameter would be called \"lambda\", but that is\n        a reserved word in Python.)  Returned values range from 0 to\n        positive infinity if lambd is positive, and from negative\n        infinity to 0 if lambd is negative.\n\n        \"\"\"\n        # lambd: rate lambd = 1/mean\n        # ('lambda' is a Python reserved word)\n\n        # we use 1-random() instead of random() to preclude the\n        # possibility of taking the log of zero.\n        return -_log(1.0 - self.random())/lambd\n\n## -------------------- von Mises distribution --------------------\n\n    def vonmisesvariate(self, mu, kappa):\n        \"\"\"Circular data distribution.\n\n        mu is the mean angle, expressed in radians between 0 and 2*pi, and\n        kappa is the concentration parameter, which must be greater than or\n        equal to zero.  If kappa is equal to zero, this distribution reduces\n        to a uniform random angle over the range 0 to 2*pi.\n\n        \"\"\"\n        # mu:    mean angle (in radians between 0 and 2*pi)\n        # kappa: concentration parameter kappa (>= 0)\n        # if kappa = 0 generate uniform random angle\n\n        # Based upon an algorithm published in: Fisher, N.I.,\n        # \"Statistical Analysis of Circular Data\", Cambridge\n        # University Press, 1993.\n\n        # Thanks to Magnus Kessler for a correction to the\n        # implementation of step 4.\n\n        random = self.random\n        if kappa <= 1e-6:\n            return TWOPI * random()\n\n        s = 0.5 / kappa\n        r = s + _sqrt(1.0 + s * s)\n\n        while 1:\n            u1 = random()\n            z = _cos(_pi * u1)\n\n            d = z / (r + z)\n            u2 = random()\n            if u2 < 1.0 - d * d or u2 <= (1.0 - d) * _exp(d):\n                break\n\n        q = 1.0 / r\n        f = (q + z) / (1.0 + q * z)\n        u3 = random()\n        if u3 > 0.5:\n            theta = (mu + _acos(f)) % TWOPI\n        else:\n            theta = (mu - _acos(f)) % TWOPI\n\n        return theta\n\n## -------------------- gamma distribution --------------------\n\n    def gammavariate(self, alpha, beta):\n        \"\"\"Gamma distribution.  Not the gamma function!\n\n        Conditions on the parameters are alpha > 0 and beta > 0.\n\n        The probability distribution function is:\n\n                    x ** (alpha - 1) * math.exp(-x / beta)\n          pdf(x) =  --------------------------------------\n                      math.gamma(alpha) * beta ** alpha\n\n        \"\"\"\n\n        # alpha > 0, beta > 0, mean is alpha*beta, variance is alpha*beta**2\n\n        # Warning: a few older sources define the gamma distribution in terms\n        # of alpha > -1.0\n        if alpha <= 0.0 or beta <= 0.0:\n            raise ValueError, 'gammavariate: alpha and beta must be > 0.0'\n\n        random = self.random\n        if alpha > 1.0:\n\n            # Uses R.C.H. Cheng, \"The generation of Gamma\n            # variables with non-integral shape parameters\",\n            # Applied Statistics, (1977), 26, No. 1, p71-74\n\n            ainv = _sqrt(2.0 * alpha - 1.0)\n            bbb = alpha - LOG4\n            ccc = alpha + ainv\n\n            while 1:\n                u1 = random()\n                if not 1e-7 < u1 < .9999999:\n                    continue\n                u2 = 1.0 - random()\n                v = _log(u1/(1.0-u1))/ainv\n                x = alpha*_exp(v)\n                z = u1*u1*u2\n                r = bbb+ccc*v-x\n                if r + SG_MAGICCONST - 4.5*z >= 0.0 or r >= _log(z):\n                    return x * beta\n\n        elif alpha == 1.0:\n            # expovariate(1)\n            u = random()\n            while u <= 1e-7:\n                u = random()\n            return -_log(u) * beta\n\n        else:   # alpha is between 0 and 1 (exclusive)\n\n            # Uses ALGORITHM GS of Statistical Computing - Kennedy & Gentle\n\n            while 1:\n                u = random()\n                b = (_e + alpha)/_e\n                p = b*u\n                if p <= 1.0:\n                    x = p ** (1.0/alpha)\n                else:\n                    x = -_log((b-p)/alpha)\n                u1 = random()\n                if p > 1.0:\n                    if u1 <= x ** (alpha - 1.0):\n                        break\n                elif u1 <= _exp(-x):\n                    break\n            return x * beta\n\n## -------------------- Gauss (faster alternative) --------------------\n\n    def gauss(self, mu, sigma):\n        \"\"\"Gaussian distribution.\n\n        mu is the mean, and sigma is the standard deviation.  This is\n        slightly faster than the normalvariate() function.\n\n        Not thread-safe without a lock around calls.\n\n        \"\"\"\n\n        # When x and y are two variables from [0, 1), uniformly\n        # distributed, then\n        #\n        #    cos(2*pi*x)*sqrt(-2*log(1-y))\n        #    sin(2*pi*x)*sqrt(-2*log(1-y))\n        #\n        # are two *independent* variables with normal distribution\n        # (mu = 0, sigma = 1).\n        # (Lambert Meertens)\n        # (corrected version; bug discovered by Mike Miller, fixed by LM)\n\n        # Multithreading note: When two threads call this function\n        # simultaneously, it is possible that they will receive the\n        # same return value.  The window is very small though.  To\n        # avoid this, you have to use a lock around all calls.  (I\n        # didn't want to slow this down in the serial case by using a\n        # lock here.)\n\n        random = self.random\n        z = self.gauss_next\n        self.gauss_next = None\n        if z is None:\n            x2pi = random() * TWOPI\n            g2rad = _sqrt(-2.0 * _log(1.0 - random()))\n            z = _cos(x2pi) * g2rad\n            self.gauss_next = _sin(x2pi) * g2rad\n\n        return mu + z*sigma\n\n## -------------------- beta --------------------\n## See\n## http://mail.python.org/pipermail/python-bugs-list/2001-January/003752.html\n## for Ivan Frohne's insightful analysis of why the original implementation:\n##\n##    def betavariate(self, alpha, beta):\n##        # Discrete Event Simulation in C, pp 87-88.\n##\n##        y = self.expovariate(alpha)\n##        z = self.expovariate(1.0/beta)\n##        return z/(y+z)\n##\n## was dead wrong, and how it probably got that way.\n\n    def betavariate(self, alpha, beta):\n        \"\"\"Beta distribution.\n\n        Conditions on the parameters are alpha > 0 and beta > 0.\n        Returned values range between 0 and 1.\n\n        \"\"\"\n\n        # This version due to Janne Sinkkonen, and matches all the std\n        # texts (e.g., Knuth Vol 2 Ed 3 pg 134 \"the beta distribution\").\n        y = self.gammavariate(alpha, 1.)\n        if y == 0:\n            return 0.0\n        else:\n            return y / (y + self.gammavariate(beta, 1.))\n\n## -------------------- Pareto --------------------\n\n    def paretovariate(self, alpha):\n        \"\"\"Pareto distribution.  alpha is the shape parameter.\"\"\"\n        # Jain, pg. 495\n\n        u = 1.0 - self.random()\n        return 1.0 / pow(u, 1.0/alpha)\n\n## -------------------- Weibull --------------------\n\n    def weibullvariate(self, alpha, beta):\n        \"\"\"Weibull distribution.\n\n        alpha is the scale parameter and beta is the shape parameter.\n\n        \"\"\"\n        # Jain, pg. 499; bug fix courtesy Bill Arms\n\n        u = 1.0 - self.random()\n        return alpha * pow(-_log(u), 1.0/beta)\n\n## -------------------- Wichmann-Hill -------------------\n\nclass WichmannHill(Random):\n\n    VERSION = 1     # used by getstate/setstate\n\n    def seed(self, a=None):\n        \"\"\"Initialize internal state from hashable object.\n\n        None or no argument seeds from current time or from an operating\n        system specific randomness source if available.\n\n        If a is not None or an int or long, hash(a) is used instead.\n\n        If a is an int or long, a is used directly.  Distinct values between\n        0 and 27814431486575L inclusive are guaranteed to yield distinct\n        internal states (this guarantee is specific to the default\n        Wichmann-Hill generator).\n        \"\"\"\n\n        if a is None:\n            try:\n                a = long(_hexlify(_urandom(16)), 16)\n            except NotImplementedError:\n                import time\n                a = long(time.time() * 256) # use fractional seconds\n\n        if not isinstance(a, (int, long)):\n            a = hash(a)\n\n        a, x = divmod(a, 30268)\n        a, y = divmod(a, 30306)\n        a, z = divmod(a, 30322)\n        self._seed = int(x)+1, int(y)+1, int(z)+1\n\n        self.gauss_next = None\n\n    def random(self):\n        \"\"\"Get the next random number in the range [0.0, 1.0).\"\"\"\n\n        # Wichman-Hill random number generator.\n        #\n        # Wichmann, B. A. & Hill, I. D. (1982)\n        # Algorithm AS 183:\n        # An efficient and portable pseudo-random number generator\n        # Applied Statistics 31 (1982) 188-190\n        #\n        # see also:\n        #        Correction to Algorithm AS 183\n        #        Applied Statistics 33 (1984) 123\n        #\n        #        McLeod, A. I. (1985)\n        #        A remark on Algorithm AS 183\n        #        Applied Statistics 34 (1985),198-200\n\n        # This part is thread-unsafe:\n        # BEGIN CRITICAL SECTION\n        x, y, z = self._seed\n        x = (171 * x) % 30269\n        y = (172 * y) % 30307\n        z = (170 * z) % 30323\n        self._seed = x, y, z\n        # END CRITICAL SECTION\n\n        # Note:  on a platform using IEEE-754 double arithmetic, this can\n        # never return 0.0 (asserted by Tim; proof too long for a comment).\n        return (x/30269.0 + y/30307.0 + z/30323.0) % 1.0\n\n    def getstate(self):\n        \"\"\"Return internal state; can be passed to setstate() later.\"\"\"\n        return self.VERSION, self._seed, self.gauss_next\n\n    def setstate(self, state):\n        \"\"\"Restore internal state from object returned by getstate().\"\"\"\n        version = state[0]\n        if version == 1:\n            version, self._seed, self.gauss_next = state\n        else:\n            raise ValueError(\"state with version %s passed to \"\n                             \"Random.setstate() of version %s\" %\n                             (version, self.VERSION))\n\n    def jumpahead(self, n):\n        \"\"\"Act as if n calls to random() were made, but quickly.\n\n        n is an int, greater than or equal to 0.\n\n        Example use:  If you have 2 threads and know that each will\n        consume no more than a million random numbers, create two Random\n        objects r1 and r2, then do\n            r2.setstate(r1.getstate())\n            r2.jumpahead(1000000)\n        Then r1 and r2 will use guaranteed-disjoint segments of the full\n        period.\n        \"\"\"\n\n        if not n >= 0:\n            raise ValueError(\"n must be >= 0\")\n        x, y, z = self._seed\n        x = int(x * pow(171, n, 30269)) % 30269\n        y = int(y * pow(172, n, 30307)) % 30307\n        z = int(z * pow(170, n, 30323)) % 30323\n        self._seed = x, y, z\n\n    def __whseed(self, x=0, y=0, z=0):\n        \"\"\"Set the Wichmann-Hill seed from (x, y, z).\n\n        These must be integers in the range [0, 256).\n        \"\"\"\n\n        if not type(x) == type(y) == type(z) == int:\n            raise TypeError('seeds must be integers')\n        if not (0 <= x < 256 and 0 <= y < 256 and 0 <= z < 256):\n            raise ValueError('seeds must be in range(0, 256)')\n        if 0 == x == y == z:\n            # Initialize from current time\n            import time\n            t = long(time.time() * 256)\n            t = int((t&0xffffff) ^ (t>>24))\n            t, x = divmod(t, 256)\n            t, y = divmod(t, 256)\n            t, z = divmod(t, 256)\n        # Zero is a poor seed, so substitute 1\n        self._seed = (x or 1, y or 1, z or 1)\n\n        self.gauss_next = None\n\n    def whseed(self, a=None):\n        \"\"\"Seed from hashable object's hash code.\n\n        None or no argument seeds from current time.  It is not guaranteed\n        that objects with distinct hash codes lead to distinct internal\n        states.\n\n        This is obsolete, provided for compatibility with the seed routine\n        used prior to Python 2.1.  Use the .seed() method instead.\n        \"\"\"\n\n        if a is None:\n            self.__whseed()\n            return\n        a = hash(a)\n        a, x = divmod(a, 256)\n        a, y = divmod(a, 256)\n        a, z = divmod(a, 256)\n        x = (x + a) % 256 or 1\n        y = (y + a) % 256 or 1\n        z = (z + a) % 256 or 1\n        self.__whseed(x, y, z)\n\n## --------------- Operating System Random Source  ------------------\n\nclass SystemRandom(Random):\n    \"\"\"Alternate random number generator using sources provided\n    by the operating system (such as /dev/urandom on Unix or\n    CryptGenRandom on Windows).\n\n     Not available on all systems (see os.urandom() for details).\n    \"\"\"\n\n    def random(self):\n        \"\"\"Get the next random number in the range [0.0, 1.0).\"\"\"\n        return (long(_hexlify(_urandom(7)), 16) >> 3) * RECIP_BPF\n\n    def getrandbits(self, k):\n        \"\"\"getrandbits(k) -> x.  Generates a long int with k random bits.\"\"\"\n        if k <= 0:\n            raise ValueError('number of bits must be greater than zero')\n        if k != int(k):\n            raise TypeError('number of bits should be an integer')\n        bytes = (k + 7) // 8                    # bits / 8 and rounded up\n        x = long(_hexlify(_urandom(bytes)), 16)\n        return x >> (bytes * 8 - k)             # trim excess bits\n\n    def _stub(self, *args, **kwds):\n        \"Stub method.  Not used for a system random number generator.\"\n        return None\n    seed = jumpahead = _stub\n\n    def _notimplemented(self, *args, **kwds):\n        \"Method should not be called for a system random number generator.\"\n        raise NotImplementedError('System entropy source does not have state.')\n    getstate = setstate = _notimplemented\n\n## -------------------- test program --------------------\n\ndef _test_generator(n, func, args):\n    import time\n    print n, 'times', func.__name__\n    total = 0.0\n    sqsum = 0.0\n    smallest = 1e10\n    largest = -1e10\n    t0 = time.time()\n    for i in range(n):\n        x = func(*args)\n        total += x\n        sqsum = sqsum + x*x\n        smallest = min(x, smallest)\n        largest = max(x, largest)\n    t1 = time.time()\n    print round(t1-t0, 3), 'sec,',\n    avg = total/n\n    stddev = _sqrt(sqsum/n - avg*avg)\n    print 'avg %g, stddev %g, min %g, max %g' % \\\n              (avg, stddev, smallest, largest)\n\n\ndef _test(N=2000):\n    _test_generator(N, random, ())\n    _test_generator(N, normalvariate, (0.0, 1.0))\n    _test_generator(N, lognormvariate, (0.0, 1.0))\n    _test_generator(N, vonmisesvariate, (0.0, 1.0))\n    _test_generator(N, gammavariate, (0.01, 1.0))\n    _test_generator(N, gammavariate, (0.1, 1.0))\n    _test_generator(N, gammavariate, (0.1, 2.0))\n    _test_generator(N, gammavariate, (0.5, 1.0))\n    _test_generator(N, gammavariate, (0.9, 1.0))\n    _test_generator(N, gammavariate, (1.0, 1.0))\n    _test_generator(N, gammavariate, (2.0, 1.0))\n    _test_generator(N, gammavariate, (20.0, 1.0))\n    _test_generator(N, gammavariate, (200.0, 1.0))\n    _test_generator(N, gauss, (0.0, 1.0))\n    _test_generator(N, betavariate, (3.0, 3.0))\n    _test_generator(N, triangular, (0.0, 1.0, 1.0/3.0))\n\n# Create one instance, seeded from current time, and export its methods\n# as module-level functions.  The functions share state across all uses\n#(both in the user's code and in the Python libraries), but that's fine\n# for most programs and is easier for the casual user than making them\n# instantiate their own Random() instance.\n\n_inst = Random()\nseed = _inst.seed\nrandom = _inst.random\nuniform = _inst.uniform\ntriangular = _inst.triangular\nrandint = _inst.randint\nchoice = _inst.choice\nrandrange = _inst.randrange\nsample = _inst.sample\nshuffle = _inst.shuffle\nnormalvariate = _inst.normalvariate\nlognormvariate = _inst.lognormvariate\nexpovariate = _inst.expovariate\nvonmisesvariate = _inst.vonmisesvariate\ngammavariate = _inst.gammavariate\ngauss = _inst.gauss\nbetavariate = _inst.betavariate\nparetovariate = _inst.paretovariate\nweibullvariate = _inst.weibullvariate\ngetstate = _inst.getstate\nsetstate = _inst.setstate\njumpahead = _inst.jumpahead\ngetrandbits = _inst.getrandbits\n\nif __name__ == '__main__':\n    _test()\n", 
    "re": "#\n# Secret Labs' Regular Expression Engine\n#\n# re-compatible interface for the sre matching engine\n#\n# Copyright (c) 1998-2001 by Secret Labs AB.  All rights reserved.\n#\n# This version of the SRE library can be redistributed under CNRI's\n# Python 1.6 license.  For any other use, please contact Secret Labs\n# AB (info@pythonware.com).\n#\n# Portions of this engine have been developed in cooperation with\n# CNRI.  Hewlett-Packard provided funding for 1.6 integration and\n# other compatibility work.\n#\n\nr\"\"\"Support for regular expressions (RE).\n\nThis module provides regular expression matching operations similar to\nthose found in Perl.  It supports both 8-bit and Unicode strings; both\nthe pattern and the strings being processed can contain null bytes and\ncharacters outside the US ASCII range.\n\nRegular expressions can contain both special and ordinary characters.\nMost ordinary characters, like \"A\", \"a\", or \"0\", are the simplest\nregular expressions; they simply match themselves.  You can\nconcatenate ordinary characters, so last matches the string 'last'.\n\nThe special characters are:\n    \".\"      Matches any character except a newline.\n    \"^\"      Matches the start of the string.\n    \"$\"      Matches the end of the string or just before the newline at\n             the end of the string.\n    \"*\"      Matches 0 or more (greedy) repetitions of the preceding RE.\n             Greedy means that it will match as many repetitions as possible.\n    \"+\"      Matches 1 or more (greedy) repetitions of the preceding RE.\n    \"?\"      Matches 0 or 1 (greedy) of the preceding RE.\n    *?,+?,?? Non-greedy versions of the previous three special characters.\n    {m,n}    Matches from m to n repetitions of the preceding RE.\n    {m,n}?   Non-greedy version of the above.\n    \"\\\\\"     Either escapes special characters or signals a special sequence.\n    []       Indicates a set of characters.\n             A \"^\" as the first character indicates a complementing set.\n    \"|\"      A|B, creates an RE that will match either A or B.\n    (...)    Matches the RE inside the parentheses.\n             The contents can be retrieved or matched later in the string.\n    (?iLmsux) Set the I, L, M, S, U, or X flag for the RE (see below).\n    (?:...)  Non-grouping version of regular parentheses.\n    (?P<name>...) The substring matched by the group is accessible by name.\n    (?P=name)     Matches the text matched earlier by the group named name.\n    (?#...)  A comment; ignored.\n    (?=...)  Matches if ... matches next, but doesn't consume the string.\n    (?!...)  Matches if ... doesn't match next.\n    (?<=...) Matches if preceded by ... (must be fixed length).\n    (?<!...) Matches if not preceded by ... (must be fixed length).\n    (?(id/name)yes|no) Matches yes pattern if the group with id/name matched,\n                       the (optional) no pattern otherwise.\n\nThe special sequences consist of \"\\\\\" and a character from the list\nbelow.  If the ordinary character is not on the list, then the\nresulting RE will match the second character.\n    \\number  Matches the contents of the group of the same number.\n    \\A       Matches only at the start of the string.\n    \\Z       Matches only at the end of the string.\n    \\b       Matches the empty string, but only at the start or end of a word.\n    \\B       Matches the empty string, but not at the start or end of a word.\n    \\d       Matches any decimal digit; equivalent to the set [0-9].\n    \\D       Matches any non-digit character; equivalent to the set [^0-9].\n    \\s       Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v].\n    \\S       Matches any non-whitespace character; equiv. to [^ \\t\\n\\r\\f\\v].\n    \\w       Matches any alphanumeric character; equivalent to [a-zA-Z0-9_].\n             With LOCALE, it will match the set [0-9_] plus characters defined\n             as letters for the current locale.\n    \\W       Matches the complement of \\w.\n    \\\\       Matches a literal backslash.\n\nThis module exports the following functions:\n    match    Match a regular expression pattern to the beginning of a string.\n    search   Search a string for the presence of a pattern.\n    sub      Substitute occurrences of a pattern found in a string.\n    subn     Same as sub, but also return the number of substitutions made.\n    split    Split a string by the occurrences of a pattern.\n    findall  Find all occurrences of a pattern in a string.\n    finditer Return an iterator yielding a match object for each match.\n    compile  Compile a pattern into a RegexObject.\n    purge    Clear the regular expression cache.\n    escape   Backslash all non-alphanumerics in a string.\n\nSome of the functions in this module takes flags as optional parameters:\n    I  IGNORECASE  Perform case-insensitive matching.\n    L  LOCALE      Make \\w, \\W, \\b, \\B, dependent on the current locale.\n    M  MULTILINE   \"^\" matches the beginning of lines (after a newline)\n                   as well as the string.\n                   \"$\" matches the end of lines (before a newline) as well\n                   as the end of the string.\n    S  DOTALL      \".\" matches any character at all, including the newline.\n    X  VERBOSE     Ignore whitespace and comments for nicer looking RE's.\n    U  UNICODE     Make \\w, \\W, \\b, \\B, dependent on the Unicode locale.\n\nThis module also defines an exception 'error'.\n\n\"\"\"\n\nimport sys\nimport sre_compile\nimport sre_parse\n\n# public symbols\n__all__ = [ \"match\", \"search\", \"sub\", \"subn\", \"split\", \"findall\",\n    \"compile\", \"purge\", \"template\", \"escape\", \"I\", \"L\", \"M\", \"S\", \"X\",\n    \"U\", \"IGNORECASE\", \"LOCALE\", \"MULTILINE\", \"DOTALL\", \"VERBOSE\",\n    \"UNICODE\", \"error\" ]\n\n__version__ = \"2.2.1\"\n\n# flags\nI = IGNORECASE = sre_compile.SRE_FLAG_IGNORECASE # ignore case\nL = LOCALE = sre_compile.SRE_FLAG_LOCALE # assume current 8-bit locale\nU = UNICODE = sre_compile.SRE_FLAG_UNICODE # assume unicode locale\nM = MULTILINE = sre_compile.SRE_FLAG_MULTILINE # make anchors look for newline\nS = DOTALL = sre_compile.SRE_FLAG_DOTALL # make dot match newline\nX = VERBOSE = sre_compile.SRE_FLAG_VERBOSE # ignore whitespace and comments\n\n# sre extensions (experimental, don't rely on these)\nT = TEMPLATE = sre_compile.SRE_FLAG_TEMPLATE # disable backtracking\nDEBUG = sre_compile.SRE_FLAG_DEBUG # dump pattern after compilation\n\n# sre exception\nerror = sre_compile.error\n\n# --------------------------------------------------------------------\n# public interface\n\ndef match(pattern, string, flags=0):\n    \"\"\"Try to apply the pattern at the start of the string, returning\n    a match object, or None if no match was found.\"\"\"\n    return _compile(pattern, flags).match(string)\n\ndef search(pattern, string, flags=0):\n    \"\"\"Scan through string looking for a match to the pattern, returning\n    a match object, or None if no match was found.\"\"\"\n    return _compile(pattern, flags).search(string)\n\ndef sub(pattern, repl, string, count=0, flags=0):\n    \"\"\"Return the string obtained by replacing the leftmost\n    non-overlapping occurrences of the pattern in string by the\n    replacement repl.  repl can be either a string or a callable;\n    if a string, backslash escapes in it are processed.  If it is\n    a callable, it's passed the match object and must return\n    a replacement string to be used.\"\"\"\n    return _compile(pattern, flags).sub(repl, string, count)\n\ndef subn(pattern, repl, string, count=0, flags=0):\n    \"\"\"Return a 2-tuple containing (new_string, number).\n    new_string is the string obtained by replacing the leftmost\n    non-overlapping occurrences of the pattern in the source\n    string by the replacement repl.  number is the number of\n    substitutions that were made. repl can be either a string or a\n    callable; if a string, backslash escapes in it are processed.\n    If it is a callable, it's passed the match object and must\n    return a replacement string to be used.\"\"\"\n    return _compile(pattern, flags).subn(repl, string, count)\n\ndef split(pattern, string, maxsplit=0, flags=0):\n    \"\"\"Split the source string by the occurrences of the pattern,\n    returning a list containing the resulting substrings.\"\"\"\n    return _compile(pattern, flags).split(string, maxsplit)\n\ndef findall(pattern, string, flags=0):\n    \"\"\"Return a list of all non-overlapping matches in the string.\n\n    If one or more groups are present in the pattern, return a\n    list of groups; this will be a list of tuples if the pattern\n    has more than one group.\n\n    Empty matches are included in the result.\"\"\"\n    return _compile(pattern, flags).findall(string)\n\nif sys.hexversion >= 0x02020000:\n    __all__.append(\"finditer\")\n    def finditer(pattern, string, flags=0):\n        \"\"\"Return an iterator over all non-overlapping matches in the\n        string.  For each match, the iterator returns a match object.\n\n        Empty matches are included in the result.\"\"\"\n        return _compile(pattern, flags).finditer(string)\n\ndef compile(pattern, flags=0):\n    \"Compile a regular expression pattern, returning a pattern object.\"\n    return _compile(pattern, flags)\n\ndef purge():\n    \"Clear the regular expression cache\"\n    _cache.clear()\n    _cache_repl.clear()\n\ndef template(pattern, flags=0):\n    \"Compile a template pattern, returning a pattern object\"\n    return _compile(pattern, flags|T)\n\n_alphanum = frozenset(\n    \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n\ndef escape(pattern):\n    \"Escape all non-alphanumeric characters in pattern.\"\n    s = list(pattern)\n    alphanum = _alphanum\n    for i, c in enumerate(pattern):\n        if c not in alphanum:\n            if c == \"\\000\":\n                s[i] = \"\\\\000\"\n            else:\n                s[i] = \"\\\\\" + c\n    return pattern[:0].join(s)\n\n# --------------------------------------------------------------------\n# internals\n\n_cache = {}\n_cache_repl = {}\n\n_pattern_type = type(sre_compile.compile(\"\", 0))\n\n_MAXCACHE = 100\n\ndef _compile(*key):\n    # internal: compile pattern\n    pattern, flags = key\n    bypass_cache = flags & DEBUG\n    if not bypass_cache:\n        cachekey = (type(key[0]),) + key\n        p = _cache.get(cachekey)\n        if p is not None:\n            return p\n    if isinstance(pattern, _pattern_type):\n        if flags:\n            raise ValueError('Cannot process flags argument with a compiled pattern')\n        return pattern\n    if not sre_compile.isstring(pattern):\n        raise TypeError, \"first argument must be string or compiled pattern\"\n    try:\n        p = sre_compile.compile(pattern, flags)\n    except error, v:\n        raise error, v # invalid expression\n    if not bypass_cache:\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        _cache[cachekey] = p\n    return p\n\ndef _compile_repl(*key):\n    # internal: compile replacement pattern\n    p = _cache_repl.get(key)\n    if p is not None:\n        return p\n    repl, pattern = key\n    try:\n        p = sre_parse.parse_template(repl, pattern)\n    except error, v:\n        raise error, v # invalid expression\n    if len(_cache_repl) >= _MAXCACHE:\n        _cache_repl.clear()\n    _cache_repl[key] = p\n    return p\n\ndef _expand(pattern, match, template):\n    # internal: match.expand implementation hook\n    template = sre_parse.parse_template(template, pattern)\n    return sre_parse.expand_template(template, match)\n\ndef _subx(pattern, template):\n    # internal: pattern.sub/subn implementation helper\n    template = _compile_repl(template, pattern)\n    if not template[0] and len(template[1]) == 1:\n        # literal replacement\n        return template[1][0]\n    def filter(match, template=template):\n        return sre_parse.expand_template(template, match)\n    return filter\n\n# register myself for pickling\n\nimport copy_reg\n\ndef _pickle(p):\n    return _compile, (p.pattern, p.flags)\n\ncopy_reg.pickle(_pattern_type, _pickle, _compile)\n\n# --------------------------------------------------------------------\n# experimental stuff (see python-dev discussions for details)\n\nclass Scanner:\n    def __init__(self, lexicon, flags=0):\n        from sre_constants import BRANCH, SUBPATTERN\n        self.lexicon = lexicon\n        # combine phrases into a compound pattern\n        p = []\n        s = sre_parse.Pattern()\n        s.flags = flags\n        for phrase, action in lexicon:\n            p.append(sre_parse.SubPattern(s, [\n                (SUBPATTERN, (len(p)+1, sre_parse.parse(phrase, flags))),\n                ]))\n        s.groups = len(p)+1\n        p = sre_parse.SubPattern(s, [(BRANCH, (None, p))])\n        self.scanner = sre_compile.compile(p)\n    def scan(self, string):\n        result = []\n        append = result.append\n        match = self.scanner.scanner(string).match\n        i = 0\n        while 1:\n            m = match()\n            if not m:\n                break\n            j = m.end()\n            if i == j:\n                break\n            action = self.lexicon[m.lastindex-1][1]\n            if hasattr(action, '__call__'):\n                self.match = m\n                action = action(self, m.group())\n            if action is not None:\n                append(action)\n            i = j\n        return result, string[i:]\n", 
    "repr": "\"\"\"Redo the builtin repr() (representation) but with limits on most sizes.\"\"\"\n\n__all__ = [\"Repr\",\"repr\"]\n\nimport __builtin__\nfrom itertools import islice\n\nclass Repr:\n\n    def __init__(self):\n        self.maxlevel = 6\n        self.maxtuple = 6\n        self.maxlist = 6\n        self.maxarray = 5\n        self.maxdict = 4\n        self.maxset = 6\n        self.maxfrozenset = 6\n        self.maxdeque = 6\n        self.maxstring = 30\n        self.maxlong = 40\n        self.maxother = 20\n\n    def repr(self, x):\n        return self.repr1(x, self.maxlevel)\n\n    def repr1(self, x, level):\n        typename = type(x).__name__\n        if ' ' in typename:\n            parts = typename.split()\n            typename = '_'.join(parts)\n        if hasattr(self, 'repr_' + typename):\n            return getattr(self, 'repr_' + typename)(x, level)\n        else:\n            s = __builtin__.repr(x)\n            if len(s) > self.maxother:\n                i = max(0, (self.maxother-3)//2)\n                j = max(0, self.maxother-3-i)\n                s = s[:i] + '...' + s[len(s)-j:]\n            return s\n\n    def _repr_iterable(self, x, level, left, right, maxiter, trail=''):\n        n = len(x)\n        if level <= 0 and n:\n            s = '...'\n        else:\n            newlevel = level - 1\n            repr1 = self.repr1\n            pieces = [repr1(elem, newlevel) for elem in islice(x, maxiter)]\n            if n > maxiter:  pieces.append('...')\n            s = ', '.join(pieces)\n            if n == 1 and trail:  right = trail + right\n        return '%s%s%s' % (left, s, right)\n\n    def repr_tuple(self, x, level):\n        return self._repr_iterable(x, level, '(', ')', self.maxtuple, ',')\n\n    def repr_list(self, x, level):\n        return self._repr_iterable(x, level, '[', ']', self.maxlist)\n\n    def repr_array(self, x, level):\n        header = \"array('%s', [\" % x.typecode\n        return self._repr_iterable(x, level, header, '])', self.maxarray)\n\n    def repr_set(self, x, level):\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, 'set([', '])', self.maxset)\n\n    def repr_frozenset(self, x, level):\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, 'frozenset([', '])',\n                                   self.maxfrozenset)\n\n    def repr_deque(self, x, level):\n        return self._repr_iterable(x, level, 'deque([', '])', self.maxdeque)\n\n    def repr_dict(self, x, level):\n        n = len(x)\n        if n == 0: return '{}'\n        if level <= 0: return '{...}'\n        newlevel = level - 1\n        repr1 = self.repr1\n        pieces = []\n        for key in islice(_possibly_sorted(x), self.maxdict):\n            keyrepr = repr1(key, newlevel)\n            valrepr = repr1(x[key], newlevel)\n            pieces.append('%s: %s' % (keyrepr, valrepr))\n        if n > self.maxdict: pieces.append('...')\n        s = ', '.join(pieces)\n        return '{%s}' % (s,)\n\n    def repr_str(self, x, level):\n        s = __builtin__.repr(x[:self.maxstring])\n        if len(s) > self.maxstring:\n            i = max(0, (self.maxstring-3)//2)\n            j = max(0, self.maxstring-3-i)\n            s = __builtin__.repr(x[:i] + x[len(x)-j:])\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s\n\n    def repr_long(self, x, level):\n        s = __builtin__.repr(x) # XXX Hope this isn't too slow...\n        if len(s) > self.maxlong:\n            i = max(0, (self.maxlong-3)//2)\n            j = max(0, self.maxlong-3-i)\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s\n\n    def repr_instance(self, x, level):\n        try:\n            s = __builtin__.repr(x)\n            # Bugs in x.__repr__() can cause arbitrary\n            # exceptions -- then make up something\n        except Exception:\n            return '<%s instance at %x>' % (x.__class__.__name__, id(x))\n        if len(s) > self.maxstring:\n            i = max(0, (self.maxstring-3)//2)\n            j = max(0, self.maxstring-3-i)\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s\n\n\ndef _possibly_sorted(x):\n    # Since not all sequences of items can be sorted and comparison\n    # functions may raise arbitrary exceptions, return an unsorted\n    # sequence in that case.\n    try:\n        return sorted(x)\n    except Exception:\n        return list(x)\n\naRepr = Repr()\nrepr = aRepr.repr\n", 
    "rfc822": "\"\"\"RFC 2822 message manipulation.\n\nNote: This is only a very rough sketch of a full RFC-822 parser; in particular\nthe tokenizing of addresses does not adhere to all the quoting rules.\n\nNote: RFC 2822 is a long awaited update to RFC 822.  This module should\nconform to RFC 2822, and is thus mis-named (it's not worth renaming it).  Some\neffort at RFC 2822 updates have been made, but a thorough audit has not been\nperformed.  Consider any RFC 2822 non-conformance to be a bug.\n\n    RFC 2822: http://www.faqs.org/rfcs/rfc2822.html\n    RFC 822 : http://www.faqs.org/rfcs/rfc822.html (obsolete)\n\nDirections for use:\n\nTo create a Message object: first open a file, e.g.:\n\n  fp = open(file, 'r')\n\nYou can use any other legal way of getting an open file object, e.g. use\nsys.stdin or call os.popen().  Then pass the open file object to the Message()\nconstructor:\n\n  m = Message(fp)\n\nThis class can work with any input object that supports a readline method.  If\nthe input object has seek and tell capability, the rewindbody method will\nwork; also illegal lines will be pushed back onto the input stream.  If the\ninput object lacks seek but has an `unread' method that can push back a line\nof input, Message will use that to push back illegal lines.  Thus this class\ncan be used to parse messages coming from a buffered stream.\n\nThe optional `seekable' argument is provided as a workaround for certain stdio\nlibraries in which tell() discards buffered data before discovering that the\nlseek() system call doesn't work.  For maximum portability, you should set the\nseekable argument to zero to prevent that initial \\code{tell} when passing in\nan unseekable object such as a file object created from a socket object.  If\nit is 1 on entry -- which it is by default -- the tell() method of the open\nfile object is called once; if this raises an exception, seekable is reset to\n0.  For other nonzero values of seekable, this test is not made.\n\nTo get the text of a particular header there are several methods:\n\n  str = m.getheader(name)\n  str = m.getrawheader(name)\n\nwhere name is the name of the header, e.g. 'Subject'.  The difference is that\ngetheader() strips the leading and trailing whitespace, while getrawheader()\ndoesn't.  Both functions retain embedded whitespace (including newlines)\nexactly as they are specified in the header, and leave the case of the text\nunchanged.\n\nFor addresses and address lists there are functions\n\n  realname, mailaddress = m.getaddr(name)\n  list = m.getaddrlist(name)\n\nwhere the latter returns a list of (realname, mailaddr) tuples.\n\nThere is also a method\n\n  time = m.getdate(name)\n\nwhich parses a Date-like field and returns a time-compatible tuple,\ni.e. a tuple such as returned by time.localtime() or accepted by\ntime.mktime().\n\nSee the class definition for lower level access methods.\n\nThere are also some utility functions here.\n\"\"\"\n# Cleanup and extensions by Eric S. Raymond <esr@thyrsus.com>\n\nimport time\n\nfrom warnings import warnpy3k\nwarnpy3k(\"in 3.x, rfc822 has been removed in favor of the email package\",\n         stacklevel=2)\n\n__all__ = [\"Message\",\"AddressList\",\"parsedate\",\"parsedate_tz\",\"mktime_tz\"]\n\n_blanklines = ('\\r\\n', '\\n')            # Optimization for islast()\n\n\nclass Message:\n    \"\"\"Represents a single RFC 2822-compliant message.\"\"\"\n\n    def __init__(self, fp, seekable = 1):\n        \"\"\"Initialize the class instance and read the headers.\"\"\"\n        if seekable == 1:\n            # Exercise tell() to make sure it works\n            # (and then assume seek() works, too)\n            try:\n                fp.tell()\n            except (AttributeError, IOError):\n                seekable = 0\n        self.fp = fp\n        self.seekable = seekable\n        self.startofheaders = None\n        self.startofbody = None\n        #\n        if self.seekable:\n            try:\n                self.startofheaders = self.fp.tell()\n            except IOError:\n                self.seekable = 0\n        #\n        self.readheaders()\n        #\n        if self.seekable:\n            try:\n                self.startofbody = self.fp.tell()\n            except IOError:\n                self.seekable = 0\n\n    def rewindbody(self):\n        \"\"\"Rewind the file to the start of the body (if seekable).\"\"\"\n        if not self.seekable:\n            raise IOError, \"unseekable file\"\n        self.fp.seek(self.startofbody)\n\n    def readheaders(self):\n        \"\"\"Read header lines.\n\n        Read header lines up to the entirely blank line that terminates them.\n        The (normally blank) line that ends the headers is skipped, but not\n        included in the returned list.  If a non-header line ends the headers,\n        (which is an error), an attempt is made to backspace over it; it is\n        never included in the returned list.\n\n        The variable self.status is set to the empty string if all went well,\n        otherwise it is an error message.  The variable self.headers is a\n        completely uninterpreted list of lines contained in the header (so\n        printing them will reproduce the header exactly as it appears in the\n        file).\n        \"\"\"\n        self.dict = {}\n        self.unixfrom = ''\n        self.headers = lst = []\n        self.status = ''\n        headerseen = \"\"\n        firstline = 1\n        startofline = unread = tell = None\n        if hasattr(self.fp, 'unread'):\n            unread = self.fp.unread\n        elif self.seekable:\n            tell = self.fp.tell\n        while 1:\n            if tell:\n                try:\n                    startofline = tell()\n                except IOError:\n                    startofline = tell = None\n                    self.seekable = 0\n            line = self.fp.readline()\n            if not line:\n                self.status = 'EOF in headers'\n                break\n            # Skip unix From name time lines\n            if firstline and line.startswith('From '):\n                self.unixfrom = self.unixfrom + line\n                continue\n            firstline = 0\n            if headerseen and line[0] in ' \\t':\n                # It's a continuation line.\n                lst.append(line)\n                x = (self.dict[headerseen] + \"\\n \" + line.strip())\n                self.dict[headerseen] = x.strip()\n                continue\n            elif self.iscomment(line):\n                # It's a comment.  Ignore it.\n                continue\n            elif self.islast(line):\n                # Note! No pushback here!  The delimiter line gets eaten.\n                break\n            headerseen = self.isheader(line)\n            if headerseen:\n                # It's a legal header line, save it.\n                lst.append(line)\n                self.dict[headerseen] = line[len(headerseen)+1:].strip()\n                continue\n            else:\n                # It's not a header line; throw it back and stop here.\n                if not self.dict:\n                    self.status = 'No headers'\n                else:\n                    self.status = 'Non-header line where header expected'\n                # Try to undo the read.\n                if unread:\n                    unread(line)\n                elif tell:\n                    self.fp.seek(startofline)\n                else:\n                    self.status = self.status + '; bad seek'\n                break\n\n    def isheader(self, line):\n        \"\"\"Determine whether a given line is a legal header.\n\n        This method should return the header name, suitably canonicalized.\n        You may override this method in order to use Message parsing on tagged\n        data in RFC 2822-like formats with special header formats.\n        \"\"\"\n        i = line.find(':')\n        if i > 0:\n            return line[:i].lower()\n        return None\n\n    def islast(self, line):\n        \"\"\"Determine whether a line is a legal end of RFC 2822 headers.\n\n        You may override this method if your application wants to bend the\n        rules, e.g. to strip trailing whitespace, or to recognize MH template\n        separators ('--------').  For convenience (e.g. for code reading from\n        sockets) a line consisting of \\\\r\\\\n also matches.\n        \"\"\"\n        return line in _blanklines\n\n    def iscomment(self, line):\n        \"\"\"Determine whether a line should be skipped entirely.\n\n        You may override this method in order to use Message parsing on tagged\n        data in RFC 2822-like formats that support embedded comments or\n        free-text data.\n        \"\"\"\n        return False\n\n    def getallmatchingheaders(self, name):\n        \"\"\"Find all header lines matching a given header name.\n\n        Look through the list of headers and find all lines matching a given\n        header name (and their continuation lines).  A list of the lines is\n        returned, without interpretation.  If the header does not occur, an\n        empty list is returned.  If the header occurs multiple times, all\n        occurrences are returned.  Case is not important in the header name.\n        \"\"\"\n        name = name.lower() + ':'\n        n = len(name)\n        lst = []\n        hit = 0\n        for line in self.headers:\n            if line[:n].lower() == name:\n                hit = 1\n            elif not line[:1].isspace():\n                hit = 0\n            if hit:\n                lst.append(line)\n        return lst\n\n    def getfirstmatchingheader(self, name):\n        \"\"\"Get the first header line matching name.\n\n        This is similar to getallmatchingheaders, but it returns only the\n        first matching header (and its continuation lines).\n        \"\"\"\n        name = name.lower() + ':'\n        n = len(name)\n        lst = []\n        hit = 0\n        for line in self.headers:\n            if hit:\n                if not line[:1].isspace():\n                    break\n            elif line[:n].lower() == name:\n                hit = 1\n            if hit:\n                lst.append(line)\n        return lst\n\n    def getrawheader(self, name):\n        \"\"\"A higher-level interface to getfirstmatchingheader().\n\n        Return a string containing the literal text of the header but with the\n        keyword stripped.  All leading, trailing and embedded whitespace is\n        kept in the string, however.  Return None if the header does not\n        occur.\n        \"\"\"\n\n        lst = self.getfirstmatchingheader(name)\n        if not lst:\n            return None\n        lst[0] = lst[0][len(name) + 1:]\n        return ''.join(lst)\n\n    def getheader(self, name, default=None):\n        \"\"\"Get the header value for a name.\n\n        This is the normal interface: it returns a stripped version of the\n        header value for a given header name, or None if it doesn't exist.\n        This uses the dictionary version which finds the *last* such header.\n        \"\"\"\n        return self.dict.get(name.lower(), default)\n    get = getheader\n\n    def getheaders(self, name):\n        \"\"\"Get all values for a header.\n\n        This returns a list of values for headers given more than once; each\n        value in the result list is stripped in the same way as the result of\n        getheader().  If the header is not given, return an empty list.\n        \"\"\"\n        result = []\n        current = ''\n        have_header = 0\n        for s in self.getallmatchingheaders(name):\n            if s[0].isspace():\n                if current:\n                    current = \"%s\\n %s\" % (current, s.strip())\n                else:\n                    current = s.strip()\n            else:\n                if have_header:\n                    result.append(current)\n                current = s[s.find(\":\") + 1:].strip()\n                have_header = 1\n        if have_header:\n            result.append(current)\n        return result\n\n    def getaddr(self, name):\n        \"\"\"Get a single address from a header, as a tuple.\n\n        An example return value:\n        ('Guido van Rossum', 'guido@cwi.nl')\n        \"\"\"\n        # New, by Ben Escoto\n        alist = self.getaddrlist(name)\n        if alist:\n            return alist[0]\n        else:\n            return (None, None)\n\n    def getaddrlist(self, name):\n        \"\"\"Get a list of addresses from a header.\n\n        Retrieves a list of addresses from a header, where each address is a\n        tuple as returned by getaddr().  Scans all named headers, so it works\n        properly with multiple To: or Cc: headers for example.\n        \"\"\"\n        raw = []\n        for h in self.getallmatchingheaders(name):\n            if h[0] in ' \\t':\n                raw.append(h)\n            else:\n                if raw:\n                    raw.append(', ')\n                i = h.find(':')\n                if i > 0:\n                    addr = h[i+1:]\n                raw.append(addr)\n        alladdrs = ''.join(raw)\n        a = AddressList(alladdrs)\n        return a.addresslist\n\n    def getdate(self, name):\n        \"\"\"Retrieve a date field from a header.\n\n        Retrieves a date field from the named header, returning a tuple\n        compatible with time.mktime().\n        \"\"\"\n        try:\n            data = self[name]\n        except KeyError:\n            return None\n        return parsedate(data)\n\n    def getdate_tz(self, name):\n        \"\"\"Retrieve a date field from a header as a 10-tuple.\n\n        The first 9 elements make up a tuple compatible with time.mktime(),\n        and the 10th is the offset of the poster's time zone from GMT/UTC.\n        \"\"\"\n        try:\n            data = self[name]\n        except KeyError:\n            return None\n        return parsedate_tz(data)\n\n\n    # Access as a dictionary (only finds *last* header of each type):\n\n    def __len__(self):\n        \"\"\"Get the number of headers in a message.\"\"\"\n        return len(self.dict)\n\n    def __getitem__(self, name):\n        \"\"\"Get a specific header, as from a dictionary.\"\"\"\n        return self.dict[name.lower()]\n\n    def __setitem__(self, name, value):\n        \"\"\"Set the value of a header.\n\n        Note: This is not a perfect inversion of __getitem__, because any\n        changed headers get stuck at the end of the raw-headers list rather\n        than where the altered header was.\n        \"\"\"\n        del self[name] # Won't fail if it doesn't exist\n        self.dict[name.lower()] = value\n        text = name + \": \" + value\n        for line in text.split(\"\\n\"):\n            self.headers.append(line + \"\\n\")\n\n    def __delitem__(self, name):\n        \"\"\"Delete all occurrences of a specific header, if it is present.\"\"\"\n        name = name.lower()\n        if not name in self.dict:\n            return\n        del self.dict[name]\n        name = name + ':'\n        n = len(name)\n        lst = []\n        hit = 0\n        for i in range(len(self.headers)):\n            line = self.headers[i]\n            if line[:n].lower() == name:\n                hit = 1\n            elif not line[:1].isspace():\n                hit = 0\n            if hit:\n                lst.append(i)\n        for i in reversed(lst):\n            del self.headers[i]\n\n    def setdefault(self, name, default=\"\"):\n        lowername = name.lower()\n        if lowername in self.dict:\n            return self.dict[lowername]\n        else:\n            text = name + \": \" + default\n            for line in text.split(\"\\n\"):\n                self.headers.append(line + \"\\n\")\n            self.dict[lowername] = default\n            return default\n\n    def has_key(self, name):\n        \"\"\"Determine whether a message contains the named header.\"\"\"\n        return name.lower() in self.dict\n\n    def __contains__(self, name):\n        \"\"\"Determine whether a message contains the named header.\"\"\"\n        return name.lower() in self.dict\n\n    def __iter__(self):\n        return iter(self.dict)\n\n    def keys(self):\n        \"\"\"Get all of a message's header field names.\"\"\"\n        return self.dict.keys()\n\n    def values(self):\n        \"\"\"Get all of a message's header field values.\"\"\"\n        return self.dict.values()\n\n    def items(self):\n        \"\"\"Get all of a message's headers.\n\n        Returns a list of name, value tuples.\n        \"\"\"\n        return self.dict.items()\n\n    def __str__(self):\n        return ''.join(self.headers)\n\n\n# Utility functions\n# -----------------\n\n# XXX Should fix unquote() and quote() to be really conformant.\n# XXX The inverses of the parse functions may also be useful.\n\n\ndef unquote(s):\n    \"\"\"Remove quotes from a string.\"\"\"\n    if len(s) > 1:\n        if s.startswith('\"') and s.endswith('\"'):\n            return s[1:-1].replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n        if s.startswith('<') and s.endswith('>'):\n            return s[1:-1]\n    return s\n\n\ndef quote(s):\n    \"\"\"Add quotes around a string.\"\"\"\n    return s.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n\n\ndef parseaddr(address):\n    \"\"\"Parse an address into a (realname, mailaddr) tuple.\"\"\"\n    a = AddressList(address)\n    lst = a.addresslist\n    if not lst:\n        return (None, None)\n    return lst[0]\n\n\nclass AddrlistClass:\n    \"\"\"Address parser class by Ben Escoto.\n\n    To understand what this class does, it helps to have a copy of\n    RFC 2822 in front of you.\n\n    http://www.faqs.org/rfcs/rfc2822.html\n\n    Note: this class interface is deprecated and may be removed in the future.\n    Use rfc822.AddressList instead.\n    \"\"\"\n\n    def __init__(self, field):\n        \"\"\"Initialize a new instance.\n\n        `field' is an unparsed address header field, containing one or more\n        addresses.\n        \"\"\"\n        self.specials = '()<>@,:;.\\\"[]'\n        self.pos = 0\n        self.LWS = ' \\t'\n        self.CR = '\\r\\n'\n        self.atomends = self.specials + self.LWS + self.CR\n        # Note that RFC 2822 now specifies `.' as obs-phrase, meaning that it\n        # is obsolete syntax.  RFC 2822 requires that we recognize obsolete\n        # syntax, so allow dots in phrases.\n        self.phraseends = self.atomends.replace('.', '')\n        self.field = field\n        self.commentlist = []\n\n    def gotonext(self):\n        \"\"\"Parse up to the start of the next address.\"\"\"\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS + '\\n\\r':\n                self.pos = self.pos + 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            else: break\n\n    def getaddrlist(self):\n        \"\"\"Parse all addresses.\n\n        Returns a list containing all of the addresses.\n        \"\"\"\n        result = []\n        ad = self.getaddress()\n        while ad:\n            result += ad\n            ad = self.getaddress()\n        return result\n\n    def getaddress(self):\n        \"\"\"Parse the next address.\"\"\"\n        self.commentlist = []\n        self.gotonext()\n\n        oldpos = self.pos\n        oldcl = self.commentlist\n        plist = self.getphraselist()\n\n        self.gotonext()\n        returnlist = []\n\n        if self.pos >= len(self.field):\n            # Bad email address technically, no domain.\n            if plist:\n                returnlist = [(' '.join(self.commentlist), plist[0])]\n\n        elif self.field[self.pos] in '.@':\n            # email address is just an addrspec\n            # this isn't very efficient since we start over\n            self.pos = oldpos\n            self.commentlist = oldcl\n            addrspec = self.getaddrspec()\n            returnlist = [(' '.join(self.commentlist), addrspec)]\n\n        elif self.field[self.pos] == ':':\n            # address is a group\n            returnlist = []\n\n            fieldlen = len(self.field)\n            self.pos += 1\n            while self.pos < len(self.field):\n                self.gotonext()\n                if self.pos < fieldlen and self.field[self.pos] == ';':\n                    self.pos += 1\n                    break\n                returnlist = returnlist + self.getaddress()\n\n        elif self.field[self.pos] == '<':\n            # Address is a phrase then a route addr\n            routeaddr = self.getrouteaddr()\n\n            if self.commentlist:\n                returnlist = [(' '.join(plist) + ' (' + \\\n                         ' '.join(self.commentlist) + ')', routeaddr)]\n            else: returnlist = [(' '.join(plist), routeaddr)]\n\n        else:\n            if plist:\n                returnlist = [(' '.join(self.commentlist), plist[0])]\n            elif self.field[self.pos] in self.specials:\n                self.pos += 1\n\n        self.gotonext()\n        if self.pos < len(self.field) and self.field[self.pos] == ',':\n            self.pos += 1\n        return returnlist\n\n    def getrouteaddr(self):\n        \"\"\"Parse a route address (Return-path value).\n\n        This method just skips all the route stuff and returns the addrspec.\n        \"\"\"\n        if self.field[self.pos] != '<':\n            return\n\n        expectroute = 0\n        self.pos += 1\n        self.gotonext()\n        adlist = \"\"\n        while self.pos < len(self.field):\n            if expectroute:\n                self.getdomain()\n                expectroute = 0\n            elif self.field[self.pos] == '>':\n                self.pos += 1\n                break\n            elif self.field[self.pos] == '@':\n                self.pos += 1\n                expectroute = 1\n            elif self.field[self.pos] == ':':\n                self.pos += 1\n            else:\n                adlist = self.getaddrspec()\n                self.pos += 1\n                break\n            self.gotonext()\n\n        return adlist\n\n    def getaddrspec(self):\n        \"\"\"Parse an RFC 2822 addr-spec.\"\"\"\n        aslist = []\n\n        self.gotonext()\n        while self.pos < len(self.field):\n            if self.field[self.pos] == '.':\n                aslist.append('.')\n                self.pos += 1\n            elif self.field[self.pos] == '\"':\n                aslist.append('\"%s\"' % self.getquote())\n            elif self.field[self.pos] in self.atomends:\n                break\n            else: aslist.append(self.getatom())\n            self.gotonext()\n\n        if self.pos >= len(self.field) or self.field[self.pos] != '@':\n            return ''.join(aslist)\n\n        aslist.append('@')\n        self.pos += 1\n        self.gotonext()\n        return ''.join(aslist) + self.getdomain()\n\n    def getdomain(self):\n        \"\"\"Get the complete domain name from an address.\"\"\"\n        sdlist = []\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS:\n                self.pos += 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            elif self.field[self.pos] == '[':\n                sdlist.append(self.getdomainliteral())\n            elif self.field[self.pos] == '.':\n                self.pos += 1\n                sdlist.append('.')\n            elif self.field[self.pos] in self.atomends:\n                break\n            else: sdlist.append(self.getatom())\n        return ''.join(sdlist)\n\n    def getdelimited(self, beginchar, endchars, allowcomments = 1):\n        \"\"\"Parse a header fragment delimited by special characters.\n\n        `beginchar' is the start character for the fragment.  If self is not\n        looking at an instance of `beginchar' then getdelimited returns the\n        empty string.\n\n        `endchars' is a sequence of allowable end-delimiting characters.\n        Parsing stops when one of these is encountered.\n\n        If `allowcomments' is non-zero, embedded RFC 2822 comments are allowed\n        within the parsed fragment.\n        \"\"\"\n        if self.field[self.pos] != beginchar:\n            return ''\n\n        slist = ['']\n        quote = 0\n        self.pos += 1\n        while self.pos < len(self.field):\n            if quote == 1:\n                slist.append(self.field[self.pos])\n                quote = 0\n            elif self.field[self.pos] in endchars:\n                self.pos += 1\n                break\n            elif allowcomments and self.field[self.pos] == '(':\n                slist.append(self.getcomment())\n                continue        # have already advanced pos from getcomment\n            elif self.field[self.pos] == '\\\\':\n                quote = 1\n            else:\n                slist.append(self.field[self.pos])\n            self.pos += 1\n\n        return ''.join(slist)\n\n    def getquote(self):\n        \"\"\"Get a quote-delimited fragment from self's field.\"\"\"\n        return self.getdelimited('\"', '\"\\r', 0)\n\n    def getcomment(self):\n        \"\"\"Get a parenthesis-delimited fragment from self's field.\"\"\"\n        return self.getdelimited('(', ')\\r', 1)\n\n    def getdomainliteral(self):\n        \"\"\"Parse an RFC 2822 domain-literal.\"\"\"\n        return '[%s]' % self.getdelimited('[', ']\\r', 0)\n\n    def getatom(self, atomends=None):\n        \"\"\"Parse an RFC 2822 atom.\n\n        Optional atomends specifies a different set of end token delimiters\n        (the default is to use self.atomends).  This is used e.g. in\n        getphraselist() since phrase endings must not include the `.' (which\n        is legal in phrases).\"\"\"\n        atomlist = ['']\n        if atomends is None:\n            atomends = self.atomends\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in atomends:\n                break\n            else: atomlist.append(self.field[self.pos])\n            self.pos += 1\n\n        return ''.join(atomlist)\n\n    def getphraselist(self):\n        \"\"\"Parse a sequence of RFC 2822 phrases.\n\n        A phrase is a sequence of words, which are in turn either RFC 2822\n        atoms or quoted-strings.  Phrases are canonicalized by squeezing all\n        runs of continuous whitespace into one space.\n        \"\"\"\n        plist = []\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS:\n                self.pos += 1\n            elif self.field[self.pos] == '\"':\n                plist.append(self.getquote())\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            elif self.field[self.pos] in self.phraseends:\n                break\n            else:\n                plist.append(self.getatom(self.phraseends))\n\n        return plist\n\nclass AddressList(AddrlistClass):\n    \"\"\"An AddressList encapsulates a list of parsed RFC 2822 addresses.\"\"\"\n    def __init__(self, field):\n        AddrlistClass.__init__(self, field)\n        if field:\n            self.addresslist = self.getaddrlist()\n        else:\n            self.addresslist = []\n\n    def __len__(self):\n        return len(self.addresslist)\n\n    def __str__(self):\n        return \", \".join(map(dump_address_pair, self.addresslist))\n\n    def __add__(self, other):\n        # Set union\n        newaddr = AddressList(None)\n        newaddr.addresslist = self.addresslist[:]\n        for x in other.addresslist:\n            if not x in self.addresslist:\n                newaddr.addresslist.append(x)\n        return newaddr\n\n    def __iadd__(self, other):\n        # Set union, in-place\n        for x in other.addresslist:\n            if not x in self.addresslist:\n                self.addresslist.append(x)\n        return self\n\n    def __sub__(self, other):\n        # Set difference\n        newaddr = AddressList(None)\n        for x in self.addresslist:\n            if not x in other.addresslist:\n                newaddr.addresslist.append(x)\n        return newaddr\n\n    def __isub__(self, other):\n        # Set difference, in-place\n        for x in other.addresslist:\n            if x in self.addresslist:\n                self.addresslist.remove(x)\n        return self\n\n    def __getitem__(self, index):\n        # Make indexing, slices, and 'in' work\n        return self.addresslist[index]\n\ndef dump_address_pair(pair):\n    \"\"\"Dump a (name, address) pair in a canonicalized form.\"\"\"\n    if pair[0]:\n        return '\"' + pair[0] + '\" <' + pair[1] + '>'\n    else:\n        return pair[1]\n\n# Parse a date field\n\n_monthnames = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul',\n               'aug', 'sep', 'oct', 'nov', 'dec',\n               'january', 'february', 'march', 'april', 'may', 'june', 'july',\n               'august', 'september', 'october', 'november', 'december']\n_daynames = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n\n# The timezone table does not include the military time zones defined\n# in RFC822, other than Z.  According to RFC1123, the description in\n# RFC822 gets the signs wrong, so we can't rely on any such time\n# zones.  RFC1123 recommends that numeric timezone indicators be used\n# instead of timezone names.\n\n_timezones = {'UT':0, 'UTC':0, 'GMT':0, 'Z':0,\n              'AST': -400, 'ADT': -300,  # Atlantic (used in Canada)\n              'EST': -500, 'EDT': -400,  # Eastern\n              'CST': -600, 'CDT': -500,  # Central\n              'MST': -700, 'MDT': -600,  # Mountain\n              'PST': -800, 'PDT': -700   # Pacific\n              }\n\n\ndef parsedate_tz(data):\n    \"\"\"Convert a date string to a time tuple.\n\n    Accounts for military timezones.\n    \"\"\"\n    if not data:\n        return None\n    data = data.split()\n    if data[0][-1] in (',', '.') or data[0].lower() in _daynames:\n        # There's a dayname here. Skip it\n        del data[0]\n    else:\n        # no space after the \"weekday,\"?\n        i = data[0].rfind(',')\n        if i >= 0:\n            data[0] = data[0][i+1:]\n    if len(data) == 3: # RFC 850 date, deprecated\n        stuff = data[0].split('-')\n        if len(stuff) == 3:\n            data = stuff + data[1:]\n    if len(data) == 4:\n        s = data[3]\n        i = s.find('+')\n        if i > 0:\n            data[3:] = [s[:i], s[i+1:]]\n        else:\n            data.append('') # Dummy tz\n    if len(data) < 5:\n        return None\n    data = data[:5]\n    [dd, mm, yy, tm, tz] = data\n    mm = mm.lower()\n    if not mm in _monthnames:\n        dd, mm = mm, dd.lower()\n        if not mm in _monthnames:\n            return None\n    mm = _monthnames.index(mm)+1\n    if mm > 12: mm = mm - 12\n    if dd[-1] == ',':\n        dd = dd[:-1]\n    i = yy.find(':')\n    if i > 0:\n        yy, tm = tm, yy\n    if yy[-1] == ',':\n        yy = yy[:-1]\n    if not yy[0].isdigit():\n        yy, tz = tz, yy\n    if tm[-1] == ',':\n        tm = tm[:-1]\n    tm = tm.split(':')\n    if len(tm) == 2:\n        [thh, tmm] = tm\n        tss = '0'\n    elif len(tm) == 3:\n        [thh, tmm, tss] = tm\n    else:\n        return None\n    try:\n        yy = int(yy)\n        dd = int(dd)\n        thh = int(thh)\n        tmm = int(tmm)\n        tss = int(tss)\n    except ValueError:\n        return None\n    tzoffset = None\n    tz = tz.upper()\n    if tz in _timezones:\n        tzoffset = _timezones[tz]\n    else:\n        try:\n            tzoffset = int(tz)\n        except ValueError:\n            pass\n    # Convert a timezone offset into seconds ; -0500 -> -18000\n    if tzoffset:\n        if tzoffset < 0:\n            tzsign = -1\n            tzoffset = -tzoffset\n        else:\n            tzsign = 1\n        tzoffset = tzsign * ( (tzoffset//100)*3600 + (tzoffset % 100)*60)\n    return (yy, mm, dd, thh, tmm, tss, 0, 1, 0, tzoffset)\n\n\ndef parsedate(data):\n    \"\"\"Convert a time string to a time tuple.\"\"\"\n    t = parsedate_tz(data)\n    if t is None:\n        return t\n    return t[:9]\n\n\ndef mktime_tz(data):\n    \"\"\"Turn a 10-tuple as returned by parsedate_tz() into a UTC timestamp.\"\"\"\n    if data[9] is None:\n        # No zone info, so localtime is better assumption than GMT\n        return time.mktime(data[:8] + (-1,))\n    else:\n        t = time.mktime(data[:8] + (0,))\n        return t - data[9] - time.timezone\n\ndef formatdate(timeval=None):\n    \"\"\"Returns time format preferred for Internet standards.\n\n    Sun, 06 Nov 1994 08:49:37 GMT  ; RFC 822, updated by RFC 1123\n\n    According to RFC 1123, day and month names must always be in\n    English.  If not for that, this code could use strftime().  It\n    can't because strftime() honors the locale and could generated\n    non-English names.\n    \"\"\"\n    if timeval is None:\n        timeval = time.time()\n    timeval = time.gmtime(timeval)\n    return \"%s, %02d %s %04d %02d:%02d:%02d GMT\" % (\n            (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")[timeval[6]],\n            timeval[2],\n            (\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n             \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")[timeval[1]-1],\n                                timeval[0], timeval[3], timeval[4], timeval[5])\n\n\n# When used as script, run a small test program.\n# The first command line argument must be a filename containing one\n# message in RFC-822 format.\n\nif __name__ == '__main__':\n    import sys, os\n    file = os.path.join(os.environ['HOME'], 'Mail/inbox/1')\n    if sys.argv[1:]: file = sys.argv[1]\n    f = open(file, 'r')\n    m = Message(f)\n    print 'From:', m.getaddr('from')\n    print 'To:', m.getaddrlist('to')\n    print 'Subject:', m.getheader('subject')\n    print 'Date:', m.getheader('date')\n    date = m.getdate_tz('date')\n    tz = date[-1]\n    date = time.localtime(mktime_tz(date))\n    if date:\n        print 'ParsedDate:', time.asctime(date),\n        hhmmss = tz\n        hhmm, ss = divmod(hhmmss, 60)\n        hh, mm = divmod(hhmm, 60)\n        print \"%+03d%02d\" % (hh, mm),\n        if ss: print \".%02d\" % ss,\n        print\n    else:\n        print 'ParsedDate:', None\n    m.rewindbody()\n    n = 0\n    while f.readline():\n        n += 1\n    print 'Lines:', n\n    print '-'*70\n    print 'len =', len(m)\n    if 'Date' in m: print 'Date =', m['Date']\n    if 'X-Nonsense' in m: pass\n    print 'keys =', m.keys()\n    print 'values =', m.values()\n    print 'items =', m.items()\n", 
    "shlex": "# -*- coding: utf-8 -*-\n\"\"\"A lexical analyzer class for simple shell-like syntaxes.\"\"\"\n\n# Module and documentation by Eric S. Raymond, 21 Dec 1998\n# Input stacking and error message cleanup added by ESR, March 2000\n# push_source() and pop_source() made explicit by ESR, January 2001.\n# Posix compliance, split(), string arguments, and\n# iterator interface by Gustavo Niemeyer, April 2003.\n\nimport os.path\nimport sys\nfrom collections import deque\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\n__all__ = [\"shlex\", \"split\"]\n\nclass shlex:\n    \"A lexical analyzer class for simple shell-like syntaxes.\"\n    def __init__(self, instream=None, infile=None, posix=False):\n        if isinstance(instream, basestring):\n            instream = StringIO(instream)\n        if instream is not None:\n            self.instream = instream\n            self.infile = infile\n        else:\n            self.instream = sys.stdin\n            self.infile = None\n        self.posix = posix\n        if posix:\n            self.eof = None\n        else:\n            self.eof = ''\n        self.commenters = '#'\n        self.wordchars = ('abcdfeghijklmnopqrstuvwxyz'\n                          'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_')\n        if self.posix:\n            self.wordchars += ('\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u00f8\u00f9\u00fa\u00fb\u00fc\u00fd\u00fe\u00ff'\n                               '\u00c0\u00c1\u00c2\u00c3\u00c4\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u00d8\u00d9\u00da\u00db\u00dc\u00dd\u00de')\n        self.whitespace = ' \\t\\r\\n'\n        self.whitespace_split = False\n        self.quotes = '\\'\"'\n        self.escape = '\\\\'\n        self.escapedquotes = '\"'\n        self.state = ' '\n        self.pushback = deque()\n        self.lineno = 1\n        self.debug = 0\n        self.token = ''\n        self.filestack = deque()\n        self.source = None\n        if self.debug:\n            print 'shlex: reading from %s, line %d' \\\n                  % (self.instream, self.lineno)\n\n    def push_token(self, tok):\n        \"Push a token onto the stack popped by the get_token method\"\n        if self.debug >= 1:\n            print \"shlex: pushing token \" + repr(tok)\n        self.pushback.appendleft(tok)\n\n    def push_source(self, newstream, newfile=None):\n        \"Push an input source onto the lexer's input source stack.\"\n        if isinstance(newstream, basestring):\n            newstream = StringIO(newstream)\n        self.filestack.appendleft((self.infile, self.instream, self.lineno))\n        self.infile = newfile\n        self.instream = newstream\n        self.lineno = 1\n        if self.debug:\n            if newfile is not None:\n                print 'shlex: pushing to file %s' % (self.infile,)\n            else:\n                print 'shlex: pushing to stream %s' % (self.instream,)\n\n    def pop_source(self):\n        \"Pop the input source stack.\"\n        self.instream.close()\n        (self.infile, self.instream, self.lineno) = self.filestack.popleft()\n        if self.debug:\n            print 'shlex: popping to %s, line %d' \\\n                  % (self.instream, self.lineno)\n        self.state = ' '\n\n    def get_token(self):\n        \"Get a token from the input stream (or from stack if it's nonempty)\"\n        if self.pushback:\n            tok = self.pushback.popleft()\n            if self.debug >= 1:\n                print \"shlex: popping token \" + repr(tok)\n            return tok\n        # No pushback.  Get a token.\n        raw = self.read_token()\n        # Handle inclusions\n        if self.source is not None:\n            while raw == self.source:\n                spec = self.sourcehook(self.read_token())\n                if spec:\n                    (newfile, newstream) = spec\n                    self.push_source(newstream, newfile)\n                raw = self.get_token()\n        # Maybe we got EOF instead?\n        while raw == self.eof:\n            if not self.filestack:\n                return self.eof\n            else:\n                self.pop_source()\n                raw = self.get_token()\n        # Neither inclusion nor EOF\n        if self.debug >= 1:\n            if raw != self.eof:\n                print \"shlex: token=\" + repr(raw)\n            else:\n                print \"shlex: token=EOF\"\n        return raw\n\n    def read_token(self):\n        quoted = False\n        escapedstate = ' '\n        while True:\n            nextchar = self.instream.read(1)\n            if nextchar == '\\n':\n                self.lineno = self.lineno + 1\n            if self.debug >= 3:\n                print \"shlex: in state\", repr(self.state), \\\n                      \"I see character:\", repr(nextchar)\n            if self.state is None:\n                self.token = ''        # past end of file\n                break\n            elif self.state == ' ':\n                if not nextchar:\n                    self.state = None  # end of file\n                    break\n                elif nextchar in self.whitespace:\n                    if self.debug >= 2:\n                        print \"shlex: I see whitespace in whitespace state\"\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n                elif nextchar in self.commenters:\n                    self.instream.readline()\n                    self.lineno = self.lineno + 1\n                elif self.posix and nextchar in self.escape:\n                    escapedstate = 'a'\n                    self.state = nextchar\n                elif nextchar in self.wordchars:\n                    self.token = nextchar\n                    self.state = 'a'\n                elif nextchar in self.quotes:\n                    if not self.posix:\n                        self.token = nextchar\n                    self.state = nextchar\n                elif self.whitespace_split:\n                    self.token = nextchar\n                    self.state = 'a'\n                else:\n                    self.token = nextchar\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n            elif self.state in self.quotes:\n                quoted = True\n                if not nextchar:      # end of file\n                    if self.debug >= 2:\n                        print \"shlex: I see EOF in quotes state\"\n                    # XXX what error should be raised here?\n                    raise ValueError, \"No closing quotation\"\n                if nextchar == self.state:\n                    if not self.posix:\n                        self.token = self.token + nextchar\n                        self.state = ' '\n                        break\n                    else:\n                        self.state = 'a'\n                elif self.posix and nextchar in self.escape and \\\n                     self.state in self.escapedquotes:\n                    escapedstate = self.state\n                    self.state = nextchar\n                else:\n                    self.token = self.token + nextchar\n            elif self.state in self.escape:\n                if not nextchar:      # end of file\n                    if self.debug >= 2:\n                        print \"shlex: I see EOF in escape state\"\n                    # XXX what error should be raised here?\n                    raise ValueError, \"No escaped character\"\n                # In posix shells, only the quote itself or the escape\n                # character may be escaped within quotes.\n                if escapedstate in self.quotes and \\\n                   nextchar != self.state and nextchar != escapedstate:\n                    self.token = self.token + self.state\n                self.token = self.token + nextchar\n                self.state = escapedstate\n            elif self.state == 'a':\n                if not nextchar:\n                    self.state = None   # end of file\n                    break\n                elif nextchar in self.whitespace:\n                    if self.debug >= 2:\n                        print \"shlex: I see whitespace in word state\"\n                    self.state = ' '\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n                elif nextchar in self.commenters:\n                    self.instream.readline()\n                    self.lineno = self.lineno + 1\n                    if self.posix:\n                        self.state = ' '\n                        if self.token or (self.posix and quoted):\n                            break   # emit current token\n                        else:\n                            continue\n                elif self.posix and nextchar in self.quotes:\n                    self.state = nextchar\n                elif self.posix and nextchar in self.escape:\n                    escapedstate = 'a'\n                    self.state = nextchar\n                elif nextchar in self.wordchars or nextchar in self.quotes \\\n                    or self.whitespace_split:\n                    self.token = self.token + nextchar\n                else:\n                    self.pushback.appendleft(nextchar)\n                    if self.debug >= 2:\n                        print \"shlex: I see punctuation in word state\"\n                    self.state = ' '\n                    if self.token:\n                        break   # emit current token\n                    else:\n                        continue\n        result = self.token\n        self.token = ''\n        if self.posix and not quoted and result == '':\n            result = None\n        if self.debug > 1:\n            if result:\n                print \"shlex: raw token=\" + repr(result)\n            else:\n                print \"shlex: raw token=EOF\"\n        return result\n\n    def sourcehook(self, newfile):\n        \"Hook called on a filename to be sourced.\"\n        if newfile[0] == '\"':\n            newfile = newfile[1:-1]\n        # This implements cpp-like semantics for relative-path inclusion.\n        if isinstance(self.infile, basestring) and not os.path.isabs(newfile):\n            newfile = os.path.join(os.path.dirname(self.infile), newfile)\n        return (newfile, open(newfile, \"r\"))\n\n    def error_leader(self, infile=None, lineno=None):\n        \"Emit a C-compiler-like, Emacs-friendly error-message leader.\"\n        if infile is None:\n            infile = self.infile\n        if lineno is None:\n            lineno = self.lineno\n        return \"\\\"%s\\\", line %d: \" % (infile, lineno)\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        token = self.get_token()\n        if token == self.eof:\n            raise StopIteration\n        return token\n\ndef split(s, comments=False, posix=True):\n    lex = shlex(s, posix=posix)\n    lex.whitespace_split = True\n    if not comments:\n        lex.commenters = ''\n    return list(lex)\n\nif __name__ == '__main__':\n    if len(sys.argv) == 1:\n        lexer = shlex()\n    else:\n        file = sys.argv[1]\n        lexer = shlex(open(file), file)\n    while 1:\n        tt = lexer.get_token()\n        if tt:\n            print \"Token: \" + repr(tt)\n        else:\n            break\n", 
    "socket": "# Wrapper module for _socket, providing some additional facilities\n# implemented in Python.\n\n\"\"\"\\\nThis module provides socket operations and some related functions.\nOn Unix, it supports IP (Internet Protocol) and Unix domain sockets.\nOn other systems, it only supports IP. Functions specific for a\nsocket are available as methods of the socket object.\n\nFunctions:\n\nsocket() -- create a new socket object\nsocketpair() -- create a pair of new socket objects [*]\nfromfd() -- create a socket object from an open file descriptor [*]\ngethostname() -- return the current hostname\ngethostbyname() -- map a hostname to its IP number\ngethostbyaddr() -- map an IP number or hostname to DNS info\ngetservbyname() -- map a service name and a protocol name to a port number\ngetprotobyname() -- map a protocol name (e.g. 'tcp') to a number\nntohs(), ntohl() -- convert 16, 32 bit int from network to host byte order\nhtons(), htonl() -- convert 16, 32 bit int from host to network byte order\ninet_aton() -- convert IP addr string (123.45.67.89) to 32-bit packed format\ninet_ntoa() -- convert 32-bit packed format IP to string (123.45.67.89)\nssl() -- secure socket layer support (only available if configured)\nsocket.getdefaulttimeout() -- get the default timeout value\nsocket.setdefaulttimeout() -- set the default timeout value\ncreate_connection() -- connects to an address, with an optional timeout and\n                       optional source address.\n\n [*] not available on all platforms!\n\nSpecial objects:\n\nSocketType -- type object for socket objects\nerror -- exception raised for I/O errors\nhas_ipv6 -- boolean value indicating if IPv6 is supported\n\nInteger constants:\n\nAF_INET, AF_UNIX -- socket domains (first argument to socket() call)\nSOCK_STREAM, SOCK_DGRAM, SOCK_RAW -- socket types (second argument)\n\nMany other constants may be defined; these may be used in calls to\nthe setsockopt() and getsockopt() methods.\n\"\"\"\n\nimport _socket\nfrom _socket import *\n\ntry:\n    import _ssl\nexcept ImportError:\n    # no SSL support\n    pass\nelse:\n    def ssl(sock, keyfile=None, certfile=None):\n        # we do an internal import here because the ssl\n        # module imports the socket module\n        import ssl as _realssl\n        warnings.warn(\"socket.ssl() is deprecated.  Use ssl.wrap_socket() instead.\",\n                      DeprecationWarning, stacklevel=2)\n        return _realssl.sslwrap_simple(sock, keyfile, certfile)\n\n    # we need to import the same constants we used to...\n    from _ssl import SSLError as sslerror\n    from _ssl import \\\n         RAND_add, \\\n         RAND_egd, \\\n         RAND_status, \\\n         SSL_ERROR_ZERO_RETURN, \\\n         SSL_ERROR_WANT_READ, \\\n         SSL_ERROR_WANT_WRITE, \\\n         SSL_ERROR_WANT_X509_LOOKUP, \\\n         SSL_ERROR_SYSCALL, \\\n         SSL_ERROR_SSL, \\\n         SSL_ERROR_WANT_CONNECT, \\\n         SSL_ERROR_EOF, \\\n         SSL_ERROR_INVALID_ERROR_CODE\n\nimport os, sys, warnings\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\ntry:\n    import errno\nexcept ImportError:\n    errno = None\nEBADF = getattr(errno, 'EBADF', 9)\nEINTR = getattr(errno, 'EINTR', 4)\n\n__all__ = [\"getfqdn\", \"create_connection\"]\n__all__.extend(os._get_exports_list(_socket))\n\n\n_realsocket = socket\n_type = type\n\n# WSA error codes\nif sys.platform.lower().startswith(\"win\"):\n    errorTab = {}\n    errorTab[10004] = \"The operation was interrupted.\"\n    errorTab[10009] = \"A bad file handle was passed.\"\n    errorTab[10013] = \"Permission denied.\"\n    errorTab[10014] = \"A fault occurred on the network??\" # WSAEFAULT\n    errorTab[10022] = \"An invalid operation was attempted.\"\n    errorTab[10035] = \"The socket operation would block\"\n    errorTab[10036] = \"A blocking operation is already in progress.\"\n    errorTab[10048] = \"The network address is in use.\"\n    errorTab[10054] = \"The connection has been reset.\"\n    errorTab[10058] = \"The network has been shut down.\"\n    errorTab[10060] = \"The operation timed out.\"\n    errorTab[10061] = \"Connection refused.\"\n    errorTab[10063] = \"The name is too long.\"\n    errorTab[10064] = \"The host is down.\"\n    errorTab[10065] = \"The host is unreachable.\"\n    __all__.append(\"errorTab\")\n\n\n\ndef getfqdn(name=''):\n    \"\"\"Get fully qualified domain name from name.\n\n    An empty argument is interpreted as meaning the local host.\n\n    First the hostname returned by gethostbyaddr() is checked, then\n    possibly existing aliases. In case no FQDN is available, hostname\n    from gethostname() is returned.\n    \"\"\"\n    name = name.strip()\n    if not name or name == '0.0.0.0':\n        name = gethostname()\n    try:\n        hostname, aliases, ipaddrs = gethostbyaddr(name)\n    except error:\n        pass\n    else:\n        aliases.insert(0, hostname)\n        for name in aliases:\n            if '.' in name:\n                break\n        else:\n            name = hostname\n    return name\n\n\n_socketmethods = (\n    'bind', 'connect', 'connect_ex', 'fileno', 'listen',\n    'getpeername', 'getsockname', 'getsockopt', 'setsockopt',\n    'sendall', 'setblocking',\n    'settimeout', 'gettimeout', 'shutdown')\n\nif os.name == \"nt\":\n    _socketmethods = _socketmethods + ('ioctl',)\n\nif sys.platform == \"riscos\":\n    _socketmethods = _socketmethods + ('sleeptaskw',)\n\nclass _closedsocket(object):\n    __slots__ = []\n    def _dummy(*args):\n        raise error(EBADF, 'Bad file descriptor')\n    # All _delegate_methods must also be initialized here.\n    send = recv = recv_into = sendto = recvfrom = recvfrom_into = _dummy\n    __getattr__ = _dummy\n    def _drop(self):\n        pass\n\n# Wrapper around platform socket objects. This implements\n# a platform-independent dup() functionality. The\n# implementation currently relies on reference counting\n# to close the underlying socket object.\nclass _socketobject(object):\n\n    __doc__ = _realsocket.__doc__\n\n    __slots__ = [\"_sock\", \"__weakref__\"]\n\n    def __init__(self, family=AF_INET, type=SOCK_STREAM, proto=0, _sock=None):\n        if _sock is None:\n            _sock = _realsocket(family, type, proto)\n        else:\n            # PyPy note about refcounting: implemented with _reuse()/_drop()\n            # on the class '_socket.socket'.  Python 3 did it differently\n            # with a reference counter on this class 'socket._socketobject'\n            # instead, but it is a less compatible change.\n            \n            # Note that a few libraries (like eventlet) poke at the\n            # private implementation of socket.py, passing custom\n            # objects to _socketobject().  These libraries need the\n            # following fix for use on PyPy: the custom objects need\n            # methods _reuse() and _drop() that maintains an explicit\n            # reference counter, starting at 0.  When it drops back to\n            # zero, close() must be called.\n            _sock._reuse()\n\n        self._sock = _sock\n\n    def send(self, data, flags=0):\n        return self._sock.send(data, flags)\n    send.__doc__ = _realsocket.send.__doc__\n\n    def recv(self, buffersize, flags=0):\n        return self._sock.recv(buffersize, flags)\n    recv.__doc__ = _realsocket.recv.__doc__\n\n    def recv_into(self, buffer, nbytes=0, flags=0):\n        return self._sock.recv_into(buffer, nbytes, flags)\n    recv_into.__doc__ = _realsocket.recv_into.__doc__\n\n    def recvfrom(self, buffersize, flags=0):\n        return self._sock.recvfrom(buffersize, flags)\n    recvfrom.__doc__ = _realsocket.recvfrom.__doc__\n\n    def recvfrom_into(self, buffer, nbytes=0, flags=0):\n        return self._sock.recvfrom_into(buffer, nbytes, flags)\n    recvfrom_into.__doc__ = _realsocket.recvfrom_into.__doc__\n\n    def sendto(self, data, param2, param3=None):\n        if param3 is None:\n            return self._sock.sendto(data, param2)\n        else:\n            return self._sock.sendto(data, param2, param3)\n    sendto.__doc__ = _realsocket.sendto.__doc__\n\n    def close(self):\n        s = self._sock\n        self._sock = _closedsocket()\n        s._drop()\n    close.__doc__ = _realsocket.close.__doc__\n\n    def accept(self):\n        sock, addr = self._sock.accept()\n        sockobj = _socketobject(_sock=sock)\n        sock._drop()    # already a copy in the _socketobject()\n        return sockobj, addr\n    accept.__doc__ = _realsocket.accept.__doc__\n\n    def dup(self):\n        \"\"\"dup() -> socket object\n\n        Return a new socket object connected to the same system resource.\"\"\"\n        return _socketobject(_sock=self._sock)\n\n    def makefile(self, mode='r', bufsize=-1):\n        \"\"\"makefile([mode[, bufsize]]) -> file object\n\n        Return a regular file object corresponding to the socket.  The mode\n        and bufsize arguments are as for the built-in open() function.\"\"\"\n        return _fileobject(self._sock, mode, bufsize)\n\n    family = property(lambda self: self._sock.family, doc=\"the socket family\")\n    type = property(lambda self: self._sock.type, doc=\"the socket type\")\n    proto = property(lambda self: self._sock.proto, doc=\"the socket protocol\")\n\n    # Delegate many calls to the raw socket object.\n    _s = (\"def %(name)s(self, %(args)s): return self._sock.%(name)s(%(args)s)\\n\\n\"\n          \"%(name)s.__doc__ = _realsocket.%(name)s.__doc__\\n\")\n    for _m in _socketmethods:\n        # yupi! we're on pypy, all code objects have this interface\n        argcount = getattr(_realsocket, _m).im_func.func_code.co_argcount - 1\n        exec _s % {'name': _m, 'args': ', '.join('arg%d' % i for i in range(argcount))}\n    del _m, _s, argcount\n\n    # Delegation methods with default arguments, that the code above\n    # cannot handle correctly\n    def sendall(self, data, flags=0):\n        self._sock.sendall(data, flags)\n    sendall.__doc__ = _realsocket.sendall.__doc__\n\n    def getsockopt(self, level, optname, buflen=None):\n        if buflen is None:\n            return self._sock.getsockopt(level, optname)\n        return self._sock.getsockopt(level, optname, buflen)\n    getsockopt.__doc__ = _realsocket.getsockopt.__doc__\n\nsocket = SocketType = _socketobject\n\nclass _fileobject(object):\n    \"\"\"Faux file object attached to a socket object.\"\"\"\n\n    default_bufsize = 8192\n    name = \"<socket>\"\n\n    __slots__ = [\"mode\", \"bufsize\", \"softspace\",\n                 # \"closed\" is a property, see below\n                 \"_sock\", \"_rbufsize\", \"_wbufsize\", \"_rbuf\", \"_wbuf\", \"_wbuf_len\",\n                 \"_close\"]\n\n    def __init__(self, sock, mode='rb', bufsize=-1, close=False):\n        # Note that a few libraries (like eventlet) poke at the\n        # private implementation of socket.py, passing custom\n        # objects to _fileobject().  These libraries need the\n        # following fix for use on PyPy: the custom objects need\n        # methods _reuse() and _drop() that maintains an explicit\n        # reference counter, starting at 0.  When it drops back to\n        # zero, close() must be called.\n        sock._reuse()\n        self._sock = sock\n        self.mode = mode # Not actually used in this version\n        if bufsize < 0:\n            bufsize = self.default_bufsize\n        self.bufsize = bufsize\n        self.softspace = False\n        # _rbufsize is the suggested recv buffer size.  It is *strictly*\n        # obeyed within readline() for recv calls.  If it is larger than\n        # default_bufsize it will be used for recv calls within read().\n        if bufsize == 0:\n            self._rbufsize = 1\n        elif bufsize == 1:\n            self._rbufsize = self.default_bufsize\n        else:\n            self._rbufsize = bufsize\n        self._wbufsize = bufsize\n        # We use StringIO for the read buffer to avoid holding a list\n        # of variously sized string objects which have been known to\n        # fragment the heap due to how they are malloc()ed and often\n        # realloc()ed down much smaller than their original allocation.\n        self._rbuf = StringIO()\n        self._wbuf = [] # A list of strings\n        self._wbuf_len = 0\n        self._close = close\n\n    def _getclosed(self):\n        return self._sock is None\n    closed = property(_getclosed, doc=\"True if the file is closed\")\n\n    def close(self):\n        try:\n            if self._sock:\n                self.flush()\n        finally:\n            s = self._sock\n            self._sock = None\n            if s is not None:\n                if self._close:\n                    s.close()\n                else:\n                    s._drop()\n\n    def __del__(self):\n        try:\n            self.close()\n        except:\n            # close() may fail if __init__ didn't complete\n            pass\n\n    def flush(self):\n        if self._wbuf:\n            data = \"\".join(self._wbuf)\n            self._wbuf = []\n            self._wbuf_len = 0\n            buffer_size = max(self._rbufsize, self.default_bufsize)\n            data_size = len(data)\n            write_offset = 0\n            view = memoryview(data)\n            try:\n                while write_offset < data_size:\n                    self._sock.sendall(view[write_offset:write_offset+buffer_size])\n                    write_offset += buffer_size\n            finally:\n                if write_offset < data_size:\n                    remainder = data[write_offset:]\n                    del view, data  # explicit free\n                    self._wbuf.append(remainder)\n                    self._wbuf_len = len(remainder)\n\n    def fileno(self):\n        return self._sock.fileno()\n\n    def write(self, data):\n        data = str(data) # XXX Should really reject non-string non-buffers\n        if not data:\n            return\n        self._wbuf.append(data)\n        self._wbuf_len += len(data)\n        if (self._wbufsize == 0 or\n            (self._wbufsize == 1 and '\\n' in data) or\n            (self._wbufsize > 1 and self._wbuf_len >= self._wbufsize)):\n            self.flush()\n\n    def writelines(self, list):\n        # XXX We could do better here for very long lists\n        # XXX Should really reject non-string non-buffers\n        lines = filter(None, map(str, list))\n        self._wbuf_len += sum(map(len, lines))\n        self._wbuf.extend(lines)\n        if (self._wbufsize <= 1 or\n            self._wbuf_len >= self._wbufsize):\n            self.flush()\n\n    def read(self, size=-1):\n        # Use max, disallow tiny reads in a loop as they are very inefficient.\n        # We never leave read() with any leftover data from a new recv() call\n        # in our internal buffer.\n        rbufsize = max(self._rbufsize, self.default_bufsize)\n        # Our use of StringIO rather than lists of string objects returned by\n        # recv() minimizes memory usage and fragmentation that occurs when\n        # rbufsize is large compared to the typical return value of recv().\n        buf = self._rbuf\n        buf.seek(0, 2)  # seek end\n        if size < 0:\n            # Read until EOF\n            self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n            while True:\n                try:\n                    data = self._sock.recv(rbufsize)\n                except error, e:\n                    if e.args[0] == EINTR:\n                        continue\n                    raise\n                if not data:\n                    break\n                buf.write(data)\n            return buf.getvalue()\n        else:\n            # Read until size bytes or EOF seen, whichever comes first\n            buf_len = buf.tell()\n            if buf_len >= size:\n                # Already have size bytes in our buffer?  Extract and return.\n                buf.seek(0)\n                rv = buf.read(size)\n                self._rbuf = StringIO()\n                self._rbuf.write(buf.read())\n                return rv\n\n            self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n            while True:\n                left = size - buf_len\n                # recv() will malloc the amount of memory given as its\n                # parameter even though it often returns much less data\n                # than that.  The returned data string is short lived\n                # as we copy it into a StringIO and free it.  This avoids\n                # fragmentation issues on many platforms.\n                try:\n                    data = self._sock.recv(left)\n                except error, e:\n                    if e.args[0] == EINTR:\n                        continue\n                    raise\n                if not data:\n                    break\n                n = len(data)\n                if n == size and not buf_len:\n                    # Shortcut.  Avoid buffer data copies when:\n                    # - We have no data in our buffer.\n                    # AND\n                    # - Our call to recv returned exactly the\n                    #   number of bytes we were asked to read.\n                    return data\n                if n == left:\n                    buf.write(data)\n                    del data  # explicit free\n                    break\n                assert n <= left, \"recv(%d) returned %d bytes\" % (left, n)\n                buf.write(data)\n                buf_len += n\n                del data  # explicit free\n                #assert buf_len == buf.tell()\n            return buf.getvalue()\n\n    def readline(self, size=-1):\n        buf = self._rbuf\n        buf.seek(0, 2)  # seek end\n        if buf.tell() > 0:\n            # check if we already have it in our buffer\n            buf.seek(0)\n            bline = buf.readline(size)\n            if bline.endswith('\\n') or len(bline) == size:\n                self._rbuf = StringIO()\n                self._rbuf.write(buf.read())\n                return bline\n            del bline\n        if size < 0:\n            # Read until \\n or EOF, whichever comes first\n            if self._rbufsize <= 1:\n                # Speed up unbuffered case\n                buf.seek(0)\n                buffers = [buf.read()]\n                self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n                data = None\n                recv = self._sock.recv\n                while True:\n                    try:\n                        while data != \"\\n\":\n                            data = recv(1)\n                            if not data:\n                                break\n                            buffers.append(data)\n                    except error, e:\n                        # The try..except to catch EINTR was moved outside the\n                        # recv loop to avoid the per byte overhead.\n                        if e.args[0] == EINTR:\n                            continue\n                        raise\n                    break\n                return \"\".join(buffers)\n\n            buf.seek(0, 2)  # seek end\n            self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n            while True:\n                try:\n                    data = self._sock.recv(self._rbufsize)\n                except error, e:\n                    if e.args[0] == EINTR:\n                        continue\n                    raise\n                if not data:\n                    break\n                nl = data.find('\\n')\n                if nl >= 0:\n                    nl += 1\n                    buf.write(data[:nl])\n                    self._rbuf.write(data[nl:])\n                    del data\n                    break\n                buf.write(data)\n            return buf.getvalue()\n        else:\n            # Read until size bytes or \\n or EOF seen, whichever comes first\n            buf.seek(0, 2)  # seek end\n            buf_len = buf.tell()\n            if buf_len >= size:\n                buf.seek(0)\n                rv = buf.read(size)\n                self._rbuf = StringIO()\n                self._rbuf.write(buf.read())\n                return rv\n            self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n            while True:\n                try:\n                    data = self._sock.recv(self._rbufsize)\n                except error, e:\n                    if e.args[0] == EINTR:\n                        continue\n                    raise\n                if not data:\n                    break\n                left = size - buf_len\n                # did we just receive a newline?\n                nl = data.find('\\n', 0, left)\n                if nl >= 0:\n                    nl += 1\n                    # save the excess data to _rbuf\n                    self._rbuf.write(data[nl:])\n                    if buf_len:\n                        buf.write(data[:nl])\n                        break\n                    else:\n                        # Shortcut.  Avoid data copy through buf when returning\n                        # a substring of our first recv().\n                        return data[:nl]\n                n = len(data)\n                if n == size and not buf_len:\n                    # Shortcut.  Avoid data copy through buf when\n                    # returning exactly all of our first recv().\n                    return data\n                if n >= left:\n                    buf.write(data[:left])\n                    self._rbuf.write(data[left:])\n                    break\n                buf.write(data)\n                buf_len += n\n                #assert buf_len == buf.tell()\n            return buf.getvalue()\n\n    def readlines(self, sizehint=0):\n        total = 0\n        list = []\n        while True:\n            line = self.readline()\n            if not line:\n                break\n            list.append(line)\n            total += len(line)\n            if sizehint and total >= sizehint:\n                break\n        return list\n\n    # Iterator protocols\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        line = self.readline()\n        if not line:\n            raise StopIteration\n        return line\n\n_GLOBAL_DEFAULT_TIMEOUT = object()\n\ndef create_connection(address, timeout=_GLOBAL_DEFAULT_TIMEOUT,\n                      source_address=None):\n    \"\"\"Connect to *address* and return the socket object.\n\n    Convenience function.  Connect to *address* (a 2-tuple ``(host,\n    port)``) and return the socket object.  Passing the optional\n    *timeout* parameter will set the timeout on the socket instance\n    before attempting to connect.  If no *timeout* is supplied, the\n    global default timeout setting returned by :func:`getdefaulttimeout`\n    is used.  If *source_address* is set it must be a tuple of (host, port)\n    for the socket to bind as a source address before making the connection.\n    An host of '' or port 0 tells the OS to use the default.\n    \"\"\"\n\n    host, port = address\n    err = None\n    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket(af, socktype, proto)\n            if timeout is not _GLOBAL_DEFAULT_TIMEOUT:\n                sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            return sock\n\n        except error as _:\n            err = _\n            if sock is not None:\n                sock.close()\n\n    if err is not None:\n        raise err\n    else:\n        raise error(\"getaddrinfo returns an empty list\")\n", 
    "sre_compile": "#\n# Secret Labs' Regular Expression Engine\n#\n# convert template to internal format\n#\n# Copyright (c) 1997-2001 by Secret Labs AB.  All rights reserved.\n#\n# See the sre.py file for information on usage and redistribution.\n#\n\n\"\"\"Internal support module for sre\"\"\"\n\nimport _sre, sys\nimport sre_parse\nfrom sre_constants import *\n\nassert _sre.MAGIC == MAGIC, \"SRE module mismatch\"\n\nif _sre.CODESIZE == 2:\n    MAXCODE = 65535\nelse:\n    MAXCODE = 0xFFFFFFFFL\n\ndef _identityfunction(x):\n    return x\n\n_LITERAL_CODES = set([LITERAL, NOT_LITERAL])\n_REPEATING_CODES = set([REPEAT, MIN_REPEAT, MAX_REPEAT])\n_SUCCESS_CODES = set([SUCCESS, FAILURE])\n_ASSERT_CODES = set([ASSERT, ASSERT_NOT])\n\ndef _compile(code, pattern, flags):\n    # internal: compile a (sub)pattern\n    emit = code.append\n    _len = len\n    LITERAL_CODES = _LITERAL_CODES\n    REPEATING_CODES = _REPEATING_CODES\n    SUCCESS_CODES = _SUCCESS_CODES\n    ASSERT_CODES = _ASSERT_CODES\n    for op, av in pattern:\n        if op in LITERAL_CODES:\n            if flags & SRE_FLAG_IGNORECASE:\n                emit(OPCODES[OP_IGNORE[op]])\n                emit(_sre.getlower(av, flags))\n            else:\n                emit(OPCODES[op])\n                emit(av)\n        elif op is IN:\n            if flags & SRE_FLAG_IGNORECASE:\n                emit(OPCODES[OP_IGNORE[op]])\n                def fixup(literal, flags=flags):\n                    return _sre.getlower(literal, flags)\n            else:\n                emit(OPCODES[op])\n                fixup = _identityfunction\n            skip = _len(code); emit(0)\n            _compile_charset(av, flags, code, fixup)\n            code[skip] = _len(code) - skip\n        elif op is ANY:\n            if flags & SRE_FLAG_DOTALL:\n                emit(OPCODES[ANY_ALL])\n            else:\n                emit(OPCODES[ANY])\n        elif op in REPEATING_CODES:\n            if flags & SRE_FLAG_TEMPLATE:\n                raise error, \"internal: unsupported template operator\"\n                emit(OPCODES[REPEAT])\n                skip = _len(code); emit(0)\n                emit(av[0])\n                emit(av[1])\n                _compile(code, av[2], flags)\n                emit(OPCODES[SUCCESS])\n                code[skip] = _len(code) - skip\n            elif _simple(av) and op is not REPEAT:\n                if op is MAX_REPEAT:\n                    emit(OPCODES[REPEAT_ONE])\n                else:\n                    emit(OPCODES[MIN_REPEAT_ONE])\n                skip = _len(code); emit(0)\n                emit(av[0])\n                emit(av[1])\n                _compile(code, av[2], flags)\n                emit(OPCODES[SUCCESS])\n                code[skip] = _len(code) - skip\n            else:\n                emit(OPCODES[REPEAT])\n                skip = _len(code); emit(0)\n                emit(av[0])\n                emit(av[1])\n                _compile(code, av[2], flags)\n                code[skip] = _len(code) - skip\n                if op is MAX_REPEAT:\n                    emit(OPCODES[MAX_UNTIL])\n                else:\n                    emit(OPCODES[MIN_UNTIL])\n        elif op is SUBPATTERN:\n            if av[0]:\n                emit(OPCODES[MARK])\n                emit((av[0]-1)*2)\n            # _compile_info(code, av[1], flags)\n            _compile(code, av[1], flags)\n            if av[0]:\n                emit(OPCODES[MARK])\n                emit((av[0]-1)*2+1)\n        elif op in SUCCESS_CODES:\n            emit(OPCODES[op])\n        elif op in ASSERT_CODES:\n            emit(OPCODES[op])\n            skip = _len(code); emit(0)\n            if av[0] >= 0:\n                emit(0) # look ahead\n            else:\n                lo, hi = av[1].getwidth()\n                if lo != hi:\n                    raise error, \"look-behind requires fixed-width pattern\"\n                emit(lo) # look behind\n            _compile(code, av[1], flags)\n            emit(OPCODES[SUCCESS])\n            code[skip] = _len(code) - skip\n        elif op is CALL:\n            emit(OPCODES[op])\n            skip = _len(code); emit(0)\n            _compile(code, av, flags)\n            emit(OPCODES[SUCCESS])\n            code[skip] = _len(code) - skip\n        elif op is AT:\n            emit(OPCODES[op])\n            if flags & SRE_FLAG_MULTILINE:\n                av = AT_MULTILINE.get(av, av)\n            if flags & SRE_FLAG_LOCALE:\n                av = AT_LOCALE.get(av, av)\n            elif flags & SRE_FLAG_UNICODE:\n                av = AT_UNICODE.get(av, av)\n            emit(ATCODES[av])\n        elif op is BRANCH:\n            emit(OPCODES[op])\n            tail = []\n            tailappend = tail.append\n            for av in av[1]:\n                skip = _len(code); emit(0)\n                # _compile_info(code, av, flags)\n                _compile(code, av, flags)\n                emit(OPCODES[JUMP])\n                tailappend(_len(code)); emit(0)\n                code[skip] = _len(code) - skip\n            emit(0) # end of branch\n            for tail in tail:\n                code[tail] = _len(code) - tail\n        elif op is CATEGORY:\n            emit(OPCODES[op])\n            if flags & SRE_FLAG_LOCALE:\n                av = CH_LOCALE[av]\n            elif flags & SRE_FLAG_UNICODE:\n                av = CH_UNICODE[av]\n            emit(CHCODES[av])\n        elif op is GROUPREF:\n            if flags & SRE_FLAG_IGNORECASE:\n                emit(OPCODES[OP_IGNORE[op]])\n            else:\n                emit(OPCODES[op])\n            emit(av-1)\n        elif op is GROUPREF_EXISTS:\n            emit(OPCODES[op])\n            emit(av[0]-1)\n            skipyes = _len(code); emit(0)\n            _compile(code, av[1], flags)\n            if av[2]:\n                emit(OPCODES[JUMP])\n                skipno = _len(code); emit(0)\n                code[skipyes] = _len(code) - skipyes + 1\n                _compile(code, av[2], flags)\n                code[skipno] = _len(code) - skipno\n            else:\n                code[skipyes] = _len(code) - skipyes + 1\n        else:\n            raise ValueError, (\"unsupported operand type\", op)\n\ndef _compile_charset(charset, flags, code, fixup=None):\n    # compile charset subprogram\n    emit = code.append\n    if fixup is None:\n        fixup = _identityfunction\n    for op, av in _optimize_charset(charset, fixup):\n        emit(OPCODES[op])\n        if op is NEGATE:\n            pass\n        elif op is LITERAL:\n            emit(fixup(av))\n        elif op is RANGE:\n            emit(fixup(av[0]))\n            emit(fixup(av[1]))\n        elif op is CHARSET:\n            code.extend(av)\n        elif op is BIGCHARSET:\n            code.extend(av)\n        elif op is CATEGORY:\n            if flags & SRE_FLAG_LOCALE:\n                emit(CHCODES[CH_LOCALE[av]])\n            elif flags & SRE_FLAG_UNICODE:\n                emit(CHCODES[CH_UNICODE[av]])\n            else:\n                emit(CHCODES[av])\n        else:\n            raise error, \"internal: unsupported set operator\"\n    emit(OPCODES[FAILURE])\n\ndef _optimize_charset(charset, fixup):\n    # internal: optimize character set\n    out = []\n    outappend = out.append\n    charmap = [0]*256\n    try:\n        for op, av in charset:\n            if op is NEGATE:\n                outappend((op, av))\n            elif op is LITERAL:\n                charmap[fixup(av)] = 1\n            elif op is RANGE:\n                for i in range(fixup(av[0]), fixup(av[1])+1):\n                    charmap[i] = 1\n            elif op is CATEGORY:\n                # XXX: could append to charmap tail\n                return charset # cannot compress\n    except IndexError:\n        # character set contains unicode characters\n        return _optimize_unicode(charset, fixup)\n    # compress character map\n    i = p = n = 0\n    runs = []\n    runsappend = runs.append\n    for c in charmap:\n        if c:\n            if n == 0:\n                p = i\n            n = n + 1\n        elif n:\n            runsappend((p, n))\n            n = 0\n        i = i + 1\n    if n:\n        runsappend((p, n))\n    if len(runs) <= 2:\n        # use literal/range\n        for p, n in runs:\n            if n == 1:\n                outappend((LITERAL, p))\n            else:\n                outappend((RANGE, (p, p+n-1)))\n        if len(out) < len(charset):\n            return out\n    else:\n        # use bitmap\n        data = _mk_bitmap(charmap)\n        outappend((CHARSET, data))\n        return out\n    return charset\n\ndef _mk_bitmap(bits):\n    data = []\n    dataappend = data.append\n    if _sre.CODESIZE == 2:\n        start = (1, 0)\n    else:\n        start = (1L, 0L)\n    m, v = start\n    for c in bits:\n        if c:\n            v = v + m\n        m = m + m\n        if m > MAXCODE:\n            dataappend(v)\n            m, v = start\n    return data\n\n# To represent a big charset, first a bitmap of all characters in the\n# set is constructed. Then, this bitmap is sliced into chunks of 256\n# characters, duplicate chunks are eliminated, and each chunk is\n# given a number. In the compiled expression, the charset is\n# represented by a 32-bit word sequence, consisting of one word for\n# the number of different chunks, a sequence of 256 bytes (64 words)\n# of chunk numbers indexed by their original chunk position, and a\n# sequence of 256-bit chunks (8 words each).\n\n# Compression is normally good: in a typical charset, large ranges of\n# Unicode will be either completely excluded (e.g. if only cyrillic\n# letters are to be matched), or completely included (e.g. if large\n# subranges of Kanji match). These ranges will be represented by\n# chunks of all one-bits or all zero-bits.\n\n# Matching can be also done efficiently: the more significant byte of\n# the Unicode character is an index into the chunk number, and the\n# less significant byte is a bit index in the chunk (just like the\n# CHARSET matching).\n\n# In UCS-4 mode, the BIGCHARSET opcode still supports only subsets\n# of the basic multilingual plane; an efficient representation\n# for all of Unicode has not yet been developed. This means,\n# in particular, that negated charsets cannot be represented as\n# bigcharsets.\n\ndef _optimize_unicode(charset, fixup):\n    try:\n        import array\n    except ImportError:\n        return charset\n    charmap = [0]*65536\n    negate = 0\n    try:\n        for op, av in charset:\n            if op is NEGATE:\n                negate = 1\n            elif op is LITERAL:\n                charmap[fixup(av)] = 1\n            elif op is RANGE:\n                for i in xrange(fixup(av[0]), fixup(av[1])+1):\n                    charmap[i] = 1\n            elif op is CATEGORY:\n                # XXX: could expand category\n                return charset # cannot compress\n    except IndexError:\n        # non-BMP characters\n        return charset\n    if negate:\n        if sys.maxunicode != 65535:\n            # XXX: negation does not work with big charsets\n            return charset\n        for i in xrange(65536):\n            charmap[i] = not charmap[i]\n    comps = {}\n    mapping = [0]*256\n    block = 0\n    data = []\n    for i in xrange(256):\n        chunk = tuple(charmap[i*256:(i+1)*256])\n        new = comps.setdefault(chunk, block)\n        mapping[i] = new\n        if new == block:\n            block = block + 1\n            data = data + _mk_bitmap(chunk)\n    header = [block]\n    if _sre.CODESIZE == 2:\n        code = 'H'\n    else:\n        code = 'I'\n    # Convert block indices to byte array of 256 bytes\n    mapping = array.array('B', mapping).tostring()\n    # Convert byte array to word array\n    mapping = array.array(code, mapping)\n    assert mapping.itemsize == _sre.CODESIZE\n    header = header + mapping.tolist()\n    data[0:0] = header\n    return [(BIGCHARSET, data)]\n\ndef _simple(av):\n    # check if av is a \"simple\" operator\n    lo, hi = av[2].getwidth()\n    return lo == hi == 1 and av[2][0][0] != SUBPATTERN\n\ndef _compile_info(code, pattern, flags):\n    # internal: compile an info block.  in the current version,\n    # this contains min/max pattern width, and an optional literal\n    # prefix or a character map\n    lo, hi = pattern.getwidth()\n    if lo == 0:\n        return # not worth it\n    # look for a literal prefix\n    prefix = []\n    prefixappend = prefix.append\n    prefix_skip = 0\n    charset = [] # not used\n    charsetappend = charset.append\n    if not (flags & SRE_FLAG_IGNORECASE):\n        # look for literal prefix\n        for op, av in pattern.data:\n            if op is LITERAL:\n                if len(prefix) == prefix_skip:\n                    prefix_skip = prefix_skip + 1\n                prefixappend(av)\n            elif op is SUBPATTERN and len(av[1]) == 1:\n                op, av = av[1][0]\n                if op is LITERAL:\n                    prefixappend(av)\n                else:\n                    break\n            else:\n                break\n        # if no prefix, look for charset prefix\n        if not prefix and pattern.data:\n            op, av = pattern.data[0]\n            if op is SUBPATTERN and av[1]:\n                op, av = av[1][0]\n                if op is LITERAL:\n                    charsetappend((op, av))\n                elif op is BRANCH:\n                    c = []\n                    cappend = c.append\n                    for p in av[1]:\n                        if not p:\n                            break\n                        op, av = p[0]\n                        if op is LITERAL:\n                            cappend((op, av))\n                        else:\n                            break\n                    else:\n                        charset = c\n            elif op is BRANCH:\n                c = []\n                cappend = c.append\n                for p in av[1]:\n                    if not p:\n                        break\n                    op, av = p[0]\n                    if op is LITERAL:\n                        cappend((op, av))\n                    else:\n                        break\n                else:\n                    charset = c\n            elif op is IN:\n                charset = av\n##     if prefix:\n##         print \"*** PREFIX\", prefix, prefix_skip\n##     if charset:\n##         print \"*** CHARSET\", charset\n    # add an info block\n    emit = code.append\n    emit(OPCODES[INFO])\n    skip = len(code); emit(0)\n    # literal flag\n    mask = 0\n    if prefix:\n        mask = SRE_INFO_PREFIX\n        if len(prefix) == prefix_skip == len(pattern.data):\n            mask = mask + SRE_INFO_LITERAL\n    elif charset:\n        mask = mask + SRE_INFO_CHARSET\n    emit(mask)\n    # pattern length\n    if lo < MAXCODE:\n        emit(lo)\n    else:\n        emit(MAXCODE)\n        prefix = prefix[:MAXCODE]\n    if hi < MAXCODE:\n        emit(hi)\n    else:\n        emit(0)\n    # add literal prefix\n    if prefix:\n        emit(len(prefix)) # length\n        emit(prefix_skip) # skip\n        code.extend(prefix)\n        # generate overlap table\n        table = [-1] + ([0]*len(prefix))\n        for i in xrange(len(prefix)):\n            table[i+1] = table[i]+1\n            while table[i+1] > 0 and prefix[i] != prefix[table[i+1]-1]:\n                table[i+1] = table[table[i+1]-1]+1\n        code.extend(table[1:]) # don't store first entry\n    elif charset:\n        _compile_charset(charset, flags, code)\n    code[skip] = len(code) - skip\n\ntry:\n    unicode\nexcept NameError:\n    STRING_TYPES = (type(\"\"),)\nelse:\n    STRING_TYPES = (type(\"\"), type(unicode(\"\")))\n\ndef isstring(obj):\n    for tp in STRING_TYPES:\n        if isinstance(obj, tp):\n            return 1\n    return 0\n\ndef _code(p, flags):\n\n    flags = p.pattern.flags | flags\n    code = []\n\n    # compile info block\n    _compile_info(code, p, flags)\n\n    # compile the pattern\n    _compile(code, p.data, flags)\n\n    code.append(OPCODES[SUCCESS])\n\n    return code\n\ndef compile(p, flags=0):\n    # internal: convert pattern list to internal format\n\n    if isstring(p):\n        pattern = p\n        p = sre_parse.parse(p, flags)\n    else:\n        pattern = None\n\n    code = _code(p, flags)\n\n    # print code\n\n    # XXX: <fl> get rid of this limitation!\n    if p.pattern.groups > 100:\n        raise AssertionError(\n            \"sorry, but this version only supports 100 named groups\"\n            )\n\n    # map in either direction\n    groupindex = p.pattern.groupdict\n    indexgroup = [None] * p.pattern.groups\n    for k, i in groupindex.items():\n        indexgroup[i] = k\n\n    return _sre.compile(\n        pattern, flags | p.pattern.flags, code,\n        p.pattern.groups-1,\n        groupindex, indexgroup\n        )\n", 
    "sre_constants": "#\n# Secret Labs' Regular Expression Engine\n#\n# various symbols used by the regular expression engine.\n# run this script to update the _sre include files!\n#\n# Copyright (c) 1998-2001 by Secret Labs AB.  All rights reserved.\n#\n# See the sre.py file for information on usage and redistribution.\n#\n\n\"\"\"Internal support module for sre\"\"\"\n\n# update when constants are added or removed\n\nMAGIC = 20031017\n\ntry:\n    from _sre import MAXREPEAT\nexcept ImportError:\n    import _sre\n    MAXREPEAT = _sre.MAXREPEAT = 65535\n\n# SRE standard exception (access as sre.error)\n# should this really be here?\n\nclass error(Exception):\n    pass\n\n# operators\n\nFAILURE = \"failure\"\nSUCCESS = \"success\"\n\nANY = \"any\"\nANY_ALL = \"any_all\"\nASSERT = \"assert\"\nASSERT_NOT = \"assert_not\"\nAT = \"at\"\nBIGCHARSET = \"bigcharset\"\nBRANCH = \"branch\"\nCALL = \"call\"\nCATEGORY = \"category\"\nCHARSET = \"charset\"\nGROUPREF = \"groupref\"\nGROUPREF_IGNORE = \"groupref_ignore\"\nGROUPREF_EXISTS = \"groupref_exists\"\nIN = \"in\"\nIN_IGNORE = \"in_ignore\"\nINFO = \"info\"\nJUMP = \"jump\"\nLITERAL = \"literal\"\nLITERAL_IGNORE = \"literal_ignore\"\nMARK = \"mark\"\nMAX_REPEAT = \"max_repeat\"\nMAX_UNTIL = \"max_until\"\nMIN_REPEAT = \"min_repeat\"\nMIN_UNTIL = \"min_until\"\nNEGATE = \"negate\"\nNOT_LITERAL = \"not_literal\"\nNOT_LITERAL_IGNORE = \"not_literal_ignore\"\nRANGE = \"range\"\nREPEAT = \"repeat\"\nREPEAT_ONE = \"repeat_one\"\nSUBPATTERN = \"subpattern\"\nMIN_REPEAT_ONE = \"min_repeat_one\"\n\n# positions\nAT_BEGINNING = \"at_beginning\"\nAT_BEGINNING_LINE = \"at_beginning_line\"\nAT_BEGINNING_STRING = \"at_beginning_string\"\nAT_BOUNDARY = \"at_boundary\"\nAT_NON_BOUNDARY = \"at_non_boundary\"\nAT_END = \"at_end\"\nAT_END_LINE = \"at_end_line\"\nAT_END_STRING = \"at_end_string\"\nAT_LOC_BOUNDARY = \"at_loc_boundary\"\nAT_LOC_NON_BOUNDARY = \"at_loc_non_boundary\"\nAT_UNI_BOUNDARY = \"at_uni_boundary\"\nAT_UNI_NON_BOUNDARY = \"at_uni_non_boundary\"\n\n# categories\nCATEGORY_DIGIT = \"category_digit\"\nCATEGORY_NOT_DIGIT = \"category_not_digit\"\nCATEGORY_SPACE = \"category_space\"\nCATEGORY_NOT_SPACE = \"category_not_space\"\nCATEGORY_WORD = \"category_word\"\nCATEGORY_NOT_WORD = \"category_not_word\"\nCATEGORY_LINEBREAK = \"category_linebreak\"\nCATEGORY_NOT_LINEBREAK = \"category_not_linebreak\"\nCATEGORY_LOC_WORD = \"category_loc_word\"\nCATEGORY_LOC_NOT_WORD = \"category_loc_not_word\"\nCATEGORY_UNI_DIGIT = \"category_uni_digit\"\nCATEGORY_UNI_NOT_DIGIT = \"category_uni_not_digit\"\nCATEGORY_UNI_SPACE = \"category_uni_space\"\nCATEGORY_UNI_NOT_SPACE = \"category_uni_not_space\"\nCATEGORY_UNI_WORD = \"category_uni_word\"\nCATEGORY_UNI_NOT_WORD = \"category_uni_not_word\"\nCATEGORY_UNI_LINEBREAK = \"category_uni_linebreak\"\nCATEGORY_UNI_NOT_LINEBREAK = \"category_uni_not_linebreak\"\n\nOPCODES = [\n\n    # failure=0 success=1 (just because it looks better that way :-)\n    FAILURE, SUCCESS,\n\n    ANY, ANY_ALL,\n    ASSERT, ASSERT_NOT,\n    AT,\n    BRANCH,\n    CALL,\n    CATEGORY,\n    CHARSET, BIGCHARSET,\n    GROUPREF, GROUPREF_EXISTS, GROUPREF_IGNORE,\n    IN, IN_IGNORE,\n    INFO,\n    JUMP,\n    LITERAL, LITERAL_IGNORE,\n    MARK,\n    MAX_UNTIL,\n    MIN_UNTIL,\n    NOT_LITERAL, NOT_LITERAL_IGNORE,\n    NEGATE,\n    RANGE,\n    REPEAT,\n    REPEAT_ONE,\n    SUBPATTERN,\n    MIN_REPEAT_ONE\n\n]\n\nATCODES = [\n    AT_BEGINNING, AT_BEGINNING_LINE, AT_BEGINNING_STRING, AT_BOUNDARY,\n    AT_NON_BOUNDARY, AT_END, AT_END_LINE, AT_END_STRING,\n    AT_LOC_BOUNDARY, AT_LOC_NON_BOUNDARY, AT_UNI_BOUNDARY,\n    AT_UNI_NON_BOUNDARY\n]\n\nCHCODES = [\n    CATEGORY_DIGIT, CATEGORY_NOT_DIGIT, CATEGORY_SPACE,\n    CATEGORY_NOT_SPACE, CATEGORY_WORD, CATEGORY_NOT_WORD,\n    CATEGORY_LINEBREAK, CATEGORY_NOT_LINEBREAK, CATEGORY_LOC_WORD,\n    CATEGORY_LOC_NOT_WORD, CATEGORY_UNI_DIGIT, CATEGORY_UNI_NOT_DIGIT,\n    CATEGORY_UNI_SPACE, CATEGORY_UNI_NOT_SPACE, CATEGORY_UNI_WORD,\n    CATEGORY_UNI_NOT_WORD, CATEGORY_UNI_LINEBREAK,\n    CATEGORY_UNI_NOT_LINEBREAK\n]\n\ndef makedict(list):\n    d = {}\n    i = 0\n    for item in list:\n        d[item] = i\n        i = i + 1\n    return d\n\nOPCODES = makedict(OPCODES)\nATCODES = makedict(ATCODES)\nCHCODES = makedict(CHCODES)\n\n# replacement operations for \"ignore case\" mode\nOP_IGNORE = {\n    GROUPREF: GROUPREF_IGNORE,\n    IN: IN_IGNORE,\n    LITERAL: LITERAL_IGNORE,\n    NOT_LITERAL: NOT_LITERAL_IGNORE\n}\n\nAT_MULTILINE = {\n    AT_BEGINNING: AT_BEGINNING_LINE,\n    AT_END: AT_END_LINE\n}\n\nAT_LOCALE = {\n    AT_BOUNDARY: AT_LOC_BOUNDARY,\n    AT_NON_BOUNDARY: AT_LOC_NON_BOUNDARY\n}\n\nAT_UNICODE = {\n    AT_BOUNDARY: AT_UNI_BOUNDARY,\n    AT_NON_BOUNDARY: AT_UNI_NON_BOUNDARY\n}\n\nCH_LOCALE = {\n    CATEGORY_DIGIT: CATEGORY_DIGIT,\n    CATEGORY_NOT_DIGIT: CATEGORY_NOT_DIGIT,\n    CATEGORY_SPACE: CATEGORY_SPACE,\n    CATEGORY_NOT_SPACE: CATEGORY_NOT_SPACE,\n    CATEGORY_WORD: CATEGORY_LOC_WORD,\n    CATEGORY_NOT_WORD: CATEGORY_LOC_NOT_WORD,\n    CATEGORY_LINEBREAK: CATEGORY_LINEBREAK,\n    CATEGORY_NOT_LINEBREAK: CATEGORY_NOT_LINEBREAK\n}\n\nCH_UNICODE = {\n    CATEGORY_DIGIT: CATEGORY_UNI_DIGIT,\n    CATEGORY_NOT_DIGIT: CATEGORY_UNI_NOT_DIGIT,\n    CATEGORY_SPACE: CATEGORY_UNI_SPACE,\n    CATEGORY_NOT_SPACE: CATEGORY_UNI_NOT_SPACE,\n    CATEGORY_WORD: CATEGORY_UNI_WORD,\n    CATEGORY_NOT_WORD: CATEGORY_UNI_NOT_WORD,\n    CATEGORY_LINEBREAK: CATEGORY_UNI_LINEBREAK,\n    CATEGORY_NOT_LINEBREAK: CATEGORY_UNI_NOT_LINEBREAK\n}\n\n# flags\nSRE_FLAG_TEMPLATE = 1 # template mode (disable backtracking)\nSRE_FLAG_IGNORECASE = 2 # case insensitive\nSRE_FLAG_LOCALE = 4 # honour system locale\nSRE_FLAG_MULTILINE = 8 # treat target as multiline string\nSRE_FLAG_DOTALL = 16 # treat target as a single string\nSRE_FLAG_UNICODE = 32 # use unicode locale\nSRE_FLAG_VERBOSE = 64 # ignore whitespace and comments\nSRE_FLAG_DEBUG = 128 # debugging\n\n# flags for INFO primitive\nSRE_INFO_PREFIX = 1 # has prefix\nSRE_INFO_LITERAL = 2 # entire pattern is literal (given by prefix)\nSRE_INFO_CHARSET = 4 # pattern starts with character from given set\n\nif __name__ == \"__main__\":\n    def dump(f, d, prefix):\n        items = d.items()\n        items.sort(key=lambda a: a[1])\n        for k, v in items:\n            f.write(\"#define %s_%s %s\\n\" % (prefix, k.upper(), v))\n    f = open(\"sre_constants.h\", \"w\")\n    f.write(\"\"\"\\\n/*\n * Secret Labs' Regular Expression Engine\n *\n * regular expression matching engine\n *\n * NOTE: This file is generated by sre_constants.py.  If you need\n * to change anything in here, edit sre_constants.py and run it.\n *\n * Copyright (c) 1997-2001 by Secret Labs AB.  All rights reserved.\n *\n * See the _sre.c file for information on usage and redistribution.\n */\n\n\"\"\")\n\n    f.write(\"#define SRE_MAGIC %d\\n\" % MAGIC)\n\n    dump(f, OPCODES, \"SRE_OP\")\n    dump(f, ATCODES, \"SRE\")\n    dump(f, CHCODES, \"SRE\")\n\n    f.write(\"#define SRE_FLAG_TEMPLATE %d\\n\" % SRE_FLAG_TEMPLATE)\n    f.write(\"#define SRE_FLAG_IGNORECASE %d\\n\" % SRE_FLAG_IGNORECASE)\n    f.write(\"#define SRE_FLAG_LOCALE %d\\n\" % SRE_FLAG_LOCALE)\n    f.write(\"#define SRE_FLAG_MULTILINE %d\\n\" % SRE_FLAG_MULTILINE)\n    f.write(\"#define SRE_FLAG_DOTALL %d\\n\" % SRE_FLAG_DOTALL)\n    f.write(\"#define SRE_FLAG_UNICODE %d\\n\" % SRE_FLAG_UNICODE)\n    f.write(\"#define SRE_FLAG_VERBOSE %d\\n\" % SRE_FLAG_VERBOSE)\n\n    f.write(\"#define SRE_INFO_PREFIX %d\\n\" % SRE_INFO_PREFIX)\n    f.write(\"#define SRE_INFO_LITERAL %d\\n\" % SRE_INFO_LITERAL)\n    f.write(\"#define SRE_INFO_CHARSET %d\\n\" % SRE_INFO_CHARSET)\n\n    f.close()\n    print \"done\"\n", 
    "sre_parse": "#\n# Secret Labs' Regular Expression Engine\n#\n# convert re-style regular expression to sre pattern\n#\n# Copyright (c) 1998-2001 by Secret Labs AB.  All rights reserved.\n#\n# See the sre.py file for information on usage and redistribution.\n#\n\n\"\"\"Internal support module for sre\"\"\"\n\n# XXX: show string offset and offending character for all errors\n\nimport sys\n\nfrom sre_constants import *\n\ntry:\n    from __pypy__ import newdict\nexcept ImportError:\n    assert '__pypy__' not in sys.builtin_module_names\n    newdict = lambda _ : {}\n\nSPECIAL_CHARS = \".\\\\[{()*+?^$|\"\nREPEAT_CHARS = \"*+?{\"\n\nDIGITS = set(\"0123456789\")\n\nOCTDIGITS = set(\"01234567\")\nHEXDIGITS = set(\"0123456789abcdefABCDEF\")\n\nWHITESPACE = set(\" \\t\\n\\r\\v\\f\")\n\nESCAPES = {\n    r\"\\a\": (LITERAL, ord(\"\\a\")),\n    r\"\\b\": (LITERAL, ord(\"\\b\")),\n    r\"\\f\": (LITERAL, ord(\"\\f\")),\n    r\"\\n\": (LITERAL, ord(\"\\n\")),\n    r\"\\r\": (LITERAL, ord(\"\\r\")),\n    r\"\\t\": (LITERAL, ord(\"\\t\")),\n    r\"\\v\": (LITERAL, ord(\"\\v\")),\n    r\"\\\\\": (LITERAL, ord(\"\\\\\"))\n}\n\nCATEGORIES = {\n    r\"\\A\": (AT, AT_BEGINNING_STRING), # start of string\n    r\"\\b\": (AT, AT_BOUNDARY),\n    r\"\\B\": (AT, AT_NON_BOUNDARY),\n    r\"\\d\": (IN, [(CATEGORY, CATEGORY_DIGIT)]),\n    r\"\\D\": (IN, [(CATEGORY, CATEGORY_NOT_DIGIT)]),\n    r\"\\s\": (IN, [(CATEGORY, CATEGORY_SPACE)]),\n    r\"\\S\": (IN, [(CATEGORY, CATEGORY_NOT_SPACE)]),\n    r\"\\w\": (IN, [(CATEGORY, CATEGORY_WORD)]),\n    r\"\\W\": (IN, [(CATEGORY, CATEGORY_NOT_WORD)]),\n    r\"\\Z\": (AT, AT_END_STRING), # end of string\n}\n\nFLAGS = {\n    # standard flags\n    \"i\": SRE_FLAG_IGNORECASE,\n    \"L\": SRE_FLAG_LOCALE,\n    \"m\": SRE_FLAG_MULTILINE,\n    \"s\": SRE_FLAG_DOTALL,\n    \"x\": SRE_FLAG_VERBOSE,\n    # extensions\n    \"t\": SRE_FLAG_TEMPLATE,\n    \"u\": SRE_FLAG_UNICODE,\n}\n\nclass Pattern:\n    # master pattern object.  keeps track of global attributes\n    def __init__(self):\n        self.flags = 0\n        self.open = []\n        self.groups = 1\n        self.groupdict = newdict(\"module\")\n    def opengroup(self, name=None):\n        gid = self.groups\n        self.groups = gid + 1\n        if name is not None:\n            ogid = self.groupdict.get(name, None)\n            if ogid is not None:\n                raise error, (\"redefinition of group name %s as group %d; \"\n                              \"was group %d\" % (repr(name), gid,  ogid))\n            self.groupdict[name] = gid\n        self.open.append(gid)\n        return gid\n    def closegroup(self, gid):\n        self.open.remove(gid)\n    def checkgroup(self, gid):\n        return gid < self.groups and gid not in self.open\n\nclass SubPattern:\n    # a subpattern, in intermediate form\n    def __init__(self, pattern, data=None):\n        self.pattern = pattern\n        if data is None:\n            data = []\n        self.data = data\n        self.width = None\n    def dump(self, level=0):\n        nl = 1\n        seqtypes = type(()), type([])\n        for op, av in self.data:\n            print level*\"  \" + op,; nl = 0\n            if op == \"in\":\n                # member sublanguage\n                print; nl = 1\n                for op, a in av:\n                    print (level+1)*\"  \" + op, a\n            elif op == \"branch\":\n                print; nl = 1\n                i = 0\n                for a in av[1]:\n                    if i > 0:\n                        print level*\"  \" + \"or\"\n                    a.dump(level+1); nl = 1\n                    i = i + 1\n            elif type(av) in seqtypes:\n                for a in av:\n                    if isinstance(a, SubPattern):\n                        if not nl: print\n                        a.dump(level+1); nl = 1\n                    else:\n                        print a, ; nl = 0\n            else:\n                print av, ; nl = 0\n            if not nl: print\n    def __repr__(self):\n        return repr(self.data)\n    def __len__(self):\n        return len(self.data)\n    def __delitem__(self, index):\n        del self.data[index]\n    def __getitem__(self, index):\n        if isinstance(index, slice):\n            return SubPattern(self.pattern, self.data[index])\n        return self.data[index]\n    def __setitem__(self, index, code):\n        self.data[index] = code\n    def insert(self, index, code):\n        self.data.insert(index, code)\n    def append(self, code):\n        self.data.append(code)\n    def getwidth(self):\n        # determine the width (min, max) for this subpattern\n        if self.width:\n            return self.width\n        lo = hi = 0\n        UNITCODES = (ANY, RANGE, IN, LITERAL, NOT_LITERAL, CATEGORY)\n        REPEATCODES = (MIN_REPEAT, MAX_REPEAT)\n        for op, av in self.data:\n            if op is BRANCH:\n                i = MAXREPEAT - 1\n                j = 0\n                for av in av[1]:\n                    l, h = av.getwidth()\n                    i = min(i, l)\n                    j = max(j, h)\n                lo = lo + i\n                hi = hi + j\n            elif op is CALL:\n                i, j = av.getwidth()\n                lo = lo + i\n                hi = hi + j\n            elif op is SUBPATTERN:\n                i, j = av[1].getwidth()\n                lo = lo + i\n                hi = hi + j\n            elif op in REPEATCODES:\n                i, j = av[2].getwidth()\n                lo = lo + i * av[0]\n                hi = hi + j * av[1]\n            elif op in UNITCODES:\n                lo = lo + 1\n                hi = hi + 1\n            elif op == SUCCESS:\n                break\n        self.width = min(lo, MAXREPEAT - 1), min(hi, MAXREPEAT)\n        return self.width\n\nclass Tokenizer:\n    def __init__(self, string):\n        self.string = string\n        self.index = 0\n        self.__next()\n    def __next(self):\n        if self.index >= len(self.string):\n            self.next = None\n            return\n        char = self.string[self.index]\n        if char[0] == \"\\\\\":\n            try:\n                c = self.string[self.index + 1]\n            except IndexError:\n                raise error, \"bogus escape (end of line)\"\n            char = char + c\n        self.index = self.index + len(char)\n        self.next = char\n    def match(self, char, skip=1):\n        if char == self.next:\n            if skip:\n                self.__next()\n            return 1\n        return 0\n    def get(self):\n        this = self.next\n        self.__next()\n        return this\n    def tell(self):\n        return self.index, self.next\n    def seek(self, index):\n        self.index, self.next = index\n\ndef isident(char):\n    return \"a\" <= char <= \"z\" or \"A\" <= char <= \"Z\" or char == \"_\"\n\ndef isdigit(char):\n    return \"0\" <= char <= \"9\"\n\ndef isname(name):\n    # check that group name is a valid string\n    if not isident(name[0]):\n        return False\n    for char in name[1:]:\n        if not isident(char) and not isdigit(char):\n            return False\n    return True\n\ndef _class_escape(source, escape):\n    # handle escape code inside character class\n    code = ESCAPES.get(escape)\n    if code:\n        return code\n    code = CATEGORIES.get(escape)\n    if code and code[0] == IN:\n        return code\n    try:\n        c = escape[1:2]\n        if c == \"x\":\n            # hexadecimal escape (exactly two digits)\n            while source.next in HEXDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            escape = escape[2:]\n            if len(escape) != 2:\n                raise error, \"bogus escape: %s\" % repr(\"\\\\\" + escape)\n            return LITERAL, int(escape, 16) & 0xff\n        elif c in OCTDIGITS:\n            # octal escape (up to three digits)\n            while source.next in OCTDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            escape = escape[1:]\n            return LITERAL, int(escape, 8) & 0xff\n        elif c in DIGITS:\n            raise error, \"bogus escape: %s\" % repr(escape)\n        if len(escape) == 2:\n            return LITERAL, ord(escape[1])\n    except ValueError:\n        pass\n    raise error, \"bogus escape: %s\" % repr(escape)\n\ndef _escape(source, escape, state):\n    # handle escape code in expression\n    code = CATEGORIES.get(escape)\n    if code:\n        return code\n    code = ESCAPES.get(escape)\n    if code:\n        return code\n    try:\n        c = escape[1:2]\n        if c == \"x\":\n            # hexadecimal escape\n            while source.next in HEXDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            if len(escape) != 4:\n                raise ValueError\n            return LITERAL, int(escape[2:], 16) & 0xff\n        elif c == \"0\":\n            # octal escape\n            while source.next in OCTDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            return LITERAL, int(escape[1:], 8) & 0xff\n        elif c in DIGITS:\n            # octal escape *or* decimal group reference (sigh)\n            if source.next in DIGITS:\n                escape = escape + source.get()\n                if (escape[1] in OCTDIGITS and escape[2] in OCTDIGITS and\n                    source.next in OCTDIGITS):\n                    # got three octal digits; this is an octal escape\n                    escape = escape + source.get()\n                    return LITERAL, int(escape[1:], 8) & 0xff\n            # not an octal escape, so this is a group reference\n            group = int(escape[1:])\n            if group < state.groups:\n                if not state.checkgroup(group):\n                    raise error, \"cannot refer to open group\"\n                return GROUPREF, group\n            raise ValueError\n        if len(escape) == 2:\n            return LITERAL, ord(escape[1])\n    except ValueError:\n        pass\n    raise error, \"bogus escape: %s\" % repr(escape)\n\ndef _parse_sub(source, state, nested=1):\n    # parse an alternation: a|b|c\n\n    items = []\n    itemsappend = items.append\n    sourcematch = source.match\n    while 1:\n        itemsappend(_parse(source, state))\n        if sourcematch(\"|\"):\n            continue\n        if not nested:\n            break\n        if not source.next or sourcematch(\")\", 0):\n            break\n        else:\n            raise error, \"pattern not properly closed\"\n\n    if len(items) == 1:\n        return items[0]\n\n    subpattern = SubPattern(state)\n    subpatternappend = subpattern.append\n\n    # check if all items share a common prefix\n    while 1:\n        prefix = None\n        for item in items:\n            if not item:\n                break\n            if prefix is None:\n                prefix = item[0]\n            elif item[0] != prefix:\n                break\n        else:\n            # all subitems start with a common \"prefix\".\n            # move it out of the branch\n            for item in items:\n                del item[0]\n            subpatternappend(prefix)\n            continue # check next one\n        break\n\n    # check if the branch can be replaced by a character set\n    for item in items:\n        if len(item) != 1 or item[0][0] != LITERAL:\n            break\n    else:\n        # we can store this as a character set instead of a\n        # branch (the compiler may optimize this even more)\n        set = []\n        setappend = set.append\n        for item in items:\n            setappend(item[0])\n        subpatternappend((IN, set))\n        return subpattern\n\n    subpattern.append((BRANCH, (None, items)))\n    return subpattern\n\ndef _parse_sub_cond(source, state, condgroup):\n    item_yes = _parse(source, state)\n    if source.match(\"|\"):\n        item_no = _parse(source, state)\n        if source.match(\"|\"):\n            raise error, \"conditional backref with more than two branches\"\n    else:\n        item_no = None\n    if source.next and not source.match(\")\", 0):\n        raise error, \"pattern not properly closed\"\n    subpattern = SubPattern(state)\n    subpattern.append((GROUPREF_EXISTS, (condgroup, item_yes, item_no)))\n    return subpattern\n\n_PATTERNENDERS = set(\"|)\")\n_ASSERTCHARS = set(\"=!<\")\n_LOOKBEHINDASSERTCHARS = set(\"=!\")\n_REPEATCODES = set([MIN_REPEAT, MAX_REPEAT])\n\ndef _parse(source, state):\n    # parse a simple pattern\n    subpattern = SubPattern(state)\n\n    # precompute constants into local variables\n    subpatternappend = subpattern.append\n    sourceget = source.get\n    sourcematch = source.match\n    _len = len\n    PATTERNENDERS = _PATTERNENDERS\n    ASSERTCHARS = _ASSERTCHARS\n    LOOKBEHINDASSERTCHARS = _LOOKBEHINDASSERTCHARS\n    REPEATCODES = _REPEATCODES\n\n    while 1:\n\n        if source.next in PATTERNENDERS:\n            break # end of subpattern\n        this = sourceget()\n        if this is None:\n            break # end of pattern\n\n        if state.flags & SRE_FLAG_VERBOSE:\n            # skip whitespace and comments\n            if this in WHITESPACE:\n                continue\n            if this == \"#\":\n                while 1:\n                    this = sourceget()\n                    if this in (None, \"\\n\"):\n                        break\n                continue\n\n        if this and this[0] not in SPECIAL_CHARS:\n            subpatternappend((LITERAL, ord(this)))\n\n        elif this == \"[\":\n            # character set\n            set = []\n            setappend = set.append\n##          if sourcematch(\":\"):\n##              pass # handle character classes\n            if sourcematch(\"^\"):\n                setappend((NEGATE, None))\n            # check remaining characters\n            start = set[:]\n            while 1:\n                this = sourceget()\n                if this == \"]\" and set != start:\n                    break\n                elif this and this[0] == \"\\\\\":\n                    code1 = _class_escape(source, this)\n                elif this:\n                    code1 = LITERAL, ord(this)\n                else:\n                    raise error, \"unexpected end of regular expression\"\n                if sourcematch(\"-\"):\n                    # potential range\n                    this = sourceget()\n                    if this == \"]\":\n                        if code1[0] is IN:\n                            code1 = code1[1][0]\n                        setappend(code1)\n                        setappend((LITERAL, ord(\"-\")))\n                        break\n                    elif this:\n                        if this[0] == \"\\\\\":\n                            code2 = _class_escape(source, this)\n                        else:\n                            code2 = LITERAL, ord(this)\n                        if code1[0] != LITERAL or code2[0] != LITERAL:\n                            raise error, \"bad character range\"\n                        lo = code1[1]\n                        hi = code2[1]\n                        if hi < lo:\n                            raise error, \"bad character range\"\n                        setappend((RANGE, (lo, hi)))\n                    else:\n                        raise error, \"unexpected end of regular expression\"\n                else:\n                    if code1[0] is IN:\n                        code1 = code1[1][0]\n                    setappend(code1)\n\n            # XXX: <fl> should move set optimization to compiler!\n            if _len(set)==1 and set[0][0] is LITERAL:\n                subpatternappend(set[0]) # optimization\n            elif _len(set)==2 and set[0][0] is NEGATE and set[1][0] is LITERAL:\n                subpatternappend((NOT_LITERAL, set[1][1])) # optimization\n            else:\n                # XXX: <fl> should add charmap optimization here\n                subpatternappend((IN, set))\n\n        elif this and this[0] in REPEAT_CHARS:\n            # repeat previous item\n            if this == \"?\":\n                min, max = 0, 1\n            elif this == \"*\":\n                min, max = 0, MAXREPEAT\n\n            elif this == \"+\":\n                min, max = 1, MAXREPEAT\n            elif this == \"{\":\n                if source.next == \"}\":\n                    subpatternappend((LITERAL, ord(this)))\n                    continue\n                here = source.tell()\n                min, max = 0, MAXREPEAT\n                lo = hi = \"\"\n                while source.next in DIGITS:\n                    lo = lo + source.get()\n                if sourcematch(\",\"):\n                    while source.next in DIGITS:\n                        hi = hi + sourceget()\n                else:\n                    hi = lo\n                if not sourcematch(\"}\"):\n                    subpatternappend((LITERAL, ord(this)))\n                    source.seek(here)\n                    continue\n                if lo:\n                    min = int(lo)\n                    if min >= MAXREPEAT:\n                        raise OverflowError(\"the repetition number is too large\")\n                if hi:\n                    max = int(hi)\n                    if max >= MAXREPEAT:\n                        raise OverflowError(\"the repetition number is too large\")\n                    if max < min:\n                        raise error(\"bad repeat interval\")\n            else:\n                raise error, \"not supported\"\n            # figure out which item to repeat\n            if subpattern:\n                item = subpattern[-1:]\n            else:\n                item = None\n            if not item or (_len(item) == 1 and item[0][0] == AT):\n                raise error, \"nothing to repeat\"\n            if item[0][0] in REPEATCODES:\n                raise error, \"multiple repeat\"\n            if sourcematch(\"?\"):\n                subpattern[-1] = (MIN_REPEAT, (min, max, item))\n            else:\n                subpattern[-1] = (MAX_REPEAT, (min, max, item))\n\n        elif this == \".\":\n            subpatternappend((ANY, None))\n\n        elif this == \"(\":\n            group = 1\n            name = None\n            condgroup = None\n            if sourcematch(\"?\"):\n                group = 0\n                # options\n                if sourcematch(\"P\"):\n                    # python extensions\n                    if sourcematch(\"<\"):\n                        # named group: skip forward to end of name\n                        name = \"\"\n                        while 1:\n                            char = sourceget()\n                            if char is None:\n                                raise error, \"unterminated name\"\n                            if char == \">\":\n                                break\n                            name = name + char\n                        group = 1\n                        if not name:\n                            raise error(\"missing group name\")\n                        if not isname(name):\n                            raise error(\"bad character in group name %r\" %\n                                        name)\n                    elif sourcematch(\"=\"):\n                        # named backreference\n                        name = \"\"\n                        while 1:\n                            char = sourceget()\n                            if char is None:\n                                raise error, \"unterminated name\"\n                            if char == \")\":\n                                break\n                            name = name + char\n                        if not name:\n                            raise error(\"missing group name\")\n                        if not isname(name):\n                            raise error(\"bad character in backref group name \"\n                                        \"%r\" % name)\n                        gid = state.groupdict.get(name)\n                        if gid is None:\n                            msg = \"unknown group name: {0!r}\".format(name)\n                            raise error(msg)\n                        subpatternappend((GROUPREF, gid))\n                        continue\n                    else:\n                        char = sourceget()\n                        if char is None:\n                            raise error, \"unexpected end of pattern\"\n                        raise error, \"unknown specifier: ?P%s\" % char\n                elif sourcematch(\":\"):\n                    # non-capturing group\n                    group = 2\n                elif sourcematch(\"#\"):\n                    # comment\n                    while 1:\n                        if source.next is None or source.next == \")\":\n                            break\n                        sourceget()\n                    if not sourcematch(\")\"):\n                        raise error, \"unbalanced parenthesis\"\n                    continue\n                elif source.next in ASSERTCHARS:\n                    # lookahead assertions\n                    char = sourceget()\n                    dir = 1\n                    if char == \"<\":\n                        if source.next not in LOOKBEHINDASSERTCHARS:\n                            raise error, \"syntax error\"\n                        dir = -1 # lookbehind\n                        char = sourceget()\n                    p = _parse_sub(source, state)\n                    if not sourcematch(\")\"):\n                        raise error, \"unbalanced parenthesis\"\n                    if char == \"=\":\n                        subpatternappend((ASSERT, (dir, p)))\n                    else:\n                        subpatternappend((ASSERT_NOT, (dir, p)))\n                    continue\n                elif sourcematch(\"(\"):\n                    # conditional backreference group\n                    condname = \"\"\n                    while 1:\n                        char = sourceget()\n                        if char is None:\n                            raise error, \"unterminated name\"\n                        if char == \")\":\n                            break\n                        condname = condname + char\n                    group = 2\n                    if not condname:\n                        raise error(\"missing group name\")\n                    if isname(condname):\n                        condgroup = state.groupdict.get(condname)\n                        if condgroup is None:\n                            msg = \"unknown group name: {0!r}\".format(condname)\n                            raise error(msg)\n                    else:\n                        try:\n                            condgroup = int(condname)\n                        except ValueError:\n                            raise error, \"bad character in group name\"\n                else:\n                    # flags\n                    if not source.next in FLAGS:\n                        raise error, \"unexpected end of pattern\"\n                    while source.next in FLAGS:\n                        state.flags = state.flags | FLAGS[sourceget()]\n            if group:\n                # parse group contents\n                if group == 2:\n                    # anonymous group\n                    group = None\n                else:\n                    group = state.opengroup(name)\n                if condgroup:\n                    p = _parse_sub_cond(source, state, condgroup)\n                else:\n                    p = _parse_sub(source, state)\n                if not sourcematch(\")\"):\n                    raise error, \"unbalanced parenthesis\"\n                if group is not None:\n                    state.closegroup(group)\n                subpatternappend((SUBPATTERN, (group, p)))\n            else:\n                while 1:\n                    char = sourceget()\n                    if char is None:\n                        raise error, \"unexpected end of pattern\"\n                    if char == \")\":\n                        break\n                    raise error, \"unknown extension\"\n\n        elif this == \"^\":\n            subpatternappend((AT, AT_BEGINNING))\n\n        elif this == \"$\":\n            subpattern.append((AT, AT_END))\n\n        elif this and this[0] == \"\\\\\":\n            code = _escape(source, this, state)\n            subpatternappend(code)\n\n        else:\n            raise error, \"parser error\"\n\n    return subpattern\n\ndef parse(str, flags=0, pattern=None):\n    # parse 're' pattern into list of (opcode, argument) tuples\n\n    source = Tokenizer(str)\n\n    if pattern is None:\n        pattern = Pattern()\n    pattern.flags = flags\n    pattern.str = str\n\n    p = _parse_sub(source, pattern, 0)\n\n    tail = source.get()\n    if tail == \")\":\n        raise error, \"unbalanced parenthesis\"\n    elif tail:\n        raise error, \"bogus characters at end of regular expression\"\n\n    if flags & SRE_FLAG_DEBUG:\n        p.dump()\n\n    if not (flags & SRE_FLAG_VERBOSE) and p.pattern.flags & SRE_FLAG_VERBOSE:\n        # the VERBOSE flag was switched on inside the pattern.  to be\n        # on the safe side, we'll parse the whole thing again...\n        return parse(str, p.pattern.flags)\n\n    return p\n\ndef parse_template(source, pattern):\n    # parse 're' replacement string into list of literals and\n    # group references\n    s = Tokenizer(source)\n    sget = s.get\n    p = []\n    a = p.append\n    def literal(literal, p=p, pappend=a):\n        if p and p[-1][0] is LITERAL:\n            p[-1] = LITERAL, p[-1][1] + literal\n        else:\n            pappend((LITERAL, literal))\n    sep = source[:0]\n    if type(sep) is type(\"\"):\n        makechar = chr\n    else:\n        makechar = unichr\n    while 1:\n        this = sget()\n        if this is None:\n            break # end of replacement string\n        if this and this[0] == \"\\\\\":\n            # group\n            c = this[1:2]\n            if c == \"g\":\n                name = \"\"\n                if s.match(\"<\"):\n                    while 1:\n                        char = sget()\n                        if char is None:\n                            raise error, \"unterminated group name\"\n                        if char == \">\":\n                            break\n                        name = name + char\n                if not name:\n                    raise error, \"missing group name\"\n                try:\n                    index = int(name)\n                    if index < 0:\n                        raise error, \"negative group number\"\n                except ValueError:\n                    if not isname(name):\n                        raise error, \"bad character in group name\"\n                    try:\n                        index = pattern.groupindex[name]\n                    except KeyError:\n                        msg = \"unknown group name: {0!r}\".format(name)\n                        raise IndexError(msg)\n                a((MARK, index))\n            elif c == \"0\":\n                if s.next in OCTDIGITS:\n                    this = this + sget()\n                    if s.next in OCTDIGITS:\n                        this = this + sget()\n                literal(makechar(int(this[1:], 8) & 0xff))\n            elif c in DIGITS:\n                isoctal = False\n                if s.next in DIGITS:\n                    this = this + sget()\n                    if (c in OCTDIGITS and this[2] in OCTDIGITS and\n                        s.next in OCTDIGITS):\n                        this = this + sget()\n                        isoctal = True\n                        literal(makechar(int(this[1:], 8) & 0xff))\n                if not isoctal:\n                    a((MARK, int(this[1:])))\n            else:\n                try:\n                    this = makechar(ESCAPES[this][1])\n                except KeyError:\n                    pass\n                literal(this)\n        else:\n            literal(this)\n    # convert template to groups and literals lists\n    i = 0\n    groups = []\n    groupsappend = groups.append\n    literals = [None] * len(p)\n    for c, s in p:\n        if c is MARK:\n            groupsappend((i, s))\n            # literal[i] is already None\n        else:\n            literals[i] = s\n        i = i + 1\n    return groups, literals\n\ndef expand_template(template, match):\n    g = match.group\n    sep = match.string[:0]\n    groups, literals = template\n    literals = literals[:]\n    try:\n        for index, group in groups:\n            literals[index] = s = g(group)\n            if s is None:\n                raise error, \"unmatched group\"\n    except IndexError:\n        raise error, \"invalid group reference\"\n    return sep.join(literals)\n", 
    "stat": "\"\"\"Constants/functions for interpreting results of os.stat() and os.lstat().\n\nSuggested usage: from stat import *\n\"\"\"\n\n# Indices for stat struct members in the tuple returned by os.stat()\n\nST_MODE  = 0\nST_INO   = 1\nST_DEV   = 2\nST_NLINK = 3\nST_UID   = 4\nST_GID   = 5\nST_SIZE  = 6\nST_ATIME = 7\nST_MTIME = 8\nST_CTIME = 9\n\n# Extract bits from the mode\n\ndef S_IMODE(mode):\n    return mode & 07777\n\ndef S_IFMT(mode):\n    return mode & 0170000\n\n# Constants used as S_IFMT() for various file types\n# (not all are implemented on all systems)\n\nS_IFDIR  = 0040000\nS_IFCHR  = 0020000\nS_IFBLK  = 0060000\nS_IFREG  = 0100000\nS_IFIFO  = 0010000\nS_IFLNK  = 0120000\nS_IFSOCK = 0140000\n\n# Functions to test for each file type\n\ndef S_ISDIR(mode):\n    return S_IFMT(mode) == S_IFDIR\n\ndef S_ISCHR(mode):\n    return S_IFMT(mode) == S_IFCHR\n\ndef S_ISBLK(mode):\n    return S_IFMT(mode) == S_IFBLK\n\ndef S_ISREG(mode):\n    return S_IFMT(mode) == S_IFREG\n\ndef S_ISFIFO(mode):\n    return S_IFMT(mode) == S_IFIFO\n\ndef S_ISLNK(mode):\n    return S_IFMT(mode) == S_IFLNK\n\ndef S_ISSOCK(mode):\n    return S_IFMT(mode) == S_IFSOCK\n\n# Names for permission bits\n\nS_ISUID = 04000\nS_ISGID = 02000\nS_ENFMT = S_ISGID\nS_ISVTX = 01000\nS_IREAD = 00400\nS_IWRITE = 00200\nS_IEXEC = 00100\nS_IRWXU = 00700\nS_IRUSR = 00400\nS_IWUSR = 00200\nS_IXUSR = 00100\nS_IRWXG = 00070\nS_IRGRP = 00040\nS_IWGRP = 00020\nS_IXGRP = 00010\nS_IRWXO = 00007\nS_IROTH = 00004\nS_IWOTH = 00002\nS_IXOTH = 00001\n\n# Names for file flags\n\nUF_NODUMP    = 0x00000001\nUF_IMMUTABLE = 0x00000002\nUF_APPEND    = 0x00000004\nUF_OPAQUE    = 0x00000008\nUF_NOUNLINK  = 0x00000010\nUF_COMPRESSED = 0x00000020  # OS X: file is hfs-compressed\nUF_HIDDEN    = 0x00008000   # OS X: file should not be displayed\nSF_ARCHIVED  = 0x00010000\nSF_IMMUTABLE = 0x00020000\nSF_APPEND    = 0x00040000\nSF_NOUNLINK  = 0x00100000\nSF_SNAPSHOT  = 0x00200000\n", 
    "string": "\"\"\"A collection of string operations (most are no longer used).\n\nWarning: most of the code you see here isn't normally used nowadays.\nBeginning with Python 1.6, many of these functions are implemented as\nmethods on the standard string object. They used to be implemented by\na built-in module called strop, but strop is now obsolete itself.\n\nPublic module variables:\n\nwhitespace -- a string containing all characters considered whitespace\nlowercase -- a string containing all characters considered lowercase letters\nuppercase -- a string containing all characters considered uppercase letters\nletters -- a string containing all characters considered letters\ndigits -- a string containing all characters considered decimal digits\nhexdigits -- a string containing all characters considered hexadecimal digits\noctdigits -- a string containing all characters considered octal digits\npunctuation -- a string containing all characters considered punctuation\nprintable -- a string containing all characters considered printable\n\n\"\"\"\n\n# Some strings for ctype-style character classification\nwhitespace = ' \\t\\n\\r\\v\\f'\nlowercase = 'abcdefghijklmnopqrstuvwxyz'\nuppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nletters = lowercase + uppercase\nascii_lowercase = lowercase\nascii_uppercase = uppercase\nascii_letters = ascii_lowercase + ascii_uppercase\ndigits = '0123456789'\nhexdigits = digits + 'abcdef' + 'ABCDEF'\noctdigits = '01234567'\npunctuation = \"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\nprintable = digits + letters + punctuation + whitespace\n\n# Case conversion helpers\n# Use str to convert Unicode literal in case of -U\nl = map(chr, xrange(256))\n_idmap = str('').join(l)\ndel l\n\n# Functions which aren't available as string methods.\n\n# Capitalize the words in a string, e.g. \" aBc  dEf \" -> \"Abc Def\".\ndef capwords(s, sep=None):\n    \"\"\"capwords(s [,sep]) -> string\n\n    Split the argument into words using split, capitalize each\n    word using capitalize, and join the capitalized words using\n    join.  If the optional second argument sep is absent or None,\n    runs of whitespace characters are replaced by a single space\n    and leading and trailing whitespace are removed, otherwise\n    sep is used to split and join the words.\n\n    \"\"\"\n    return (sep or ' ').join(x.capitalize() for x in s.split(sep))\n\n\n# Construct a translation string\n_idmapL = None\ndef maketrans(fromstr, tostr):\n    \"\"\"maketrans(frm, to) -> string\n\n    Return a translation table (a string of 256 bytes long)\n    suitable for use in string.translate.  The strings frm and to\n    must be of the same length.\n\n    \"\"\"\n    n = len(fromstr)\n    if n != len(tostr):\n        raise ValueError, \"maketrans arguments must have same length\"\n    # this function has been rewritten to suit PyPy better; it is\n    # almost 10x faster than the original.\n    buf = bytearray(256)\n    for i in range(256):\n        buf[i] = i\n    for i in range(n):\n        buf[ord(fromstr[i])] = tostr[i]\n    return str(buf)\n\n\n\n####################################################################\nimport re as _re\n\nclass _multimap:\n    \"\"\"Helper class for combining multiple mappings.\n\n    Used by .{safe_,}substitute() to combine the mapping and keyword\n    arguments.\n    \"\"\"\n    def __init__(self, primary, secondary):\n        self._primary = primary\n        self._secondary = secondary\n\n    def __getitem__(self, key):\n        try:\n            return self._primary[key]\n        except KeyError:\n            return self._secondary[key]\n\n\nclass _TemplateMetaclass(type):\n    pattern = r\"\"\"\n    %(delim)s(?:\n      (?P<escaped>%(delim)s) |   # Escape sequence of two delimiters\n      (?P<named>%(id)s)      |   # delimiter and a Python identifier\n      {(?P<braced>%(id)s)}   |   # delimiter and a braced identifier\n      (?P<invalid>)              # Other ill-formed delimiter exprs\n    )\n    \"\"\"\n\n    def __init__(cls, name, bases, dct):\n        super(_TemplateMetaclass, cls).__init__(name, bases, dct)\n        if 'pattern' in dct:\n            pattern = cls.pattern\n        else:\n            pattern = _TemplateMetaclass.pattern % {\n                'delim' : _re.escape(cls.delimiter),\n                'id'    : cls.idpattern,\n                }\n        cls.pattern = _re.compile(pattern, _re.IGNORECASE | _re.VERBOSE)\n\n\nclass Template:\n    \"\"\"A string class for supporting $-substitutions.\"\"\"\n    __metaclass__ = _TemplateMetaclass\n\n    delimiter = '$'\n    idpattern = r'[_a-z][_a-z0-9]*'\n\n    def __init__(self, template):\n        self.template = template\n\n    # Search for $$, $identifier, ${identifier}, and any bare $'s\n\n    def _invalid(self, mo):\n        i = mo.start('invalid')\n        lines = self.template[:i].splitlines(True)\n        if not lines:\n            colno = 1\n            lineno = 1\n        else:\n            colno = i - len(''.join(lines[:-1]))\n            lineno = len(lines)\n        raise ValueError('Invalid placeholder in string: line %d, col %d' %\n                         (lineno, colno))\n\n    def substitute(self, *args, **kws):\n        if len(args) > 1:\n            raise TypeError('Too many positional arguments')\n        if not args:\n            mapping = kws\n        elif kws:\n            mapping = _multimap(kws, args[0])\n        else:\n            mapping = args[0]\n        # Helper function for .sub()\n        def convert(mo):\n            # Check the most common path first.\n            named = mo.group('named') or mo.group('braced')\n            if named is not None:\n                val = mapping[named]\n                # We use this idiom instead of str() because the latter will\n                # fail if val is a Unicode containing non-ASCII characters.\n                return '%s' % (val,)\n            if mo.group('escaped') is not None:\n                return self.delimiter\n            if mo.group('invalid') is not None:\n                self._invalid(mo)\n            raise ValueError('Unrecognized named group in pattern',\n                             self.pattern)\n        return self.pattern.sub(convert, self.template)\n\n    def safe_substitute(self, *args, **kws):\n        if len(args) > 1:\n            raise TypeError('Too many positional arguments')\n        if not args:\n            mapping = kws\n        elif kws:\n            mapping = _multimap(kws, args[0])\n        else:\n            mapping = args[0]\n        # Helper function for .sub()\n        def convert(mo):\n            named = mo.group('named')\n            if named is not None:\n                try:\n                    # We use this idiom instead of str() because the latter\n                    # will fail if val is a Unicode containing non-ASCII\n                    return '%s' % (mapping[named],)\n                except KeyError:\n                    return self.delimiter + named\n            braced = mo.group('braced')\n            if braced is not None:\n                try:\n                    return '%s' % (mapping[braced],)\n                except KeyError:\n                    return self.delimiter + '{' + braced + '}'\n            if mo.group('escaped') is not None:\n                return self.delimiter\n            if mo.group('invalid') is not None:\n                return self.delimiter\n            raise ValueError('Unrecognized named group in pattern',\n                             self.pattern)\n        return self.pattern.sub(convert, self.template)\n\n\n\n####################################################################\n# NOTE: Everything below here is deprecated.  Use string methods instead.\n# This stuff will go away in Python 3.0.\n\n# Backward compatible names for exceptions\nindex_error = ValueError\natoi_error = ValueError\natof_error = ValueError\natol_error = ValueError\n\n# convert UPPER CASE letters to lower case\ndef lower(s):\n    \"\"\"lower(s) -> string\n\n    Return a copy of the string s converted to lowercase.\n\n    \"\"\"\n    return s.lower()\n\n# Convert lower case letters to UPPER CASE\ndef upper(s):\n    \"\"\"upper(s) -> string\n\n    Return a copy of the string s converted to uppercase.\n\n    \"\"\"\n    return s.upper()\n\n# Swap lower case letters and UPPER CASE\ndef swapcase(s):\n    \"\"\"swapcase(s) -> string\n\n    Return a copy of the string s with upper case characters\n    converted to lowercase and vice versa.\n\n    \"\"\"\n    return s.swapcase()\n\n# Strip leading and trailing tabs and spaces\ndef strip(s, chars=None):\n    \"\"\"strip(s [,chars]) -> string\n\n    Return a copy of the string s with leading and trailing\n    whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n    If chars is unicode, S will be converted to unicode before stripping.\n\n    \"\"\"\n    return s.strip(chars)\n\n# Strip leading tabs and spaces\ndef lstrip(s, chars=None):\n    \"\"\"lstrip(s [,chars]) -> string\n\n    Return a copy of the string s with leading whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n\n    \"\"\"\n    return s.lstrip(chars)\n\n# Strip trailing tabs and spaces\ndef rstrip(s, chars=None):\n    \"\"\"rstrip(s [,chars]) -> string\n\n    Return a copy of the string s with trailing whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n\n    \"\"\"\n    return s.rstrip(chars)\n\n\n# Split a string into a list of space/tab-separated words\ndef split(s, sep=None, maxsplit=-1):\n    \"\"\"split(s [,sep [,maxsplit]]) -> list of strings\n\n    Return a list of the words in the string s, using sep as the\n    delimiter string.  If maxsplit is given, splits at no more than\n    maxsplit places (resulting in at most maxsplit+1 words).  If sep\n    is not specified or is None, any whitespace string is a separator.\n\n    (split and splitfields are synonymous)\n\n    \"\"\"\n    return s.split(sep, maxsplit)\nsplitfields = split\n\n# Split a string into a list of space/tab-separated words\ndef rsplit(s, sep=None, maxsplit=-1):\n    \"\"\"rsplit(s [,sep [,maxsplit]]) -> list of strings\n\n    Return a list of the words in the string s, using sep as the\n    delimiter string, starting at the end of the string and working\n    to the front.  If maxsplit is given, at most maxsplit splits are\n    done. If sep is not specified or is None, any whitespace string\n    is a separator.\n    \"\"\"\n    return s.rsplit(sep, maxsplit)\n\n# Join fields with optional separator\ndef join(words, sep = ' '):\n    \"\"\"join(list [,sep]) -> string\n\n    Return a string composed of the words in list, with\n    intervening occurrences of sep.  The default separator is a\n    single space.\n\n    (joinfields and join are synonymous)\n\n    \"\"\"\n    return sep.join(words)\njoinfields = join\n\n# Find substring, raise exception if not found\ndef index(s, *args):\n    \"\"\"index(s, sub [,start [,end]]) -> int\n\n    Like find but raises ValueError when the substring is not found.\n\n    \"\"\"\n    return s.index(*args)\n\n# Find last substring, raise exception if not found\ndef rindex(s, *args):\n    \"\"\"rindex(s, sub [,start [,end]]) -> int\n\n    Like rfind but raises ValueError when the substring is not found.\n\n    \"\"\"\n    return s.rindex(*args)\n\n# Count non-overlapping occurrences of substring\ndef count(s, *args):\n    \"\"\"count(s, sub[, start[,end]]) -> int\n\n    Return the number of occurrences of substring sub in string\n    s[start:end].  Optional arguments start and end are\n    interpreted as in slice notation.\n\n    \"\"\"\n    return s.count(*args)\n\n# Find substring, return -1 if not found\ndef find(s, *args):\n    \"\"\"find(s, sub [,start [,end]]) -> in\n\n    Return the lowest index in s where substring sub is found,\n    such that sub is contained within s[start,end].  Optional\n    arguments start and end are interpreted as in slice notation.\n\n    Return -1 on failure.\n\n    \"\"\"\n    return s.find(*args)\n\n# Find last substring, return -1 if not found\ndef rfind(s, *args):\n    \"\"\"rfind(s, sub [,start [,end]]) -> int\n\n    Return the highest index in s where substring sub is found,\n    such that sub is contained within s[start,end].  Optional\n    arguments start and end are interpreted as in slice notation.\n\n    Return -1 on failure.\n\n    \"\"\"\n    return s.rfind(*args)\n\n# for a bit of speed\n_float = float\n_int = int\n_long = long\n\n# Convert string to float\ndef atof(s):\n    \"\"\"atof(s) -> float\n\n    Return the floating point number represented by the string s.\n\n    \"\"\"\n    return _float(s)\n\n\n# Convert string to integer\ndef atoi(s , base=10):\n    \"\"\"atoi(s [,base]) -> int\n\n    Return the integer represented by the string s in the given\n    base, which defaults to 10.  The string s must consist of one\n    or more digits, possibly preceded by a sign.  If base is 0, it\n    is chosen from the leading characters of s, 0 for octal, 0x or\n    0X for hexadecimal.  If base is 16, a preceding 0x or 0X is\n    accepted.\n\n    \"\"\"\n    return _int(s, base)\n\n\n# Convert string to long integer\ndef atol(s, base=10):\n    \"\"\"atol(s [,base]) -> long\n\n    Return the long integer represented by the string s in the\n    given base, which defaults to 10.  The string s must consist\n    of one or more digits, possibly preceded by a sign.  If base\n    is 0, it is chosen from the leading characters of s, 0 for\n    octal, 0x or 0X for hexadecimal.  If base is 16, a preceding\n    0x or 0X is accepted.  A trailing L or l is not accepted,\n    unless base is 0.\n\n    \"\"\"\n    return _long(s, base)\n\n\n# Left-justify a string\ndef ljust(s, width, *args):\n    \"\"\"ljust(s, width[, fillchar]) -> string\n\n    Return a left-justified version of s, in a field of the\n    specified width, padded with spaces as needed.  The string is\n    never truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.ljust(width, *args)\n\n# Right-justify a string\ndef rjust(s, width, *args):\n    \"\"\"rjust(s, width[, fillchar]) -> string\n\n    Return a right-justified version of s, in a field of the\n    specified width, padded with spaces as needed.  The string is\n    never truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.rjust(width, *args)\n\n# Center a string\ndef center(s, width, *args):\n    \"\"\"center(s, width[, fillchar]) -> string\n\n    Return a center version of s, in a field of the specified\n    width. padded with spaces as needed.  The string is never\n    truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.center(width, *args)\n\n# Zero-fill a number, e.g., (12, 3) --> '012' and (-3, 3) --> '-03'\n# Decadent feature: the argument may be a string or a number\n# (Use of this is deprecated; it should be a string as with ljust c.s.)\ndef zfill(x, width):\n    \"\"\"zfill(x, width) -> string\n\n    Pad a numeric string x with zeros on the left, to fill a field\n    of the specified width.  The string x is never truncated.\n\n    \"\"\"\n    if not isinstance(x, basestring):\n        x = repr(x)\n    return x.zfill(width)\n\n# Expand tabs in a string.\n# Doesn't take non-printing chars into account, but does understand \\n.\ndef expandtabs(s, tabsize=8):\n    \"\"\"expandtabs(s [,tabsize]) -> string\n\n    Return a copy of the string s with all tab characters replaced\n    by the appropriate number of spaces, depending on the current\n    column, and the tabsize (default 8).\n\n    \"\"\"\n    return s.expandtabs(tabsize)\n\n# Character translation through look-up table.\ndef translate(s, table, deletions=\"\"):\n    \"\"\"translate(s,table [,deletions]) -> string\n\n    Return a copy of the string s, where all characters occurring\n    in the optional argument deletions are removed, and the\n    remaining characters have been mapped through the given\n    translation table, which must be a string of length 256.  The\n    deletions argument is not allowed for Unicode strings.\n\n    \"\"\"\n    if deletions or table is None:\n        return s.translate(table, deletions)\n    else:\n        # Add s[:0] so that if s is Unicode and table is an 8-bit string,\n        # table is converted to Unicode.  This means that table *cannot*\n        # be a dictionary -- for that feature, use u.translate() directly.\n        return s.translate(table + s[:0])\n\n# Capitalize a string, e.g. \"aBc  dEf\" -> \"Abc  def\".\ndef capitalize(s):\n    \"\"\"capitalize(s) -> string\n\n    Return a copy of the string s with only its first character\n    capitalized.\n\n    \"\"\"\n    return s.capitalize()\n\n# Substring replacement (global)\ndef replace(s, old, new, maxreplace=-1):\n    \"\"\"replace (str, old, new[, maxreplace]) -> string\n\n    Return a copy of string str with all occurrences of substring\n    old replaced by new. If the optional argument maxreplace is\n    given, only the first maxreplace occurrences are replaced.\n\n    \"\"\"\n    return s.replace(old, new, maxreplace)\n\n\n# Try importing optional built-in module \"strop\" -- if it exists,\n# it redefines some string operations that are 100-1000 times faster.\n# It also defines values for whitespace, lowercase and uppercase\n# that match <ctype.h>'s definitions.\n\ntry:\n    from strop import maketrans, lowercase, uppercase, whitespace\n    letters = lowercase + uppercase\nexcept ImportError:\n    pass                                          # Use the original versions\n\n########################################################################\n# the Formatter class\n# see PEP 3101 for details and purpose of this class\n\n# The hard parts are reused from the C implementation.  They're exposed as \"_\"\n# prefixed methods of str and unicode.\n\n# The overall parser is implemented in str._formatter_parser.\n# The field name parser is implemented in str._formatter_field_name_split\n\nclass Formatter(object):\n    def format(self, format_string, *args, **kwargs):\n        return self.vformat(format_string, args, kwargs)\n\n    def vformat(self, format_string, args, kwargs):\n        used_args = set()\n        result = self._vformat(format_string, args, kwargs, used_args, 2)\n        self.check_unused_args(used_args, args, kwargs)\n        return result\n\n    def _vformat(self, format_string, args, kwargs, used_args, recursion_depth):\n        if recursion_depth < 0:\n            raise ValueError('Max string recursion exceeded')\n        result = []\n        for literal_text, field_name, format_spec, conversion in \\\n                self.parse(format_string):\n\n            # output the literal text\n            if literal_text:\n                result.append(literal_text)\n\n            # if there's a field, output it\n            if field_name is not None:\n                # this is some markup, find the object and do\n                #  the formatting\n\n                # given the field_name, find the object it references\n                #  and the argument it came from\n                obj, arg_used = self.get_field(field_name, args, kwargs)\n                used_args.add(arg_used)\n\n                # do any conversion on the resulting object\n                obj = self.convert_field(obj, conversion)\n\n                # expand the format spec, if needed\n                format_spec = self._vformat(format_spec, args, kwargs,\n                                            used_args, recursion_depth-1)\n\n                # format the object and append to the result\n                result.append(self.format_field(obj, format_spec))\n\n        return ''.join(result)\n\n\n    def get_value(self, key, args, kwargs):\n        if isinstance(key, (int, long)):\n            return args[key]\n        else:\n            return kwargs[key]\n\n\n    def check_unused_args(self, used_args, args, kwargs):\n        pass\n\n\n    def format_field(self, value, format_spec):\n        return format(value, format_spec)\n\n\n    def convert_field(self, value, conversion):\n        # do any conversion on the resulting object\n        if conversion is None:\n            return value\n        elif conversion == 's':\n            return str(value)\n        elif conversion == 'r':\n            return repr(value)\n        raise ValueError(\"Unknown conversion specifier {0!s}\".format(conversion))\n\n\n    # returns an iterable that contains tuples of the form:\n    # (literal_text, field_name, format_spec, conversion)\n    # literal_text can be zero length\n    # field_name can be None, in which case there's no\n    #  object to format and output\n    # if field_name is not None, it is looked up, formatted\n    #  with format_spec and conversion and then used\n    def parse(self, format_string):\n        return format_string._formatter_parser()\n\n\n    # given a field_name, find the object it references.\n    #  field_name:   the field being looked up, e.g. \"0.name\"\n    #                 or \"lookup[3]\"\n    #  used_args:    a set of which args have been used\n    #  args, kwargs: as passed in to vformat\n    def get_field(self, field_name, args, kwargs):\n        first, rest = field_name._formatter_field_name_split()\n\n        obj = self.get_value(first, args, kwargs)\n\n        # loop through the rest of the field_name, doing\n        #  getattr or getitem as needed\n        for is_attr, i in rest:\n            if is_attr:\n                obj = getattr(obj, i)\n            else:\n                obj = obj[i]\n\n        return obj, first\n", 
    "struct": "from _struct import *\nfrom _struct import _clearcache\nfrom _struct import __doc__\n", 
    "tempfile": "\"\"\"Temporary files.\n\nThis module provides generic, low- and high-level interfaces for\ncreating temporary files and directories.  The interfaces listed\nas \"safe\" just below can be used without fear of race conditions.\nThose listed as \"unsafe\" cannot, and are provided for backward\ncompatibility only.\n\nThis module also provides some data items to the user:\n\n  TMP_MAX  - maximum number of names that will be tried before\n             giving up.\n  template - the default prefix for all temporary names.\n             You may change this to control the default prefix.\n  tempdir  - If this is set to a string before the first use of\n             any routine from this module, it will be considered as\n             another candidate location to store temporary files.\n\"\"\"\n\n__all__ = [\n    \"NamedTemporaryFile\", \"TemporaryFile\", # high level safe interfaces\n    \"SpooledTemporaryFile\",\n    \"mkstemp\", \"mkdtemp\",                  # low level safe interfaces\n    \"mktemp\",                              # deprecated unsafe interface\n    \"TMP_MAX\", \"gettempprefix\",            # constants\n    \"tempdir\", \"gettempdir\"\n   ]\n\n\n# Imports.\n\nimport io as _io\nimport os as _os\nimport errno as _errno\nfrom random import Random as _Random\n\ntry:\n    from cStringIO import StringIO as _StringIO\nexcept ImportError:\n    from StringIO import StringIO as _StringIO\n\ntry:\n    import fcntl as _fcntl\nexcept ImportError:\n    def _set_cloexec(fd):\n        pass\nelse:\n    def _set_cloexec(fd):\n        try:\n            flags = _fcntl.fcntl(fd, _fcntl.F_GETFD, 0)\n        except IOError:\n            pass\n        else:\n            # flags read successfully, modify\n            flags |= _fcntl.FD_CLOEXEC\n            _fcntl.fcntl(fd, _fcntl.F_SETFD, flags)\n\n\ntry:\n    import thread as _thread\nexcept ImportError:\n    import dummy_thread as _thread\n_allocate_lock = _thread.allocate_lock\n\n_text_openflags = _os.O_RDWR | _os.O_CREAT | _os.O_EXCL\nif hasattr(_os, 'O_NOINHERIT'):\n    _text_openflags |= _os.O_NOINHERIT\nif hasattr(_os, 'O_NOFOLLOW'):\n    _text_openflags |= _os.O_NOFOLLOW\n\n_bin_openflags = _text_openflags\nif hasattr(_os, 'O_BINARY'):\n    _bin_openflags |= _os.O_BINARY\n\nif hasattr(_os, 'TMP_MAX'):\n    TMP_MAX = _os.TMP_MAX\nelse:\n    TMP_MAX = 10000\n\ntemplate = \"tmp\"\n\n# Internal routines.\n\n_once_lock = _allocate_lock()\n\nif hasattr(_os, \"lstat\"):\n    _stat = _os.lstat\nelif hasattr(_os, \"stat\"):\n    _stat = _os.stat\nelse:\n    # Fallback.  All we need is something that raises os.error if the\n    # file doesn't exist.\n    def _stat(fn):\n        try:\n            f = open(fn)\n        except IOError:\n            raise _os.error\n        f.close()\n\ndef _exists(fn):\n    try:\n        _stat(fn)\n    except _os.error:\n        return False\n    else:\n        return True\n\nclass _RandomNameSequence:\n    \"\"\"An instance of _RandomNameSequence generates an endless\n    sequence of unpredictable strings which can safely be incorporated\n    into file names.  Each string is six characters long.  Multiple\n    threads can safely use the same instance at the same time.\n\n    _RandomNameSequence is an iterator.\"\"\"\n\n    characters = (\"abcdefghijklmnopqrstuvwxyz\" +\n                  \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" +\n                  \"0123456789_\")\n\n    def __init__(self):\n        self.mutex = _allocate_lock()\n        self.normcase = _os.path.normcase\n\n    @property\n    def rng(self):\n        cur_pid = _os.getpid()\n        if cur_pid != getattr(self, '_rng_pid', None):\n            self._rng = _Random()\n            self._rng_pid = cur_pid\n        return self._rng\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        m = self.mutex\n        c = self.characters\n        choose = self.rng.choice\n\n        m.acquire()\n        try:\n            letters = [choose(c) for dummy in \"123456\"]\n        finally:\n            m.release()\n\n        return self.normcase(''.join(letters))\n\ndef _candidate_tempdir_list():\n    \"\"\"Generate a list of candidate temporary directories which\n    _get_default_tempdir will try.\"\"\"\n\n    dirlist = []\n\n    # First, try the environment.\n    for envname in 'TMPDIR', 'TEMP', 'TMP':\n        dirname = _os.getenv(envname)\n        if dirname: dirlist.append(dirname)\n\n    # Failing that, try OS-specific locations.\n    if _os.name == 'riscos':\n        dirname = _os.getenv('Wimp$ScrapDir')\n        if dirname: dirlist.append(dirname)\n    elif _os.name == 'nt':\n        dirlist.extend([ r'c:\\temp', r'c:\\tmp', r'\\temp', r'\\tmp' ])\n    else:\n        dirlist.extend([ '/tmp', '/var/tmp', '/usr/tmp' ])\n\n    # As a last resort, the current directory.\n    try:\n        dirlist.append(_os.getcwd())\n    except (AttributeError, _os.error):\n        dirlist.append(_os.curdir)\n\n    return dirlist\n\ndef _get_default_tempdir():\n    \"\"\"Calculate the default directory to use for temporary files.\n    This routine should be called exactly once.\n\n    We determine whether or not a candidate temp dir is usable by\n    trying to create and write to a file in that directory.  If this\n    is successful, the test file is deleted.  To prevent denial of\n    service, the name of the test file must be randomized.\"\"\"\n\n    namer = _RandomNameSequence()\n    dirlist = _candidate_tempdir_list()\n    flags = _text_openflags\n\n    for dir in dirlist:\n        if dir != _os.curdir:\n            dir = _os.path.normcase(_os.path.abspath(dir))\n        # Try only a few names per directory.\n        for seq in xrange(100):\n            name = namer.next()\n            filename = _os.path.join(dir, name)\n            try:\n                fd = _os.open(filename, flags, 0o600)\n                try:\n                    try:\n                        with _io.open(fd, 'wb', closefd=False) as fp:\n                            fp.write(b'blat')\n                    finally:\n                        _os.close(fd)\n                finally:\n                    _os.unlink(filename)\n                return dir\n            except (OSError, IOError) as e:\n                if e.args[0] != _errno.EEXIST:\n                    break # no point trying more names in this directory\n                pass\n    raise IOError, (_errno.ENOENT,\n                    (\"No usable temporary directory found in %s\" % dirlist))\n\n_name_sequence = None\n\ndef _get_candidate_names():\n    \"\"\"Common setup sequence for all user-callable interfaces.\"\"\"\n\n    global _name_sequence\n    if _name_sequence is None:\n        _once_lock.acquire()\n        try:\n            if _name_sequence is None:\n                _name_sequence = _RandomNameSequence()\n        finally:\n            _once_lock.release()\n    return _name_sequence\n\n\ndef _mkstemp_inner(dir, pre, suf, flags):\n    \"\"\"Code common to mkstemp, TemporaryFile, and NamedTemporaryFile.\"\"\"\n\n    names = _get_candidate_names()\n\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, pre + name + suf)\n        try:\n            fd = _os.open(file, flags, 0600)\n            _set_cloexec(fd)\n            return (fd, _os.path.abspath(file))\n        except OSError, e:\n            if e.errno == _errno.EEXIST:\n                continue # try again\n            if _os.name == 'nt' and e.errno == _errno.EACCES:\n                # On windows, when a directory with the chosen name already\n                # exists, EACCES error code is returned instead of EEXIST.\n                continue\n            raise\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary file name found\")\n\n\n# User visible interfaces.\n\ndef gettempprefix():\n    \"\"\"Accessor for tempdir.template.\"\"\"\n    return template\n\ntempdir = None\n\ndef gettempdir():\n    \"\"\"Accessor for tempfile.tempdir.\"\"\"\n    global tempdir\n    if tempdir is None:\n        _once_lock.acquire()\n        try:\n            if tempdir is None:\n                tempdir = _get_default_tempdir()\n        finally:\n            _once_lock.release()\n    return tempdir\n\ndef mkstemp(suffix=\"\", prefix=template, dir=None, text=False):\n    \"\"\"User-callable function to create and return a unique temporary\n    file.  The return value is a pair (fd, name) where fd is the\n    file descriptor returned by os.open, and name is the filename.\n\n    If 'suffix' is specified, the file name will end with that suffix,\n    otherwise there will be no suffix.\n\n    If 'prefix' is specified, the file name will begin with that prefix,\n    otherwise a default prefix is used.\n\n    If 'dir' is specified, the file will be created in that directory,\n    otherwise a default directory is used.\n\n    If 'text' is specified and true, the file is opened in text\n    mode.  Else (the default) the file is opened in binary mode.  On\n    some operating systems, this makes no difference.\n\n    The file is readable and writable only by the creating user ID.\n    If the operating system uses permission bits to indicate whether a\n    file is executable, the file is executable by no one. The file\n    descriptor is not inherited by children of this process.\n\n    Caller is responsible for deleting the file when done with it.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    if text:\n        flags = _text_openflags\n    else:\n        flags = _bin_openflags\n\n    return _mkstemp_inner(dir, prefix, suffix, flags)\n\n\ndef mkdtemp(suffix=\"\", prefix=template, dir=None):\n    \"\"\"User-callable function to create and return a unique temporary\n    directory.  The return value is the pathname of the directory.\n\n    Arguments are as for mkstemp, except that the 'text' argument is\n    not accepted.\n\n    The directory is readable, writable, and searchable only by the\n    creating user.\n\n    Caller is responsible for deleting the directory when done with it.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    names = _get_candidate_names()\n\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, prefix + name + suffix)\n        try:\n            _os.mkdir(file, 0700)\n            return file\n        except OSError, e:\n            if e.errno == _errno.EEXIST:\n                continue # try again\n            raise\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary directory name found\")\n\ndef mktemp(suffix=\"\", prefix=template, dir=None):\n    \"\"\"User-callable function to return a unique temporary file name.  The\n    file is not created.\n\n    Arguments are as for mkstemp, except that the 'text' argument is\n    not accepted.\n\n    This function is unsafe and should not be used.  The file name\n    refers to a file that did not exist at some point, but by the time\n    you get around to creating it, someone else may have beaten you to\n    the punch.\n    \"\"\"\n\n##    from warnings import warn as _warn\n##    _warn(\"mktemp is a potential security risk to your program\",\n##          RuntimeWarning, stacklevel=2)\n\n    if dir is None:\n        dir = gettempdir()\n\n    names = _get_candidate_names()\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, prefix + name + suffix)\n        if not _exists(file):\n            return file\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary filename found\")\n\n\nclass _TemporaryFileWrapper:\n    \"\"\"Temporary file wrapper\n\n    This class provides a wrapper around files opened for\n    temporary use.  In particular, it seeks to automatically\n    remove the file when it is no longer needed.\n    \"\"\"\n\n    def __init__(self, file, name, delete=True):\n        self.file = file\n        self.name = name\n        self.close_called = False\n        self.delete = delete\n\n    def __getattr__(self, name):\n        # Attribute lookups are delegated to the underlying file\n        # and cached for non-numeric results\n        # (i.e. methods are cached, closed and friends are not)\n        file = self.__dict__['file']\n        a = getattr(file, name)\n        if not issubclass(type(a), type(0)):\n            setattr(self, name, a)\n        return a\n\n    # The underlying __enter__ method returns the wrong object\n    # (self.file) so override it to return the wrapper\n    def __enter__(self):\n        self.file.__enter__()\n        return self\n\n    # NT provides delete-on-close as a primitive, so we don't need\n    # the wrapper to do anything special.  We still use it so that\n    # file.name is useful (i.e. not \"(fdopen)\") with NamedTemporaryFile.\n    if _os.name != 'nt':\n        # Cache the unlinker so we don't get spurious errors at\n        # shutdown when the module-level \"os\" is None'd out.  Note\n        # that this must be referenced as self.unlink, because the\n        # name TemporaryFileWrapper may also get None'd out before\n        # __del__ is called.\n        unlink = _os.unlink\n\n        def close(self):\n            if not self.close_called:\n                self.close_called = True\n                self.file.close()\n                if self.delete:\n                    self.unlink(self.name)\n\n        def __del__(self):\n            self.close()\n\n        # Need to trap __exit__ as well to ensure the file gets\n        # deleted when used in a with statement\n        def __exit__(self, exc, value, tb):\n            result = self.file.__exit__(exc, value, tb)\n            self.close()\n            return result\n    else:\n        def __exit__(self, exc, value, tb):\n            self.file.__exit__(exc, value, tb)\n\n\ndef NamedTemporaryFile(mode='w+b', bufsize=-1, suffix=\"\",\n                       prefix=template, dir=None, delete=True):\n    \"\"\"Create and return a temporary file.\n    Arguments:\n    'prefix', 'suffix', 'dir' -- as for mkstemp.\n    'mode' -- the mode argument to os.fdopen (default \"w+b\").\n    'bufsize' -- the buffer size argument to os.fdopen (default -1).\n    'delete' -- whether the file is deleted on close (default True).\n    The file is created as mkstemp() would do it.\n\n    Returns an object with a file-like interface; the name of the file\n    is accessible as file.name.  The file will be automatically deleted\n    when it is closed unless the 'delete' argument is set to False.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    if 'b' in mode:\n        flags = _bin_openflags\n    else:\n        flags = _text_openflags\n\n    # Setting O_TEMPORARY in the flags causes the OS to delete\n    # the file when it is closed.  This is only supported by Windows.\n    if _os.name == 'nt' and delete:\n        flags |= _os.O_TEMPORARY\n\n    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n    try:\n        file = _os.fdopen(fd, mode, bufsize)\n        return _TemporaryFileWrapper(file, name, delete)\n    except:\n        _os.close(fd)\n        raise\n\nif _os.name != 'posix' or _os.sys.platform == 'cygwin':\n    # On non-POSIX and Cygwin systems, assume that we cannot unlink a file\n    # while it is open.\n    TemporaryFile = NamedTemporaryFile\n\nelse:\n    def TemporaryFile(mode='w+b', bufsize=-1, suffix=\"\",\n                      prefix=template, dir=None):\n        \"\"\"Create and return a temporary file.\n        Arguments:\n        'prefix', 'suffix', 'dir' -- as for mkstemp.\n        'mode' -- the mode argument to os.fdopen (default \"w+b\").\n        'bufsize' -- the buffer size argument to os.fdopen (default -1).\n        The file is created as mkstemp() would do it.\n\n        Returns an object with a file-like interface.  The file has no\n        name, and will cease to exist when it is closed.\n        \"\"\"\n\n        if dir is None:\n            dir = gettempdir()\n\n        if 'b' in mode:\n            flags = _bin_openflags\n        else:\n            flags = _text_openflags\n\n        (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n        try:\n            _os.unlink(name)\n            return _os.fdopen(fd, mode, bufsize)\n        except:\n            _os.close(fd)\n            raise\n\nclass SpooledTemporaryFile:\n    \"\"\"Temporary file wrapper, specialized to switch from\n    StringIO to a real file when it exceeds a certain size or\n    when a fileno is needed.\n    \"\"\"\n    _rolled = False\n\n    def __init__(self, max_size=0, mode='w+b', bufsize=-1,\n                 suffix=\"\", prefix=template, dir=None):\n        self._file = _StringIO()\n        self._max_size = max_size\n        self._rolled = False\n        self._TemporaryFileArgs = (mode, bufsize, suffix, prefix, dir)\n\n    def _check(self, file):\n        if self._rolled: return\n        max_size = self._max_size\n        if max_size and file.tell() > max_size:\n            self.rollover()\n\n    def rollover(self):\n        if self._rolled: return\n        file = self._file\n        newfile = self._file = TemporaryFile(*self._TemporaryFileArgs)\n        del self._TemporaryFileArgs\n\n        newfile.write(file.getvalue())\n        newfile.seek(file.tell(), 0)\n\n        self._rolled = True\n\n    # The method caching trick from NamedTemporaryFile\n    # won't work here, because _file may change from a\n    # _StringIO instance to a real file. So we list\n    # all the methods directly.\n\n    # Context management protocol\n    def __enter__(self):\n        if self._file.closed:\n            raise ValueError(\"Cannot enter context with closed file\")\n        return self\n\n    def __exit__(self, exc, value, tb):\n        self._file.close()\n\n    # file protocol\n    def __iter__(self):\n        return self._file.__iter__()\n\n    def close(self):\n        self._file.close()\n\n    @property\n    def closed(self):\n        return self._file.closed\n\n    def fileno(self):\n        self.rollover()\n        return self._file.fileno()\n\n    def flush(self):\n        self._file.flush()\n\n    def isatty(self):\n        return self._file.isatty()\n\n    @property\n    def mode(self):\n        try:\n            return self._file.mode\n        except AttributeError:\n            return self._TemporaryFileArgs[0]\n\n    @property\n    def name(self):\n        try:\n            return self._file.name\n        except AttributeError:\n            return None\n\n    def next(self):\n        return self._file.next\n\n    def read(self, *args):\n        return self._file.read(*args)\n\n    def readline(self, *args):\n        return self._file.readline(*args)\n\n    def readlines(self, *args):\n        return self._file.readlines(*args)\n\n    def seek(self, *args):\n        self._file.seek(*args)\n\n    @property\n    def softspace(self):\n        return self._file.softspace\n\n    def tell(self):\n        return self._file.tell()\n\n    def truncate(self):\n        self._file.truncate()\n\n    def write(self, s):\n        file = self._file\n        rv = file.write(s)\n        self._check(file)\n        return rv\n\n    def writelines(self, iterable):\n        file = self._file\n        rv = file.writelines(iterable)\n        self._check(file)\n        return rv\n\n    def xreadlines(self, *args):\n        if hasattr(self._file, 'xreadlines'):  # real file\n            return iter(self._file)\n        else:  # StringIO()\n            return iter(self._file.readlines(*args))\n", 
    "textwrap": "\"\"\"Text wrapping and filling.\n\"\"\"\n\n# Copyright (C) 1999-2001 Gregory P. Ward.\n# Copyright (C) 2002, 2003 Python Software Foundation.\n# Written by Greg Ward <gward@python.net>\n\n__revision__ = \"$Id$\"\n\nimport string, re\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n# Do the right thing with boolean values for all known Python versions\n# (so this module can be copied to projects that don't depend on Python\n# 2.3, e.g. Optik and Docutils) by uncommenting the block of code below.\n#try:\n#    True, False\n#except NameError:\n#    (True, False) = (1, 0)\n\n__all__ = ['TextWrapper', 'wrap', 'fill', 'dedent']\n\n# Hardcode the recognized whitespace characters to the US-ASCII\n# whitespace characters.  The main reason for doing this is that in\n# ISO-8859-1, 0xa0 is non-breaking whitespace, so in certain locales\n# that character winds up in string.whitespace.  Respecting\n# string.whitespace in those cases would 1) make textwrap treat 0xa0 the\n# same as any other whitespace char, which is clearly wrong (it's a\n# *non-breaking* space), 2) possibly cause problems with Unicode,\n# since 0xa0 is not in range(128).\n_whitespace = '\\t\\n\\x0b\\x0c\\r '\n\nclass TextWrapper:\n    \"\"\"\n    Object for wrapping/filling text.  The public interface consists of\n    the wrap() and fill() methods; the other methods are just there for\n    subclasses to override in order to tweak the default behaviour.\n    If you want to completely replace the main wrapping algorithm,\n    you'll probably have to override _wrap_chunks().\n\n    Several instance attributes control various aspects of wrapping:\n      width (default: 70)\n        the maximum width of wrapped lines (unless break_long_words\n        is false)\n      initial_indent (default: \"\")\n        string that will be prepended to the first line of wrapped\n        output.  Counts towards the line's width.\n      subsequent_indent (default: \"\")\n        string that will be prepended to all lines save the first\n        of wrapped output; also counts towards each line's width.\n      expand_tabs (default: true)\n        Expand tabs in input text to spaces before further processing.\n        Each tab will become 1 .. 8 spaces, depending on its position in\n        its line.  If false, each tab is treated as a single character.\n      replace_whitespace (default: true)\n        Replace all whitespace characters in the input text by spaces\n        after tab expansion.  Note that if expand_tabs is false and\n        replace_whitespace is true, every tab will be converted to a\n        single space!\n      fix_sentence_endings (default: false)\n        Ensure that sentence-ending punctuation is always followed\n        by two spaces.  Off by default because the algorithm is\n        (unavoidably) imperfect.\n      break_long_words (default: true)\n        Break words longer than 'width'.  If false, those words will not\n        be broken, and some lines might be longer than 'width'.\n      break_on_hyphens (default: true)\n        Allow breaking hyphenated words. If true, wrapping will occur\n        preferably on whitespaces and right after hyphens part of\n        compound words.\n      drop_whitespace (default: true)\n        Drop leading and trailing whitespace from lines.\n    \"\"\"\n\n    whitespace_trans = string.maketrans(_whitespace, ' ' * len(_whitespace))\n\n    unicode_whitespace_trans = {}\n    uspace = ord(u' ')\n    for x in map(ord, _whitespace):\n        unicode_whitespace_trans[x] = uspace\n\n    # This funky little regex is just the trick for splitting\n    # text up into word-wrappable chunks.  E.g.\n    #   \"Hello there -- you goof-ball, use the -b option!\"\n    # splits into\n    #   Hello/ /there/ /--/ /you/ /goof-/ball,/ /use/ /the/ /-b/ /option!\n    # (after stripping out empty strings).\n    wordsep_re = re.compile(\n        r'(\\s+|'                                  # any whitespace\n        r'[^\\s\\w]*\\w+[^0-9\\W]-(?=\\w+[^0-9\\W])|'   # hyphenated words\n        r'(?<=[\\w\\!\\\"\\'\\&\\.\\,\\?])-{2,}(?=\\w))')   # em-dash\n\n    # This less funky little regex just split on recognized spaces. E.g.\n    #   \"Hello there -- you goof-ball, use the -b option!\"\n    # splits into\n    #   Hello/ /there/ /--/ /you/ /goof-ball,/ /use/ /the/ /-b/ /option!/\n    wordsep_simple_re = re.compile(r'(\\s+)')\n\n    # XXX this is not locale- or charset-aware -- string.lowercase\n    # is US-ASCII only (and therefore English-only)\n    sentence_end_re = re.compile(r'[%s]'              # lowercase letter\n                                 r'[\\.\\!\\?]'          # sentence-ending punct.\n                                 r'[\\\"\\']?'           # optional end-of-quote\n                                 r'\\Z'                # end of chunk\n                                 % string.lowercase)\n\n\n    def __init__(self,\n                 width=70,\n                 initial_indent=\"\",\n                 subsequent_indent=\"\",\n                 expand_tabs=True,\n                 replace_whitespace=True,\n                 fix_sentence_endings=False,\n                 break_long_words=True,\n                 drop_whitespace=True,\n                 break_on_hyphens=True):\n        self.width = width\n        self.initial_indent = initial_indent\n        self.subsequent_indent = subsequent_indent\n        self.expand_tabs = expand_tabs\n        self.replace_whitespace = replace_whitespace\n        self.fix_sentence_endings = fix_sentence_endings\n        self.break_long_words = break_long_words\n        self.drop_whitespace = drop_whitespace\n        self.break_on_hyphens = break_on_hyphens\n\n        # recompile the regexes for Unicode mode -- done in this clumsy way for\n        # backwards compatibility because it's rather common to monkey-patch\n        # the TextWrapper class' wordsep_re attribute.\n        self.wordsep_re_uni = re.compile(self.wordsep_re.pattern, re.U)\n        self.wordsep_simple_re_uni = re.compile(\n            self.wordsep_simple_re.pattern, re.U)\n\n\n    # -- Private methods -----------------------------------------------\n    # (possibly useful for subclasses to override)\n\n    def _munge_whitespace(self, text):\n        \"\"\"_munge_whitespace(text : string) -> string\n\n        Munge whitespace in text: expand tabs and convert all other\n        whitespace characters to spaces.  Eg. \" foo\\tbar\\n\\nbaz\"\n        becomes \" foo    bar  baz\".\n        \"\"\"\n        if self.expand_tabs:\n            text = text.expandtabs()\n        if self.replace_whitespace:\n            if isinstance(text, str):\n                text = text.translate(self.whitespace_trans)\n            elif isinstance(text, _unicode):\n                text = text.translate(self.unicode_whitespace_trans)\n        return text\n\n\n    def _split(self, text):\n        \"\"\"_split(text : string) -> [string]\n\n        Split the text to wrap into indivisible chunks.  Chunks are\n        not quite the same as words; see _wrap_chunks() for full\n        details.  As an example, the text\n          Look, goof-ball -- use the -b option!\n        breaks into the following chunks:\n          'Look,', ' ', 'goof-', 'ball', ' ', '--', ' ',\n          'use', ' ', 'the', ' ', '-b', ' ', 'option!'\n        if break_on_hyphens is True, or in:\n          'Look,', ' ', 'goof-ball', ' ', '--', ' ',\n          'use', ' ', 'the', ' ', '-b', ' ', option!'\n        otherwise.\n        \"\"\"\n        if isinstance(text, _unicode):\n            if self.break_on_hyphens:\n                pat = self.wordsep_re_uni\n            else:\n                pat = self.wordsep_simple_re_uni\n        else:\n            if self.break_on_hyphens:\n                pat = self.wordsep_re\n            else:\n                pat = self.wordsep_simple_re\n        chunks = pat.split(text)\n        chunks = filter(None, chunks)  # remove empty chunks\n        return chunks\n\n    def _fix_sentence_endings(self, chunks):\n        \"\"\"_fix_sentence_endings(chunks : [string])\n\n        Correct for sentence endings buried in 'chunks'.  Eg. when the\n        original text contains \"... foo.\\nBar ...\", munge_whitespace()\n        and split() will convert that to [..., \"foo.\", \" \", \"Bar\", ...]\n        which has one too few spaces; this method simply changes the one\n        space to two.\n        \"\"\"\n        i = 0\n        patsearch = self.sentence_end_re.search\n        while i < len(chunks)-1:\n            if chunks[i+1] == \" \" and patsearch(chunks[i]):\n                chunks[i+1] = \"  \"\n                i += 2\n            else:\n                i += 1\n\n    def _handle_long_word(self, reversed_chunks, cur_line, cur_len, width):\n        \"\"\"_handle_long_word(chunks : [string],\n                             cur_line : [string],\n                             cur_len : int, width : int)\n\n        Handle a chunk of text (most likely a word, not whitespace) that\n        is too long to fit in any line.\n        \"\"\"\n        # Figure out when indent is larger than the specified width, and make\n        # sure at least one character is stripped off on every pass\n        if width < 1:\n            space_left = 1\n        else:\n            space_left = width - cur_len\n\n        # If we're allowed to break long words, then do so: put as much\n        # of the next chunk onto the current line as will fit.\n        if self.break_long_words:\n            cur_line.append(reversed_chunks[-1][:space_left])\n            reversed_chunks[-1] = reversed_chunks[-1][space_left:]\n\n        # Otherwise, we have to preserve the long word intact.  Only add\n        # it to the current line if there's nothing already there --\n        # that minimizes how much we violate the width constraint.\n        elif not cur_line:\n            cur_line.append(reversed_chunks.pop())\n\n        # If we're not allowed to break long words, and there's already\n        # text on the current line, do nothing.  Next time through the\n        # main loop of _wrap_chunks(), we'll wind up here again, but\n        # cur_len will be zero, so the next line will be entirely\n        # devoted to the long word that we can't handle right now.\n\n    def _wrap_chunks(self, chunks):\n        \"\"\"_wrap_chunks(chunks : [string]) -> [string]\n\n        Wrap a sequence of text chunks and return a list of lines of\n        length 'self.width' or less.  (If 'break_long_words' is false,\n        some lines may be longer than this.)  Chunks correspond roughly\n        to words and the whitespace between them: each chunk is\n        indivisible (modulo 'break_long_words'), but a line break can\n        come between any two chunks.  Chunks should not have internal\n        whitespace; ie. a chunk is either all whitespace or a \"word\".\n        Whitespace chunks will be removed from the beginning and end of\n        lines, but apart from that whitespace is preserved.\n        \"\"\"\n        lines = []\n        if self.width <= 0:\n            raise ValueError(\"invalid width %r (must be > 0)\" % self.width)\n\n        # Arrange in reverse order so items can be efficiently popped\n        # from a stack of chucks.\n        chunks.reverse()\n\n        while chunks:\n\n            # Start the list of chunks that will make up the current line.\n            # cur_len is just the length of all the chunks in cur_line.\n            cur_line = []\n            cur_len = 0\n\n            # Figure out which static string will prefix this line.\n            if lines:\n                indent = self.subsequent_indent\n            else:\n                indent = self.initial_indent\n\n            # Maximum width for this line.\n            width = self.width - len(indent)\n\n            # First chunk on line is whitespace -- drop it, unless this\n            # is the very beginning of the text (ie. no lines started yet).\n            if self.drop_whitespace and chunks[-1].strip() == '' and lines:\n                del chunks[-1]\n\n            while chunks:\n                l = len(chunks[-1])\n\n                # Can at least squeeze this chunk onto the current line.\n                if cur_len + l <= width:\n                    cur_line.append(chunks.pop())\n                    cur_len += l\n\n                # Nope, this line is full.\n                else:\n                    break\n\n            # The current line is full, and the next chunk is too big to\n            # fit on *any* line (not just this one).\n            if chunks and len(chunks[-1]) > width:\n                self._handle_long_word(chunks, cur_line, cur_len, width)\n\n            # If the last chunk on this line is all whitespace, drop it.\n            if self.drop_whitespace and cur_line and cur_line[-1].strip() == '':\n                del cur_line[-1]\n\n            # Convert current line back to a string and store it in list\n            # of all lines (return value).\n            if cur_line:\n                lines.append(indent + ''.join(cur_line))\n\n        return lines\n\n\n    # -- Public interface ----------------------------------------------\n\n    def wrap(self, text):\n        \"\"\"wrap(text : string) -> [string]\n\n        Reformat the single paragraph in 'text' so it fits in lines of\n        no more than 'self.width' columns, and return a list of wrapped\n        lines.  Tabs in 'text' are expanded with string.expandtabs(),\n        and all other whitespace characters (including newline) are\n        converted to space.\n        \"\"\"\n        text = self._munge_whitespace(text)\n        chunks = self._split(text)\n        if self.fix_sentence_endings:\n            self._fix_sentence_endings(chunks)\n        return self._wrap_chunks(chunks)\n\n    def fill(self, text):\n        \"\"\"fill(text : string) -> string\n\n        Reformat the single paragraph in 'text' to fit in lines of no\n        more than 'self.width' columns, and return a new string\n        containing the entire wrapped paragraph.\n        \"\"\"\n        return \"\\n\".join(self.wrap(text))\n\n\n# -- Convenience interface ---------------------------------------------\n\ndef wrap(text, width=70, **kwargs):\n    \"\"\"Wrap a single paragraph of text, returning a list of wrapped lines.\n\n    Reformat the single paragraph in 'text' so it fits in lines of no\n    more than 'width' columns, and return a list of wrapped lines.  By\n    default, tabs in 'text' are expanded with string.expandtabs(), and\n    all other whitespace characters (including newline) are converted to\n    space.  See TextWrapper class for available keyword args to customize\n    wrapping behaviour.\n    \"\"\"\n    w = TextWrapper(width=width, **kwargs)\n    return w.wrap(text)\n\ndef fill(text, width=70, **kwargs):\n    \"\"\"Fill a single paragraph of text, returning a new string.\n\n    Reformat the single paragraph in 'text' to fit in lines of no more\n    than 'width' columns, and return a new string containing the entire\n    wrapped paragraph.  As with wrap(), tabs are expanded and other\n    whitespace characters converted to space.  See TextWrapper class for\n    available keyword args to customize wrapping behaviour.\n    \"\"\"\n    w = TextWrapper(width=width, **kwargs)\n    return w.fill(text)\n\n\n# -- Loosely related functionality -------------------------------------\n\n_whitespace_only_re = re.compile('^[ \\t]+$', re.MULTILINE)\n_leading_whitespace_re = re.compile('(^[ \\t]*)(?:[^ \\t\\n])', re.MULTILINE)\n\ndef dedent(text):\n    \"\"\"Remove any common leading whitespace from every line in `text`.\n\n    This can be used to make triple-quoted strings line up with the left\n    edge of the display, while still presenting them in the source code\n    in indented form.\n\n    Note that tabs and spaces are both treated as whitespace, but they\n    are not equal: the lines \"  hello\" and \"\\thello\" are\n    considered to have no common leading whitespace.  (This behaviour is\n    new in Python 2.5; older versions of this module incorrectly\n    expanded tabs before searching for common leading whitespace.)\n    \"\"\"\n    # Look for the longest leading string of spaces and tabs common to\n    # all lines.\n    margin = None\n    text = _whitespace_only_re.sub('', text)\n    indents = _leading_whitespace_re.findall(text)\n    for indent in indents:\n        if margin is None:\n            margin = indent\n\n        # Current line more deeply indented than previous winner:\n        # no change (previous winner is still on top).\n        elif indent.startswith(margin):\n            pass\n\n        # Current line consistent with and no deeper than previous winner:\n        # it's the new winner.\n        elif margin.startswith(indent):\n            margin = indent\n\n        # Current line and previous winner have no common whitespace:\n        # there is no margin.\n        else:\n            margin = \"\"\n            break\n\n    # sanity check (testing/debugging only)\n    if 0 and margin:\n        for line in text.split(\"\\n\"):\n            assert not line or line.startswith(margin), \\\n                   \"line = %r, margin = %r\" % (line, margin)\n\n    if margin:\n        text = re.sub(r'(?m)^' + margin, '', text)\n    return text\n\nif __name__ == \"__main__\":\n    #print dedent(\"\\tfoo\\n\\tbar\")\n    #print dedent(\"  \\thello there\\n  \\t  how are you?\")\n    print dedent(\"Hello there.\\n  This is indented.\")\n", 
    "token": "\"\"\"Token constants (from \"token.h\").\"\"\"\n\n#  This file is automatically generated; please don't muck it up!\n#\n#  To update the symbols in this file, 'cd' to the top directory of\n#  the python source tree after building the interpreter and run:\n#\n#    ./python Lib/token.py\n\n#--start constants--\nENDMARKER = 0\nNAME = 1\nNUMBER = 2\nSTRING = 3\nNEWLINE = 4\nINDENT = 5\nDEDENT = 6\nLPAR = 7\nRPAR = 8\nLSQB = 9\nRSQB = 10\nCOLON = 11\nCOMMA = 12\nSEMI = 13\nPLUS = 14\nMINUS = 15\nSTAR = 16\nSLASH = 17\nVBAR = 18\nAMPER = 19\nLESS = 20\nGREATER = 21\nEQUAL = 22\nDOT = 23\nPERCENT = 24\nBACKQUOTE = 25\nLBRACE = 26\nRBRACE = 27\nEQEQUAL = 28\nNOTEQUAL = 29\nLESSEQUAL = 30\nGREATEREQUAL = 31\nTILDE = 32\nCIRCUMFLEX = 33\nLEFTSHIFT = 34\nRIGHTSHIFT = 35\nDOUBLESTAR = 36\nPLUSEQUAL = 37\nMINEQUAL = 38\nSTAREQUAL = 39\nSLASHEQUAL = 40\nPERCENTEQUAL = 41\nAMPEREQUAL = 42\nVBAREQUAL = 43\nCIRCUMFLEXEQUAL = 44\nLEFTSHIFTEQUAL = 45\nRIGHTSHIFTEQUAL = 46\nDOUBLESTAREQUAL = 47\nDOUBLESLASH = 48\nDOUBLESLASHEQUAL = 49\nAT = 50\nOP = 51\nERRORTOKEN = 52\nN_TOKENS = 53\nNT_OFFSET = 256\n#--end constants--\n\ntok_name = {}\nfor _name, _value in globals().items():\n    if type(_value) is type(0):\n        tok_name[_value] = _name\ndel _name, _value\n\n\ndef ISTERMINAL(x):\n    return x < NT_OFFSET\n\ndef ISNONTERMINAL(x):\n    return x >= NT_OFFSET\n\ndef ISEOF(x):\n    return x == ENDMARKER\n\n\ndef main():\n    import re\n    import sys\n    args = sys.argv[1:]\n    inFileName = args and args[0] or \"Include/token.h\"\n    outFileName = \"Lib/token.py\"\n    if len(args) > 1:\n        outFileName = args[1]\n    try:\n        fp = open(inFileName)\n    except IOError, err:\n        sys.stdout.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(1)\n    lines = fp.read().split(\"\\n\")\n    fp.close()\n    prog = re.compile(\n        \"#define[ \\t][ \\t]*([A-Z0-9][A-Z0-9_]*)[ \\t][ \\t]*([0-9][0-9]*)\",\n        re.IGNORECASE)\n    tokens = {}\n    for line in lines:\n        match = prog.match(line)\n        if match:\n            name, val = match.group(1, 2)\n            val = int(val)\n            tokens[val] = name          # reverse so we can sort them...\n    keys = tokens.keys()\n    keys.sort()\n    # load the output skeleton from the target:\n    try:\n        fp = open(outFileName)\n    except IOError, err:\n        sys.stderr.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(2)\n    format = fp.read().split(\"\\n\")\n    fp.close()\n    try:\n        start = format.index(\"#--start constants--\") + 1\n        end = format.index(\"#--end constants--\")\n    except ValueError:\n        sys.stderr.write(\"target does not contain format markers\")\n        sys.exit(3)\n    lines = []\n    for val in keys:\n        lines.append(\"%s = %d\" % (tokens[val], val))\n    format[start:end] = lines\n    try:\n        fp = open(outFileName, 'w')\n    except IOError, err:\n        sys.stderr.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(4)\n    fp.write(\"\\n\".join(format))\n    fp.close()\n\n\nif __name__ == \"__main__\":\n    main()\n", 
    "tokenize": "\"\"\"Tokenization help for Python programs.\n\ngenerate_tokens(readline) is a generator that breaks a stream of\ntext into Python tokens.  It accepts a readline-like method which is called\nrepeatedly to get the next line of input (or \"\" for EOF).  It generates\n5-tuples with these members:\n\n    the token type (see token.py)\n    the token (a string)\n    the starting (row, column) indices of the token (a 2-tuple of ints)\n    the ending (row, column) indices of the token (a 2-tuple of ints)\n    the original line (string)\n\nIt is designed to match the working of the Python tokenizer exactly, except\nthat it produces COMMENT tokens for comments and gives type OP for all\noperators\n\nOlder entry points\n    tokenize_loop(readline, tokeneater)\n    tokenize(readline, tokeneater=printtoken)\nare the same, except instead of generating tokens, tokeneater is a callback\nfunction to which the 5 fields described above are passed as 5 arguments,\neach time a new token is found.\"\"\"\n\n__author__ = 'Ka-Ping Yee <ping@lfw.org>'\n__credits__ = ('GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, '\n               'Skip Montanaro, Raymond Hettinger')\n\nfrom itertools import chain\nimport string, re\nfrom token import *\n\nimport token\n__all__ = [x for x in dir(token) if not x.startswith(\"_\")]\n__all__ += [\"COMMENT\", \"tokenize\", \"generate_tokens\", \"NL\", \"untokenize\"]\ndel x\ndel token\n\nCOMMENT = N_TOKENS\ntok_name[COMMENT] = 'COMMENT'\nNL = N_TOKENS + 1\ntok_name[NL] = 'NL'\nN_TOKENS += 2\n\ndef group(*choices): return '(' + '|'.join(choices) + ')'\ndef any(*choices): return group(*choices) + '*'\ndef maybe(*choices): return group(*choices) + '?'\n\nWhitespace = r'[ \\f\\t]*'\nComment = r'#[^\\r\\n]*'\nIgnore = Whitespace + any(r'\\\\\\r?\\n' + Whitespace) + maybe(Comment)\nName = r'[a-zA-Z_]\\w*'\n\nHexnumber = r'0[xX][\\da-fA-F]+[lL]?'\nOctnumber = r'(0[oO][0-7]+)|(0[0-7]*)[lL]?'\nBinnumber = r'0[bB][01]+[lL]?'\nDecnumber = r'[1-9]\\d*[lL]?'\nIntnumber = group(Hexnumber, Binnumber, Octnumber, Decnumber)\nExponent = r'[eE][-+]?\\d+'\nPointfloat = group(r'\\d+\\.\\d*', r'\\.\\d+') + maybe(Exponent)\nExpfloat = r'\\d+' + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r'\\d+[jJ]', Floatnumber + r'[jJ]')\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\n# Tail end of ' string.\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\n# Tail end of \" string.\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\n# Tail end of ''' string.\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\n# Tail end of \"\"\" string.\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\nTriple = group(\"[uUbB]?[rR]?'''\", '[uUbB]?[rR]?\"\"\"')\n# Single-line ' or \" string.\nString = group(r\"[uUbB]?[rR]?'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n               r'[uUbB]?[rR]?\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"')\n\n# Because of leftmost-then-longest match semantics, be sure to put the\n# longest operators first (e.g., if = came before ==, == would get\n# recognized as two instances of =).\nOperator = group(r\"\\*\\*=?\", r\">>=?\", r\"<<=?\", r\"<>\", r\"!=\",\n                 r\"//=?\",\n                 r\"[+\\-*/%&|^=<>]=?\",\n                 r\"~\")\n\nBracket = '[][(){}]'\nSpecial = group(r'\\r?\\n', r'[:;.,`@]')\nFunny = group(Operator, Bracket, Special)\n\nPlainToken = group(Number, Funny, String, Name)\nToken = Ignore + PlainToken\n\n# First (or only) line of ' or \" string.\nContStr = group(r\"[uUbB]?[rR]?'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" +\n                group(\"'\", r'\\\\\\r?\\n'),\n                r'[uUbB]?[rR]?\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' +\n                group('\"', r'\\\\\\r?\\n'))\nPseudoExtras = group(r'\\\\\\r?\\n|\\Z', Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\ntokenprog, pseudoprog, single3prog, double3prog = map(\n    re.compile, (Token, PseudoToken, Single3, Double3))\nendprogs = {\"'\": re.compile(Single), '\"': re.compile(Double),\n            \"'''\": single3prog, '\"\"\"': double3prog,\n            \"r'''\": single3prog, 'r\"\"\"': double3prog,\n            \"u'''\": single3prog, 'u\"\"\"': double3prog,\n            \"ur'''\": single3prog, 'ur\"\"\"': double3prog,\n            \"R'''\": single3prog, 'R\"\"\"': double3prog,\n            \"U'''\": single3prog, 'U\"\"\"': double3prog,\n            \"uR'''\": single3prog, 'uR\"\"\"': double3prog,\n            \"Ur'''\": single3prog, 'Ur\"\"\"': double3prog,\n            \"UR'''\": single3prog, 'UR\"\"\"': double3prog,\n            \"b'''\": single3prog, 'b\"\"\"': double3prog,\n            \"br'''\": single3prog, 'br\"\"\"': double3prog,\n            \"B'''\": single3prog, 'B\"\"\"': double3prog,\n            \"bR'''\": single3prog, 'bR\"\"\"': double3prog,\n            \"Br'''\": single3prog, 'Br\"\"\"': double3prog,\n            \"BR'''\": single3prog, 'BR\"\"\"': double3prog,\n            'r': None, 'R': None, 'u': None, 'U': None,\n            'b': None, 'B': None}\n\ntriple_quoted = {}\nfor t in (\"'''\", '\"\"\"',\n          \"r'''\", 'r\"\"\"', \"R'''\", 'R\"\"\"',\n          \"u'''\", 'u\"\"\"', \"U'''\", 'U\"\"\"',\n          \"ur'''\", 'ur\"\"\"', \"Ur'''\", 'Ur\"\"\"',\n          \"uR'''\", 'uR\"\"\"', \"UR'''\", 'UR\"\"\"',\n          \"b'''\", 'b\"\"\"', \"B'''\", 'B\"\"\"',\n          \"br'''\", 'br\"\"\"', \"Br'''\", 'Br\"\"\"',\n          \"bR'''\", 'bR\"\"\"', \"BR'''\", 'BR\"\"\"'):\n    triple_quoted[t] = t\nsingle_quoted = {}\nfor t in (\"'\", '\"',\n          \"r'\", 'r\"', \"R'\", 'R\"',\n          \"u'\", 'u\"', \"U'\", 'U\"',\n          \"ur'\", 'ur\"', \"Ur'\", 'Ur\"',\n          \"uR'\", 'uR\"', \"UR'\", 'UR\"',\n          \"b'\", 'b\"', \"B'\", 'B\"',\n          \"br'\", 'br\"', \"Br'\", 'Br\"',\n          \"bR'\", 'bR\"', \"BR'\", 'BR\"' ):\n    single_quoted[t] = t\n\ntabsize = 8\n\nclass TokenError(Exception): pass\n\nclass StopTokenizing(Exception): pass\n\ndef printtoken(type, token, srow_scol, erow_ecol, line): # for testing\n    srow, scol = srow_scol\n    erow, ecol = erow_ecol\n    print \"%d,%d-%d,%d:\\t%s\\t%s\" % \\\n        (srow, scol, erow, ecol, tok_name[type], repr(token))\n\ndef tokenize(readline, tokeneater=printtoken):\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n# backwards compatible interface\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\nclass Untokenizer:\n\n    def __init__(self):\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start):\n        row, col = start\n        if row < self.prev_row or row == self.prev_row and col < self.prev_col:\n            raise ValueError(\"start ({},{}) precedes previous end ({},{})\"\n                             .format(row, col, self.prev_row, self.prev_col))\n        row_offset = row - self.prev_row\n        if row_offset:\n            self.tokens.append(\"\\\\\\n\" * row_offset)\n            self.prev_col = 0\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable):\n        it = iter(iterable)\n        for t in it:\n            if len(t) == 2:\n                self.compat(t, it)\n                break\n            tok_type, token, start, end, line = t\n            if tok_type == ENDMARKER:\n                break\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token, iterable):\n        indents = []\n        toks_append = self.tokens.append\n        startline = token[0] in (NEWLINE, NL)\n        prevstring = False\n\n        for tok in chain([token], iterable):\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER):\n                tokval += ' '\n\n            # Insert a space between two consecutive strings\n            if toknum == STRING:\n                if prevstring:\n                    tokval = ' ' + tokval\n                prevstring = True\n            else:\n                prevstring = False\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\ndef untokenize(iterable):\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited intput:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tok in generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\ndef generate_tokens(readline):\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    namechars, numchars = string.ascii_letters + '_', '0123456789'\n    contstr, needcont = '', 0\n    contline = None\n    indents = [0]\n\n    while 1:                                   # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = ''\n        lnum += 1\n        pos, max = 0, len(line)\n\n        if contstr:                            # continued string\n            if not line:\n                raise TokenError, (\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (STRING, contstr + line[:end],\n                       strstart, (lnum, end), contline + line)\n                contstr, needcont = '', 0\n                contline = None\n            elif needcont and line[-2:] != '\\\\\\n' and line[-3:] != '\\\\\\r\\n':\n                yield (ERRORTOKEN, contstr + line,\n                           strstart, (lnum, len(line)), contline)\n                contstr = ''\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line: break\n            column = 0\n            while pos < max:                   # measure leading whitespace\n                if line[pos] == ' ':\n                    column += 1\n                elif line[pos] == '\\t':\n                    column = (column//tabsize + 1)*tabsize\n                elif line[pos] == '\\f':\n                    column = 0\n                else:\n                    break\n                pos += 1\n            if pos == max:\n                break\n\n            if line[pos] in '#\\r\\n':           # skip comments or blank lines\n                if line[pos] == '#':\n                    comment_token = line[pos:].rstrip('\\r\\n')\n                    nl_pos = pos + len(comment_token)\n                    yield (COMMENT, comment_token,\n                           (lnum, pos), (lnum, pos + len(comment_token)), line)\n                    yield (NL, line[nl_pos:],\n                           (lnum, nl_pos), (lnum, len(line)), line)\n                else:\n                    yield ((NL, COMMENT)[line[pos] == '#'], line[pos:],\n                           (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:           # count indents or dedents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n            while column < indents[-1]:\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line))\n                indents = indents[:-1]\n                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)\n\n        else:                                  # continued statement\n            if not line:\n                raise TokenError, (\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:                                # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                if start == end:\n                    continue\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or \\\n                   (initial == '.' and token != '.'):      # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in '\\r\\n':\n                    yield (NL if parenlev > 0 else NEWLINE,\n                           token, spos, epos, line)\n                elif initial == '#':\n                    assert not token.endswith(\"\\n\")\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:                           # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)           # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif initial in single_quoted or \\\n                    token[:2] in single_quoted or \\\n                    token[:3] in single_quoted:\n                    if token[-1] == '\\n':                  # continued string\n                        strstart = (lnum, start)\n                        endprog = (endprogs[initial] or endprogs[token[1]] or\n                                   endprogs[token[2]])\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:                                  # ordinary string\n                        yield (STRING, token, spos, epos, line)\n                elif initial in namechars:                 # ordinary name\n                    yield (NAME, token, spos, epos, line)\n                elif initial == '\\\\':                      # continued stmt\n                    continued = 1\n                else:\n                    if initial in '([{':\n                        parenlev += 1\n                    elif initial in ')]}':\n                        parenlev -= 1\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos],\n                           (lnum, pos), (lnum, pos+1), line)\n                pos += 1\n\n    for indent in indents[1:]:                 # pop remaining indent levels\n        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')\n    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')\n\nif __name__ == '__main__':                     # testing\n    import sys\n    if len(sys.argv) > 1:\n        tokenize(open(sys.argv[1]).readline)\n    else:\n        tokenize(sys.stdin.readline)\n", 
    "traceback": "\"\"\"Extract, format and print information about Python stack traces.\"\"\"\n\nimport linecache\nimport sys\nimport types\n\n__all__ = ['extract_stack', 'extract_tb', 'format_exception',\n           'format_exception_only', 'format_list', 'format_stack',\n           'format_tb', 'print_exc', 'format_exc', 'print_exception',\n           'print_last', 'print_stack', 'print_tb', 'tb_lineno']\n\ndef _print(file, str='', terminator='\\n'):\n    file.write(str+terminator)\n\n\ndef print_list(extracted_list, file=None):\n    \"\"\"Print the list of tuples as returned by extract_tb() or\n    extract_stack() as a formatted stack trace to the given file.\"\"\"\n    if file is None:\n        file = sys.stderr\n    for filename, lineno, name, line in extracted_list:\n        _print(file,\n               '  File \"%s\", line %d, in %s' % (filename,lineno,name))\n        if line:\n            _print(file, '    %s' % line.strip())\n\ndef format_list(extracted_list):\n    \"\"\"Format a list of traceback entry tuples for printing.\n\n    Given a list of tuples as returned by extract_tb() or\n    extract_stack(), return a list of strings ready for printing.\n    Each string in the resulting list corresponds to the item with the\n    same index in the argument list.  Each string ends in a newline;\n    the strings may contain internal newlines as well, for those items\n    whose source text line is not None.\n    \"\"\"\n    list = []\n    for filename, lineno, name, line in extracted_list:\n        item = '  File \"%s\", line %d, in %s\\n' % (filename,lineno,name)\n        if line:\n            item = item + '    %s\\n' % line.strip()\n        list.append(item)\n    return list\n\n\ndef print_tb(tb, limit=None, file=None):\n    \"\"\"Print up to 'limit' stack trace entries from the traceback 'tb'.\n\n    If 'limit' is omitted or None, all entries are printed.  If 'file'\n    is omitted or None, the output goes to sys.stderr; otherwise\n    'file' should be an open file or file-like object with a write()\n    method.\n    \"\"\"\n    if file is None:\n        file = sys.stderr\n    if limit is None:\n        if hasattr(sys, 'tracebacklimit'):\n            limit = sys.tracebacklimit\n    n = 0\n    while tb is not None and (limit is None or n < limit):\n        f = tb.tb_frame\n        lineno = tb.tb_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        _print(file,\n               '  File \"%s\", line %d, in %s' % (filename, lineno, name))\n        linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        if line: _print(file, '    ' + line.strip())\n        tb = tb.tb_next\n        n = n+1\n\ndef format_tb(tb, limit = None):\n    \"\"\"A shorthand for 'format_list(extract_tb(tb, limit))'.\"\"\"\n    return format_list(extract_tb(tb, limit))\n\ndef extract_tb(tb, limit = None):\n    \"\"\"Return list of up to limit pre-processed entries from traceback.\n\n    This is useful for alternate formatting of stack traces.  If\n    'limit' is omitted or None, all entries are extracted.  A\n    pre-processed stack trace entry is a quadruple (filename, line\n    number, function name, text) representing the information that is\n    usually printed for a stack trace.  The text is a string with\n    leading and trailing whitespace stripped; if the source is not\n    available it is None.\n    \"\"\"\n    if limit is None:\n        if hasattr(sys, 'tracebacklimit'):\n            limit = sys.tracebacklimit\n    list = []\n    n = 0\n    while tb is not None and (limit is None or n < limit):\n        f = tb.tb_frame\n        lineno = tb.tb_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        if line: line = line.strip()\n        else: line = None\n        list.append((filename, lineno, name, line))\n        tb = tb.tb_next\n        n = n+1\n    return list\n\n\ndef print_exception(etype, value, tb, limit=None, file=None, _encoding=None):\n    \"\"\"Print exception up to 'limit' stack trace entries from 'tb' to 'file'.\n\n    This differs from print_tb() in the following ways: (1) if\n    traceback is not None, it prints a header \"Traceback (most recent\n    call last):\"; (2) it prints the exception type and value after the\n    stack trace; (3) if type is SyntaxError and value has the\n    appropriate format, it prints the line where the syntax error\n    occurred with a caret on the next line indicating the approximate\n    position of the error.\n    \"\"\"\n    if file is None:\n        file = sys.stderr\n    if tb:\n        _print(file, 'Traceback (most recent call last):')\n        print_tb(tb, limit, file)\n    lines = format_exception_only(etype, value, _encoding)\n    for line in lines:\n        _print(file, line, '')\n\ndef format_exception(etype, value, tb, limit = None):\n    \"\"\"Format a stack trace and the exception information.\n\n    The arguments have the same meaning as the corresponding arguments\n    to print_exception().  The return value is a list of strings, each\n    ending in a newline and some containing internal newlines.  When\n    these lines are concatenated and printed, exactly the same text is\n    printed as does print_exception().\n    \"\"\"\n    if tb:\n        list = ['Traceback (most recent call last):\\n']\n        list = list + format_tb(tb, limit)\n    else:\n        list = []\n    list = list + format_exception_only(etype, value)\n    return list\n\ndef format_exception_only(etype, value, _encoding=None):\n    \"\"\"Format the exception part of a traceback.\n\n    The arguments are the exception type and value such as given by\n    sys.last_type and sys.last_value. The return value is a list of\n    strings, each ending in a newline.\n\n    Normally, the list contains a single string; however, for\n    SyntaxError exceptions, it contains several lines that (when\n    printed) display detailed information about where the syntax\n    error occurred.\n\n    The message indicating which exception occurred is always the last\n    string in the list.\n\n    \"\"\"\n\n    # An instance should not have a meaningful value parameter, but\n    # sometimes does, particularly for string exceptions, such as\n    # >>> raise string1, string2  # deprecated\n    #\n    # Clear these out first because issubtype(string1, SyntaxError)\n    # would raise another exception and mask the original problem.\n    if (isinstance(etype, BaseException) or\n        isinstance(etype, types.InstanceType) or\n        etype is None or type(etype) is str):\n        return [_format_final_exc_line(etype, value, _encoding)]\n\n    stype = etype.__name__\n\n    if not issubclass(etype, SyntaxError):\n        return [_format_final_exc_line(stype, value, _encoding)]\n\n    # It was a syntax error; show exactly where the problem was found.\n    lines = []\n    try:\n        msg, (filename, lineno, offset, badline) = value.args\n    except Exception:\n        pass\n    else:\n        filename = filename or \"<string>\"\n        lines.append('  File \"%s\", line %d\\n' % (filename, lineno))\n        if badline is not None:\n            lines.append('    %s\\n' % badline.strip())\n            if offset is not None:\n                caretspace = badline.rstrip('\\n')\n                offset = min(len(caretspace), offset) - 1\n                caretspace = caretspace[:offset].lstrip()\n                # non-space whitespace (likes tabs) must be kept for alignment\n                caretspace = ((c.isspace() and c or ' ') for c in caretspace)\n                lines.append('    %s^\\n' % ''.join(caretspace))\n        value = msg\n\n    lines.append(_format_final_exc_line(stype, value, _encoding))\n    return lines\n\ndef _format_final_exc_line(etype, value, _encoding=None):\n    \"\"\"Return a list of a single line -- normal case for format_exception_only\"\"\"\n    valuestr = _some_str(value, _encoding)\n    if value is None or not valuestr:\n        line = \"%s\\n\" % etype\n    else:\n        line = \"%s: %s\\n\" % (etype, valuestr)\n    return line\n\ndef _some_str(value, _encoding=None):\n    try:\n        return str(value)\n    except Exception:\n        pass\n    try:\n        value = unicode(value)\n        return value.encode(_encoding or \"ascii\", \"backslashreplace\")\n    except Exception:\n        pass\n    return '<unprintable %s object>' % type(value).__name__\n\n\ndef print_exc(limit=None, file=None):\n    \"\"\"Shorthand for 'print_exception(sys.exc_type, sys.exc_value, sys.exc_traceback, limit, file)'.\n    (In fact, it uses sys.exc_info() to retrieve the same information\n    in a thread-safe way.)\"\"\"\n    if file is None:\n        file = sys.stderr\n    try:\n        etype, value, tb = sys.exc_info()\n        print_exception(etype, value, tb, limit, file)\n    finally:\n        etype = value = tb = None\n\n\ndef format_exc(limit=None):\n    \"\"\"Like print_exc() but return a string.\"\"\"\n    try:\n        etype, value, tb = sys.exc_info()\n        return ''.join(format_exception(etype, value, tb, limit))\n    finally:\n        etype = value = tb = None\n\n\ndef print_last(limit=None, file=None):\n    \"\"\"This is a shorthand for 'print_exception(sys.last_type,\n    sys.last_value, sys.last_traceback, limit, file)'.\"\"\"\n    if not hasattr(sys, \"last_type\"):\n        raise ValueError(\"no last exception\")\n    if file is None:\n        file = sys.stderr\n    print_exception(sys.last_type, sys.last_value, sys.last_traceback,\n                    limit, file)\n\n\ndef print_stack(f=None, limit=None, file=None):\n    \"\"\"Print a stack trace from its invocation point.\n\n    The optional 'f' argument can be used to specify an alternate\n    stack frame at which to start. The optional 'limit' and 'file'\n    arguments have the same meaning as for print_exception().\n    \"\"\"\n    if f is None:\n        try:\n            raise ZeroDivisionError\n        except ZeroDivisionError:\n            f = sys.exc_info()[2].tb_frame.f_back\n    print_list(extract_stack(f, limit), file)\n\ndef format_stack(f=None, limit=None):\n    \"\"\"Shorthand for 'format_list(extract_stack(f, limit))'.\"\"\"\n    if f is None:\n        try:\n            raise ZeroDivisionError\n        except ZeroDivisionError:\n            f = sys.exc_info()[2].tb_frame.f_back\n    return format_list(extract_stack(f, limit))\n\ndef extract_stack(f=None, limit = None):\n    \"\"\"Extract the raw traceback from the current stack frame.\n\n    The return value has the same format as for extract_tb().  The\n    optional 'f' and 'limit' arguments have the same meaning as for\n    print_stack().  Each item in the list is a quadruple (filename,\n    line number, function name, text), and the entries are in order\n    from oldest to newest stack frame.\n    \"\"\"\n    if f is None:\n        try:\n            raise ZeroDivisionError\n        except ZeroDivisionError:\n            f = sys.exc_info()[2].tb_frame.f_back\n    if limit is None:\n        if hasattr(sys, 'tracebacklimit'):\n            limit = sys.tracebacklimit\n    list = []\n    n = 0\n    while f is not None and (limit is None or n < limit):\n        lineno = f.f_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        if line: line = line.strip()\n        else: line = None\n        list.append((filename, lineno, name, line))\n        f = f.f_back\n        n = n+1\n    list.reverse()\n    return list\n\ndef tb_lineno(tb):\n    \"\"\"Calculate correct line number of traceback given in tb.\n\n    Obsolete in 2.3.\n    \"\"\"\n    return tb.tb_lineno\n", 
    "types": "\"\"\"Define names for all type symbols known in the standard interpreter.\n\nTypes that are part of optional modules (e.g. array) are not listed.\n\"\"\"\nimport sys\n\n# Iterators in Python aren't a matter of type but of protocol.  A large\n# and changing number of builtin types implement *some* flavor of\n# iterator.  Don't check the type!  Use hasattr to check for both\n# \"__iter__\" and \"next\" attributes instead.\n\nNoneType = type(None)\nTypeType = type\nObjectType = object\n\nIntType = int\nLongType = long\nFloatType = float\nBooleanType = bool\ntry:\n    ComplexType = complex\nexcept NameError:\n    pass\n\nStringType = str\n\n# StringTypes is already outdated.  Instead of writing \"type(x) in\n# types.StringTypes\", you should use \"isinstance(x, basestring)\".  But\n# we keep around for compatibility with Python 2.2.\ntry:\n    UnicodeType = unicode\n    StringTypes = (StringType, UnicodeType)\nexcept NameError:\n    StringTypes = (StringType,)\n\nBufferType = buffer\n\nTupleType = tuple\nListType = list\nDictType = DictionaryType = dict\n\ndef _f(): pass\nFunctionType = type(_f)\nLambdaType = type(lambda: None)         # Same as FunctionType\nCodeType = type(_f.func_code)\n\ndef _g():\n    yield 1\nGeneratorType = type(_g())\n\nclass _C:\n    def _m(self): pass\nClassType = type(_C)\nUnboundMethodType = type(_C._m)         # Same as MethodType\n_x = _C()\nInstanceType = type(_x)\nMethodType = type(_x._m)\n\nBuiltinFunctionType = type(len)\nBuiltinMethodType = type([].append)     # Same as BuiltinFunctionType\n\nModuleType = type(sys)\nFileType = file\nXRangeType = xrange\n\ntry:\n    raise TypeError\nexcept TypeError:\n    tb = sys.exc_info()[2]\n    TracebackType = type(tb)\n    FrameType = type(tb.tb_frame)\n    del tb\n\nSliceType = slice\nEllipsisType = type(Ellipsis)\n\nDictProxyType = type(TypeType.__dict__)\nNotImplementedType = type(NotImplemented)\n\n# For Jython, the following two types are identical\nGetSetDescriptorType = type(FunctionType.func_code)\nMemberDescriptorType = type(FunctionType.func_globals)\n\ndel sys, _f, _g, _C, _x                           # Not for export\n", 
    "unittest.__init__": "\"\"\"\nPython unit testing framework, based on Erich Gamma's JUnit and Kent Beck's\nSmalltalk testing framework.\n\nThis module contains the core framework classes that form the basis of\nspecific test cases and suites (TestCase, TestSuite etc.), and also a\ntext-based utility class for running the tests and reporting the results\n (TextTestRunner).\n\nSimple usage:\n\n    import unittest\n\n    class IntegerArithmeticTestCase(unittest.TestCase):\n        def testAdd(self):  ## test method names begin 'test*'\n            self.assertEqual((1 + 2), 3)\n            self.assertEqual(0 + 1, 1)\n        def testMultiply(self):\n            self.assertEqual((0 * 10), 0)\n            self.assertEqual((5 * 8), 40)\n\n    if __name__ == '__main__':\n        unittest.main()\n\nFurther information is available in the bundled documentation, and from\n\n  http://docs.python.org/library/unittest.html\n\nCopyright (c) 1999-2003 Steve Purcell\nCopyright (c) 2003-2010 Python Software Foundation\nThis module is free software, and you may redistribute it and/or modify\nit under the same terms as Python itself, so long as this copyright message\nand disclaimer are retained in their original form.\n\nIN NO EVENT SHALL THE AUTHOR BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT,\nSPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OF\nTHIS CODE, EVEN IF THE AUTHOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH\nDAMAGE.\n\nTHE AUTHOR SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE.  THE CODE PROVIDED HEREUNDER IS ON AN \"AS IS\" BASIS,\nAND THERE IS NO OBLIGATION WHATSOEVER TO PROVIDE MAINTENANCE,\nSUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.\n\"\"\"\n\n__all__ = ['TestResult', 'TestCase', 'TestSuite',\n           'TextTestRunner', 'TestLoader', 'FunctionTestCase', 'main',\n           'defaultTestLoader', 'SkipTest', 'skip', 'skipIf', 'skipUnless',\n           'expectedFailure', 'TextTestResult', 'installHandler',\n           'registerResult', 'removeResult', 'removeHandler']\n\n# Expose obsolete functions for backwards compatibility\n__all__.extend(['getTestCaseNames', 'makeSuite', 'findTestCases'])\n\n__unittest = True\n\nfrom .result import TestResult\nfrom .case import (TestCase, FunctionTestCase, SkipTest, skip, skipIf,\n                   skipUnless, expectedFailure)\nfrom .suite import BaseTestSuite, TestSuite\nfrom .loader import (TestLoader, defaultTestLoader, makeSuite, getTestCaseNames,\n                     findTestCases)\nfrom .main import TestProgram, main\nfrom .runner import TextTestRunner, TextTestResult\nfrom .signals import installHandler, registerResult, removeResult, removeHandler\n\n# deprecated\n_TextTestResult = TextTestResult\n", 
    "unittest.case": "\"\"\"Test case implementation\"\"\"\n\nimport collections\nimport sys\nimport functools\nimport difflib\nimport pprint\nimport re\nimport types\nimport warnings\n\nfrom . import result\nfrom .util import (\n    strclass, safe_repr, unorderable_list_difference,\n    _count_diff_all_purpose, _count_diff_hashable\n)\n\n\n__unittest = True\n\n\nDIFF_OMITTED = ('\\nDiff is %s characters long. '\n                 'Set self.maxDiff to None to see it.')\n\nclass SkipTest(Exception):\n    \"\"\"\n    Raise this exception in a test to skip it.\n\n    Usually you can use TestCase.skipTest() or one of the skipping decorators\n    instead of raising this directly.\n    \"\"\"\n    pass\n\nclass _ExpectedFailure(Exception):\n    \"\"\"\n    Raise this when a test is expected to fail.\n\n    This is an implementation detail.\n    \"\"\"\n\n    def __init__(self, exc_info):\n        super(_ExpectedFailure, self).__init__()\n        self.exc_info = exc_info\n\nclass _UnexpectedSuccess(Exception):\n    \"\"\"\n    The test was supposed to fail, but it didn't!\n    \"\"\"\n    pass\n\ndef _id(obj):\n    return obj\n\ndef skip(reason):\n    \"\"\"\n    Unconditionally skip a test.\n    \"\"\"\n    def decorator(test_item):\n        if not isinstance(test_item, (type, types.ClassType)):\n            @functools.wraps(test_item)\n            def skip_wrapper(*args, **kwargs):\n                raise SkipTest(reason)\n            test_item = skip_wrapper\n\n        test_item.__unittest_skip__ = True\n        test_item.__unittest_skip_why__ = reason\n        return test_item\n    return decorator\n\ndef skipIf(condition, reason):\n    \"\"\"\n    Skip a test if the condition is true.\n    \"\"\"\n    if condition:\n        return skip(reason)\n    return _id\n\ndef skipUnless(condition, reason):\n    \"\"\"\n    Skip a test unless the condition is true.\n    \"\"\"\n    if not condition:\n        return skip(reason)\n    return _id\n\n\ndef expectedFailure(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            func(*args, **kwargs)\n        except Exception:\n            raise _ExpectedFailure(sys.exc_info())\n        raise _UnexpectedSuccess\n    return wrapper\n\n\nclass _AssertRaisesContext(object):\n    \"\"\"A context manager used to implement TestCase.assertRaises* methods.\"\"\"\n\n    def __init__(self, expected, test_case, expected_regexp=None):\n        self.expected = expected\n        self.failureException = test_case.failureException\n        self.expected_regexp = expected_regexp\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, tb):\n        if exc_type is None:\n            try:\n                exc_name = self.expected.__name__\n            except AttributeError:\n                exc_name = str(self.expected)\n            raise self.failureException(\n                \"{0} not raised\".format(exc_name))\n        if not issubclass(exc_type, self.expected):\n            # let unexpected exceptions pass through\n            return False\n        self.exception = exc_value # store for later retrieval\n        if self.expected_regexp is None:\n            return True\n\n        expected_regexp = self.expected_regexp\n        if not expected_regexp.search(str(exc_value)):\n            raise self.failureException('\"%s\" does not match \"%s\"' %\n                     (expected_regexp.pattern, str(exc_value)))\n        return True\n\n\nclass TestCase(object):\n    \"\"\"A class whose instances are single test cases.\n\n    By default, the test code itself should be placed in a method named\n    'runTest'.\n\n    If the fixture may be used for many test cases, create as\n    many test methods as are needed. When instantiating such a TestCase\n    subclass, specify in the constructor arguments the name of the test method\n    that the instance is to execute.\n\n    Test authors should subclass TestCase for their own tests. Construction\n    and deconstruction of the test's environment ('fixture') can be\n    implemented by overriding the 'setUp' and 'tearDown' methods respectively.\n\n    If it is necessary to override the __init__ method, the base class\n    __init__ method must always be called. It is important that subclasses\n    should not change the signature of their __init__ method, since instances\n    of the classes are instantiated automatically by parts of the framework\n    in order to be run.\n\n    When subclassing TestCase, you can set these attributes:\n    * failureException: determines which exception will be raised when\n        the instance's assertion methods fail; test methods raising this\n        exception will be deemed to have 'failed' rather than 'errored'.\n    * longMessage: determines whether long messages (including repr of\n        objects used in assert methods) will be printed on failure in *addition*\n        to any explicit message passed.\n    * maxDiff: sets the maximum length of a diff in failure messages\n        by assert methods using difflib. It is looked up as an instance\n        attribute so can be configured by individual tests if required.\n    \"\"\"\n\n    failureException = AssertionError\n\n    longMessage = False\n\n    maxDiff = 80*8\n\n    # If a string is longer than _diffThreshold, use normal comparison instead\n    # of difflib.  See #11763.\n    _diffThreshold = 2**16\n\n    # Attribute used by TestSuite for classSetUp\n\n    _classSetupFailed = False\n\n    def __init__(self, methodName='runTest'):\n        \"\"\"Create an instance of the class that will use the named test\n           method when executed. Raises a ValueError if the instance does\n           not have a method with the specified name.\n        \"\"\"\n        self._testMethodName = methodName\n        self._resultForDoCleanups = None\n        try:\n            testMethod = getattr(self, methodName)\n        except AttributeError:\n            raise ValueError(\"no such test method in %s: %s\" %\n                  (self.__class__, methodName))\n        self._testMethodDoc = testMethod.__doc__\n        self._cleanups = []\n\n        # Map types to custom assertEqual functions that will compare\n        # instances of said type in more detail to generate a more useful\n        # error message.\n        self._type_equality_funcs = {}\n        self.addTypeEqualityFunc(dict, 'assertDictEqual')\n        self.addTypeEqualityFunc(list, 'assertListEqual')\n        self.addTypeEqualityFunc(tuple, 'assertTupleEqual')\n        self.addTypeEqualityFunc(set, 'assertSetEqual')\n        self.addTypeEqualityFunc(frozenset, 'assertSetEqual')\n        try:\n            self.addTypeEqualityFunc(unicode, 'assertMultiLineEqual')\n        except NameError:\n            # No unicode support in this build\n            pass\n\n    def addTypeEqualityFunc(self, typeobj, function):\n        \"\"\"Add a type specific assertEqual style function to compare a type.\n\n        This method is for use by TestCase subclasses that need to register\n        their own type equality functions to provide nicer error messages.\n\n        Args:\n            typeobj: The data type to call this function on when both values\n                    are of the same type in assertEqual().\n            function: The callable taking two arguments and an optional\n                    msg= argument that raises self.failureException with a\n                    useful error message when the two arguments are not equal.\n        \"\"\"\n        self._type_equality_funcs[typeobj] = function\n\n    def addCleanup(self, function, *args, **kwargs):\n        \"\"\"Add a function, with arguments, to be called when the test is\n        completed. Functions added are called on a LIFO basis and are\n        called after tearDown on test failure or success.\n\n        Cleanup items are called even if setUp fails (unlike tearDown).\"\"\"\n        self._cleanups.append((function, args, kwargs))\n\n    def setUp(self):\n        \"Hook method for setting up the test fixture before exercising it.\"\n        pass\n\n    def tearDown(self):\n        \"Hook method for deconstructing the test fixture after testing it.\"\n        pass\n\n    @classmethod\n    def setUpClass(cls):\n        \"Hook method for setting up class fixture before running tests in the class.\"\n\n    @classmethod\n    def tearDownClass(cls):\n        \"Hook method for deconstructing the class fixture after running all tests in the class.\"\n\n    def countTestCases(self):\n        return 1\n\n    def defaultTestResult(self):\n        return result.TestResult()\n\n    def shortDescription(self):\n        \"\"\"Returns a one-line description of the test, or None if no\n        description has been provided.\n\n        The default implementation of this method returns the first line of\n        the specified test method's docstring.\n        \"\"\"\n        doc = self._testMethodDoc\n        return doc and doc.split(\"\\n\")[0].strip() or None\n\n\n    def id(self):\n        return \"%s.%s\" % (strclass(self.__class__), self._testMethodName)\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self._testMethodName == other._testMethodName\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((type(self), self._testMethodName))\n\n    def __str__(self):\n        return \"%s (%s)\" % (self._testMethodName, strclass(self.__class__))\n\n    def __repr__(self):\n        return \"<%s testMethod=%s>\" % \\\n               (strclass(self.__class__), self._testMethodName)\n\n    def _addSkip(self, result, reason):\n        addSkip = getattr(result, 'addSkip', None)\n        if addSkip is not None:\n            addSkip(self, reason)\n        else:\n            warnings.warn(\"TestResult has no addSkip method, skips not reported\",\n                          RuntimeWarning, 2)\n            result.addSuccess(self)\n\n    def run(self, result=None):\n        orig_result = result\n        if result is None:\n            result = self.defaultTestResult()\n            startTestRun = getattr(result, 'startTestRun', None)\n            if startTestRun is not None:\n                startTestRun()\n\n        self._resultForDoCleanups = result\n        result.startTest(self)\n\n        testMethod = getattr(self, self._testMethodName)\n        if (getattr(self.__class__, \"__unittest_skip__\", False) or\n            getattr(testMethod, \"__unittest_skip__\", False)):\n            # If the class or method was skipped.\n            try:\n                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')\n                            or getattr(testMethod, '__unittest_skip_why__', ''))\n                self._addSkip(result, skip_why)\n            finally:\n                result.stopTest(self)\n            return\n        try:\n            success = False\n            try:\n                self.setUp()\n            except SkipTest as e:\n                self._addSkip(result, str(e))\n            except KeyboardInterrupt:\n                raise\n            except:\n                result.addError(self, sys.exc_info())\n            else:\n                try:\n                    testMethod()\n                except KeyboardInterrupt:\n                    raise\n                except self.failureException:\n                    result.addFailure(self, sys.exc_info())\n                except _ExpectedFailure as e:\n                    addExpectedFailure = getattr(result, 'addExpectedFailure', None)\n                    if addExpectedFailure is not None:\n                        addExpectedFailure(self, e.exc_info)\n                    else:\n                        warnings.warn(\"TestResult has no addExpectedFailure method, reporting as passes\",\n                                      RuntimeWarning)\n                        result.addSuccess(self)\n                except _UnexpectedSuccess:\n                    addUnexpectedSuccess = getattr(result, 'addUnexpectedSuccess', None)\n                    if addUnexpectedSuccess is not None:\n                        addUnexpectedSuccess(self)\n                    else:\n                        warnings.warn(\"TestResult has no addUnexpectedSuccess method, reporting as failures\",\n                                      RuntimeWarning)\n                        result.addFailure(self, sys.exc_info())\n                except SkipTest as e:\n                    self._addSkip(result, str(e))\n                except:\n                    result.addError(self, sys.exc_info())\n                else:\n                    success = True\n\n                try:\n                    self.tearDown()\n                except KeyboardInterrupt:\n                    raise\n                except:\n                    result.addError(self, sys.exc_info())\n                    success = False\n\n            cleanUpSuccess = self.doCleanups()\n            success = success and cleanUpSuccess\n            if success:\n                result.addSuccess(self)\n        finally:\n            result.stopTest(self)\n            if orig_result is None:\n                stopTestRun = getattr(result, 'stopTestRun', None)\n                if stopTestRun is not None:\n                    stopTestRun()\n\n    def doCleanups(self):\n        \"\"\"Execute all cleanup functions. Normally called for you after\n        tearDown.\"\"\"\n        result = self._resultForDoCleanups\n        ok = True\n        while self._cleanups:\n            function, args, kwargs = self._cleanups.pop(-1)\n            try:\n                function(*args, **kwargs)\n            except KeyboardInterrupt:\n                raise\n            except:\n                ok = False\n                result.addError(self, sys.exc_info())\n        return ok\n\n    def __call__(self, *args, **kwds):\n        return self.run(*args, **kwds)\n\n    def debug(self):\n        \"\"\"Run the test without collecting errors in a TestResult\"\"\"\n        self.setUp()\n        getattr(self, self._testMethodName)()\n        self.tearDown()\n        while self._cleanups:\n            function, args, kwargs = self._cleanups.pop(-1)\n            function(*args, **kwargs)\n\n    def skipTest(self, reason):\n        \"\"\"Skip this test.\"\"\"\n        raise SkipTest(reason)\n\n    def fail(self, msg=None):\n        \"\"\"Fail immediately, with the given message.\"\"\"\n        raise self.failureException(msg)\n\n    def assertFalse(self, expr, msg=None):\n        \"\"\"Check that the expression is false.\"\"\"\n        if expr:\n            msg = self._formatMessage(msg, \"%s is not false\" % safe_repr(expr))\n            raise self.failureException(msg)\n\n    def assertTrue(self, expr, msg=None):\n        \"\"\"Check that the expression is true.\"\"\"\n        if not expr:\n            msg = self._formatMessage(msg, \"%s is not true\" % safe_repr(expr))\n            raise self.failureException(msg)\n\n    def _formatMessage(self, msg, standardMsg):\n        \"\"\"Honour the longMessage attribute when generating failure messages.\n        If longMessage is False this means:\n        * Use only an explicit message if it is provided\n        * Otherwise use the standard message for the assert\n\n        If longMessage is True:\n        * Use the standard message\n        * If an explicit message is provided, plus ' : ' and the explicit message\n        \"\"\"\n        if not self.longMessage:\n            return msg or standardMsg\n        if msg is None:\n            return standardMsg\n        try:\n            # don't switch to '{}' formatting in Python 2.X\n            # it changes the way unicode input is handled\n            return '%s : %s' % (standardMsg, msg)\n        except UnicodeDecodeError:\n            return  '%s : %s' % (safe_repr(standardMsg), safe_repr(msg))\n\n\n    def assertRaises(self, excClass, callableObj=None, *args, **kwargs):\n        \"\"\"Fail unless an exception of class excClass is raised\n           by callableObj when invoked with arguments args and keyword\n           arguments kwargs. If a different type of exception is\n           raised, it will not be caught, and the test case will be\n           deemed to have suffered an error, exactly as for an\n           unexpected exception.\n\n           If called with callableObj omitted or None, will return a\n           context object used like this::\n\n                with self.assertRaises(SomeException):\n                    do_something()\n\n           The context manager keeps a reference to the exception as\n           the 'exception' attribute. This allows you to inspect the\n           exception after the assertion::\n\n               with self.assertRaises(SomeException) as cm:\n                   do_something()\n               the_exception = cm.exception\n               self.assertEqual(the_exception.error_code, 3)\n        \"\"\"\n        context = _AssertRaisesContext(excClass, self)\n        if callableObj is None:\n            return context\n        with context:\n            callableObj(*args, **kwargs)\n\n    def _getAssertEqualityFunc(self, first, second):\n        \"\"\"Get a detailed comparison function for the types of the two args.\n\n        Returns: A callable accepting (first, second, msg=None) that will\n        raise a failure exception if first != second with a useful human\n        readable error message for those types.\n        \"\"\"\n        #\n        # NOTE(gregory.p.smith): I considered isinstance(first, type(second))\n        # and vice versa.  I opted for the conservative approach in case\n        # subclasses are not intended to be compared in detail to their super\n        # class instances using a type equality func.  This means testing\n        # subtypes won't automagically use the detailed comparison.  Callers\n        # should use their type specific assertSpamEqual method to compare\n        # subclasses if the detailed comparison is desired and appropriate.\n        # See the discussion in http://bugs.python.org/issue2578.\n        #\n        if type(first) is type(second):\n            asserter = self._type_equality_funcs.get(type(first))\n            if asserter is not None:\n                if isinstance(asserter, basestring):\n                    asserter = getattr(self, asserter)\n                return asserter\n\n        return self._baseAssertEqual\n\n    def _baseAssertEqual(self, first, second, msg=None):\n        \"\"\"The default assertEqual implementation, not type specific.\"\"\"\n        if not first == second:\n            standardMsg = '%s != %s' % (safe_repr(first), safe_repr(second))\n            msg = self._formatMessage(msg, standardMsg)\n            raise self.failureException(msg)\n\n    def assertEqual(self, first, second, msg=None):\n        \"\"\"Fail if the two objects are unequal as determined by the '=='\n           operator.\n        \"\"\"\n        assertion_func = self._getAssertEqualityFunc(first, second)\n        assertion_func(first, second, msg=msg)\n\n    def assertNotEqual(self, first, second, msg=None):\n        \"\"\"Fail if the two objects are equal as determined by the '!='\n           operator.\n        \"\"\"\n        if not first != second:\n            msg = self._formatMessage(msg, '%s == %s' % (safe_repr(first),\n                                                          safe_repr(second)))\n            raise self.failureException(msg)\n\n\n    def assertAlmostEqual(self, first, second, places=None, msg=None, delta=None):\n        \"\"\"Fail if the two objects are unequal as determined by their\n           difference rounded to the given number of decimal places\n           (default 7) and comparing to zero, or by comparing that the\n           between the two objects is more than the given delta.\n\n           Note that decimal places (from zero) are usually not the same\n           as significant digits (measured from the most signficant digit).\n\n           If the two objects compare equal then they will automatically\n           compare almost equal.\n        \"\"\"\n        if first == second:\n            # shortcut\n            return\n        if delta is not None and places is not None:\n            raise TypeError(\"specify delta or places not both\")\n\n        if delta is not None:\n            if abs(first - second) <= delta:\n                return\n\n            standardMsg = '%s != %s within %s delta' % (safe_repr(first),\n                                                        safe_repr(second),\n                                                        safe_repr(delta))\n        else:\n            if places is None:\n                places = 7\n\n            if round(abs(second-first), places) == 0:\n                return\n\n            standardMsg = '%s != %s within %r places' % (safe_repr(first),\n                                                          safe_repr(second),\n                                                          places)\n        msg = self._formatMessage(msg, standardMsg)\n        raise self.failureException(msg)\n\n    def assertNotAlmostEqual(self, first, second, places=None, msg=None, delta=None):\n        \"\"\"Fail if the two objects are equal as determined by their\n           difference rounded to the given number of decimal places\n           (default 7) and comparing to zero, or by comparing that the\n           between the two objects is less than the given delta.\n\n           Note that decimal places (from zero) are usually not the same\n           as significant digits (measured from the most signficant digit).\n\n           Objects that are equal automatically fail.\n        \"\"\"\n        if delta is not None and places is not None:\n            raise TypeError(\"specify delta or places not both\")\n        if delta is not None:\n            if not (first == second) and abs(first - second) > delta:\n                return\n            standardMsg = '%s == %s within %s delta' % (safe_repr(first),\n                                                        safe_repr(second),\n                                                        safe_repr(delta))\n        else:\n            if places is None:\n                places = 7\n            if not (first == second) and round(abs(second-first), places) != 0:\n                return\n            standardMsg = '%s == %s within %r places' % (safe_repr(first),\n                                                         safe_repr(second),\n                                                         places)\n\n        msg = self._formatMessage(msg, standardMsg)\n        raise self.failureException(msg)\n\n    # Synonyms for assertion methods\n\n    # The plurals are undocumented.  Keep them that way to discourage use.\n    # Do not add more.  Do not remove.\n    # Going through a deprecation cycle on these would annoy many people.\n    assertEquals = assertEqual\n    assertNotEquals = assertNotEqual\n    assertAlmostEquals = assertAlmostEqual\n    assertNotAlmostEquals = assertNotAlmostEqual\n    assert_ = assertTrue\n\n    # These fail* assertion method names are pending deprecation and will\n    # be a DeprecationWarning in 3.2; http://bugs.python.org/issue2578\n    def _deprecate(original_func):\n        def deprecated_func(*args, **kwargs):\n            warnings.warn(\n                'Please use {0} instead.'.format(original_func.__name__),\n                PendingDeprecationWarning, 2)\n            return original_func(*args, **kwargs)\n        return deprecated_func\n\n    failUnlessEqual = _deprecate(assertEqual)\n    failIfEqual = _deprecate(assertNotEqual)\n    failUnlessAlmostEqual = _deprecate(assertAlmostEqual)\n    failIfAlmostEqual = _deprecate(assertNotAlmostEqual)\n    failUnless = _deprecate(assertTrue)\n    failUnlessRaises = _deprecate(assertRaises)\n    failIf = _deprecate(assertFalse)\n\n    def assertSequenceEqual(self, seq1, seq2, msg=None, seq_type=None):\n        \"\"\"An equality assertion for ordered sequences (like lists and tuples).\n\n        For the purposes of this function, a valid ordered sequence type is one\n        which can be indexed, has a length, and has an equality operator.\n\n        Args:\n            seq1: The first sequence to compare.\n            seq2: The second sequence to compare.\n            seq_type: The expected datatype of the sequences, or None if no\n                    datatype should be enforced.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n        \"\"\"\n        if seq_type is not None:\n            seq_type_name = seq_type.__name__\n            if not isinstance(seq1, seq_type):\n                raise self.failureException('First sequence is not a %s: %s'\n                                        % (seq_type_name, safe_repr(seq1)))\n            if not isinstance(seq2, seq_type):\n                raise self.failureException('Second sequence is not a %s: %s'\n                                        % (seq_type_name, safe_repr(seq2)))\n        else:\n            seq_type_name = \"sequence\"\n\n        differing = None\n        try:\n            len1 = len(seq1)\n        except (TypeError, NotImplementedError):\n            differing = 'First %s has no length.    Non-sequence?' % (\n                    seq_type_name)\n\n        if differing is None:\n            try:\n                len2 = len(seq2)\n            except (TypeError, NotImplementedError):\n                differing = 'Second %s has no length.    Non-sequence?' % (\n                        seq_type_name)\n\n        if differing is None:\n            if seq1 == seq2:\n                return\n\n            seq1_repr = safe_repr(seq1)\n            seq2_repr = safe_repr(seq2)\n            if len(seq1_repr) > 30:\n                seq1_repr = seq1_repr[:30] + '...'\n            if len(seq2_repr) > 30:\n                seq2_repr = seq2_repr[:30] + '...'\n            elements = (seq_type_name.capitalize(), seq1_repr, seq2_repr)\n            differing = '%ss differ: %s != %s\\n' % elements\n\n            for i in xrange(min(len1, len2)):\n                try:\n                    item1 = seq1[i]\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('\\nUnable to index element %d of first %s\\n' %\n                                 (i, seq_type_name))\n                    break\n\n                try:\n                    item2 = seq2[i]\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('\\nUnable to index element %d of second %s\\n' %\n                                 (i, seq_type_name))\n                    break\n\n                if item1 != item2:\n                    differing += ('\\nFirst differing element %d:\\n%s\\n%s\\n' %\n                                 (i, item1, item2))\n                    break\n            else:\n                if (len1 == len2 and seq_type is None and\n                    type(seq1) != type(seq2)):\n                    # The sequences are the same, but have differing types.\n                    return\n\n            if len1 > len2:\n                differing += ('\\nFirst %s contains %d additional '\n                             'elements.\\n' % (seq_type_name, len1 - len2))\n                try:\n                    differing += ('First extra element %d:\\n%s\\n' %\n                                  (len2, seq1[len2]))\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('Unable to index element %d '\n                                  'of first %s\\n' % (len2, seq_type_name))\n            elif len1 < len2:\n                differing += ('\\nSecond %s contains %d additional '\n                             'elements.\\n' % (seq_type_name, len2 - len1))\n                try:\n                    differing += ('First extra element %d:\\n%s\\n' %\n                                  (len1, seq2[len1]))\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('Unable to index element %d '\n                                  'of second %s\\n' % (len1, seq_type_name))\n        standardMsg = differing\n        diffMsg = '\\n' + '\\n'.join(\n            difflib.ndiff(pprint.pformat(seq1).splitlines(),\n                          pprint.pformat(seq2).splitlines()))\n        standardMsg = self._truncateMessage(standardMsg, diffMsg)\n        msg = self._formatMessage(msg, standardMsg)\n        self.fail(msg)\n\n    def _truncateMessage(self, message, diff):\n        max_diff = self.maxDiff\n        if max_diff is None or len(diff) <= max_diff:\n            return message + diff\n        return message + (DIFF_OMITTED % len(diff))\n\n    def assertListEqual(self, list1, list2, msg=None):\n        \"\"\"A list-specific equality assertion.\n\n        Args:\n            list1: The first list to compare.\n            list2: The second list to compare.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n\n        \"\"\"\n        self.assertSequenceEqual(list1, list2, msg, seq_type=list)\n\n    def assertTupleEqual(self, tuple1, tuple2, msg=None):\n        \"\"\"A tuple-specific equality assertion.\n\n        Args:\n            tuple1: The first tuple to compare.\n            tuple2: The second tuple to compare.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n        \"\"\"\n        self.assertSequenceEqual(tuple1, tuple2, msg, seq_type=tuple)\n\n    def assertSetEqual(self, set1, set2, msg=None):\n        \"\"\"A set-specific equality assertion.\n\n        Args:\n            set1: The first set to compare.\n            set2: The second set to compare.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n\n        assertSetEqual uses ducktyping to support different types of sets, and\n        is optimized for sets specifically (parameters must support a\n        difference method).\n        \"\"\"\n        try:\n            difference1 = set1.difference(set2)\n        except TypeError, e:\n            self.fail('invalid type when attempting set difference: %s' % e)\n        except AttributeError, e:\n            self.fail('first argument does not support set difference: %s' % e)\n\n        try:\n            difference2 = set2.difference(set1)\n        except TypeError, e:\n            self.fail('invalid type when attempting set difference: %s' % e)\n        except AttributeError, e:\n            self.fail('second argument does not support set difference: %s' % e)\n\n        if not (difference1 or difference2):\n            return\n\n        lines = []\n        if difference1:\n            lines.append('Items in the first set but not the second:')\n            for item in difference1:\n                lines.append(repr(item))\n        if difference2:\n            lines.append('Items in the second set but not the first:')\n            for item in difference2:\n                lines.append(repr(item))\n\n        standardMsg = '\\n'.join(lines)\n        self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIn(self, member, container, msg=None):\n        \"\"\"Just like self.assertTrue(a in b), but with a nicer default message.\"\"\"\n        if member not in container:\n            standardMsg = '%s not found in %s' % (safe_repr(member),\n                                                  safe_repr(container))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertNotIn(self, member, container, msg=None):\n        \"\"\"Just like self.assertTrue(a not in b), but with a nicer default message.\"\"\"\n        if member in container:\n            standardMsg = '%s unexpectedly found in %s' % (safe_repr(member),\n                                                        safe_repr(container))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIs(self, expr1, expr2, msg=None):\n        \"\"\"Just like self.assertTrue(a is b), but with a nicer default message.\"\"\"\n        if expr1 is not expr2:\n            standardMsg = '%s is not %s' % (safe_repr(expr1),\n                                             safe_repr(expr2))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIsNot(self, expr1, expr2, msg=None):\n        \"\"\"Just like self.assertTrue(a is not b), but with a nicer default message.\"\"\"\n        if expr1 is expr2:\n            standardMsg = 'unexpectedly identical: %s' % (safe_repr(expr1),)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertDictEqual(self, d1, d2, msg=None):\n        self.assertIsInstance(d1, dict, 'First argument is not a dictionary')\n        self.assertIsInstance(d2, dict, 'Second argument is not a dictionary')\n\n        if d1 != d2:\n            standardMsg = '%s != %s' % (safe_repr(d1, True), safe_repr(d2, True))\n            diff = ('\\n' + '\\n'.join(difflib.ndiff(\n                           pprint.pformat(d1).splitlines(),\n                           pprint.pformat(d2).splitlines())))\n            standardMsg = self._truncateMessage(standardMsg, diff)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertDictContainsSubset(self, expected, actual, msg=None):\n        \"\"\"Checks whether actual is a superset of expected.\"\"\"\n        missing = []\n        mismatched = []\n        for key, value in expected.iteritems():\n            if key not in actual:\n                missing.append(key)\n            elif value != actual[key]:\n                mismatched.append('%s, expected: %s, actual: %s' %\n                                  (safe_repr(key), safe_repr(value),\n                                   safe_repr(actual[key])))\n\n        if not (missing or mismatched):\n            return\n\n        standardMsg = ''\n        if missing:\n            standardMsg = 'Missing: %s' % ','.join(safe_repr(m) for m in\n                                                    missing)\n        if mismatched:\n            if standardMsg:\n                standardMsg += '; '\n            standardMsg += 'Mismatched values: %s' % ','.join(mismatched)\n\n        self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertItemsEqual(self, expected_seq, actual_seq, msg=None):\n        \"\"\"An unordered sequence specific comparison. It asserts that\n        actual_seq and expected_seq have the same element counts.\n        Equivalent to::\n\n            self.assertEqual(Counter(iter(actual_seq)),\n                             Counter(iter(expected_seq)))\n\n        Asserts that each element has the same count in both sequences.\n        Example:\n            - [0, 1, 1] and [1, 0, 1] compare equal.\n            - [0, 0, 1] and [0, 1] compare unequal.\n        \"\"\"\n        first_seq, second_seq = list(expected_seq), list(actual_seq)\n        with warnings.catch_warnings():\n            if sys.py3kwarning:\n                # Silence Py3k warning raised during the sorting\n                for _msg in [\"(code|dict|type) inequality comparisons\",\n                             \"builtin_function_or_method order comparisons\",\n                             \"comparing unequal types\"]:\n                    warnings.filterwarnings(\"ignore\", _msg, DeprecationWarning)\n            try:\n                first = collections.Counter(first_seq)\n                second = collections.Counter(second_seq)\n            except TypeError:\n                # Handle case with unhashable elements\n                differences = _count_diff_all_purpose(first_seq, second_seq)\n            else:\n                if first == second:\n                    return\n                differences = _count_diff_hashable(first_seq, second_seq)\n\n        if differences:\n            standardMsg = 'Element counts were not equal:\\n'\n            lines = ['First has %d, Second has %d:  %r' % diff for diff in differences]\n            diffMsg = '\\n'.join(lines)\n            standardMsg = self._truncateMessage(standardMsg, diffMsg)\n            msg = self._formatMessage(msg, standardMsg)\n            self.fail(msg)\n\n    def assertMultiLineEqual(self, first, second, msg=None):\n        \"\"\"Assert that two multi-line strings are equal.\"\"\"\n        self.assertIsInstance(first, basestring,\n                'First argument is not a string')\n        self.assertIsInstance(second, basestring,\n                'Second argument is not a string')\n\n        if first != second:\n            # don't use difflib if the strings are too long\n            if (len(first) > self._diffThreshold or\n                len(second) > self._diffThreshold):\n                self._baseAssertEqual(first, second, msg)\n            firstlines = first.splitlines(True)\n            secondlines = second.splitlines(True)\n            if len(firstlines) == 1 and first.strip('\\r\\n') == first:\n                firstlines = [first + '\\n']\n                secondlines = [second + '\\n']\n            standardMsg = '%s != %s' % (safe_repr(first, True),\n                                        safe_repr(second, True))\n            diff = '\\n' + ''.join(difflib.ndiff(firstlines, secondlines))\n            standardMsg = self._truncateMessage(standardMsg, diff)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertLess(self, a, b, msg=None):\n        \"\"\"Just like self.assertTrue(a < b), but with a nicer default message.\"\"\"\n        if not a < b:\n            standardMsg = '%s not less than %s' % (safe_repr(a), safe_repr(b))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertLessEqual(self, a, b, msg=None):\n        \"\"\"Just like self.assertTrue(a <= b), but with a nicer default message.\"\"\"\n        if not a <= b:\n            standardMsg = '%s not less than or equal to %s' % (safe_repr(a), safe_repr(b))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertGreater(self, a, b, msg=None):\n        \"\"\"Just like self.assertTrue(a > b), but with a nicer default message.\"\"\"\n        if not a > b:\n            standardMsg = '%s not greater than %s' % (safe_repr(a), safe_repr(b))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertGreaterEqual(self, a, b, msg=None):\n        \"\"\"Just like self.assertTrue(a >= b), but with a nicer default message.\"\"\"\n        if not a >= b:\n            standardMsg = '%s not greater than or equal to %s' % (safe_repr(a), safe_repr(b))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIsNone(self, obj, msg=None):\n        \"\"\"Same as self.assertTrue(obj is None), with a nicer default message.\"\"\"\n        if obj is not None:\n            standardMsg = '%s is not None' % (safe_repr(obj),)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIsNotNone(self, obj, msg=None):\n        \"\"\"Included for symmetry with assertIsNone.\"\"\"\n        if obj is None:\n            standardMsg = 'unexpectedly None'\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIsInstance(self, obj, cls, msg=None):\n        \"\"\"Same as self.assertTrue(isinstance(obj, cls)), with a nicer\n        default message.\"\"\"\n        if not isinstance(obj, cls):\n            standardMsg = '%s is not an instance of %r' % (safe_repr(obj), cls)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertNotIsInstance(self, obj, cls, msg=None):\n        \"\"\"Included for symmetry with assertIsInstance.\"\"\"\n        if isinstance(obj, cls):\n            standardMsg = '%s is an instance of %r' % (safe_repr(obj), cls)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertRaisesRegexp(self, expected_exception, expected_regexp,\n                           callable_obj=None, *args, **kwargs):\n        \"\"\"Asserts that the message in a raised exception matches a regexp.\n\n        Args:\n            expected_exception: Exception class expected to be raised.\n            expected_regexp: Regexp (re pattern object or string) expected\n                    to be found in error message.\n            callable_obj: Function to be called.\n            args: Extra args.\n            kwargs: Extra kwargs.\n        \"\"\"\n        if expected_regexp is not None:\n            expected_regexp = re.compile(expected_regexp)\n        context = _AssertRaisesContext(expected_exception, self, expected_regexp)\n        if callable_obj is None:\n            return context\n        with context:\n            callable_obj(*args, **kwargs)\n\n    def assertRegexpMatches(self, text, expected_regexp, msg=None):\n        \"\"\"Fail the test unless the text matches the regular expression.\"\"\"\n        if isinstance(expected_regexp, basestring):\n            expected_regexp = re.compile(expected_regexp)\n        if not expected_regexp.search(text):\n            msg = msg or \"Regexp didn't match\"\n            msg = '%s: %r not found in %r' % (msg, expected_regexp.pattern, text)\n            raise self.failureException(msg)\n\n    def assertNotRegexpMatches(self, text, unexpected_regexp, msg=None):\n        \"\"\"Fail the test if the text matches the regular expression.\"\"\"\n        if isinstance(unexpected_regexp, basestring):\n            unexpected_regexp = re.compile(unexpected_regexp)\n        match = unexpected_regexp.search(text)\n        if match:\n            msg = msg or \"Regexp matched\"\n            msg = '%s: %r matches %r in %r' % (msg,\n                                               text[match.start():match.end()],\n                                               unexpected_regexp.pattern,\n                                               text)\n            raise self.failureException(msg)\n\n\nclass FunctionTestCase(TestCase):\n    \"\"\"A test case that wraps a test function.\n\n    This is useful for slipping pre-existing test functions into the\n    unittest framework. Optionally, set-up and tidy-up functions can be\n    supplied. As with TestCase, the tidy-up ('tearDown') function will\n    always be called if the set-up ('setUp') function ran successfully.\n    \"\"\"\n\n    def __init__(self, testFunc, setUp=None, tearDown=None, description=None):\n        super(FunctionTestCase, self).__init__()\n        self._setUpFunc = setUp\n        self._tearDownFunc = tearDown\n        self._testFunc = testFunc\n        self._description = description\n\n    def setUp(self):\n        if self._setUpFunc is not None:\n            self._setUpFunc()\n\n    def tearDown(self):\n        if self._tearDownFunc is not None:\n            self._tearDownFunc()\n\n    def runTest(self):\n        self._testFunc()\n\n    def id(self):\n        return self._testFunc.__name__\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n\n        return self._setUpFunc == other._setUpFunc and \\\n               self._tearDownFunc == other._tearDownFunc and \\\n               self._testFunc == other._testFunc and \\\n               self._description == other._description\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((type(self), self._setUpFunc, self._tearDownFunc,\n                     self._testFunc, self._description))\n\n    def __str__(self):\n        return \"%s (%s)\" % (strclass(self.__class__),\n                            self._testFunc.__name__)\n\n    def __repr__(self):\n        return \"<%s tec=%s>\" % (strclass(self.__class__),\n                                     self._testFunc)\n\n    def shortDescription(self):\n        if self._description is not None:\n            return self._description\n        doc = self._testFunc.__doc__\n        return doc and doc.split(\"\\n\")[0].strip() or None\n", 
    "unittest.loader": "\"\"\"Loading unittests.\"\"\"\n\nimport os\nimport re\nimport sys\nimport traceback\nimport types\n\nfrom functools import cmp_to_key as _CmpToKey\nfrom fnmatch import fnmatch\n\nfrom . import case, suite\n\n__unittest = True\n\n# what about .pyc or .pyo (etc)\n# we would need to avoid loading the same tests multiple times\n# from '.py', '.pyc' *and* '.pyo'\nVALID_MODULE_NAME = re.compile(r'[_a-z]\\w*\\.py$', re.IGNORECASE)\n\n\ndef _make_failed_import_test(name, suiteClass):\n    message = 'Failed to import test module: %s\\n%s' % (name, traceback.format_exc())\n    return _make_failed_test('ModuleImportFailure', name, ImportError(message),\n                             suiteClass)\n\ndef _make_failed_load_tests(name, exception, suiteClass):\n    return _make_failed_test('LoadTestsFailure', name, exception, suiteClass)\n\ndef _make_failed_test(classname, methodname, exception, suiteClass):\n    def testFailure(self):\n        raise exception\n    attrs = {methodname: testFailure}\n    TestClass = type(classname, (case.TestCase,), attrs)\n    return suiteClass((TestClass(methodname),))\n\n\nclass TestLoader(object):\n    \"\"\"\n    This class is responsible for loading tests according to various criteria\n    and returning them wrapped in a TestSuite\n    \"\"\"\n    testMethodPrefix = 'test'\n    sortTestMethodsUsing = cmp\n    suiteClass = suite.TestSuite\n    _top_level_dir = None\n\n    def loadTestsFromTestCase(self, testCaseClass):\n        \"\"\"Return a suite of all tests cases contained in testCaseClass\"\"\"\n        if issubclass(testCaseClass, suite.TestSuite):\n            raise TypeError(\"Test cases should not be derived from TestSuite.\" \\\n                                \" Maybe you meant to derive from TestCase?\")\n        testCaseNames = self.getTestCaseNames(testCaseClass)\n        if not testCaseNames and hasattr(testCaseClass, 'runTest'):\n            testCaseNames = ['runTest']\n        loaded_suite = self.suiteClass(map(testCaseClass, testCaseNames))\n        return loaded_suite\n\n    def loadTestsFromModule(self, module, use_load_tests=True):\n        \"\"\"Return a suite of all tests cases contained in the given module\"\"\"\n        tests = []\n        for name in dir(module):\n            obj = getattr(module, name)\n            if isinstance(obj, type) and issubclass(obj, case.TestCase):\n                tests.append(self.loadTestsFromTestCase(obj))\n\n        load_tests = getattr(module, 'load_tests', None)\n        tests = self.suiteClass(tests)\n        if use_load_tests and load_tests is not None:\n            try:\n                return load_tests(self, tests, None)\n            except Exception, e:\n                return _make_failed_load_tests(module.__name__, e,\n                                               self.suiteClass)\n        return tests\n\n    def loadTestsFromName(self, name, module=None):\n        \"\"\"Return a suite of all tests cases given a string specifier.\n\n        The name may resolve either to a module, a test case class, a\n        test method within a test case class, or a callable object which\n        returns a TestCase or TestSuite instance.\n\n        The method optionally resolves the names relative to a given module.\n        \"\"\"\n        parts = name.split('.')\n        if module is None:\n            parts_copy = parts[:]\n            while parts_copy:\n                try:\n                    module = __import__('.'.join(parts_copy))\n                    break\n                except ImportError:\n                    del parts_copy[-1]\n                    if not parts_copy:\n                        raise\n            parts = parts[1:]\n        obj = module\n        for part in parts:\n            parent, obj = obj, getattr(obj, part)\n\n        if isinstance(obj, types.ModuleType):\n            return self.loadTestsFromModule(obj)\n        elif isinstance(obj, type) and issubclass(obj, case.TestCase):\n            return self.loadTestsFromTestCase(obj)\n        elif (isinstance(obj, types.UnboundMethodType) and\n              isinstance(parent, type) and\n              issubclass(parent, case.TestCase)):\n            name = parts[-1]\n            inst = parent(name)\n            return self.suiteClass([inst])\n        elif isinstance(obj, suite.TestSuite):\n            return obj\n        elif hasattr(obj, '__call__'):\n            test = obj()\n            if isinstance(test, suite.TestSuite):\n                return test\n            elif isinstance(test, case.TestCase):\n                return self.suiteClass([test])\n            else:\n                raise TypeError(\"calling %s returned %s, not a test\" %\n                                (obj, test))\n        else:\n            raise TypeError(\"don't know how to make test from: %s\" % obj)\n\n    def loadTestsFromNames(self, names, module=None):\n        \"\"\"Return a suite of all tests cases found using the given sequence\n        of string specifiers. See 'loadTestsFromName()'.\n        \"\"\"\n        suites = [self.loadTestsFromName(name, module) for name in names]\n        return self.suiteClass(suites)\n\n    def getTestCaseNames(self, testCaseClass):\n        \"\"\"Return a sorted sequence of method names found within testCaseClass\n        \"\"\"\n        def isTestMethod(attrname, testCaseClass=testCaseClass,\n                         prefix=self.testMethodPrefix):\n            return attrname.startswith(prefix) and \\\n                hasattr(getattr(testCaseClass, attrname), '__call__')\n        testFnNames = filter(isTestMethod, dir(testCaseClass))\n        if self.sortTestMethodsUsing:\n            testFnNames.sort(key=_CmpToKey(self.sortTestMethodsUsing))\n        return testFnNames\n\n    def discover(self, start_dir, pattern='test*.py', top_level_dir=None):\n        \"\"\"Find and return all test modules from the specified start\n        directory, recursing into subdirectories to find them. Only test files\n        that match the pattern will be loaded. (Using shell style pattern\n        matching.)\n\n        All test modules must be importable from the top level of the project.\n        If the start directory is not the top level directory then the top\n        level directory must be specified separately.\n\n        If a test package name (directory with '__init__.py') matches the\n        pattern then the package will be checked for a 'load_tests' function. If\n        this exists then it will be called with loader, tests, pattern.\n\n        If load_tests exists then discovery does  *not* recurse into the package,\n        load_tests is responsible for loading all tests in the package.\n\n        The pattern is deliberately not stored as a loader attribute so that\n        packages can continue discovery themselves. top_level_dir is stored so\n        load_tests does not need to pass this argument in to loader.discover().\n        \"\"\"\n        set_implicit_top = False\n        if top_level_dir is None and self._top_level_dir is not None:\n            # make top_level_dir optional if called from load_tests in a package\n            top_level_dir = self._top_level_dir\n        elif top_level_dir is None:\n            set_implicit_top = True\n            top_level_dir = start_dir\n\n        top_level_dir = os.path.abspath(top_level_dir)\n\n        if not top_level_dir in sys.path:\n            # all test modules must be importable from the top level directory\n            # should we *unconditionally* put the start directory in first\n            # in sys.path to minimise likelihood of conflicts between installed\n            # modules and development versions?\n            sys.path.insert(0, top_level_dir)\n        self._top_level_dir = top_level_dir\n\n        is_not_importable = False\n        if os.path.isdir(os.path.abspath(start_dir)):\n            start_dir = os.path.abspath(start_dir)\n            if start_dir != top_level_dir:\n                is_not_importable = not os.path.isfile(os.path.join(start_dir, '__init__.py'))\n        else:\n            # support for discovery from dotted module names\n            try:\n                __import__(start_dir)\n            except ImportError:\n                is_not_importable = True\n            else:\n                the_module = sys.modules[start_dir]\n                top_part = start_dir.split('.')[0]\n                start_dir = os.path.abspath(os.path.dirname((the_module.__file__)))\n                if set_implicit_top:\n                    self._top_level_dir = self._get_directory_containing_module(top_part)\n                    sys.path.remove(top_level_dir)\n\n        if is_not_importable:\n            raise ImportError('Start directory is not importable: %r' % start_dir)\n\n        tests = list(self._find_tests(start_dir, pattern))\n        return self.suiteClass(tests)\n\n    def _get_directory_containing_module(self, module_name):\n        module = sys.modules[module_name]\n        full_path = os.path.abspath(module.__file__)\n\n        if os.path.basename(full_path).lower().startswith('__init__.py'):\n            return os.path.dirname(os.path.dirname(full_path))\n        else:\n            # here we have been given a module rather than a package - so\n            # all we can do is search the *same* directory the module is in\n            # should an exception be raised instead\n            return os.path.dirname(full_path)\n\n    def _get_name_from_path(self, path):\n        path = os.path.splitext(os.path.normpath(path))[0]\n\n        _relpath = os.path.relpath(path, self._top_level_dir)\n        assert not os.path.isabs(_relpath), \"Path must be within the project\"\n        assert not _relpath.startswith('..'), \"Path must be within the project\"\n\n        name = _relpath.replace(os.path.sep, '.')\n        return name\n\n    def _get_module_from_name(self, name):\n        __import__(name)\n        return sys.modules[name]\n\n    def _match_path(self, path, full_path, pattern):\n        # override this method to use alternative matching strategy\n        return fnmatch(path, pattern)\n\n    def _find_tests(self, start_dir, pattern):\n        \"\"\"Used by discovery. Yields test suites it loads.\"\"\"\n        paths = os.listdir(start_dir)\n\n        for path in paths:\n            full_path = os.path.join(start_dir, path)\n            if os.path.isfile(full_path):\n                if not VALID_MODULE_NAME.match(path):\n                    # valid Python identifiers only\n                    continue\n                if not self._match_path(path, full_path, pattern):\n                    continue\n                # if the test file matches, load it\n                name = self._get_name_from_path(full_path)\n                try:\n                    module = self._get_module_from_name(name)\n                except:\n                    yield _make_failed_import_test(name, self.suiteClass)\n                else:\n                    mod_file = os.path.abspath(getattr(module, '__file__', full_path))\n                    realpath = os.path.splitext(os.path.realpath(mod_file))[0]\n                    fullpath_noext = os.path.splitext(os.path.realpath(full_path))[0]\n                    if realpath.lower() != fullpath_noext.lower():\n                        module_dir = os.path.dirname(realpath)\n                        mod_name = os.path.splitext(os.path.basename(full_path))[0]\n                        expected_dir = os.path.dirname(full_path)\n                        msg = (\"%r module incorrectly imported from %r. Expected %r. \"\n                               \"Is this module globally installed?\")\n                        raise ImportError(msg % (mod_name, module_dir, expected_dir))\n                    yield self.loadTestsFromModule(module)\n            elif os.path.isdir(full_path):\n                if not os.path.isfile(os.path.join(full_path, '__init__.py')):\n                    continue\n\n                load_tests = None\n                tests = None\n                if fnmatch(path, pattern):\n                    # only check load_tests if the package directory itself matches the filter\n                    name = self._get_name_from_path(full_path)\n                    package = self._get_module_from_name(name)\n                    load_tests = getattr(package, 'load_tests', None)\n                    tests = self.loadTestsFromModule(package, use_load_tests=False)\n\n                if load_tests is None:\n                    if tests is not None:\n                        # tests loaded from package file\n                        yield tests\n                    # recurse into the package\n                    for test in self._find_tests(full_path, pattern):\n                        yield test\n                else:\n                    try:\n                        yield load_tests(self, tests, pattern)\n                    except Exception, e:\n                        yield _make_failed_load_tests(package.__name__, e,\n                                                      self.suiteClass)\n\ndefaultTestLoader = TestLoader()\n\n\ndef _makeLoader(prefix, sortUsing, suiteClass=None):\n    loader = TestLoader()\n    loader.sortTestMethodsUsing = sortUsing\n    loader.testMethodPrefix = prefix\n    if suiteClass:\n        loader.suiteClass = suiteClass\n    return loader\n\ndef getTestCaseNames(testCaseClass, prefix, sortUsing=cmp):\n    return _makeLoader(prefix, sortUsing).getTestCaseNames(testCaseClass)\n\ndef makeSuite(testCaseClass, prefix='test', sortUsing=cmp,\n              suiteClass=suite.TestSuite):\n    return _makeLoader(prefix, sortUsing, suiteClass).loadTestsFromTestCase(testCaseClass)\n\ndef findTestCases(module, prefix='test', sortUsing=cmp,\n                  suiteClass=suite.TestSuite):\n    return _makeLoader(prefix, sortUsing, suiteClass).loadTestsFromModule(module)\n", 
    "unittest.main": "\"\"\"Unittest main program\"\"\"\n\nimport sys\nimport os\nimport types\n\nfrom . import loader, runner\nfrom .signals import installHandler\n\n__unittest = True\n\nFAILFAST     = \"  -f, --failfast   Stop on first failure\\n\"\nCATCHBREAK   = \"  -c, --catch      Catch control-C and display results\\n\"\nBUFFEROUTPUT = \"  -b, --buffer     Buffer stdout and stderr during test runs\\n\"\n\nUSAGE_AS_MAIN = \"\"\"\\\nUsage: %(progName)s [options] [tests]\n\nOptions:\n  -h, --help       Show this message\n  -v, --verbose    Verbose output\n  -q, --quiet      Minimal output\n%(failfast)s%(catchbreak)s%(buffer)s\nExamples:\n  %(progName)s test_module               - run tests from test_module\n  %(progName)s module.TestClass          - run tests from module.TestClass\n  %(progName)s module.Class.test_method  - run specified test method\n\n[tests] can be a list of any number of test modules, classes and test\nmethods.\n\nAlternative Usage: %(progName)s discover [options]\n\nOptions:\n  -v, --verbose    Verbose output\n%(failfast)s%(catchbreak)s%(buffer)s  -s directory     Directory to start discovery ('.' default)\n  -p pattern       Pattern to match test files ('test*.py' default)\n  -t directory     Top level directory of project (default to\n                   start directory)\n\nFor test discovery all test modules must be importable from the top\nlevel directory of the project.\n\"\"\"\n\nUSAGE_FROM_MODULE = \"\"\"\\\nUsage: %(progName)s [options] [test] [...]\n\nOptions:\n  -h, --help       Show this message\n  -v, --verbose    Verbose output\n  -q, --quiet      Minimal output\n%(failfast)s%(catchbreak)s%(buffer)s\nExamples:\n  %(progName)s                               - run default set of tests\n  %(progName)s MyTestSuite                   - run suite 'MyTestSuite'\n  %(progName)s MyTestCase.testSomething      - run MyTestCase.testSomething\n  %(progName)s MyTestCase                    - run all 'test*' test methods\n                                               in MyTestCase\n\"\"\"\n\n\n\nclass TestProgram(object):\n    \"\"\"A command-line program that runs a set of tests; this is primarily\n       for making test modules conveniently executable.\n    \"\"\"\n    USAGE = USAGE_FROM_MODULE\n\n    # defaults for testing\n    failfast = catchbreak = buffer = progName = None\n\n    def __init__(self, module='__main__', defaultTest=None, argv=None,\n                    testRunner=None, testLoader=loader.defaultTestLoader,\n                    exit=True, verbosity=1, failfast=None, catchbreak=None,\n                    buffer=None):\n        if isinstance(module, basestring):\n            self.module = __import__(module)\n            for part in module.split('.')[1:]:\n                self.module = getattr(self.module, part)\n        else:\n            self.module = module\n        if argv is None:\n            argv = sys.argv\n\n        self.exit = exit\n        self.failfast = failfast\n        self.catchbreak = catchbreak\n        self.verbosity = verbosity\n        self.buffer = buffer\n        self.defaultTest = defaultTest\n        self.testRunner = testRunner\n        self.testLoader = testLoader\n        self.progName = os.path.basename(argv[0])\n        self.parseArgs(argv)\n        self.runTests()\n\n    def usageExit(self, msg=None):\n        if msg:\n            print msg\n        usage = {'progName': self.progName, 'catchbreak': '', 'failfast': '',\n                 'buffer': ''}\n        if self.failfast != False:\n            usage['failfast'] = FAILFAST\n        if self.catchbreak != False:\n            usage['catchbreak'] = CATCHBREAK\n        if self.buffer != False:\n            usage['buffer'] = BUFFEROUTPUT\n        print self.USAGE % usage\n        sys.exit(2)\n\n    def parseArgs(self, argv):\n        if len(argv) > 1 and argv[1].lower() == 'discover':\n            self._do_discovery(argv[2:])\n            return\n\n        import getopt\n        long_opts = ['help', 'verbose', 'quiet', 'failfast', 'catch', 'buffer']\n        try:\n            options, args = getopt.getopt(argv[1:], 'hHvqfcb', long_opts)\n            for opt, value in options:\n                if opt in ('-h','-H','--help'):\n                    self.usageExit()\n                if opt in ('-q','--quiet'):\n                    self.verbosity = 0\n                if opt in ('-v','--verbose'):\n                    self.verbosity = 2\n                if opt in ('-f','--failfast'):\n                    if self.failfast is None:\n                        self.failfast = True\n                    # Should this raise an exception if -f is not valid?\n                if opt in ('-c','--catch'):\n                    if self.catchbreak is None:\n                        self.catchbreak = True\n                    # Should this raise an exception if -c is not valid?\n                if opt in ('-b','--buffer'):\n                    if self.buffer is None:\n                        self.buffer = True\n                    # Should this raise an exception if -b is not valid?\n            if len(args) == 0 and self.defaultTest is None:\n                # createTests will load tests from self.module\n                self.testNames = None\n            elif len(args) > 0:\n                self.testNames = args\n                if __name__ == '__main__':\n                    # to support python -m unittest ...\n                    self.module = None\n            else:\n                self.testNames = (self.defaultTest,)\n            self.createTests()\n        except getopt.error, msg:\n            self.usageExit(msg)\n\n    def createTests(self):\n        if self.testNames is None:\n            self.test = self.testLoader.loadTestsFromModule(self.module)\n        else:\n            self.test = self.testLoader.loadTestsFromNames(self.testNames,\n                                                           self.module)\n\n    def _do_discovery(self, argv, Loader=None):\n        if Loader is None:\n            Loader = lambda: self.testLoader\n\n        # handle command line args for test discovery\n        self.progName = '%s discover' % self.progName\n        import optparse\n        parser = optparse.OptionParser()\n        parser.prog = self.progName\n        parser.add_option('-v', '--verbose', dest='verbose', default=False,\n                          help='Verbose output', action='store_true')\n        if self.failfast != False:\n            parser.add_option('-f', '--failfast', dest='failfast', default=False,\n                              help='Stop on first fail or error',\n                              action='store_true')\n        if self.catchbreak != False:\n            parser.add_option('-c', '--catch', dest='catchbreak', default=False,\n                              help='Catch ctrl-C and display results so far',\n                              action='store_true')\n        if self.buffer != False:\n            parser.add_option('-b', '--buffer', dest='buffer', default=False,\n                              help='Buffer stdout and stderr during tests',\n                              action='store_true')\n        parser.add_option('-s', '--start-directory', dest='start', default='.',\n                          help=\"Directory to start discovery ('.' default)\")\n        parser.add_option('-p', '--pattern', dest='pattern', default='test*.py',\n                          help=\"Pattern to match tests ('test*.py' default)\")\n        parser.add_option('-t', '--top-level-directory', dest='top', default=None,\n                          help='Top level directory of project (defaults to start directory)')\n\n        options, args = parser.parse_args(argv)\n        if len(args) > 3:\n            self.usageExit()\n\n        for name, value in zip(('start', 'pattern', 'top'), args):\n            setattr(options, name, value)\n\n        # only set options from the parsing here\n        # if they weren't set explicitly in the constructor\n        if self.failfast is None:\n            self.failfast = options.failfast\n        if self.catchbreak is None:\n            self.catchbreak = options.catchbreak\n        if self.buffer is None:\n            self.buffer = options.buffer\n\n        if options.verbose:\n            self.verbosity = 2\n\n        start_dir = options.start\n        pattern = options.pattern\n        top_level_dir = options.top\n\n        loader = Loader()\n        self.test = loader.discover(start_dir, pattern, top_level_dir)\n\n    def runTests(self):\n        if self.catchbreak:\n            installHandler()\n        if self.testRunner is None:\n            self.testRunner = runner.TextTestRunner\n        if isinstance(self.testRunner, (type, types.ClassType)):\n            try:\n                testRunner = self.testRunner(verbosity=self.verbosity,\n                                             failfast=self.failfast,\n                                             buffer=self.buffer)\n            except TypeError:\n                # didn't accept the verbosity, buffer or failfast arguments\n                testRunner = self.testRunner()\n        else:\n            # it is assumed to be a TestRunner instance\n            testRunner = self.testRunner\n        self.result = testRunner.run(self.test)\n        if self.exit:\n            sys.exit(not self.result.wasSuccessful())\n\nmain = TestProgram\n", 
    "unittest.result": "\"\"\"Test result object\"\"\"\n\nimport os\nimport sys\nimport traceback\n\nfrom StringIO import StringIO\n\nfrom . import util\nfrom functools import wraps\n\n__unittest = True\n\ndef failfast(method):\n    @wraps(method)\n    def inner(self, *args, **kw):\n        if getattr(self, 'failfast', False):\n            self.stop()\n        return method(self, *args, **kw)\n    return inner\n\nSTDOUT_LINE = '\\nStdout:\\n%s'\nSTDERR_LINE = '\\nStderr:\\n%s'\n\n\nclass TestResult(object):\n    \"\"\"Holder for test result information.\n\n    Test results are automatically managed by the TestCase and TestSuite\n    classes, and do not need to be explicitly manipulated by writers of tests.\n\n    Each instance holds the total number of tests run, and collections of\n    failures and errors that occurred among those test runs. The collections\n    contain tuples of (testcase, exceptioninfo), where exceptioninfo is the\n    formatted traceback of the error that occurred.\n    \"\"\"\n    _previousTestClass = None\n    _testRunEntered = False\n    _moduleSetUpFailed = False\n    def __init__(self, stream=None, descriptions=None, verbosity=None):\n        self.failfast = False\n        self.failures = []\n        self.errors = []\n        self.testsRun = 0\n        self.skipped = []\n        self.expectedFailures = []\n        self.unexpectedSuccesses = []\n        self.shouldStop = False\n        self.buffer = False\n        self._stdout_buffer = None\n        self._stderr_buffer = None\n        self._original_stdout = sys.stdout\n        self._original_stderr = sys.stderr\n        self._mirrorOutput = False\n\n    def printErrors(self):\n        \"Called by TestRunner after test run\"\n\n    def startTest(self, test):\n        \"Called when the given test is about to be run\"\n        self.testsRun += 1\n        self._mirrorOutput = False\n        self._setupStdout()\n\n    def _setupStdout(self):\n        if self.buffer:\n            if self._stderr_buffer is None:\n                self._stderr_buffer = StringIO()\n                self._stdout_buffer = StringIO()\n            sys.stdout = self._stdout_buffer\n            sys.stderr = self._stderr_buffer\n\n    def startTestRun(self):\n        \"\"\"Called once before any tests are executed.\n\n        See startTest for a method called before each test.\n        \"\"\"\n\n    def stopTest(self, test):\n        \"\"\"Called when the given test has been run\"\"\"\n        self._restoreStdout()\n        self._mirrorOutput = False\n\n    def _restoreStdout(self):\n        if self.buffer:\n            if self._mirrorOutput:\n                output = sys.stdout.getvalue()\n                error = sys.stderr.getvalue()\n                if output:\n                    if not output.endswith('\\n'):\n                        output += '\\n'\n                    self._original_stdout.write(STDOUT_LINE % output)\n                if error:\n                    if not error.endswith('\\n'):\n                        error += '\\n'\n                    self._original_stderr.write(STDERR_LINE % error)\n\n            sys.stdout = self._original_stdout\n            sys.stderr = self._original_stderr\n            self._stdout_buffer.seek(0)\n            self._stdout_buffer.truncate()\n            self._stderr_buffer.seek(0)\n            self._stderr_buffer.truncate()\n\n    def stopTestRun(self):\n        \"\"\"Called once after all tests are executed.\n\n        See stopTest for a method called after each test.\n        \"\"\"\n\n    @failfast\n    def addError(self, test, err):\n        \"\"\"Called when an error has occurred. 'err' is a tuple of values as\n        returned by sys.exc_info().\n        \"\"\"\n        self.errors.append((test, self._exc_info_to_string(err, test)))\n        self._mirrorOutput = True\n\n    @failfast\n    def addFailure(self, test, err):\n        \"\"\"Called when an error has occurred. 'err' is a tuple of values as\n        returned by sys.exc_info().\"\"\"\n        self.failures.append((test, self._exc_info_to_string(err, test)))\n        self._mirrorOutput = True\n\n    def addSuccess(self, test):\n        \"Called when a test has completed successfully\"\n        pass\n\n    def addSkip(self, test, reason):\n        \"\"\"Called when a test is skipped.\"\"\"\n        self.skipped.append((test, reason))\n\n    def addExpectedFailure(self, test, err):\n        \"\"\"Called when an expected failure/error occured.\"\"\"\n        self.expectedFailures.append(\n            (test, self._exc_info_to_string(err, test)))\n\n    @failfast\n    def addUnexpectedSuccess(self, test):\n        \"\"\"Called when a test was expected to fail, but succeed.\"\"\"\n        self.unexpectedSuccesses.append(test)\n\n    def wasSuccessful(self):\n        \"Tells whether or not this result was a success\"\n        return len(self.failures) == len(self.errors) == 0\n\n    def stop(self):\n        \"Indicates that the tests should be aborted\"\n        self.shouldStop = True\n\n    def _exc_info_to_string(self, err, test):\n        \"\"\"Converts a sys.exc_info()-style tuple of values into a string.\"\"\"\n        exctype, value, tb = err\n        # Skip test runner traceback levels\n        while tb and self._is_relevant_tb_level(tb):\n            tb = tb.tb_next\n\n        if exctype is test.failureException:\n            # Skip assert*() traceback levels\n            length = self._count_relevant_tb_levels(tb)\n            msgLines = traceback.format_exception(exctype, value, tb, length)\n        else:\n            msgLines = traceback.format_exception(exctype, value, tb)\n\n        if self.buffer:\n            output = sys.stdout.getvalue()\n            error = sys.stderr.getvalue()\n            if output:\n                if not output.endswith('\\n'):\n                    output += '\\n'\n                msgLines.append(STDOUT_LINE % output)\n            if error:\n                if not error.endswith('\\n'):\n                    error += '\\n'\n                msgLines.append(STDERR_LINE % error)\n        return ''.join(msgLines)\n\n\n    def _is_relevant_tb_level(self, tb):\n        return '__unittest' in tb.tb_frame.f_globals\n\n    def _count_relevant_tb_levels(self, tb):\n        length = 0\n        while tb and not self._is_relevant_tb_level(tb):\n            length += 1\n            tb = tb.tb_next\n        return length\n\n    def __repr__(self):\n        return (\"<%s run=%i errors=%i failures=%i>\" %\n               (util.strclass(self.__class__), self.testsRun, len(self.errors),\n                len(self.failures)))\n", 
    "unittest.runner": "\"\"\"Running tests\"\"\"\n\nimport sys\nimport time\n\nfrom . import result\nfrom .signals import registerResult\n\n__unittest = True\n\n\nclass _WritelnDecorator(object):\n    \"\"\"Used to decorate file-like objects with a handy 'writeln' method\"\"\"\n    def __init__(self,stream):\n        self.stream = stream\n\n    def __getattr__(self, attr):\n        if attr in ('stream', '__getstate__'):\n            raise AttributeError(attr)\n        return getattr(self.stream,attr)\n\n    def writeln(self, arg=None):\n        if arg:\n            self.write(arg)\n        self.write('\\n') # text-mode streams translate to \\r\\n if needed\n\n\nclass TextTestResult(result.TestResult):\n    \"\"\"A test result class that can print formatted text results to a stream.\n\n    Used by TextTestRunner.\n    \"\"\"\n    separator1 = '=' * 70\n    separator2 = '-' * 70\n\n    def __init__(self, stream, descriptions, verbosity):\n        super(TextTestResult, self).__init__(stream, descriptions, verbosity)\n        self.stream = stream\n        self.showAll = verbosity > 1\n        self.dots = verbosity == 1\n        self.descriptions = descriptions\n\n    def getDescription(self, test):\n        doc_first_line = test.shortDescription()\n        if self.descriptions and doc_first_line:\n            return '\\n'.join((str(test), doc_first_line))\n        else:\n            return str(test)\n\n    def startTest(self, test):\n        super(TextTestResult, self).startTest(test)\n        if self.showAll:\n            self.stream.write(self.getDescription(test))\n            self.stream.write(\" ... \")\n            self.stream.flush()\n\n    def addSuccess(self, test):\n        super(TextTestResult, self).addSuccess(test)\n        if self.showAll:\n            self.stream.writeln(\"ok\")\n        elif self.dots:\n            self.stream.write('.')\n            self.stream.flush()\n\n    def addError(self, test, err):\n        super(TextTestResult, self).addError(test, err)\n        if self.showAll:\n            self.stream.writeln(\"ERROR\")\n        elif self.dots:\n            self.stream.write('E')\n            self.stream.flush()\n\n    def addFailure(self, test, err):\n        super(TextTestResult, self).addFailure(test, err)\n        if self.showAll:\n            self.stream.writeln(\"FAIL\")\n        elif self.dots:\n            self.stream.write('F')\n            self.stream.flush()\n\n    def addSkip(self, test, reason):\n        super(TextTestResult, self).addSkip(test, reason)\n        if self.showAll:\n            self.stream.writeln(\"skipped {0!r}\".format(reason))\n        elif self.dots:\n            self.stream.write(\"s\")\n            self.stream.flush()\n\n    def addExpectedFailure(self, test, err):\n        super(TextTestResult, self).addExpectedFailure(test, err)\n        if self.showAll:\n            self.stream.writeln(\"expected failure\")\n        elif self.dots:\n            self.stream.write(\"x\")\n            self.stream.flush()\n\n    def addUnexpectedSuccess(self, test):\n        super(TextTestResult, self).addUnexpectedSuccess(test)\n        if self.showAll:\n            self.stream.writeln(\"unexpected success\")\n        elif self.dots:\n            self.stream.write(\"u\")\n            self.stream.flush()\n\n    def printErrors(self):\n        if self.dots or self.showAll:\n            self.stream.writeln()\n        self.printErrorList('ERROR', self.errors)\n        self.printErrorList('FAIL', self.failures)\n\n    def printErrorList(self, flavour, errors):\n        for test, err in errors:\n            self.stream.writeln(self.separator1)\n            self.stream.writeln(\"%s: %s\" % (flavour,self.getDescription(test)))\n            self.stream.writeln(self.separator2)\n            self.stream.writeln(\"%s\" % err)\n\n\nclass TextTestRunner(object):\n    \"\"\"A test runner class that displays results in textual form.\n\n    It prints out the names of tests as they are run, errors as they\n    occur, and a summary of the results at the end of the test run.\n    \"\"\"\n    resultclass = TextTestResult\n\n    def __init__(self, stream=sys.stderr, descriptions=True, verbosity=1,\n                 failfast=False, buffer=False, resultclass=None):\n        self.stream = _WritelnDecorator(stream)\n        self.descriptions = descriptions\n        self.verbosity = verbosity\n        self.failfast = failfast\n        self.buffer = buffer\n        if resultclass is not None:\n            self.resultclass = resultclass\n\n    def _makeResult(self):\n        return self.resultclass(self.stream, self.descriptions, self.verbosity)\n\n    def run(self, test):\n        \"Run the given test case or test suite.\"\n        result = self._makeResult()\n        registerResult(result)\n        result.failfast = self.failfast\n        result.buffer = self.buffer\n        startTime = time.time()\n        startTestRun = getattr(result, 'startTestRun', None)\n        if startTestRun is not None:\n            startTestRun()\n        try:\n            test(result)\n        finally:\n            stopTestRun = getattr(result, 'stopTestRun', None)\n            if stopTestRun is not None:\n                stopTestRun()\n        stopTime = time.time()\n        timeTaken = stopTime - startTime\n        result.printErrors()\n        if hasattr(result, 'separator2'):\n            self.stream.writeln(result.separator2)\n        run = result.testsRun\n        self.stream.writeln(\"Ran %d test%s in %.3fs\" %\n                            (run, run != 1 and \"s\" or \"\", timeTaken))\n        self.stream.writeln()\n\n        expectedFails = unexpectedSuccesses = skipped = 0\n        try:\n            results = map(len, (result.expectedFailures,\n                                result.unexpectedSuccesses,\n                                result.skipped))\n        except AttributeError:\n            pass\n        else:\n            expectedFails, unexpectedSuccesses, skipped = results\n\n        infos = []\n        if not result.wasSuccessful():\n            self.stream.write(\"FAILED\")\n            failed, errored = map(len, (result.failures, result.errors))\n            if failed:\n                infos.append(\"failures=%d\" % failed)\n            if errored:\n                infos.append(\"errors=%d\" % errored)\n        else:\n            self.stream.write(\"OK\")\n        if skipped:\n            infos.append(\"skipped=%d\" % skipped)\n        if expectedFails:\n            infos.append(\"expected failures=%d\" % expectedFails)\n        if unexpectedSuccesses:\n            infos.append(\"unexpected successes=%d\" % unexpectedSuccesses)\n        if infos:\n            self.stream.writeln(\" (%s)\" % (\", \".join(infos),))\n        else:\n            self.stream.write(\"\\n\")\n        return result\n", 
    "unittest.signals": "import signal\nimport weakref\n\nfrom functools import wraps\n\n__unittest = True\n\n\nclass _InterruptHandler(object):\n    def __init__(self, default_handler):\n        self.called = False\n        self.original_handler = default_handler\n        if isinstance(default_handler, int):\n            if default_handler == signal.SIG_DFL:\n                # Pretend it's signal.default_int_handler instead.\n                default_handler = signal.default_int_handler\n            elif default_handler == signal.SIG_IGN:\n                # Not quite the same thing as SIG_IGN, but the closest we\n                # can make it: do nothing.\n                def default_handler(unused_signum, unused_frame):\n                    pass\n            else:\n                raise TypeError(\"expected SIGINT signal handler to be \"\n                                \"signal.SIG_IGN, signal.SIG_DFL, or a \"\n                                \"callable object\")\n        self.default_handler = default_handler\n\n    def __call__(self, signum, frame):\n        installed_handler = signal.getsignal(signal.SIGINT)\n        if installed_handler is not self:\n            # if we aren't the installed handler, then delegate immediately\n            # to the default handler\n            self.default_handler(signum, frame)\n\n        if self.called:\n            self.default_handler(signum, frame)\n        self.called = True\n        for result in _results.keys():\n            result.stop()\n\n_results = weakref.WeakKeyDictionary()\ndef registerResult(result):\n    _results[result] = 1\n\ndef removeResult(result):\n    return bool(_results.pop(result, None))\n\n_interrupt_handler = None\ndef installHandler():\n    global _interrupt_handler\n    if _interrupt_handler is None:\n        default_handler = signal.getsignal(signal.SIGINT)\n        _interrupt_handler = _InterruptHandler(default_handler)\n        signal.signal(signal.SIGINT, _interrupt_handler)\n\n\ndef removeHandler(method=None):\n    if method is not None:\n        @wraps(method)\n        def inner(*args, **kwargs):\n            initial = signal.getsignal(signal.SIGINT)\n            removeHandler()\n            try:\n                return method(*args, **kwargs)\n            finally:\n                signal.signal(signal.SIGINT, initial)\n        return inner\n\n    global _interrupt_handler\n    if _interrupt_handler is not None:\n        signal.signal(signal.SIGINT, _interrupt_handler.original_handler)\n", 
    "unittest.suite": "\"\"\"TestSuite\"\"\"\n\nimport sys\n\nfrom . import case\nfrom . import util\n\n__unittest = True\n\n\ndef _call_if_exists(parent, attr):\n    func = getattr(parent, attr, lambda: None)\n    func()\n\n\nclass BaseTestSuite(object):\n    \"\"\"A simple test suite that doesn't provide class or module shared fixtures.\n    \"\"\"\n    def __init__(self, tests=()):\n        self._tests = []\n        self.addTests(tests)\n\n    def __repr__(self):\n        return \"<%s tests=%s>\" % (util.strclass(self.__class__), list(self))\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return list(self) == list(other)\n\n    def __ne__(self, other):\n        return not self == other\n\n    # Can't guarantee hash invariant, so flag as unhashable\n    __hash__ = None\n\n    def __iter__(self):\n        return iter(self._tests)\n\n    def countTestCases(self):\n        cases = 0\n        for test in self:\n            cases += test.countTestCases()\n        return cases\n\n    def addTest(self, test):\n        # sanity checks\n        if not hasattr(test, '__call__'):\n            raise TypeError(\"{} is not callable\".format(repr(test)))\n        if isinstance(test, type) and issubclass(test,\n                                                 (case.TestCase, TestSuite)):\n            raise TypeError(\"TestCases and TestSuites must be instantiated \"\n                            \"before passing them to addTest()\")\n        self._tests.append(test)\n\n    def addTests(self, tests):\n        if isinstance(tests, basestring):\n            raise TypeError(\"tests must be an iterable of tests, not a string\")\n        for test in tests:\n            self.addTest(test)\n\n    def run(self, result):\n        for test in self:\n            if result.shouldStop:\n                break\n            test(result)\n        return result\n\n    def __call__(self, *args, **kwds):\n        return self.run(*args, **kwds)\n\n    def debug(self):\n        \"\"\"Run the tests without collecting errors in a TestResult\"\"\"\n        for test in self:\n            test.debug()\n\n\nclass TestSuite(BaseTestSuite):\n    \"\"\"A test suite is a composite test consisting of a number of TestCases.\n\n    For use, create an instance of TestSuite, then add test case instances.\n    When all tests have been added, the suite can be passed to a test\n    runner, such as TextTestRunner. It will run the individual test cases\n    in the order in which they were added, aggregating the results. When\n    subclassing, do not forget to call the base class constructor.\n    \"\"\"\n\n    def run(self, result, debug=False):\n        topLevel = False\n        if getattr(result, '_testRunEntered', False) is False:\n            result._testRunEntered = topLevel = True\n\n        for test in self:\n            if result.shouldStop:\n                break\n\n            if _isnotsuite(test):\n                self._tearDownPreviousClass(test, result)\n                self._handleModuleFixture(test, result)\n                self._handleClassSetUp(test, result)\n                result._previousTestClass = test.__class__\n\n                if (getattr(test.__class__, '_classSetupFailed', False) or\n                    getattr(result, '_moduleSetUpFailed', False)):\n                    continue\n\n            if not debug:\n                test(result)\n            else:\n                test.debug()\n\n        if topLevel:\n            self._tearDownPreviousClass(None, result)\n            self._handleModuleTearDown(result)\n            result._testRunEntered = False\n        return result\n\n    def debug(self):\n        \"\"\"Run the tests without collecting errors in a TestResult\"\"\"\n        debug = _DebugResult()\n        self.run(debug, True)\n\n    ################################\n\n    def _handleClassSetUp(self, test, result):\n        previousClass = getattr(result, '_previousTestClass', None)\n        currentClass = test.__class__\n        if currentClass == previousClass:\n            return\n        if result._moduleSetUpFailed:\n            return\n        if getattr(currentClass, \"__unittest_skip__\", False):\n            return\n\n        try:\n            currentClass._classSetupFailed = False\n        except TypeError:\n            # test may actually be a function\n            # so its class will be a builtin-type\n            pass\n\n        setUpClass = getattr(currentClass, 'setUpClass', None)\n        if setUpClass is not None:\n            _call_if_exists(result, '_setupStdout')\n            try:\n                setUpClass()\n            except Exception as e:\n                if isinstance(result, _DebugResult):\n                    raise\n                currentClass._classSetupFailed = True\n                className = util.strclass(currentClass)\n                errorName = 'setUpClass (%s)' % className\n                self._addClassOrModuleLevelException(result, e, errorName)\n            finally:\n                _call_if_exists(result, '_restoreStdout')\n\n    def _get_previous_module(self, result):\n        previousModule = None\n        previousClass = getattr(result, '_previousTestClass', None)\n        if previousClass is not None:\n            previousModule = previousClass.__module__\n        return previousModule\n\n\n    def _handleModuleFixture(self, test, result):\n        previousModule = self._get_previous_module(result)\n        currentModule = test.__class__.__module__\n        if currentModule == previousModule:\n            return\n\n        self._handleModuleTearDown(result)\n\n        result._moduleSetUpFailed = False\n        try:\n            module = sys.modules[currentModule]\n        except KeyError:\n            return\n        setUpModule = getattr(module, 'setUpModule', None)\n        if setUpModule is not None:\n            _call_if_exists(result, '_setupStdout')\n            try:\n                setUpModule()\n            except Exception, e:\n                if isinstance(result, _DebugResult):\n                    raise\n                result._moduleSetUpFailed = True\n                errorName = 'setUpModule (%s)' % currentModule\n                self._addClassOrModuleLevelException(result, e, errorName)\n            finally:\n                _call_if_exists(result, '_restoreStdout')\n\n    def _addClassOrModuleLevelException(self, result, exception, errorName):\n        error = _ErrorHolder(errorName)\n        addSkip = getattr(result, 'addSkip', None)\n        if addSkip is not None and isinstance(exception, case.SkipTest):\n            addSkip(error, str(exception))\n        else:\n            result.addError(error, sys.exc_info())\n\n    def _handleModuleTearDown(self, result):\n        previousModule = self._get_previous_module(result)\n        if previousModule is None:\n            return\n        if result._moduleSetUpFailed:\n            return\n\n        try:\n            module = sys.modules[previousModule]\n        except KeyError:\n            return\n\n        tearDownModule = getattr(module, 'tearDownModule', None)\n        if tearDownModule is not None:\n            _call_if_exists(result, '_setupStdout')\n            try:\n                tearDownModule()\n            except Exception as e:\n                if isinstance(result, _DebugResult):\n                    raise\n                errorName = 'tearDownModule (%s)' % previousModule\n                self._addClassOrModuleLevelException(result, e, errorName)\n            finally:\n                _call_if_exists(result, '_restoreStdout')\n\n    def _tearDownPreviousClass(self, test, result):\n        previousClass = getattr(result, '_previousTestClass', None)\n        currentClass = test.__class__\n        if currentClass == previousClass:\n            return\n        if getattr(previousClass, '_classSetupFailed', False):\n            return\n        if getattr(result, '_moduleSetUpFailed', False):\n            return\n        if getattr(previousClass, \"__unittest_skip__\", False):\n            return\n\n        tearDownClass = getattr(previousClass, 'tearDownClass', None)\n        if tearDownClass is not None:\n            _call_if_exists(result, '_setupStdout')\n            try:\n                tearDownClass()\n            except Exception, e:\n                if isinstance(result, _DebugResult):\n                    raise\n                className = util.strclass(previousClass)\n                errorName = 'tearDownClass (%s)' % className\n                self._addClassOrModuleLevelException(result, e, errorName)\n            finally:\n                _call_if_exists(result, '_restoreStdout')\n\n\nclass _ErrorHolder(object):\n    \"\"\"\n    Placeholder for a TestCase inside a result. As far as a TestResult\n    is concerned, this looks exactly like a unit test. Used to insert\n    arbitrary errors into a test suite run.\n    \"\"\"\n    # Inspired by the ErrorHolder from Twisted:\n    # http://twistedmatrix.com/trac/browser/trunk/twisted/trial/runner.py\n\n    # attribute used by TestResult._exc_info_to_string\n    failureException = None\n\n    def __init__(self, description):\n        self.description = description\n\n    def id(self):\n        return self.description\n\n    def shortDescription(self):\n        return None\n\n    def __repr__(self):\n        return \"<ErrorHolder description=%r>\" % (self.description,)\n\n    def __str__(self):\n        return self.id()\n\n    def run(self, result):\n        # could call result.addError(...) - but this test-like object\n        # shouldn't be run anyway\n        pass\n\n    def __call__(self, result):\n        return self.run(result)\n\n    def countTestCases(self):\n        return 0\n\ndef _isnotsuite(test):\n    \"A crude way to tell apart testcases and suites with duck-typing\"\n    try:\n        iter(test)\n    except TypeError:\n        return True\n    return False\n\n\nclass _DebugResult(object):\n    \"Used by the TestSuite to hold previous class when running in debug.\"\n    _previousTestClass = None\n    _moduleSetUpFailed = False\n    shouldStop = False\n", 
    "unittest.util": "\"\"\"Various utility functions.\"\"\"\nfrom collections import namedtuple, OrderedDict\n\n\n__unittest = True\n\n_MAX_LENGTH = 80\ndef safe_repr(obj, short=False):\n    try:\n        result = repr(obj)\n    except Exception:\n        result = object.__repr__(obj)\n    if not short or len(result) < _MAX_LENGTH:\n        return result\n    return result[:_MAX_LENGTH] + ' [truncated]...'\n\n\ndef strclass(cls):\n    return \"%s.%s\" % (cls.__module__, cls.__name__)\n\ndef sorted_list_difference(expected, actual):\n    \"\"\"Finds elements in only one or the other of two, sorted input lists.\n\n    Returns a two-element tuple of lists.    The first list contains those\n    elements in the \"expected\" list but not in the \"actual\" list, and the\n    second contains those elements in the \"actual\" list but not in the\n    \"expected\" list.    Duplicate elements in either input list are ignored.\n    \"\"\"\n    i = j = 0\n    missing = []\n    unexpected = []\n    while True:\n        try:\n            e = expected[i]\n            a = actual[j]\n            if e < a:\n                missing.append(e)\n                i += 1\n                while expected[i] == e:\n                    i += 1\n            elif e > a:\n                unexpected.append(a)\n                j += 1\n                while actual[j] == a:\n                    j += 1\n            else:\n                i += 1\n                try:\n                    while expected[i] == e:\n                        i += 1\n                finally:\n                    j += 1\n                    while actual[j] == a:\n                        j += 1\n        except IndexError:\n            missing.extend(expected[i:])\n            unexpected.extend(actual[j:])\n            break\n    return missing, unexpected\n\n\ndef unorderable_list_difference(expected, actual, ignore_duplicate=False):\n    \"\"\"Same behavior as sorted_list_difference but\n    for lists of unorderable items (like dicts).\n\n    As it does a linear search per item (remove) it\n    has O(n*n) performance.\n    \"\"\"\n    missing = []\n    unexpected = []\n    while expected:\n        item = expected.pop()\n        try:\n            actual.remove(item)\n        except ValueError:\n            missing.append(item)\n        if ignore_duplicate:\n            for lst in expected, actual:\n                try:\n                    while True:\n                        lst.remove(item)\n                except ValueError:\n                    pass\n    if ignore_duplicate:\n        while actual:\n            item = actual.pop()\n            unexpected.append(item)\n            try:\n                while True:\n                    actual.remove(item)\n            except ValueError:\n                pass\n        return missing, unexpected\n\n    # anything left in actual is unexpected\n    return missing, actual\n\n_Mismatch = namedtuple('Mismatch', 'actual expected value')\n\ndef _count_diff_all_purpose(actual, expected):\n    'Returns list of (cnt_act, cnt_exp, elem) triples where the counts differ'\n    # elements need not be hashable\n    s, t = list(actual), list(expected)\n    m, n = len(s), len(t)\n    NULL = object()\n    result = []\n    for i, elem in enumerate(s):\n        if elem is NULL:\n            continue\n        cnt_s = cnt_t = 0\n        for j in range(i, m):\n            if s[j] == elem:\n                cnt_s += 1\n                s[j] = NULL\n        for j, other_elem in enumerate(t):\n            if other_elem == elem:\n                cnt_t += 1\n                t[j] = NULL\n        if cnt_s != cnt_t:\n            diff = _Mismatch(cnt_s, cnt_t, elem)\n            result.append(diff)\n\n    for i, elem in enumerate(t):\n        if elem is NULL:\n            continue\n        cnt_t = 0\n        for j in range(i, n):\n            if t[j] == elem:\n                cnt_t += 1\n                t[j] = NULL\n        diff = _Mismatch(0, cnt_t, elem)\n        result.append(diff)\n    return result\n\ndef _ordered_count(iterable):\n    'Return dict of element counts, in the order they were first seen'\n    c = OrderedDict()\n    for elem in iterable:\n        c[elem] = c.get(elem, 0) + 1\n    return c\n\ndef _count_diff_hashable(actual, expected):\n    'Returns list of (cnt_act, cnt_exp, elem) triples where the counts differ'\n    # elements must be hashable\n    s, t = _ordered_count(actual), _ordered_count(expected)\n    result = []\n    for elem, cnt_s in s.items():\n        cnt_t = t.get(elem, 0)\n        if cnt_s != cnt_t:\n            diff = _Mismatch(cnt_s, cnt_t, elem)\n            result.append(diff)\n    for elem, cnt_t in t.items():\n        if elem not in s:\n            diff = _Mismatch(0, cnt_t, elem)\n            result.append(diff)\n    return result\n", 
    "urllib": "\"\"\"Open an arbitrary URL.\n\nSee the following document for more info on URLs:\n\"Names and Addresses, URIs, URLs, URNs, URCs\", at\nhttp://www.w3.org/pub/WWW/Addressing/Overview.html\n\nSee also the HTTP spec (from which the error codes are derived):\n\"HTTP - Hypertext Transfer Protocol\", at\nhttp://www.w3.org/pub/WWW/Protocols/\n\nRelated standards and specs:\n- RFC1808: the \"relative URL\" spec. (authoritative status)\n- RFC1738 - the \"URL standard\". (authoritative status)\n- RFC1630 - the \"URI spec\". (informational status)\n\nThe object returned by URLopener().open(file) will differ per\nprotocol.  All you know is that is has methods read(), readline(),\nreadlines(), fileno(), close() and info().  The read*(), fileno()\nand close() methods work like those of open files.\nThe info() method returns a mimetools.Message object which can be\nused to query various info about the object, if available.\n(mimetools.Message objects are queried with the getheader() method.)\n\"\"\"\n\nimport string\nimport socket\nimport os\nimport time\nimport sys\nimport base64\nimport re\n\nfrom urlparse import urljoin as basejoin\n\n__all__ = [\"urlopen\", \"URLopener\", \"FancyURLopener\", \"urlretrieve\",\n           \"urlcleanup\", \"quote\", \"quote_plus\", \"unquote\", \"unquote_plus\",\n           \"urlencode\", \"url2pathname\", \"pathname2url\", \"splittag\",\n           \"localhost\", \"thishost\", \"ftperrors\", \"basejoin\", \"unwrap\",\n           \"splittype\", \"splithost\", \"splituser\", \"splitpasswd\", \"splitport\",\n           \"splitnport\", \"splitquery\", \"splitattr\", \"splitvalue\",\n           \"getproxies\"]\n\n__version__ = '1.17'    # XXX This version is not always updated :-(\n\nMAXFTPCACHE = 10        # Trim the ftp cache beyond this size\n\n# Helper for non-unix systems\nif os.name == 'nt':\n    from nturl2path import url2pathname, pathname2url\nelif os.name == 'riscos':\n    from rourl2path import url2pathname, pathname2url\nelse:\n    def url2pathname(pathname):\n        \"\"\"OS-specific conversion from a relative URL of the 'file' scheme\n        to a file system path; not recommended for general use.\"\"\"\n        return unquote(pathname)\n\n    def pathname2url(pathname):\n        \"\"\"OS-specific conversion from a file system path to a relative URL\n        of the 'file' scheme; not recommended for general use.\"\"\"\n        return quote(pathname)\n\n# This really consists of two pieces:\n# (1) a class which handles opening of all sorts of URLs\n#     (plus assorted utilities etc.)\n# (2) a set of functions for parsing URLs\n# XXX Should these be separated out into different modules?\n\n\n# Shortcut for basic usage\n_urlopener = None\ndef urlopen(url, data=None, proxies=None):\n    \"\"\"Create a file-like object for the specified URL to read from.\"\"\"\n    from warnings import warnpy3k\n    warnpy3k(\"urllib.urlopen() has been removed in Python 3.0 in \"\n             \"favor of urllib2.urlopen()\", stacklevel=2)\n\n    global _urlopener\n    if proxies is not None:\n        opener = FancyURLopener(proxies=proxies)\n    elif not _urlopener:\n        opener = FancyURLopener()\n        _urlopener = opener\n    else:\n        opener = _urlopener\n    if data is None:\n        return opener.open(url)\n    else:\n        return opener.open(url, data)\ndef urlretrieve(url, filename=None, reporthook=None, data=None):\n    global _urlopener\n    if not _urlopener:\n        _urlopener = FancyURLopener()\n    return _urlopener.retrieve(url, filename, reporthook, data)\ndef urlcleanup():\n    if _urlopener:\n        _urlopener.cleanup()\n    _safe_quoters.clear()\n    ftpcache.clear()\n\n# check for SSL\ntry:\n    import ssl\nexcept:\n    _have_ssl = False\nelse:\n    _have_ssl = True\n\n# exception raised when downloaded size does not match content-length\nclass ContentTooShortError(IOError):\n    def __init__(self, message, content):\n        IOError.__init__(self, message)\n        self.content = content\n\nftpcache = {}\nclass URLopener:\n    \"\"\"Class to open URLs.\n    This is a class rather than just a subroutine because we may need\n    more than one set of global protocol-specific options.\n    Note -- this is a base class for those who don't want the\n    automatic handling of errors type 302 (relocated) and 401\n    (authorization needed).\"\"\"\n\n    __tempfiles = None\n\n    version = \"Python-urllib/%s\" % __version__\n\n    # Constructor\n    def __init__(self, proxies=None, **x509):\n        if proxies is None:\n            proxies = getproxies()\n        assert hasattr(proxies, 'has_key'), \"proxies must be a mapping\"\n        self.proxies = proxies\n        self.key_file = x509.get('key_file')\n        self.cert_file = x509.get('cert_file')\n        self.addheaders = [('User-Agent', self.version)]\n        self.__tempfiles = []\n        self.__unlink = os.unlink # See cleanup()\n        self.tempcache = None\n        # Undocumented feature: if you assign {} to tempcache,\n        # it is used to cache files retrieved with\n        # self.retrieve().  This is not enabled by default\n        # since it does not work for changing documents (and I\n        # haven't got the logic to check expiration headers\n        # yet).\n        self.ftpcache = ftpcache\n        # Undocumented feature: you can use a different\n        # ftp cache by assigning to the .ftpcache member;\n        # in case you want logically independent URL openers\n        # XXX This is not threadsafe.  Bah.\n\n    def __del__(self):\n        self.close()\n\n    def close(self):\n        self.cleanup()\n\n    def cleanup(self):\n        # This code sometimes runs when the rest of this module\n        # has already been deleted, so it can't use any globals\n        # or import anything.\n        if self.__tempfiles:\n            for file in self.__tempfiles:\n                try:\n                    self.__unlink(file)\n                except OSError:\n                    pass\n            del self.__tempfiles[:]\n        if self.tempcache:\n            self.tempcache.clear()\n\n    def addheader(self, *args):\n        \"\"\"Add a header to be used by the HTTP interface only\n        e.g. u.addheader('Accept', 'sound/basic')\"\"\"\n        self.addheaders.append(args)\n\n    # External interface\n    def open(self, fullurl, data=None):\n        \"\"\"Use URLopener().open(file) instead of open(file, 'r').\"\"\"\n        fullurl = unwrap(toBytes(fullurl))\n        # percent encode url, fixing lame server errors for e.g, like space\n        # within url paths.\n        fullurl = quote(fullurl, safe=\"%/:=&?~#+!$,;'@()*[]|\")\n        if self.tempcache and fullurl in self.tempcache:\n            filename, headers = self.tempcache[fullurl]\n            fp = open(filename, 'rb')\n            return addinfourl(fp, headers, fullurl)\n        urltype, url = splittype(fullurl)\n        if not urltype:\n            urltype = 'file'\n        if urltype in self.proxies:\n            proxy = self.proxies[urltype]\n            urltype, proxyhost = splittype(proxy)\n            host, selector = splithost(proxyhost)\n            url = (host, fullurl) # Signal special case to open_*()\n        else:\n            proxy = None\n        name = 'open_' + urltype\n        self.type = urltype\n        name = name.replace('-', '_')\n        if not hasattr(self, name):\n            if proxy:\n                return self.open_unknown_proxy(proxy, fullurl, data)\n            else:\n                return self.open_unknown(fullurl, data)\n        try:\n            if data is None:\n                return getattr(self, name)(url)\n            else:\n                return getattr(self, name)(url, data)\n        except socket.error, msg:\n            raise IOError, ('socket error', msg), sys.exc_info()[2]\n\n    def open_unknown(self, fullurl, data=None):\n        \"\"\"Overridable interface to open unknown URL type.\"\"\"\n        type, url = splittype(fullurl)\n        raise IOError, ('url error', 'unknown url type', type)\n\n    def open_unknown_proxy(self, proxy, fullurl, data=None):\n        \"\"\"Overridable interface to open unknown URL type.\"\"\"\n        type, url = splittype(fullurl)\n        raise IOError, ('url error', 'invalid proxy for %s' % type, proxy)\n\n    # External interface\n    def retrieve(self, url, filename=None, reporthook=None, data=None):\n        \"\"\"retrieve(url) returns (filename, headers) for a local object\n        or (tempfilename, headers) for a remote object.\"\"\"\n        url = unwrap(toBytes(url))\n        if self.tempcache and url in self.tempcache:\n            return self.tempcache[url]\n        type, url1 = splittype(url)\n        if filename is None and (not type or type == 'file'):\n            try:\n                fp = self.open_local_file(url1)\n                hdrs = fp.info()\n                fp.close()\n                return url2pathname(splithost(url1)[1]), hdrs\n            except IOError:\n                pass\n        fp = self.open(url, data)\n        try:\n            headers = fp.info()\n            if filename:\n                tfp = open(filename, 'wb')\n            else:\n                import tempfile\n                garbage, path = splittype(url)\n                garbage, path = splithost(path or \"\")\n                path, garbage = splitquery(path or \"\")\n                path, garbage = splitattr(path or \"\")\n                suffix = os.path.splitext(path)[1]\n                (fd, filename) = tempfile.mkstemp(suffix)\n                self.__tempfiles.append(filename)\n                tfp = os.fdopen(fd, 'wb')\n            try:\n                result = filename, headers\n                if self.tempcache is not None:\n                    self.tempcache[url] = result\n                bs = 1024*8\n                size = -1\n                read = 0\n                blocknum = 0\n                if \"content-length\" in headers:\n                    size = int(headers[\"Content-Length\"])\n                if reporthook:\n                    reporthook(blocknum, bs, size)\n                while 1:\n                    block = fp.read(bs)\n                    if block == \"\":\n                        break\n                    read += len(block)\n                    tfp.write(block)\n                    blocknum += 1\n                    if reporthook:\n                        reporthook(blocknum, bs, size)\n            finally:\n                tfp.close()\n        finally:\n            fp.close()\n\n        # raise exception if actual size does not match content-length header\n        if size >= 0 and read < size:\n            raise ContentTooShortError(\"retrieval incomplete: got only %i out \"\n                                       \"of %i bytes\" % (read, size), result)\n\n        return result\n\n    # Each method named open_<type> knows how to open that type of URL\n\n    def open_http(self, url, data=None):\n        \"\"\"Use HTTP protocol.\"\"\"\n        import httplib\n        user_passwd = None\n        proxy_passwd= None\n        if isinstance(url, str):\n            host, selector = splithost(url)\n            if host:\n                user_passwd, host = splituser(host)\n                host = unquote(host)\n            realhost = host\n        else:\n            host, selector = url\n            # check whether the proxy contains authorization information\n            proxy_passwd, host = splituser(host)\n            # now we proceed with the url we want to obtain\n            urltype, rest = splittype(selector)\n            url = rest\n            user_passwd = None\n            if urltype.lower() != 'http':\n                realhost = None\n            else:\n                realhost, rest = splithost(rest)\n                if realhost:\n                    user_passwd, realhost = splituser(realhost)\n                if user_passwd:\n                    selector = \"%s://%s%s\" % (urltype, realhost, rest)\n                if proxy_bypass(realhost):\n                    host = realhost\n\n            #print \"proxy via http:\", host, selector\n        if not host: raise IOError, ('http error', 'no host given')\n\n        if proxy_passwd:\n            proxy_passwd = unquote(proxy_passwd)\n            proxy_auth = base64.b64encode(proxy_passwd).strip()\n        else:\n            proxy_auth = None\n\n        if user_passwd:\n            user_passwd = unquote(user_passwd)\n            auth = base64.b64encode(user_passwd).strip()\n        else:\n            auth = None\n        h = httplib.HTTP(host)\n        if data is not None:\n            h.putrequest('POST', selector)\n            h.putheader('Content-Type', 'application/x-www-form-urlencoded')\n            h.putheader('Content-Length', '%d' % len(data))\n        else:\n            h.putrequest('GET', selector)\n        if proxy_auth: h.putheader('Proxy-Authorization', 'Basic %s' % proxy_auth)\n        if auth: h.putheader('Authorization', 'Basic %s' % auth)\n        if realhost: h.putheader('Host', realhost)\n        for args in self.addheaders: h.putheader(*args)\n        h.endheaders(data)\n        errcode, errmsg, headers = h.getreply()\n        fp = h.getfile()\n        if errcode == -1:\n            if fp: fp.close()\n            # something went wrong with the HTTP status line\n            raise IOError, ('http protocol error', 0,\n                            'got a bad status line', None)\n        # According to RFC 2616, \"2xx\" code indicates that the client's\n        # request was successfully received, understood, and accepted.\n        if (200 <= errcode < 300):\n            return addinfourl(fp, headers, \"http:\" + url, errcode)\n        else:\n            if data is None:\n                return self.http_error(url, fp, errcode, errmsg, headers)\n            else:\n                return self.http_error(url, fp, errcode, errmsg, headers, data)\n\n    def http_error(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Handle http errors.\n        Derived class can override this, or provide specific handlers\n        named http_error_DDD where DDD is the 3-digit error code.\"\"\"\n        # First check if there's a specific handler for this error\n        name = 'http_error_%d' % errcode\n        if hasattr(self, name):\n            method = getattr(self, name)\n            if data is None:\n                result = method(url, fp, errcode, errmsg, headers)\n            else:\n                result = method(url, fp, errcode, errmsg, headers, data)\n            if result: return result\n        return self.http_error_default(url, fp, errcode, errmsg, headers)\n\n    def http_error_default(self, url, fp, errcode, errmsg, headers):\n        \"\"\"Default error handler: close the connection and raise IOError.\"\"\"\n        fp.close()\n        raise IOError, ('http error', errcode, errmsg, headers)\n\n    if _have_ssl:\n        def open_https(self, url, data=None):\n            \"\"\"Use HTTPS protocol.\"\"\"\n\n            import httplib\n            user_passwd = None\n            proxy_passwd = None\n            if isinstance(url, str):\n                host, selector = splithost(url)\n                if host:\n                    user_passwd, host = splituser(host)\n                    host = unquote(host)\n                realhost = host\n            else:\n                host, selector = url\n                # here, we determine, whether the proxy contains authorization information\n                proxy_passwd, host = splituser(host)\n                urltype, rest = splittype(selector)\n                url = rest\n                user_passwd = None\n                if urltype.lower() != 'https':\n                    realhost = None\n                else:\n                    realhost, rest = splithost(rest)\n                    if realhost:\n                        user_passwd, realhost = splituser(realhost)\n                    if user_passwd:\n                        selector = \"%s://%s%s\" % (urltype, realhost, rest)\n                #print \"proxy via https:\", host, selector\n            if not host: raise IOError, ('https error', 'no host given')\n            if proxy_passwd:\n                proxy_passwd = unquote(proxy_passwd)\n                proxy_auth = base64.b64encode(proxy_passwd).strip()\n            else:\n                proxy_auth = None\n            if user_passwd:\n                user_passwd = unquote(user_passwd)\n                auth = base64.b64encode(user_passwd).strip()\n            else:\n                auth = None\n            h = httplib.HTTPS(host, 0,\n                              key_file=self.key_file,\n                              cert_file=self.cert_file)\n            if data is not None:\n                h.putrequest('POST', selector)\n                h.putheader('Content-Type',\n                            'application/x-www-form-urlencoded')\n                h.putheader('Content-Length', '%d' % len(data))\n            else:\n                h.putrequest('GET', selector)\n            if proxy_auth: h.putheader('Proxy-Authorization', 'Basic %s' % proxy_auth)\n            if auth: h.putheader('Authorization', 'Basic %s' % auth)\n            if realhost: h.putheader('Host', realhost)\n            for args in self.addheaders: h.putheader(*args)\n            h.endheaders(data)\n            errcode, errmsg, headers = h.getreply()\n            fp = h.getfile()\n            if errcode == -1:\n                if fp: fp.close()\n                # something went wrong with the HTTP status line\n                raise IOError, ('http protocol error', 0,\n                                'got a bad status line', None)\n            # According to RFC 2616, \"2xx\" code indicates that the client's\n            # request was successfully received, understood, and accepted.\n            if (200 <= errcode < 300):\n                return addinfourl(fp, headers, \"https:\" + url, errcode)\n            else:\n                if data is None:\n                    return self.http_error(url, fp, errcode, errmsg, headers)\n                else:\n                    return self.http_error(url, fp, errcode, errmsg, headers,\n                                           data)\n\n    def open_file(self, url):\n        \"\"\"Use local file or FTP depending on form of URL.\"\"\"\n        if not isinstance(url, str):\n            raise IOError, ('file error', 'proxy support for file protocol currently not implemented')\n        if url[:2] == '//' and url[2:3] != '/' and url[2:12].lower() != 'localhost/':\n            return self.open_ftp(url)\n        else:\n            return self.open_local_file(url)\n\n    def open_local_file(self, url):\n        \"\"\"Use local file.\"\"\"\n        import mimetypes, mimetools, email.utils\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        host, file = splithost(url)\n        localname = url2pathname(file)\n        try:\n            stats = os.stat(localname)\n        except OSError, e:\n            raise IOError(e.errno, e.strerror, e.filename)\n        size = stats.st_size\n        modified = email.utils.formatdate(stats.st_mtime, usegmt=True)\n        mtype = mimetypes.guess_type(url)[0]\n        headers = mimetools.Message(StringIO(\n            'Content-Type: %s\\nContent-Length: %d\\nLast-modified: %s\\n' %\n            (mtype or 'text/plain', size, modified)))\n        if not host:\n            urlfile = file\n            if file[:1] == '/':\n                urlfile = 'file://' + file\n            elif file[:2] == './':\n                raise ValueError(\"local file url may start with / or file:. Unknown url of type: %s\" % url)\n            return addinfourl(open(localname, 'rb'),\n                              headers, urlfile)\n        host, port = splitport(host)\n        if not port \\\n           and socket.gethostbyname(host) in (localhost(), thishost()):\n            urlfile = file\n            if file[:1] == '/':\n                urlfile = 'file://' + file\n            return addinfourl(open(localname, 'rb'),\n                              headers, urlfile)\n        raise IOError, ('local file error', 'not on local host')\n\n    def open_ftp(self, url):\n        \"\"\"Use FTP protocol.\"\"\"\n        if not isinstance(url, str):\n            raise IOError, ('ftp error', 'proxy support for ftp protocol currently not implemented')\n        import mimetypes, mimetools\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        host, path = splithost(url)\n        if not host: raise IOError, ('ftp error', 'no host given')\n        host, port = splitport(host)\n        user, host = splituser(host)\n        if user: user, passwd = splitpasswd(user)\n        else: passwd = None\n        host = unquote(host)\n        user = user or ''\n        passwd = passwd or ''\n        host = socket.gethostbyname(host)\n        if not port:\n            import ftplib\n            port = ftplib.FTP_PORT\n        else:\n            port = int(port)\n        path, attrs = splitattr(path)\n        path = unquote(path)\n        dirs = path.split('/')\n        dirs, file = dirs[:-1], dirs[-1]\n        if dirs and not dirs[0]: dirs = dirs[1:]\n        if dirs and not dirs[0]: dirs[0] = '/'\n        key = user, host, port, '/'.join(dirs)\n        # XXX thread unsafe!\n        if len(self.ftpcache) > MAXFTPCACHE:\n            # Prune the cache, rather arbitrarily\n            for k in self.ftpcache.keys():\n                if k != key:\n                    v = self.ftpcache[k]\n                    del self.ftpcache[k]\n                    v.close()\n        try:\n            if not key in self.ftpcache:\n                self.ftpcache[key] = \\\n                    ftpwrapper(user, passwd, host, port, dirs)\n            if not file: type = 'D'\n            else: type = 'I'\n            for attr in attrs:\n                attr, value = splitvalue(attr)\n                if attr.lower() == 'type' and \\\n                   value in ('a', 'A', 'i', 'I', 'd', 'D'):\n                    type = value.upper()\n            (fp, retrlen) = self.ftpcache[key].retrfile(file, type)\n            mtype = mimetypes.guess_type(\"ftp:\" + url)[0]\n            headers = \"\"\n            if mtype:\n                headers += \"Content-Type: %s\\n\" % mtype\n            if retrlen is not None and retrlen >= 0:\n                headers += \"Content-Length: %d\\n\" % retrlen\n            headers = mimetools.Message(StringIO(headers))\n            return addinfourl(fp, headers, \"ftp:\" + url)\n        except ftperrors(), msg:\n            raise IOError, ('ftp error', msg), sys.exc_info()[2]\n\n    def open_data(self, url, data=None):\n        \"\"\"Use \"data\" URL.\"\"\"\n        if not isinstance(url, str):\n            raise IOError, ('data error', 'proxy support for data protocol currently not implemented')\n        # ignore POSTed data\n        #\n        # syntax of data URLs:\n        # dataurl   := \"data:\" [ mediatype ] [ \";base64\" ] \",\" data\n        # mediatype := [ type \"/\" subtype ] *( \";\" parameter )\n        # data      := *urlchar\n        # parameter := attribute \"=\" value\n        import mimetools\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        try:\n            [type, data] = url.split(',', 1)\n        except ValueError:\n            raise IOError, ('data error', 'bad data URL')\n        if not type:\n            type = 'text/plain;charset=US-ASCII'\n        semi = type.rfind(';')\n        if semi >= 0 and '=' not in type[semi:]:\n            encoding = type[semi+1:]\n            type = type[:semi]\n        else:\n            encoding = ''\n        msg = []\n        msg.append('Date: %s'%time.strftime('%a, %d %b %Y %H:%M:%S GMT',\n                                            time.gmtime(time.time())))\n        msg.append('Content-type: %s' % type)\n        if encoding == 'base64':\n            data = base64.decodestring(data)\n        else:\n            data = unquote(data)\n        msg.append('Content-Length: %d' % len(data))\n        msg.append('')\n        msg.append(data)\n        msg = '\\n'.join(msg)\n        f = StringIO(msg)\n        headers = mimetools.Message(f, 0)\n        #f.fileno = None     # needed for addinfourl\n        return addinfourl(f, headers, url)\n\n\nclass FancyURLopener(URLopener):\n    \"\"\"Derived class with handlers for errors we can handle (perhaps).\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        URLopener.__init__(self, *args, **kwargs)\n        self.auth_cache = {}\n        self.tries = 0\n        self.maxtries = 10\n\n    def http_error_default(self, url, fp, errcode, errmsg, headers):\n        \"\"\"Default error handling -- don't raise an exception.\"\"\"\n        return addinfourl(fp, headers, \"http:\" + url, errcode)\n\n    def http_error_302(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Error 302 -- relocated (temporarily).\"\"\"\n        self.tries += 1\n        if self.maxtries and self.tries >= self.maxtries:\n            if hasattr(self, \"http_error_500\"):\n                meth = self.http_error_500\n            else:\n                meth = self.http_error_default\n            self.tries = 0\n            return meth(url, fp, 500,\n                        \"Internal Server Error: Redirect Recursion\", headers)\n        result = self.redirect_internal(url, fp, errcode, errmsg, headers,\n                                        data)\n        self.tries = 0\n        return result\n\n    def redirect_internal(self, url, fp, errcode, errmsg, headers, data):\n        if 'location' in headers:\n            newurl = headers['location']\n        elif 'uri' in headers:\n            newurl = headers['uri']\n        else:\n            return\n        fp.close()\n        # In case the server sent a relative URL, join with original:\n        newurl = basejoin(self.type + \":\" + url, newurl)\n\n        # For security reasons we do not allow redirects to protocols\n        # other than HTTP, HTTPS or FTP.\n        newurl_lower = newurl.lower()\n        if not (newurl_lower.startswith('http://') or\n                newurl_lower.startswith('https://') or\n                newurl_lower.startswith('ftp://')):\n            raise IOError('redirect error', errcode,\n                          errmsg + \" - Redirection to url '%s' is not allowed\" %\n                          newurl,\n                          headers)\n\n        return self.open(newurl)\n\n    def http_error_301(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Error 301 -- also relocated (permanently).\"\"\"\n        return self.http_error_302(url, fp, errcode, errmsg, headers, data)\n\n    def http_error_303(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Error 303 -- also relocated (essentially identical to 302).\"\"\"\n        return self.http_error_302(url, fp, errcode, errmsg, headers, data)\n\n    def http_error_307(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Error 307 -- relocated, but turn POST into error.\"\"\"\n        if data is None:\n            return self.http_error_302(url, fp, errcode, errmsg, headers, data)\n        else:\n            return self.http_error_default(url, fp, errcode, errmsg, headers)\n\n    def http_error_401(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Error 401 -- authentication required.\n        This function supports Basic authentication only.\"\"\"\n        if not 'www-authenticate' in headers:\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        stuff = headers['www-authenticate']\n        import re\n        match = re.match('[ \\t]*([^ \\t]+)[ \\t]+realm=\"([^\"]*)\"', stuff)\n        if not match:\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        scheme, realm = match.groups()\n        if scheme.lower() != 'basic':\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        name = 'retry_' + self.type + '_basic_auth'\n        if data is None:\n            return getattr(self,name)(url, realm)\n        else:\n            return getattr(self,name)(url, realm, data)\n\n    def http_error_407(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Error 407 -- proxy authentication required.\n        This function supports Basic authentication only.\"\"\"\n        if not 'proxy-authenticate' in headers:\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        stuff = headers['proxy-authenticate']\n        import re\n        match = re.match('[ \\t]*([^ \\t]+)[ \\t]+realm=\"([^\"]*)\"', stuff)\n        if not match:\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        scheme, realm = match.groups()\n        if scheme.lower() != 'basic':\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        name = 'retry_proxy_' + self.type + '_basic_auth'\n        if data is None:\n            return getattr(self,name)(url, realm)\n        else:\n            return getattr(self,name)(url, realm, data)\n\n    def retry_proxy_http_basic_auth(self, url, realm, data=None):\n        host, selector = splithost(url)\n        newurl = 'http://' + host + selector\n        proxy = self.proxies['http']\n        urltype, proxyhost = splittype(proxy)\n        proxyhost, proxyselector = splithost(proxyhost)\n        i = proxyhost.find('@') + 1\n        proxyhost = proxyhost[i:]\n        user, passwd = self.get_user_passwd(proxyhost, realm, i)\n        if not (user or passwd): return None\n        proxyhost = quote(user, safe='') + ':' + quote(passwd, safe='') + '@' + proxyhost\n        self.proxies['http'] = 'http://' + proxyhost + proxyselector\n        if data is None:\n            return self.open(newurl)\n        else:\n            return self.open(newurl, data)\n\n    def retry_proxy_https_basic_auth(self, url, realm, data=None):\n        host, selector = splithost(url)\n        newurl = 'https://' + host + selector\n        proxy = self.proxies['https']\n        urltype, proxyhost = splittype(proxy)\n        proxyhost, proxyselector = splithost(proxyhost)\n        i = proxyhost.find('@') + 1\n        proxyhost = proxyhost[i:]\n        user, passwd = self.get_user_passwd(proxyhost, realm, i)\n        if not (user or passwd): return None\n        proxyhost = quote(user, safe='') + ':' + quote(passwd, safe='') + '@' + proxyhost\n        self.proxies['https'] = 'https://' + proxyhost + proxyselector\n        if data is None:\n            return self.open(newurl)\n        else:\n            return self.open(newurl, data)\n\n    def retry_http_basic_auth(self, url, realm, data=None):\n        host, selector = splithost(url)\n        i = host.find('@') + 1\n        host = host[i:]\n        user, passwd = self.get_user_passwd(host, realm, i)\n        if not (user or passwd): return None\n        host = quote(user, safe='') + ':' + quote(passwd, safe='') + '@' + host\n        newurl = 'http://' + host + selector\n        if data is None:\n            return self.open(newurl)\n        else:\n            return self.open(newurl, data)\n\n    def retry_https_basic_auth(self, url, realm, data=None):\n        host, selector = splithost(url)\n        i = host.find('@') + 1\n        host = host[i:]\n        user, passwd = self.get_user_passwd(host, realm, i)\n        if not (user or passwd): return None\n        host = quote(user, safe='') + ':' + quote(passwd, safe='') + '@' + host\n        newurl = 'https://' + host + selector\n        if data is None:\n            return self.open(newurl)\n        else:\n            return self.open(newurl, data)\n\n    def get_user_passwd(self, host, realm, clear_cache=0):\n        key = realm + '@' + host.lower()\n        if key in self.auth_cache:\n            if clear_cache:\n                del self.auth_cache[key]\n            else:\n                return self.auth_cache[key]\n        user, passwd = self.prompt_user_passwd(host, realm)\n        if user or passwd: self.auth_cache[key] = (user, passwd)\n        return user, passwd\n\n    def prompt_user_passwd(self, host, realm):\n        \"\"\"Override this in a GUI environment!\"\"\"\n        import getpass\n        try:\n            user = raw_input(\"Enter username for %s at %s: \" % (realm,\n                                                                host))\n            passwd = getpass.getpass(\"Enter password for %s in %s at %s: \" %\n                (user, realm, host))\n            return user, passwd\n        except KeyboardInterrupt:\n            print\n            return None, None\n\n\n# Utility functions\n\n_localhost = None\ndef localhost():\n    \"\"\"Return the IP address of the magic hostname 'localhost'.\"\"\"\n    global _localhost\n    if _localhost is None:\n        _localhost = socket.gethostbyname('localhost')\n    return _localhost\n\n_thishost = None\ndef thishost():\n    \"\"\"Return the IP address of the current host.\"\"\"\n    global _thishost\n    if _thishost is None:\n        try:\n            _thishost = socket.gethostbyname(socket.gethostname())\n        except socket.gaierror:\n            _thishost = socket.gethostbyname('localhost')\n    return _thishost\n\n_ftperrors = None\ndef ftperrors():\n    \"\"\"Return the set of errors raised by the FTP class.\"\"\"\n    global _ftperrors\n    if _ftperrors is None:\n        import ftplib\n        _ftperrors = ftplib.all_errors\n    return _ftperrors\n\n_noheaders = None\ndef noheaders():\n    \"\"\"Return an empty mimetools.Message object.\"\"\"\n    global _noheaders\n    if _noheaders is None:\n        import mimetools\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        _noheaders = mimetools.Message(StringIO(), 0)\n        _noheaders.fp.close()   # Recycle file descriptor\n    return _noheaders\n\n\n# Utility classes\n\nclass ftpwrapper:\n    \"\"\"Class used by open_ftp() for cache of open FTP connections.\"\"\"\n\n    def __init__(self, user, passwd, host, port, dirs,\n                 timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\n                 persistent=True):\n        self.user = user\n        self.passwd = passwd\n        self.host = host\n        self.port = port\n        self.dirs = dirs\n        self.timeout = timeout\n        self.refcount = 0\n        self.keepalive = persistent\n        self.init()\n\n    def init(self):\n        import ftplib\n        self.busy = 0\n        self.ftp = ftplib.FTP()\n        self.ftp.connect(self.host, self.port, self.timeout)\n        self.ftp.login(self.user, self.passwd)\n        _target = '/'.join(self.dirs)\n        self.ftp.cwd(_target)\n\n    def retrfile(self, file, type):\n        import ftplib\n        self.endtransfer()\n        if type in ('d', 'D'): cmd = 'TYPE A'; isdir = 1\n        else: cmd = 'TYPE ' + type; isdir = 0\n        try:\n            self.ftp.voidcmd(cmd)\n        except ftplib.all_errors:\n            self.init()\n            self.ftp.voidcmd(cmd)\n        conn = None\n        if file and not isdir:\n            # Try to retrieve as a file\n            try:\n                cmd = 'RETR ' + file\n                conn, retrlen = self.ftp.ntransfercmd(cmd)\n            except ftplib.error_perm, reason:\n                if str(reason)[:3] != '550':\n                    raise IOError, ('ftp error', reason), sys.exc_info()[2]\n        if not conn:\n            # Set transfer mode to ASCII!\n            self.ftp.voidcmd('TYPE A')\n            # Try a directory listing. Verify that directory exists.\n            if file:\n                pwd = self.ftp.pwd()\n                try:\n                    try:\n                        self.ftp.cwd(file)\n                    except ftplib.error_perm, reason:\n                        raise IOError, ('ftp error', reason), sys.exc_info()[2]\n                finally:\n                    self.ftp.cwd(pwd)\n                cmd = 'LIST ' + file\n            else:\n                cmd = 'LIST'\n            conn, retrlen = self.ftp.ntransfercmd(cmd)\n        self.busy = 1\n        ftpobj = addclosehook(conn.makefile('rb'), self.file_close)\n        self.refcount += 1\n        conn.close()\n        # Pass back both a suitably decorated object and a retrieval length\n        return (ftpobj, retrlen)\n\n    def endtransfer(self):\n        if not self.busy:\n            return\n        self.busy = 0\n        try:\n            self.ftp.voidresp()\n        except ftperrors():\n            pass\n\n    def close(self):\n        self.keepalive = False\n        if self.refcount <= 0:\n            self.real_close()\n\n    def file_close(self):\n        self.endtransfer()\n        self.refcount -= 1\n        if self.refcount <= 0 and not self.keepalive:\n            self.real_close()\n\n    def real_close(self):\n        self.endtransfer()\n        try:\n            self.ftp.close()\n        except ftperrors():\n            pass\n\nclass addbase:\n    \"\"\"Base class for addinfo and addclosehook.\"\"\"\n\n    def __init__(self, fp):\n        self.fp = fp\n        self.read = self.fp.read\n        self.readline = self.fp.readline\n        if hasattr(self.fp, \"readlines\"): self.readlines = self.fp.readlines\n        if hasattr(self.fp, \"fileno\"):\n            self.fileno = self.fp.fileno\n        else:\n            self.fileno = lambda: None\n        if hasattr(self.fp, \"__iter__\"):\n            self.__iter__ = self.fp.__iter__\n            if hasattr(self.fp, \"next\"):\n                self.next = self.fp.next\n\n    def __repr__(self):\n        return '<%s at %r whose fp = %r>' % (self.__class__.__name__,\n                                             id(self), self.fp)\n\n    def close(self):\n        self.read = None\n        self.readline = None\n        self.readlines = None\n        self.fileno = None\n        if self.fp: self.fp.close()\n        self.fp = None\n\nclass addclosehook(addbase):\n    \"\"\"Class to add a close hook to an open file.\"\"\"\n\n    def __init__(self, fp, closehook, *hookargs):\n        addbase.__init__(self, fp)\n        self.closehook = closehook\n        self.hookargs = hookargs\n\n    def close(self):\n        if self.closehook:\n            self.closehook(*self.hookargs)\n            self.closehook = None\n            self.hookargs = None\n        addbase.close(self)\n\nclass addinfo(addbase):\n    \"\"\"class to add an info() method to an open file.\"\"\"\n\n    def __init__(self, fp, headers):\n        addbase.__init__(self, fp)\n        self.headers = headers\n\n    def info(self):\n        return self.headers\n\nclass addinfourl(addbase):\n    \"\"\"class to add info() and geturl() methods to an open file.\"\"\"\n\n    def __init__(self, fp, headers, url, code=None):\n        addbase.__init__(self, fp)\n        self.headers = headers\n        self.url = url\n        self.code = code\n\n    def info(self):\n        return self.headers\n\n    def getcode(self):\n        return self.code\n\n    def geturl(self):\n        return self.url\n\n\n# Utilities to parse URLs (most of these return None for missing parts):\n# unwrap('<URL:type://host/path>') --> 'type://host/path'\n# splittype('type:opaquestring') --> 'type', 'opaquestring'\n# splithost('//host[:port]/path') --> 'host[:port]', '/path'\n# splituser('user[:passwd]@host[:port]') --> 'user[:passwd]', 'host[:port]'\n# splitpasswd('user:passwd') -> 'user', 'passwd'\n# splitport('host:port') --> 'host', 'port'\n# splitquery('/path?query') --> '/path', 'query'\n# splittag('/path#tag') --> '/path', 'tag'\n# splitattr('/path;attr1=value1;attr2=value2;...') ->\n#   '/path', ['attr1=value1', 'attr2=value2', ...]\n# splitvalue('attr=value') --> 'attr', 'value'\n# unquote('abc%20def') -> 'abc def'\n# quote('abc def') -> 'abc%20def')\n\ntry:\n    unicode\nexcept NameError:\n    def _is_unicode(x):\n        return 0\nelse:\n    def _is_unicode(x):\n        return isinstance(x, unicode)\n\ndef toBytes(url):\n    \"\"\"toBytes(u\"URL\") --> 'URL'.\"\"\"\n    # Most URL schemes require ASCII. If that changes, the conversion\n    # can be relaxed\n    if _is_unicode(url):\n        try:\n            url = url.encode(\"ASCII\")\n        except UnicodeError:\n            raise UnicodeError(\"URL \" + repr(url) +\n                               \" contains non-ASCII characters\")\n    return url\n\ndef unwrap(url):\n    \"\"\"unwrap('<URL:type://host/path>') --> 'type://host/path'.\"\"\"\n    url = url.strip()\n    if url[:1] == '<' and url[-1:] == '>':\n        url = url[1:-1].strip()\n    if url[:4] == 'URL:': url = url[4:].strip()\n    return url\n\n_typeprog = None\ndef splittype(url):\n    \"\"\"splittype('type:opaquestring') --> 'type', 'opaquestring'.\"\"\"\n    global _typeprog\n    if _typeprog is None:\n        import re\n        _typeprog = re.compile('^([^/:]+):')\n\n    match = _typeprog.match(url)\n    if match:\n        scheme = match.group(1)\n        return scheme.lower(), url[len(scheme) + 1:]\n    return None, url\n\n_hostprog = None\ndef splithost(url):\n    \"\"\"splithost('//host[:port]/path') --> 'host[:port]', '/path'.\"\"\"\n    global _hostprog\n    if _hostprog is None:\n        import re\n        _hostprog = re.compile('^//([^/?]*)(.*)$')\n\n    match = _hostprog.match(url)\n    if match:\n        host_port = match.group(1)\n        path = match.group(2)\n        if path and not path.startswith('/'):\n            path = '/' + path\n        return host_port, path\n    return None, url\n\n_userprog = None\ndef splituser(host):\n    \"\"\"splituser('user[:passwd]@host[:port]') --> 'user[:passwd]', 'host[:port]'.\"\"\"\n    global _userprog\n    if _userprog is None:\n        import re\n        _userprog = re.compile('^(.*)@(.*)$')\n\n    match = _userprog.match(host)\n    if match: return match.group(1, 2)\n    return None, host\n\n_passwdprog = None\ndef splitpasswd(user):\n    \"\"\"splitpasswd('user:passwd') -> 'user', 'passwd'.\"\"\"\n    global _passwdprog\n    if _passwdprog is None:\n        import re\n        _passwdprog = re.compile('^([^:]*):(.*)$',re.S)\n\n    match = _passwdprog.match(user)\n    if match: return match.group(1, 2)\n    return user, None\n\n# splittag('/path#tag') --> '/path', 'tag'\n_portprog = None\ndef splitport(host):\n    \"\"\"splitport('host:port') --> 'host', 'port'.\"\"\"\n    global _portprog\n    if _portprog is None:\n        import re\n        _portprog = re.compile('^(.*):([0-9]*)$')\n\n    match = _portprog.match(host)\n    if match:\n        host, port = match.groups()\n        if port:\n            return host, port\n    return host, None\n\n_nportprog = None\ndef splitnport(host, defport=-1):\n    \"\"\"Split host and port, returning numeric port.\n    Return given default port if no ':' found; defaults to -1.\n    Return numerical port if a valid number are found after ':'.\n    Return None if ':' but not a valid number.\"\"\"\n    global _nportprog\n    if _nportprog is None:\n        import re\n        _nportprog = re.compile('^(.*):(.*)$')\n\n    match = _nportprog.match(host)\n    if match:\n        host, port = match.group(1, 2)\n        if port:\n            try:\n                nport = int(port)\n            except ValueError:\n                nport = None\n            return host, nport\n    return host, defport\n\n_queryprog = None\ndef splitquery(url):\n    \"\"\"splitquery('/path?query') --> '/path', 'query'.\"\"\"\n    global _queryprog\n    if _queryprog is None:\n        import re\n        _queryprog = re.compile('^(.*)\\?([^?]*)$')\n\n    match = _queryprog.match(url)\n    if match: return match.group(1, 2)\n    return url, None\n\n_tagprog = None\ndef splittag(url):\n    \"\"\"splittag('/path#tag') --> '/path', 'tag'.\"\"\"\n    global _tagprog\n    if _tagprog is None:\n        import re\n        _tagprog = re.compile('^(.*)#([^#]*)$')\n\n    match = _tagprog.match(url)\n    if match: return match.group(1, 2)\n    return url, None\n\ndef splitattr(url):\n    \"\"\"splitattr('/path;attr1=value1;attr2=value2;...') ->\n        '/path', ['attr1=value1', 'attr2=value2', ...].\"\"\"\n    words = url.split(';')\n    return words[0], words[1:]\n\n_valueprog = None\ndef splitvalue(attr):\n    \"\"\"splitvalue('attr=value') --> 'attr', 'value'.\"\"\"\n    global _valueprog\n    if _valueprog is None:\n        import re\n        _valueprog = re.compile('^([^=]*)=(.*)$')\n\n    match = _valueprog.match(attr)\n    if match: return match.group(1, 2)\n    return attr, None\n\n# urlparse contains a duplicate of this method to avoid a circular import.  If\n# you update this method, also update the copy in urlparse.  This code\n# duplication does not exist in Python3.\n\n_hexdig = '0123456789ABCDEFabcdef'\n_hextochr = dict((a + b, chr(int(a + b, 16)))\n                 for a in _hexdig for b in _hexdig)\n_asciire = re.compile('([\\x00-\\x7f]+)')\n\ndef unquote(s):\n    \"\"\"unquote('abc%20def') -> 'abc def'.\"\"\"\n    if _is_unicode(s):\n        if '%' not in s:\n            return s\n        bits = _asciire.split(s)\n        res = [bits[0]]\n        append = res.append\n        for i in range(1, len(bits), 2):\n            append(unquote(str(bits[i])).decode('latin1'))\n            append(bits[i + 1])\n        return ''.join(res)\n\n    bits = s.split('%')\n    # fastpath\n    if len(bits) == 1:\n        return s\n    res = [bits[0]]\n    append = res.append\n    for j in xrange(1, len(bits)):\n        item = bits[j]\n        try:\n            append(_hextochr[item[:2]])\n            append(item[2:])\n        except KeyError:\n            append('%')\n            append(item)\n    return ''.join(res)\n\ndef unquote_plus(s):\n    \"\"\"unquote('%7e/abc+def') -> '~/abc def'\"\"\"\n    s = s.replace('+', ' ')\n    return unquote(s)\n\nalways_safe = ('ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n               'abcdefghijklmnopqrstuvwxyz'\n               '0123456789' '_.-')\n_safe_map = {}\nfor i, c in zip(xrange(256), str(bytearray(xrange(256)))):\n    _safe_map[c] = c if (i < 128 and c in always_safe) else '%{:02X}'.format(i)\n_safe_quoters = {}\n\ndef quote(s, safe='/'):\n    \"\"\"quote('abc def') -> 'abc%20def'\n\n    Each part of a URL, e.g. the path info, the query, etc., has a\n    different set of reserved characters that must be quoted.\n\n    RFC 2396 Uniform Resource Identifiers (URI): Generic Syntax lists\n    the following reserved characters.\n\n    reserved    = \";\" | \"/\" | \"?\" | \":\" | \"@\" | \"&\" | \"=\" | \"+\" |\n                  \"$\" | \",\"\n\n    Each of these characters is reserved in some component of a URL,\n    but not necessarily in all of them.\n\n    By default, the quote function is intended for quoting the path\n    section of a URL.  Thus, it will not encode '/'.  This character\n    is reserved, but in typical usage the quote function is being\n    called on a path where the existing slash characters are used as\n    reserved characters.\n    \"\"\"\n    # fastpath\n    if not s:\n        if s is None:\n            raise TypeError('None object cannot be quoted')\n        return s\n    cachekey = (safe, always_safe)\n    try:\n        (quoter, safe) = _safe_quoters[cachekey]\n    except KeyError:\n        safe_map = _safe_map.copy()\n        safe_map.update([(c, c) for c in safe])\n        quoter = safe_map.__getitem__\n        safe = always_safe + safe\n        _safe_quoters[cachekey] = (quoter, safe)\n    if not s.rstrip(safe):\n        return s\n    return ''.join(map(quoter, s))\n\ndef quote_plus(s, safe=''):\n    \"\"\"Quote the query fragment of a URL; replacing ' ' with '+'\"\"\"\n    if ' ' in s:\n        s = quote(s, safe + ' ')\n        return s.replace(' ', '+')\n    return quote(s, safe)\n\ndef urlencode(query, doseq=0):\n    \"\"\"Encode a sequence of two-element tuples or dictionary into a URL query string.\n\n    If any values in the query arg are sequences and doseq is true, each\n    sequence element is converted to a separate parameter.\n\n    If the query arg is a sequence of two-element tuples, the order of the\n    parameters in the output will match the order of parameters in the\n    input.\n    \"\"\"\n\n    if hasattr(query,\"items\"):\n        # mapping objects\n        query = query.items()\n    else:\n        # it's a bother at times that strings and string-like objects are\n        # sequences...\n        try:\n            # non-sequence items should not work with len()\n            # non-empty strings will fail this\n            if len(query) and not isinstance(query[0], tuple):\n                raise TypeError\n            # zero-length sequences of all types will get here and succeed,\n            # but that's a minor nit - since the original implementation\n            # allowed empty dicts that type of behavior probably should be\n            # preserved for consistency\n        except TypeError:\n            ty,va,tb = sys.exc_info()\n            raise TypeError, \"not a valid non-string sequence or mapping object\", tb\n\n    l = []\n    if not doseq:\n        # preserve old behavior\n        for k, v in query:\n            k = quote_plus(str(k))\n            v = quote_plus(str(v))\n            l.append(k + '=' + v)\n    else:\n        for k, v in query:\n            k = quote_plus(str(k))\n            if isinstance(v, str):\n                v = quote_plus(v)\n                l.append(k + '=' + v)\n            elif _is_unicode(v):\n                # is there a reasonable way to convert to ASCII?\n                # encode generates a string, but \"replace\" or \"ignore\"\n                # lose information and \"strict\" can raise UnicodeError\n                v = quote_plus(v.encode(\"ASCII\",\"replace\"))\n                l.append(k + '=' + v)\n            else:\n                try:\n                    # is this a sufficient test for sequence-ness?\n                    len(v)\n                except TypeError:\n                    # not a sequence\n                    v = quote_plus(str(v))\n                    l.append(k + '=' + v)\n                else:\n                    # loop over the sequence\n                    for elt in v:\n                        l.append(k + '=' + quote_plus(str(elt)))\n    return '&'.join(l)\n\n# Proxy handling\ndef getproxies_environment():\n    \"\"\"Return a dictionary of scheme -> proxy server URL mappings.\n\n    Scan the environment for variables named <scheme>_proxy;\n    this seems to be the standard convention.  If you need a\n    different way, you can pass a proxies dictionary to the\n    [Fancy]URLopener constructor.\n\n    \"\"\"\n    proxies = {}\n    for name, value in os.environ.items():\n        name = name.lower()\n        if value and name[-6:] == '_proxy':\n            proxies[name[:-6]] = value\n    return proxies\n\ndef proxy_bypass_environment(host):\n    \"\"\"Test if proxies should not be used for a particular host.\n\n    Checks the environment for a variable named no_proxy, which should\n    be a list of DNS suffixes separated by commas, or '*' for all hosts.\n    \"\"\"\n    no_proxy = os.environ.get('no_proxy', '') or os.environ.get('NO_PROXY', '')\n    # '*' is special case for always bypass\n    if no_proxy == '*':\n        return 1\n    # strip port off host\n    hostonly, port = splitport(host)\n    # check if the host ends with any of the DNS suffixes\n    no_proxy_list = [proxy.strip() for proxy in no_proxy.split(',')]\n    for name in no_proxy_list:\n        if name and (hostonly.endswith(name) or host.endswith(name)):\n            return 1\n    # otherwise, don't bypass\n    return 0\n\n\nif sys.platform == 'darwin':\n    from _scproxy import _get_proxy_settings, _get_proxies\n\n    def proxy_bypass_macosx_sysconf(host):\n        \"\"\"\n        Return True iff this host shouldn't be accessed using a proxy\n\n        This function uses the MacOSX framework SystemConfiguration\n        to fetch the proxy information.\n        \"\"\"\n        import re\n        import socket\n        from fnmatch import fnmatch\n\n        hostonly, port = splitport(host)\n\n        def ip2num(ipAddr):\n            parts = ipAddr.split('.')\n            parts = map(int, parts)\n            if len(parts) != 4:\n                parts = (parts + [0, 0, 0, 0])[:4]\n            return (parts[0] << 24) | (parts[1] << 16) | (parts[2] << 8) | parts[3]\n\n        proxy_settings = _get_proxy_settings()\n\n        # Check for simple host names:\n        if '.' not in host:\n            if proxy_settings['exclude_simple']:\n                return True\n\n        hostIP = None\n\n        for value in proxy_settings.get('exceptions', ()):\n            # Items in the list are strings like these: *.local, 169.254/16\n            if not value: continue\n\n            m = re.match(r\"(\\d+(?:\\.\\d+)*)(/\\d+)?\", value)\n            if m is not None:\n                if hostIP is None:\n                    try:\n                        hostIP = socket.gethostbyname(hostonly)\n                        hostIP = ip2num(hostIP)\n                    except socket.error:\n                        continue\n\n                base = ip2num(m.group(1))\n                mask = m.group(2)\n                if mask is None:\n                    mask = 8 * (m.group(1).count('.') + 1)\n\n                else:\n                    mask = int(mask[1:])\n                mask = 32 - mask\n\n                if (hostIP >> mask) == (base >> mask):\n                    return True\n\n            elif fnmatch(host, value):\n                return True\n\n        return False\n\n    def getproxies_macosx_sysconf():\n        \"\"\"Return a dictionary of scheme -> proxy server URL mappings.\n\n        This function uses the MacOSX framework SystemConfiguration\n        to fetch the proxy information.\n        \"\"\"\n        return _get_proxies()\n\n    def proxy_bypass(host):\n        if getproxies_environment():\n            return proxy_bypass_environment(host)\n        else:\n            return proxy_bypass_macosx_sysconf(host)\n\n    def getproxies():\n        return getproxies_environment() or getproxies_macosx_sysconf()\n\nelif os.name == 'nt':\n    def getproxies_registry():\n        \"\"\"Return a dictionary of scheme -> proxy server URL mappings.\n\n        Win32 uses the registry to store proxies.\n\n        \"\"\"\n        proxies = {}\n        try:\n            import _winreg\n        except ImportError:\n            # Std module, so should be around - but you never know!\n            return proxies\n        try:\n            internetSettings = _winreg.OpenKey(_winreg.HKEY_CURRENT_USER,\n                r'Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings')\n            proxyEnable = _winreg.QueryValueEx(internetSettings,\n                                               'ProxyEnable')[0]\n            if proxyEnable:\n                # Returned as Unicode but problems if not converted to ASCII\n                proxyServer = str(_winreg.QueryValueEx(internetSettings,\n                                                       'ProxyServer')[0])\n                if '=' in proxyServer:\n                    # Per-protocol settings\n                    for p in proxyServer.split(';'):\n                        protocol, address = p.split('=', 1)\n                        # See if address has a type:// prefix\n                        import re\n                        if not re.match('^([^/:]+)://', address):\n                            address = '%s://%s' % (protocol, address)\n                        proxies[protocol] = address\n                else:\n                    # Use one setting for all protocols\n                    if proxyServer[:5] == 'http:':\n                        proxies['http'] = proxyServer\n                    else:\n                        proxies['http'] = 'http://%s' % proxyServer\n                        proxies['https'] = 'https://%s' % proxyServer\n                        proxies['ftp'] = 'ftp://%s' % proxyServer\n            internetSettings.Close()\n        except (WindowsError, ValueError, TypeError):\n            # Either registry key not found etc, or the value in an\n            # unexpected format.\n            # proxies already set up to be empty so nothing to do\n            pass\n        return proxies\n\n    def getproxies():\n        \"\"\"Return a dictionary of scheme -> proxy server URL mappings.\n\n        Returns settings gathered from the environment, if specified,\n        or the registry.\n\n        \"\"\"\n        return getproxies_environment() or getproxies_registry()\n\n    def proxy_bypass_registry(host):\n        try:\n            import _winreg\n            import re\n        except ImportError:\n            # Std modules, so should be around - but you never know!\n            return 0\n        try:\n            internetSettings = _winreg.OpenKey(_winreg.HKEY_CURRENT_USER,\n                r'Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings')\n            proxyEnable = _winreg.QueryValueEx(internetSettings,\n                                               'ProxyEnable')[0]\n            proxyOverride = str(_winreg.QueryValueEx(internetSettings,\n                                                     'ProxyOverride')[0])\n            # ^^^^ Returned as Unicode but problems if not converted to ASCII\n        except WindowsError:\n            return 0\n        if not proxyEnable or not proxyOverride:\n            return 0\n        # try to make a host list from name and IP address.\n        rawHost, port = splitport(host)\n        host = [rawHost]\n        try:\n            addr = socket.gethostbyname(rawHost)\n            if addr != rawHost:\n                host.append(addr)\n        except socket.error:\n            pass\n        try:\n            fqdn = socket.getfqdn(rawHost)\n            if fqdn != rawHost:\n                host.append(fqdn)\n        except socket.error:\n            pass\n        # make a check value list from the registry entry: replace the\n        # '<local>' string by the localhost entry and the corresponding\n        # canonical entry.\n        proxyOverride = proxyOverride.split(';')\n        # now check if we match one of the registry values.\n        for test in proxyOverride:\n            if test == '<local>':\n                if '.' not in rawHost:\n                    return 1\n            test = test.replace(\".\", r\"\\.\")     # mask dots\n            test = test.replace(\"*\", r\".*\")     # change glob sequence\n            test = test.replace(\"?\", r\".\")      # change glob char\n            for val in host:\n                # print \"%s <--> %s\" %( test, val )\n                if re.match(test, val, re.I):\n                    return 1\n        return 0\n\n    def proxy_bypass(host):\n        \"\"\"Return a dictionary of scheme -> proxy server URL mappings.\n\n        Returns settings gathered from the environment, if specified,\n        or the registry.\n\n        \"\"\"\n        if getproxies_environment():\n            return proxy_bypass_environment(host)\n        else:\n            return proxy_bypass_registry(host)\n\nelse:\n    # By default use environment variables\n    getproxies = getproxies_environment\n    proxy_bypass = proxy_bypass_environment\n\n# Test and time quote() and unquote()\ndef test1():\n    s = ''\n    for i in range(256): s = s + chr(i)\n    s = s*4\n    t0 = time.time()\n    qs = quote(s)\n    uqs = unquote(qs)\n    t1 = time.time()\n    if uqs != s:\n        print 'Wrong!'\n    print repr(s)\n    print repr(qs)\n    print repr(uqs)\n    print round(t1 - t0, 3), 'sec'\n\n\ndef reporthook(blocknum, blocksize, totalsize):\n    # Report during remote transfers\n    print \"Block number: %d, Block size: %d, Total size: %d\" % (\n        blocknum, blocksize, totalsize)\n", 
    "urllib2": "\"\"\"An extensible library for opening URLs using a variety of protocols\n\nThe simplest way to use this module is to call the urlopen function,\nwhich accepts a string containing a URL or a Request object (described\nbelow).  It opens the URL and returns the results as file-like\nobject; the returned object has some extra methods described below.\n\nThe OpenerDirector manages a collection of Handler objects that do\nall the actual work.  Each Handler implements a particular protocol or\noption.  The OpenerDirector is a composite object that invokes the\nHandlers needed to open the requested URL.  For example, the\nHTTPHandler performs HTTP GET and POST requests and deals with\nnon-error returns.  The HTTPRedirectHandler automatically deals with\nHTTP 301, 302, 303 and 307 redirect errors, and the HTTPDigestAuthHandler\ndeals with digest authentication.\n\nurlopen(url, data=None) -- Basic usage is the same as original\nurllib.  pass the url and optionally data to post to an HTTP URL, and\nget a file-like object back.  One difference is that you can also pass\na Request instance instead of URL.  Raises a URLError (subclass of\nIOError); for HTTP errors, raises an HTTPError, which can also be\ntreated as a valid response.\n\nbuild_opener -- Function that creates a new OpenerDirector instance.\nWill install the default handlers.  Accepts one or more Handlers as\narguments, either instances or Handler classes that it will\ninstantiate.  If one of the argument is a subclass of the default\nhandler, the argument will be installed instead of the default.\n\ninstall_opener -- Installs a new opener as the default opener.\n\nobjects of interest:\n\nOpenerDirector -- Sets up the User Agent as the Python-urllib client and manages\nthe Handler classes, while dealing with requests and responses.\n\nRequest -- An object that encapsulates the state of a request.  The\nstate can be as simple as the URL.  It can also include extra HTTP\nheaders, e.g. a User-Agent.\n\nBaseHandler --\n\nexceptions:\nURLError -- A subclass of IOError, individual protocols have their own\nspecific subclass.\n\nHTTPError -- Also a valid HTTP response, so you can treat an HTTP error\nas an exceptional event or valid response.\n\ninternals:\nBaseHandler and parent\n_call_chain conventions\n\nExample usage:\n\nimport urllib2\n\n# set up authentication info\nauthinfo = urllib2.HTTPBasicAuthHandler()\nauthinfo.add_password(realm='PDQ Application',\n                      uri='https://mahler:8092/site-updates.py',\n                      user='klem',\n                      passwd='geheim$parole')\n\nproxy_support = urllib2.ProxyHandler({\"http\" : \"http://ahad-haam:3128\"})\n\n# build a new opener that adds authentication and caching FTP handlers\nopener = urllib2.build_opener(proxy_support, authinfo, urllib2.CacheFTPHandler)\n\n# install it\nurllib2.install_opener(opener)\n\nf = urllib2.urlopen('http://www.python.org/')\n\n\n\"\"\"\n\n# XXX issues:\n# If an authentication error handler that tries to perform\n# authentication for some reason but fails, how should the error be\n# signalled?  The client needs to know the HTTP error code.  But if\n# the handler knows that the problem was, e.g., that it didn't know\n# that hash algo that requested in the challenge, it would be good to\n# pass that information along to the client, too.\n# ftp errors aren't handled cleanly\n# check digest against correct (i.e. non-apache) implementation\n\n# Possible extensions:\n# complex proxies  XXX not sure what exactly was meant by this\n# abstract factory for opener\n\nimport base64\nimport hashlib\nimport httplib\nimport mimetools\nimport os\nimport posixpath\nimport random\nimport re\nimport socket\nimport sys\nimport time\nimport urlparse\nimport bisect\nimport warnings\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\nfrom urllib import (unwrap, unquote, splittype, splithost, quote,\n     addinfourl, splitport, splittag, toBytes,\n     splitattr, ftpwrapper, splituser, splitpasswd, splitvalue)\n\n# support for FileHandler, proxies via environment variables\nfrom urllib import localhost, url2pathname, getproxies, proxy_bypass\n\n# used in User-Agent header sent\n__version__ = sys.version[:3]\n\n_opener = None\ndef urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT):\n    global _opener\n    if _opener is None:\n        _opener = build_opener()\n    return _opener.open(url, data, timeout)\n\ndef install_opener(opener):\n    global _opener\n    _opener = opener\n\n# do these error classes make sense?\n# make sure all of the IOError stuff is overridden.  we just want to be\n# subtypes.\n\nclass URLError(IOError):\n    # URLError is a sub-type of IOError, but it doesn't share any of\n    # the implementation.  need to override __init__ and __str__.\n    # It sets self.args for compatibility with other EnvironmentError\n    # subclasses, but args doesn't have the typical format with errno in\n    # slot 0 and strerror in slot 1.  This may be better than nothing.\n    def __init__(self, reason):\n        self.args = reason,\n        self.reason = reason\n\n    def __str__(self):\n        return '<urlopen error %s>' % self.reason\n\nclass HTTPError(URLError, addinfourl):\n    \"\"\"Raised when HTTP error occurs, but also acts like non-error return\"\"\"\n    __super_init = addinfourl.__init__\n\n    def __init__(self, url, code, msg, hdrs, fp):\n        self.code = code\n        self.msg = msg\n        self.hdrs = hdrs\n        self.fp = fp\n        self.filename = url\n        # The addinfourl classes depend on fp being a valid file\n        # object.  In some cases, the HTTPError may not have a valid\n        # file object.  If this happens, the simplest workaround is to\n        # not initialize the base classes.\n        if fp is not None:\n            self.__super_init(fp, hdrs, url, code)\n\n    def __str__(self):\n        return 'HTTP Error %s: %s' % (self.code, self.msg)\n\n    # since URLError specifies a .reason attribute, HTTPError should also\n    #  provide this attribute. See issue13211 fo discussion.\n    @property\n    def reason(self):\n        return self.msg\n\n    def info(self):\n        return self.hdrs\n\n# copied from cookielib.py\n_cut_port_re = re.compile(r\":\\d+$\")\ndef request_host(request):\n    \"\"\"Return request-host, as defined by RFC 2965.\n\n    Variation from RFC: returned value is lowercased, for convenient\n    comparison.\n\n    \"\"\"\n    url = request.get_full_url()\n    host = urlparse.urlparse(url)[1]\n    if host == \"\":\n        host = request.get_header(\"Host\", \"\")\n\n    # remove port, if present\n    host = _cut_port_re.sub(\"\", host, 1)\n    return host.lower()\n\nclass Request:\n\n    def __init__(self, url, data=None, headers={},\n                 origin_req_host=None, unverifiable=False):\n        # unwrap('<URL:type://host/path>') --> 'type://host/path'\n        self.__original = unwrap(url)\n        self.__original, self.__fragment = splittag(self.__original)\n        self.type = None\n        # self.__r_type is what's left after doing the splittype\n        self.host = None\n        self.port = None\n        self._tunnel_host = None\n        self.data = data\n        self.headers = {}\n        for key, value in headers.items():\n            self.add_header(key, value)\n        self.unredirected_hdrs = {}\n        if origin_req_host is None:\n            origin_req_host = request_host(self)\n        self.origin_req_host = origin_req_host\n        self.unverifiable = unverifiable\n\n    def __getattr__(self, attr):\n        # XXX this is a fallback mechanism to guard against these\n        # methods getting called in a non-standard order.  this may be\n        # too complicated and/or unnecessary.\n        # XXX should the __r_XXX attributes be public?\n        if attr[:12] == '_Request__r_':\n            name = attr[12:]\n            if hasattr(Request, 'get_' + name):\n                getattr(self, 'get_' + name)()\n                return getattr(self, attr)\n        raise AttributeError, attr\n\n    def get_method(self):\n        if self.has_data():\n            return \"POST\"\n        else:\n            return \"GET\"\n\n    # XXX these helper methods are lame\n\n    def add_data(self, data):\n        self.data = data\n\n    def has_data(self):\n        return self.data is not None\n\n    def get_data(self):\n        return self.data\n\n    def get_full_url(self):\n        if self.__fragment:\n            return '%s#%s' % (self.__original, self.__fragment)\n        else:\n            return self.__original\n\n    def get_type(self):\n        if self.type is None:\n            self.type, self.__r_type = splittype(self.__original)\n            if self.type is None:\n                raise ValueError, \"unknown url type: %s\" % self.__original\n        return self.type\n\n    def get_host(self):\n        if self.host is None:\n            self.host, self.__r_host = splithost(self.__r_type)\n            if self.host:\n                self.host = unquote(self.host)\n        return self.host\n\n    def get_selector(self):\n        return self.__r_host\n\n    def set_proxy(self, host, type):\n        if self.type == 'https' and not self._tunnel_host:\n            self._tunnel_host = self.host\n        else:\n            self.type = type\n            self.__r_host = self.__original\n\n        self.host = host\n\n    def has_proxy(self):\n        return self.__r_host == self.__original\n\n    def get_origin_req_host(self):\n        return self.origin_req_host\n\n    def is_unverifiable(self):\n        return self.unverifiable\n\n    def add_header(self, key, val):\n        # useful for something like authentication\n        self.headers[key.capitalize()] = val\n\n    def add_unredirected_header(self, key, val):\n        # will not be added to a redirected request\n        self.unredirected_hdrs[key.capitalize()] = val\n\n    def has_header(self, header_name):\n        return (header_name in self.headers or\n                header_name in self.unredirected_hdrs)\n\n    def get_header(self, header_name, default=None):\n        return self.headers.get(\n            header_name,\n            self.unredirected_hdrs.get(header_name, default))\n\n    def header_items(self):\n        hdrs = self.unredirected_hdrs.copy()\n        hdrs.update(self.headers)\n        return hdrs.items()\n\nclass OpenerDirector:\n    def __init__(self):\n        client_version = \"Python-urllib/%s\" % __version__\n        self.addheaders = [('User-agent', client_version)]\n        # self.handlers is retained only for backward compatibility\n        self.handlers = []\n        # manage the individual handlers\n        self.handle_open = {}\n        self.handle_error = {}\n        self.process_response = {}\n        self.process_request = {}\n\n    def add_handler(self, handler):\n        if not hasattr(handler, \"add_parent\"):\n            raise TypeError(\"expected BaseHandler instance, got %r\" %\n                            type(handler))\n\n        added = False\n        for meth in dir(handler):\n            if meth in [\"redirect_request\", \"do_open\", \"proxy_open\"]:\n                # oops, coincidental match\n                continue\n\n            i = meth.find(\"_\")\n            protocol = meth[:i]\n            condition = meth[i+1:]\n\n            if condition.startswith(\"error\"):\n                j = condition.find(\"_\") + i + 1\n                kind = meth[j+1:]\n                try:\n                    kind = int(kind)\n                except ValueError:\n                    pass\n                lookup = self.handle_error.get(protocol, {})\n                self.handle_error[protocol] = lookup\n            elif condition == \"open\":\n                kind = protocol\n                lookup = self.handle_open\n            elif condition == \"response\":\n                kind = protocol\n                lookup = self.process_response\n            elif condition == \"request\":\n                kind = protocol\n                lookup = self.process_request\n            else:\n                continue\n\n            handlers = lookup.setdefault(kind, [])\n            if handlers:\n                bisect.insort(handlers, handler)\n            else:\n                handlers.append(handler)\n            added = True\n\n        if added:\n            bisect.insort(self.handlers, handler)\n            handler.add_parent(self)\n\n    def close(self):\n        # Only exists for backwards compatibility.\n        pass\n\n    def _call_chain(self, chain, kind, meth_name, *args):\n        # Handlers raise an exception if no one else should try to handle\n        # the request, or return None if they can't but another handler\n        # could.  Otherwise, they return the response.\n        handlers = chain.get(kind, ())\n        for handler in handlers:\n            func = getattr(handler, meth_name)\n\n            result = func(*args)\n            if result is not None:\n                return result\n\n    def open(self, fullurl, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT):\n        # accept a URL or a Request object\n        if isinstance(fullurl, basestring):\n            req = Request(fullurl, data)\n        else:\n            req = fullurl\n            if data is not None:\n                req.add_data(data)\n\n        req.timeout = timeout\n        protocol = req.get_type()\n\n        # pre-process request\n        meth_name = protocol+\"_request\"\n        for processor in self.process_request.get(protocol, []):\n            meth = getattr(processor, meth_name)\n            req = meth(req)\n\n        response = self._open(req, data)\n\n        # post-process response\n        meth_name = protocol+\"_response\"\n        for processor in self.process_response.get(protocol, []):\n            meth = getattr(processor, meth_name)\n            response = meth(req, response)\n\n        return response\n\n    def _open(self, req, data=None):\n        result = self._call_chain(self.handle_open, 'default',\n                                  'default_open', req)\n        if result:\n            return result\n\n        protocol = req.get_type()\n        result = self._call_chain(self.handle_open, protocol, protocol +\n                                  '_open', req)\n        if result:\n            return result\n\n        return self._call_chain(self.handle_open, 'unknown',\n                                'unknown_open', req)\n\n    def error(self, proto, *args):\n        if proto in ('http', 'https'):\n            # XXX http[s] protocols are special-cased\n            dict = self.handle_error['http'] # https is not different than http\n            proto = args[2]  # YUCK!\n            meth_name = 'http_error_%s' % proto\n            http_err = 1\n            orig_args = args\n        else:\n            dict = self.handle_error\n            meth_name = proto + '_error'\n            http_err = 0\n        args = (dict, proto, meth_name) + args\n        result = self._call_chain(*args)\n        if result:\n            return result\n\n        if http_err:\n            args = (dict, 'default', 'http_error_default') + orig_args\n            return self._call_chain(*args)\n\n# XXX probably also want an abstract factory that knows when it makes\n# sense to skip a superclass in favor of a subclass and when it might\n# make sense to include both\n\ndef build_opener(*handlers):\n    \"\"\"Create an opener object from a list of handlers.\n\n    The opener will use several default handlers, including support\n    for HTTP, FTP and when applicable, HTTPS.\n\n    If any of the handlers passed as arguments are subclasses of the\n    default handlers, the default handlers will not be used.\n    \"\"\"\n    import types\n    def isclass(obj):\n        return isinstance(obj, (types.ClassType, type))\n\n    opener = OpenerDirector()\n    default_classes = [ProxyHandler, UnknownHandler, HTTPHandler,\n                       HTTPDefaultErrorHandler, HTTPRedirectHandler,\n                       FTPHandler, FileHandler, HTTPErrorProcessor]\n    if hasattr(httplib, 'HTTPS'):\n        default_classes.append(HTTPSHandler)\n    skip = set()\n    for klass in default_classes:\n        for check in handlers:\n            if isclass(check):\n                if issubclass(check, klass):\n                    skip.add(klass)\n            elif isinstance(check, klass):\n                skip.add(klass)\n    for klass in skip:\n        default_classes.remove(klass)\n\n    for klass in default_classes:\n        opener.add_handler(klass())\n\n    for h in handlers:\n        if isclass(h):\n            h = h()\n        opener.add_handler(h)\n    return opener\n\nclass BaseHandler:\n    handler_order = 500\n\n    def add_parent(self, parent):\n        self.parent = parent\n\n    def close(self):\n        # Only exists for backwards compatibility\n        pass\n\n    def __lt__(self, other):\n        if not hasattr(other, \"handler_order\"):\n            # Try to preserve the old behavior of having custom classes\n            # inserted after default ones (works only for custom user\n            # classes which are not aware of handler_order).\n            return True\n        return self.handler_order < other.handler_order\n\n\nclass HTTPErrorProcessor(BaseHandler):\n    \"\"\"Process HTTP error responses.\"\"\"\n    handler_order = 1000  # after all other processing\n\n    def http_response(self, request, response):\n        code, msg, hdrs = response.code, response.msg, response.info()\n\n        # According to RFC 2616, \"2xx\" code indicates that the client's\n        # request was successfully received, understood, and accepted.\n        if not (200 <= code < 300):\n            response = self.parent.error(\n                'http', request, response, code, msg, hdrs)\n\n        return response\n\n    https_response = http_response\n\nclass HTTPDefaultErrorHandler(BaseHandler):\n    def http_error_default(self, req, fp, code, msg, hdrs):\n        raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)\n\nclass HTTPRedirectHandler(BaseHandler):\n    # maximum number of redirections to any single URL\n    # this is needed because of the state that cookies introduce\n    max_repeats = 4\n    # maximum total number of redirections (regardless of URL) before\n    # assuming we're in a loop\n    max_redirections = 10\n\n    def redirect_request(self, req, fp, code, msg, headers, newurl):\n        \"\"\"Return a Request or None in response to a redirect.\n\n        This is called by the http_error_30x methods when a\n        redirection response is received.  If a redirection should\n        take place, return a new Request to allow http_error_30x to\n        perform the redirect.  Otherwise, raise HTTPError if no-one\n        else should try to handle this url.  Return None if you can't\n        but another Handler might.\n        \"\"\"\n        m = req.get_method()\n        if (code in (301, 302, 303, 307) and m in (\"GET\", \"HEAD\")\n            or code in (301, 302, 303) and m == \"POST\"):\n            # Strictly (according to RFC 2616), 301 or 302 in response\n            # to a POST MUST NOT cause a redirection without confirmation\n            # from the user (of urllib2, in this case).  In practice,\n            # essentially all clients do redirect in this case, so we\n            # do the same.\n            # be conciliant with URIs containing a space\n            newurl = newurl.replace(' ', '%20')\n            newheaders = dict((k,v) for k,v in req.headers.items()\n                              if k.lower() not in (\"content-length\", \"content-type\")\n                             )\n            return Request(newurl,\n                           headers=newheaders,\n                           origin_req_host=req.get_origin_req_host(),\n                           unverifiable=True)\n        else:\n            raise HTTPError(req.get_full_url(), code, msg, headers, fp)\n\n    # Implementation note: To avoid the server sending us into an\n    # infinite loop, the request object needs to track what URLs we\n    # have already seen.  Do this by adding a handler-specific\n    # attribute to the Request object.\n    def http_error_302(self, req, fp, code, msg, headers):\n        # Some servers (incorrectly) return multiple Location headers\n        # (so probably same goes for URI).  Use first header.\n        if 'location' in headers:\n            newurl = headers.getheaders('location')[0]\n        elif 'uri' in headers:\n            newurl = headers.getheaders('uri')[0]\n        else:\n            return\n\n        # fix a possible malformed URL\n        urlparts = urlparse.urlparse(newurl)\n        if not urlparts.path:\n            urlparts = list(urlparts)\n            urlparts[2] = \"/\"\n        newurl = urlparse.urlunparse(urlparts)\n\n        newurl = urlparse.urljoin(req.get_full_url(), newurl)\n\n        # For security reasons we do not allow redirects to protocols\n        # other than HTTP, HTTPS or FTP.\n        newurl_lower = newurl.lower()\n        if not (newurl_lower.startswith('http://') or\n                newurl_lower.startswith('https://') or\n                newurl_lower.startswith('ftp://')):\n            raise HTTPError(newurl, code,\n                            msg + \" - Redirection to url '%s' is not allowed\" %\n                            newurl,\n                            headers, fp)\n\n        # XXX Probably want to forget about the state of the current\n        # request, although that might interact poorly with other\n        # handlers that also use handler-specific request attributes\n        new = self.redirect_request(req, fp, code, msg, headers, newurl)\n        if new is None:\n            return\n\n        # loop detection\n        # .redirect_dict has a key url if url was previously visited.\n        if hasattr(req, 'redirect_dict'):\n            visited = new.redirect_dict = req.redirect_dict\n            if (visited.get(newurl, 0) >= self.max_repeats or\n                len(visited) >= self.max_redirections):\n                raise HTTPError(req.get_full_url(), code,\n                                self.inf_msg + msg, headers, fp)\n        else:\n            visited = new.redirect_dict = req.redirect_dict = {}\n        visited[newurl] = visited.get(newurl, 0) + 1\n\n        # Don't close the fp until we are sure that we won't use it\n        # with HTTPError.\n        fp.read()\n        fp.close()\n\n        return self.parent.open(new, timeout=req.timeout)\n\n    http_error_301 = http_error_303 = http_error_307 = http_error_302\n\n    inf_msg = \"The HTTP server returned a redirect error that would \" \\\n              \"lead to an infinite loop.\\n\" \\\n              \"The last 30x error message was:\\n\"\n\n\ndef _parse_proxy(proxy):\n    \"\"\"Return (scheme, user, password, host/port) given a URL or an authority.\n\n    If a URL is supplied, it must have an authority (host:port) component.\n    According to RFC 3986, having an authority component means the URL must\n    have two slashes after the scheme:\n\n    >>> _parse_proxy('file:/ftp.example.com/')\n    Traceback (most recent call last):\n    ValueError: proxy URL with no authority: 'file:/ftp.example.com/'\n\n    The first three items of the returned tuple may be None.\n\n    Examples of authority parsing:\n\n    >>> _parse_proxy('proxy.example.com')\n    (None, None, None, 'proxy.example.com')\n    >>> _parse_proxy('proxy.example.com:3128')\n    (None, None, None, 'proxy.example.com:3128')\n\n    The authority component may optionally include userinfo (assumed to be\n    username:password):\n\n    >>> _parse_proxy('joe:password@proxy.example.com')\n    (None, 'joe', 'password', 'proxy.example.com')\n    >>> _parse_proxy('joe:password@proxy.example.com:3128')\n    (None, 'joe', 'password', 'proxy.example.com:3128')\n\n    Same examples, but with URLs instead:\n\n    >>> _parse_proxy('http://proxy.example.com/')\n    ('http', None, None, 'proxy.example.com')\n    >>> _parse_proxy('http://proxy.example.com:3128/')\n    ('http', None, None, 'proxy.example.com:3128')\n    >>> _parse_proxy('http://joe:password@proxy.example.com/')\n    ('http', 'joe', 'password', 'proxy.example.com')\n    >>> _parse_proxy('http://joe:password@proxy.example.com:3128')\n    ('http', 'joe', 'password', 'proxy.example.com:3128')\n\n    Everything after the authority is ignored:\n\n    >>> _parse_proxy('ftp://joe:password@proxy.example.com/rubbish:3128')\n    ('ftp', 'joe', 'password', 'proxy.example.com')\n\n    Test for no trailing '/' case:\n\n    >>> _parse_proxy('http://joe:password@proxy.example.com')\n    ('http', 'joe', 'password', 'proxy.example.com')\n\n    \"\"\"\n    scheme, r_scheme = splittype(proxy)\n    if not r_scheme.startswith(\"/\"):\n        # authority\n        scheme = None\n        authority = proxy\n    else:\n        # URL\n        if not r_scheme.startswith(\"//\"):\n            raise ValueError(\"proxy URL with no authority: %r\" % proxy)\n        # We have an authority, so for RFC 3986-compliant URLs (by ss 3.\n        # and 3.3.), path is empty or starts with '/'\n        end = r_scheme.find(\"/\", 2)\n        if end == -1:\n            end = None\n        authority = r_scheme[2:end]\n    userinfo, hostport = splituser(authority)\n    if userinfo is not None:\n        user, password = splitpasswd(userinfo)\n    else:\n        user = password = None\n    return scheme, user, password, hostport\n\nclass ProxyHandler(BaseHandler):\n    # Proxies must be in front\n    handler_order = 100\n\n    def __init__(self, proxies=None):\n        if proxies is None:\n            proxies = getproxies()\n        assert hasattr(proxies, 'has_key'), \"proxies must be a mapping\"\n        self.proxies = proxies\n        for type, url in proxies.items():\n            setattr(self, '%s_open' % type,\n                    lambda r, proxy=url, type=type, meth=self.proxy_open: \\\n                    meth(r, proxy, type))\n\n    def proxy_open(self, req, proxy, type):\n        orig_type = req.get_type()\n        proxy_type, user, password, hostport = _parse_proxy(proxy)\n\n        if proxy_type is None:\n            proxy_type = orig_type\n\n        if req.host and proxy_bypass(req.host):\n            return None\n\n        if user and password:\n            user_pass = '%s:%s' % (unquote(user), unquote(password))\n            creds = base64.b64encode(user_pass).strip()\n            req.add_header('Proxy-authorization', 'Basic ' + creds)\n        hostport = unquote(hostport)\n        req.set_proxy(hostport, proxy_type)\n\n        if orig_type == proxy_type or orig_type == 'https':\n            # let other handlers take care of it\n            return None\n        else:\n            # need to start over, because the other handlers don't\n            # grok the proxy's URL type\n            # e.g. if we have a constructor arg proxies like so:\n            # {'http': 'ftp://proxy.example.com'}, we may end up turning\n            # a request for http://acme.example.com/a into one for\n            # ftp://proxy.example.com/a\n            return self.parent.open(req, timeout=req.timeout)\n\nclass HTTPPasswordMgr:\n\n    def __init__(self):\n        self.passwd = {}\n\n    def add_password(self, realm, uri, user, passwd):\n        # uri could be a single URI or a sequence\n        if isinstance(uri, basestring):\n            uri = [uri]\n        if not realm in self.passwd:\n            self.passwd[realm] = {}\n        for default_port in True, False:\n            reduced_uri = tuple(\n                [self.reduce_uri(u, default_port) for u in uri])\n            self.passwd[realm][reduced_uri] = (user, passwd)\n\n    def find_user_password(self, realm, authuri):\n        domains = self.passwd.get(realm, {})\n        for default_port in True, False:\n            reduced_authuri = self.reduce_uri(authuri, default_port)\n            for uris, authinfo in domains.iteritems():\n                for uri in uris:\n                    if self.is_suburi(uri, reduced_authuri):\n                        return authinfo\n        return None, None\n\n    def reduce_uri(self, uri, default_port=True):\n        \"\"\"Accept authority or URI and extract only the authority and path.\"\"\"\n        # note HTTP URLs do not have a userinfo component\n        parts = urlparse.urlsplit(uri)\n        if parts[1]:\n            # URI\n            scheme = parts[0]\n            authority = parts[1]\n            path = parts[2] or '/'\n        else:\n            # host or host:port\n            scheme = None\n            authority = uri\n            path = '/'\n        host, port = splitport(authority)\n        if default_port and port is None and scheme is not None:\n            dport = {\"http\": 80,\n                     \"https\": 443,\n                     }.get(scheme)\n            if dport is not None:\n                authority = \"%s:%d\" % (host, dport)\n        return authority, path\n\n    def is_suburi(self, base, test):\n        \"\"\"Check if test is below base in a URI tree\n\n        Both args must be URIs in reduced form.\n        \"\"\"\n        if base == test:\n            return True\n        if base[0] != test[0]:\n            return False\n        common = posixpath.commonprefix((base[1], test[1]))\n        if len(common) == len(base[1]):\n            return True\n        return False\n\n\nclass HTTPPasswordMgrWithDefaultRealm(HTTPPasswordMgr):\n\n    def find_user_password(self, realm, authuri):\n        user, password = HTTPPasswordMgr.find_user_password(self, realm,\n                                                            authuri)\n        if user is not None:\n            return user, password\n        return HTTPPasswordMgr.find_user_password(self, None, authuri)\n\n\nclass AbstractBasicAuthHandler:\n\n    # XXX this allows for multiple auth-schemes, but will stupidly pick\n    # the last one with a realm specified.\n\n    # allow for double- and single-quoted realm values\n    # (single quotes are a violation of the RFC, but appear in the wild)\n    rx = re.compile('(?:.*,)*[ \\t]*([^ \\t]+)[ \\t]+'\n                    'realm=([\"\\']?)([^\"\\']*)\\\\2', re.I)\n\n    # XXX could pre-emptively send auth info already accepted (RFC 2617,\n    # end of section 2, and section 1.2 immediately after \"credentials\"\n    # production).\n\n    def __init__(self, password_mgr=None):\n        if password_mgr is None:\n            password_mgr = HTTPPasswordMgr()\n        self.passwd = password_mgr\n        self.add_password = self.passwd.add_password\n        self.retried = 0\n\n    def reset_retry_count(self):\n        self.retried = 0\n\n    def http_error_auth_reqed(self, authreq, host, req, headers):\n        # host may be an authority (without userinfo) or a URL with an\n        # authority\n        # XXX could be multiple headers\n        authreq = headers.get(authreq, None)\n\n        if self.retried > 5:\n            # retry sending the username:password 5 times before failing.\n            raise HTTPError(req.get_full_url(), 401, \"basic auth failed\",\n                            headers, None)\n        else:\n            self.retried += 1\n\n        if authreq:\n            mo = AbstractBasicAuthHandler.rx.search(authreq)\n            if mo:\n                scheme, quote, realm = mo.groups()\n                if quote not in ['\"', \"'\"]:\n                    warnings.warn(\"Basic Auth Realm was unquoted\",\n                                  UserWarning, 2)\n                if scheme.lower() == 'basic':\n                    response = self.retry_http_basic_auth(host, req, realm)\n                    if response and response.code != 401:\n                        self.retried = 0\n                    return response\n\n    def retry_http_basic_auth(self, host, req, realm):\n        user, pw = self.passwd.find_user_password(realm, host)\n        if pw is not None:\n            raw = \"%s:%s\" % (user, pw)\n            auth = 'Basic %s' % base64.b64encode(raw).strip()\n            if req.headers.get(self.auth_header, None) == auth:\n                return None\n            req.add_unredirected_header(self.auth_header, auth)\n            return self.parent.open(req, timeout=req.timeout)\n        else:\n            return None\n\n\nclass HTTPBasicAuthHandler(AbstractBasicAuthHandler, BaseHandler):\n\n    auth_header = 'Authorization'\n\n    def http_error_401(self, req, fp, code, msg, headers):\n        url = req.get_full_url()\n        response = self.http_error_auth_reqed('www-authenticate',\n                                              url, req, headers)\n        self.reset_retry_count()\n        return response\n\n\nclass ProxyBasicAuthHandler(AbstractBasicAuthHandler, BaseHandler):\n\n    auth_header = 'Proxy-authorization'\n\n    def http_error_407(self, req, fp, code, msg, headers):\n        # http_error_auth_reqed requires that there is no userinfo component in\n        # authority.  Assume there isn't one, since urllib2 does not (and\n        # should not, RFC 3986 s. 3.2.1) support requests for URLs containing\n        # userinfo.\n        authority = req.get_host()\n        response = self.http_error_auth_reqed('proxy-authenticate',\n                                          authority, req, headers)\n        self.reset_retry_count()\n        return response\n\n\ndef randombytes(n):\n    \"\"\"Return n random bytes.\"\"\"\n    # Use /dev/urandom if it is available.  Fall back to random module\n    # if not.  It might be worthwhile to extend this function to use\n    # other platform-specific mechanisms for getting random bytes.\n    if os.path.exists(\"/dev/urandom\"):\n        f = open(\"/dev/urandom\")\n        s = f.read(n)\n        f.close()\n        return s\n    else:\n        L = [chr(random.randrange(0, 256)) for i in range(n)]\n        return \"\".join(L)\n\nclass AbstractDigestAuthHandler:\n    # Digest authentication is specified in RFC 2617.\n\n    # XXX The client does not inspect the Authentication-Info header\n    # in a successful response.\n\n    # XXX It should be possible to test this implementation against\n    # a mock server that just generates a static set of challenges.\n\n    # XXX qop=\"auth-int\" supports is shaky\n\n    def __init__(self, passwd=None):\n        if passwd is None:\n            passwd = HTTPPasswordMgr()\n        self.passwd = passwd\n        self.add_password = self.passwd.add_password\n        self.retried = 0\n        self.nonce_count = 0\n        self.last_nonce = None\n\n    def reset_retry_count(self):\n        self.retried = 0\n\n    def http_error_auth_reqed(self, auth_header, host, req, headers):\n        authreq = headers.get(auth_header, None)\n        if self.retried > 5:\n            # Don't fail endlessly - if we failed once, we'll probably\n            # fail a second time. Hm. Unless the Password Manager is\n            # prompting for the information. Crap. This isn't great\n            # but it's better than the current 'repeat until recursion\n            # depth exceeded' approach <wink>\n            raise HTTPError(req.get_full_url(), 401, \"digest auth failed\",\n                            headers, None)\n        else:\n            self.retried += 1\n        if authreq:\n            scheme = authreq.split()[0]\n            if scheme.lower() == 'digest':\n                return self.retry_http_digest_auth(req, authreq)\n\n    def retry_http_digest_auth(self, req, auth):\n        token, challenge = auth.split(' ', 1)\n        chal = parse_keqv_list(parse_http_list(challenge))\n        auth = self.get_authorization(req, chal)\n        if auth:\n            auth_val = 'Digest %s' % auth\n            if req.headers.get(self.auth_header, None) == auth_val:\n                return None\n            req.add_unredirected_header(self.auth_header, auth_val)\n            resp = self.parent.open(req, timeout=req.timeout)\n            return resp\n\n    def get_cnonce(self, nonce):\n        # The cnonce-value is an opaque\n        # quoted string value provided by the client and used by both client\n        # and server to avoid chosen plaintext attacks, to provide mutual\n        # authentication, and to provide some message integrity protection.\n        # This isn't a fabulous effort, but it's probably Good Enough.\n        dig = hashlib.sha1(\"%s:%s:%s:%s\" % (self.nonce_count, nonce, time.ctime(),\n                                            randombytes(8))).hexdigest()\n        return dig[:16]\n\n    def get_authorization(self, req, chal):\n        try:\n            realm = chal['realm']\n            nonce = chal['nonce']\n            qop = chal.get('qop')\n            algorithm = chal.get('algorithm', 'MD5')\n            # mod_digest doesn't send an opaque, even though it isn't\n            # supposed to be optional\n            opaque = chal.get('opaque', None)\n        except KeyError:\n            return None\n\n        H, KD = self.get_algorithm_impls(algorithm)\n        if H is None:\n            return None\n\n        user, pw = self.passwd.find_user_password(realm, req.get_full_url())\n        if user is None:\n            return None\n\n        # XXX not implemented yet\n        if req.has_data():\n            entdig = self.get_entity_digest(req.get_data(), chal)\n        else:\n            entdig = None\n\n        A1 = \"%s:%s:%s\" % (user, realm, pw)\n        A2 = \"%s:%s\" % (req.get_method(),\n                        # XXX selector: what about proxies and full urls\n                        req.get_selector())\n        if qop == 'auth':\n            if nonce == self.last_nonce:\n                self.nonce_count += 1\n            else:\n                self.nonce_count = 1\n                self.last_nonce = nonce\n\n            ncvalue = '%08x' % self.nonce_count\n            cnonce = self.get_cnonce(nonce)\n            noncebit = \"%s:%s:%s:%s:%s\" % (nonce, ncvalue, cnonce, qop, H(A2))\n            respdig = KD(H(A1), noncebit)\n        elif qop is None:\n            respdig = KD(H(A1), \"%s:%s\" % (nonce, H(A2)))\n        else:\n            # XXX handle auth-int.\n            raise URLError(\"qop '%s' is not supported.\" % qop)\n\n        # XXX should the partial digests be encoded too?\n\n        base = 'username=\"%s\", realm=\"%s\", nonce=\"%s\", uri=\"%s\", ' \\\n               'response=\"%s\"' % (user, realm, nonce, req.get_selector(),\n                                  respdig)\n        if opaque:\n            base += ', opaque=\"%s\"' % opaque\n        if entdig:\n            base += ', digest=\"%s\"' % entdig\n        base += ', algorithm=\"%s\"' % algorithm\n        if qop:\n            base += ', qop=auth, nc=%s, cnonce=\"%s\"' % (ncvalue, cnonce)\n        return base\n\n    def get_algorithm_impls(self, algorithm):\n        # algorithm should be case-insensitive according to RFC2617\n        algorithm = algorithm.upper()\n        # lambdas assume digest modules are imported at the top level\n        if algorithm == 'MD5':\n            H = lambda x: hashlib.md5(x).hexdigest()\n        elif algorithm == 'SHA':\n            H = lambda x: hashlib.sha1(x).hexdigest()\n        # XXX MD5-sess\n        KD = lambda s, d: H(\"%s:%s\" % (s, d))\n        return H, KD\n\n    def get_entity_digest(self, data, chal):\n        # XXX not implemented yet\n        return None\n\n\nclass HTTPDigestAuthHandler(BaseHandler, AbstractDigestAuthHandler):\n    \"\"\"An authentication protocol defined by RFC 2069\n\n    Digest authentication improves on basic authentication because it\n    does not transmit passwords in the clear.\n    \"\"\"\n\n    auth_header = 'Authorization'\n    handler_order = 490  # before Basic auth\n\n    def http_error_401(self, req, fp, code, msg, headers):\n        host = urlparse.urlparse(req.get_full_url())[1]\n        retry = self.http_error_auth_reqed('www-authenticate',\n                                           host, req, headers)\n        self.reset_retry_count()\n        return retry\n\n\nclass ProxyDigestAuthHandler(BaseHandler, AbstractDigestAuthHandler):\n\n    auth_header = 'Proxy-Authorization'\n    handler_order = 490  # before Basic auth\n\n    def http_error_407(self, req, fp, code, msg, headers):\n        host = req.get_host()\n        retry = self.http_error_auth_reqed('proxy-authenticate',\n                                           host, req, headers)\n        self.reset_retry_count()\n        return retry\n\nclass AbstractHTTPHandler(BaseHandler):\n\n    def __init__(self, debuglevel=0):\n        self._debuglevel = debuglevel\n\n    def set_http_debuglevel(self, level):\n        self._debuglevel = level\n\n    def do_request_(self, request):\n        host = request.get_host()\n        if not host:\n            raise URLError('no host given')\n\n        if request.has_data():  # POST\n            data = request.get_data()\n            if not request.has_header('Content-type'):\n                request.add_unredirected_header(\n                    'Content-type',\n                    'application/x-www-form-urlencoded')\n            if not request.has_header('Content-length'):\n                request.add_unredirected_header(\n                    'Content-length', '%d' % len(data))\n\n        sel_host = host\n        if request.has_proxy():\n            scheme, sel = splittype(request.get_selector())\n            sel_host, sel_path = splithost(sel)\n\n        if not request.has_header('Host'):\n            request.add_unredirected_header('Host', sel_host)\n        for name, value in self.parent.addheaders:\n            name = name.capitalize()\n            if not request.has_header(name):\n                request.add_unredirected_header(name, value)\n\n        return request\n\n    def do_open(self, http_class, req):\n        \"\"\"Return an addinfourl object for the request, using http_class.\n\n        http_class must implement the HTTPConnection API from httplib.\n        The addinfourl return value is a file-like object.  It also\n        has methods and attributes including:\n            - info(): return a mimetools.Message object for the headers\n            - geturl(): return the original request URL\n            - code: HTTP status code\n        \"\"\"\n        host = req.get_host()\n        if not host:\n            raise URLError('no host given')\n\n        h = http_class(host, timeout=req.timeout) # will parse host:port\n        h.set_debuglevel(self._debuglevel)\n\n        headers = dict(req.unredirected_hdrs)\n        headers.update(dict((k, v) for k, v in req.headers.items()\n                            if k not in headers))\n\n        # We want to make an HTTP/1.1 request, but the addinfourl\n        # class isn't prepared to deal with a persistent connection.\n        # It will try to read all remaining data from the socket,\n        # which will block while the server waits for the next request.\n        # So make sure the connection gets closed after the (only)\n        # request.\n        headers[\"Connection\"] = \"close\"\n        headers = dict(\n            (name.title(), val) for name, val in headers.items())\n\n        if req._tunnel_host:\n            tunnel_headers = {}\n            proxy_auth_hdr = \"Proxy-Authorization\"\n            if proxy_auth_hdr in headers:\n                tunnel_headers[proxy_auth_hdr] = headers[proxy_auth_hdr]\n                # Proxy-Authorization should not be sent to origin\n                # server.\n                del headers[proxy_auth_hdr]\n            h.set_tunnel(req._tunnel_host, headers=tunnel_headers)\n\n        try:\n            h.request(req.get_method(), req.get_selector(), req.data, headers)\n        except socket.error, err: # XXX what error?\n            h.close()\n            raise URLError(err)\n        else:\n            try:\n                r = h.getresponse(buffering=True)\n            except TypeError: # buffering kw not supported\n                r = h.getresponse()\n\n        # Pick apart the HTTPResponse object to get the addinfourl\n        # object initialized properly.\n\n        # Wrap the HTTPResponse object in socket's file object adapter\n        # for Windows.  That adapter calls recv(), so delegate recv()\n        # to read().  This weird wrapping allows the returned object to\n        # have readline() and readlines() methods.\n\n        # XXX It might be better to extract the read buffering code\n        # out of socket._fileobject() and into a base class.\n\n        r.recv = r.read\n        r._reuse = lambda: None\n        r._drop = lambda: None\n        fp = socket._fileobject(r, close=True)\n\n        resp = addinfourl(fp, r.msg, req.get_full_url())\n        resp.code = r.status\n        resp.msg = r.reason\n        return resp\n\n\nclass HTTPHandler(AbstractHTTPHandler):\n\n    def http_open(self, req):\n        return self.do_open(httplib.HTTPConnection, req)\n\n    http_request = AbstractHTTPHandler.do_request_\n\nif hasattr(httplib, 'HTTPS'):\n    class HTTPSHandler(AbstractHTTPHandler):\n\n        def https_open(self, req):\n            return self.do_open(httplib.HTTPSConnection, req)\n\n        https_request = AbstractHTTPHandler.do_request_\n\nclass HTTPCookieProcessor(BaseHandler):\n    def __init__(self, cookiejar=None):\n        import cookielib\n        if cookiejar is None:\n            cookiejar = cookielib.CookieJar()\n        self.cookiejar = cookiejar\n\n    def http_request(self, request):\n        self.cookiejar.add_cookie_header(request)\n        return request\n\n    def http_response(self, request, response):\n        self.cookiejar.extract_cookies(response, request)\n        return response\n\n    https_request = http_request\n    https_response = http_response\n\nclass UnknownHandler(BaseHandler):\n    def unknown_open(self, req):\n        type = req.get_type()\n        raise URLError('unknown url type: %s' % type)\n\ndef parse_keqv_list(l):\n    \"\"\"Parse list of key=value strings where keys are not duplicated.\"\"\"\n    parsed = {}\n    for elt in l:\n        k, v = elt.split('=', 1)\n        if v[0] == '\"' and v[-1] == '\"':\n            v = v[1:-1]\n        parsed[k] = v\n    return parsed\n\ndef parse_http_list(s):\n    \"\"\"Parse lists as described by RFC 2068 Section 2.\n\n    In particular, parse comma-separated lists where the elements of\n    the list may include quoted-strings.  A quoted-string could\n    contain a comma.  A non-quoted string could have quotes in the\n    middle.  Neither commas nor quotes count if they are escaped.\n    Only double-quotes count, not single-quotes.\n    \"\"\"\n    res = []\n    part = ''\n\n    escape = quote = False\n    for cur in s:\n        if escape:\n            part += cur\n            escape = False\n            continue\n        if quote:\n            if cur == '\\\\':\n                escape = True\n                continue\n            elif cur == '\"':\n                quote = False\n            part += cur\n            continue\n\n        if cur == ',':\n            res.append(part)\n            part = ''\n            continue\n\n        if cur == '\"':\n            quote = True\n\n        part += cur\n\n    # append last part\n    if part:\n        res.append(part)\n\n    return [part.strip() for part in res]\n\ndef _safe_gethostbyname(host):\n    try:\n        return socket.gethostbyname(host)\n    except socket.gaierror:\n        return None\n\nclass FileHandler(BaseHandler):\n    # Use local file or FTP depending on form of URL\n    def file_open(self, req):\n        url = req.get_selector()\n        if url[:2] == '//' and url[2:3] != '/' and (req.host and\n                req.host != 'localhost'):\n            req.type = 'ftp'\n            return self.parent.open(req)\n        else:\n            return self.open_local_file(req)\n\n    # names for the localhost\n    names = None\n    def get_names(self):\n        if FileHandler.names is None:\n            try:\n                FileHandler.names = tuple(\n                    socket.gethostbyname_ex('localhost')[2] +\n                    socket.gethostbyname_ex(socket.gethostname())[2])\n            except socket.gaierror:\n                FileHandler.names = (socket.gethostbyname('localhost'),)\n        return FileHandler.names\n\n    # not entirely sure what the rules are here\n    def open_local_file(self, req):\n        import email.utils\n        import mimetypes\n        host = req.get_host()\n        filename = req.get_selector()\n        localfile = url2pathname(filename)\n        try:\n            stats = os.stat(localfile)\n            size = stats.st_size\n            modified = email.utils.formatdate(stats.st_mtime, usegmt=True)\n            mtype = mimetypes.guess_type(filename)[0]\n            headers = mimetools.Message(StringIO(\n                'Content-type: %s\\nContent-length: %d\\nLast-modified: %s\\n' %\n                (mtype or 'text/plain', size, modified)))\n            if host:\n                host, port = splitport(host)\n            if not host or \\\n                (not port and _safe_gethostbyname(host) in self.get_names()):\n                if host:\n                    origurl = 'file://' + host + filename\n                else:\n                    origurl = 'file://' + filename\n                return addinfourl(open(localfile, 'rb'), headers, origurl)\n        except OSError, msg:\n            # urllib2 users shouldn't expect OSErrors coming from urlopen()\n            raise URLError(msg)\n        raise URLError('file not on local host')\n\nclass FTPHandler(BaseHandler):\n    def ftp_open(self, req):\n        import ftplib\n        import mimetypes\n        host = req.get_host()\n        if not host:\n            raise URLError('ftp error: no host given')\n        host, port = splitport(host)\n        if port is None:\n            port = ftplib.FTP_PORT\n        else:\n            port = int(port)\n\n        # username/password handling\n        user, host = splituser(host)\n        if user:\n            user, passwd = splitpasswd(user)\n        else:\n            passwd = None\n        host = unquote(host)\n        user = user or ''\n        passwd = passwd or ''\n\n        try:\n            host = socket.gethostbyname(host)\n        except socket.error, msg:\n            raise URLError(msg)\n        path, attrs = splitattr(req.get_selector())\n        dirs = path.split('/')\n        dirs = map(unquote, dirs)\n        dirs, file = dirs[:-1], dirs[-1]\n        if dirs and not dirs[0]:\n            dirs = dirs[1:]\n        try:\n            fw = self.connect_ftp(user, passwd, host, port, dirs, req.timeout)\n            type = file and 'I' or 'D'\n            for attr in attrs:\n                attr, value = splitvalue(attr)\n                if attr.lower() == 'type' and \\\n                   value in ('a', 'A', 'i', 'I', 'd', 'D'):\n                    type = value.upper()\n            fp, retrlen = fw.retrfile(file, type)\n            headers = \"\"\n            mtype = mimetypes.guess_type(req.get_full_url())[0]\n            if mtype:\n                headers += \"Content-type: %s\\n\" % mtype\n            if retrlen is not None and retrlen >= 0:\n                headers += \"Content-length: %d\\n\" % retrlen\n            sf = StringIO(headers)\n            headers = mimetools.Message(sf)\n            return addinfourl(fp, headers, req.get_full_url())\n        except ftplib.all_errors, msg:\n            raise URLError, ('ftp error: %s' % msg), sys.exc_info()[2]\n\n    def connect_ftp(self, user, passwd, host, port, dirs, timeout):\n        fw = ftpwrapper(user, passwd, host, port, dirs, timeout,\n                        persistent=False)\n##        fw.ftp.set_debuglevel(1)\n        return fw\n\nclass CacheFTPHandler(FTPHandler):\n    # XXX would be nice to have pluggable cache strategies\n    # XXX this stuff is definitely not thread safe\n    def __init__(self):\n        self.cache = {}\n        self.timeout = {}\n        self.soonest = 0\n        self.delay = 60\n        self.max_conns = 16\n\n    def setTimeout(self, t):\n        self.delay = t\n\n    def setMaxConns(self, m):\n        self.max_conns = m\n\n    def connect_ftp(self, user, passwd, host, port, dirs, timeout):\n        key = user, host, port, '/'.join(dirs), timeout\n        if key in self.cache:\n            self.timeout[key] = time.time() + self.delay\n        else:\n            self.cache[key] = ftpwrapper(user, passwd, host, port, dirs, timeout)\n            self.timeout[key] = time.time() + self.delay\n        self.check_cache()\n        return self.cache[key]\n\n    def check_cache(self):\n        # first check for old ones\n        t = time.time()\n        if self.soonest <= t:\n            for k, v in self.timeout.items():\n                if v < t:\n                    self.cache[k].close()\n                    del self.cache[k]\n                    del self.timeout[k]\n        self.soonest = min(self.timeout.values())\n\n        # then check the size\n        if len(self.cache) == self.max_conns:\n            for k, v in self.timeout.items():\n                if v == self.soonest:\n                    del self.cache[k]\n                    del self.timeout[k]\n                    break\n            self.soonest = min(self.timeout.values())\n\n    def clear_cache(self):\n        for conn in self.cache.values():\n            conn.close()\n        self.cache.clear()\n        self.timeout.clear()\n", 
    "urlparse": "\"\"\"Parse (absolute and relative) URLs.\n\nurlparse module is based upon the following RFC specifications.\n\nRFC 3986 (STD66): \"Uniform Resource Identifiers\" by T. Berners-Lee, R. Fielding\nand L.  Masinter, January 2005.\n\nRFC 2732 : \"Format for Literal IPv6 Addresses in URL's by R.Hinden, B.Carpenter\nand L.Masinter, December 1999.\n\nRFC 2396:  \"Uniform Resource Identifiers (URI)\": Generic Syntax by T.\nBerners-Lee, R. Fielding, and L. Masinter, August 1998.\n\nRFC 2368: \"The mailto URL scheme\", by P.Hoffman , L Masinter, J. Zwinski, July 1998.\n\nRFC 1808: \"Relative Uniform Resource Locators\", by R. Fielding, UC Irvine, June\n1995.\n\nRFC 1738: \"Uniform Resource Locators (URL)\" by T. Berners-Lee, L. Masinter, M.\nMcCahill, December 1994\n\nRFC 3986 is considered the current standard and any future changes to\nurlparse module should conform with it.  The urlparse module is\ncurrently not entirely compliant with this RFC due to defacto\nscenarios for parsing, and for backward compatibility purposes, some\nparsing quirks from older RFCs are retained. The testcases in\ntest_urlparse.py provides a good indicator of parsing behavior.\n\n\"\"\"\n\nimport re\n\n__all__ = [\"urlparse\", \"urlunparse\", \"urljoin\", \"urldefrag\",\n           \"urlsplit\", \"urlunsplit\", \"parse_qs\", \"parse_qsl\"]\n\n# A classification of schemes ('' means apply by default)\nuses_relative = ['ftp', 'http', 'gopher', 'nntp', 'imap',\n                 'wais', 'file', 'https', 'shttp', 'mms',\n                 'prospero', 'rtsp', 'rtspu', '', 'sftp',\n                 'svn', 'svn+ssh']\nuses_netloc = ['ftp', 'http', 'gopher', 'nntp', 'telnet',\n               'imap', 'wais', 'file', 'mms', 'https', 'shttp',\n               'snews', 'prospero', 'rtsp', 'rtspu', 'rsync', '',\n               'svn', 'svn+ssh', 'sftp','nfs','git', 'git+ssh']\nuses_params = ['ftp', 'hdl', 'prospero', 'http', 'imap',\n               'https', 'shttp', 'rtsp', 'rtspu', 'sip', 'sips',\n               'mms', '', 'sftp', 'tel']\n\n# These are not actually used anymore, but should stay for backwards\n# compatibility.  (They are undocumented, but have a public-looking name.)\nnon_hierarchical = ['gopher', 'hdl', 'mailto', 'news',\n                    'telnet', 'wais', 'imap', 'snews', 'sip', 'sips']\nuses_query = ['http', 'wais', 'imap', 'https', 'shttp', 'mms',\n              'gopher', 'rtsp', 'rtspu', 'sip', 'sips', '']\nuses_fragment = ['ftp', 'hdl', 'http', 'gopher', 'news',\n                 'nntp', 'wais', 'https', 'shttp', 'snews',\n                 'file', 'prospero', '']\n\n# Characters valid in scheme names\nscheme_chars = ('abcdefghijklmnopqrstuvwxyz'\n                'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n                '0123456789'\n                '+-.')\n\nMAX_CACHE_SIZE = 20\n_parse_cache = {}\n\ndef clear_cache():\n    \"\"\"Clear the parse cache.\"\"\"\n    _parse_cache.clear()\n\n\nclass ResultMixin(object):\n    \"\"\"Shared methods for the parsed result objects.\"\"\"\n\n    @property\n    def username(self):\n        netloc = self.netloc\n        if \"@\" in netloc:\n            userinfo = netloc.rsplit(\"@\", 1)[0]\n            if \":\" in userinfo:\n                userinfo = userinfo.split(\":\", 1)[0]\n            return userinfo\n        return None\n\n    @property\n    def password(self):\n        netloc = self.netloc\n        if \"@\" in netloc:\n            userinfo = netloc.rsplit(\"@\", 1)[0]\n            if \":\" in userinfo:\n                return userinfo.split(\":\", 1)[1]\n        return None\n\n    @property\n    def hostname(self):\n        netloc = self.netloc.split('@')[-1]\n        if '[' in netloc and ']' in netloc:\n            return netloc.split(']')[0][1:].lower()\n        elif ':' in netloc:\n            return netloc.split(':')[0].lower()\n        elif netloc == '':\n            return None\n        else:\n            return netloc.lower()\n\n    @property\n    def port(self):\n        netloc = self.netloc.split('@')[-1].split(']')[-1]\n        if ':' in netloc:\n            port = netloc.split(':')[1]\n            if port:\n                port = int(port, 10)\n                # verify legal port\n                if (0 <= port <= 65535):\n                    return port\n        return None\n\nfrom collections import namedtuple\n\nclass SplitResult(namedtuple('SplitResult', 'scheme netloc path query fragment'), ResultMixin):\n\n    __slots__ = ()\n\n    def geturl(self):\n        return urlunsplit(self)\n\n\nclass ParseResult(namedtuple('ParseResult', 'scheme netloc path params query fragment'), ResultMixin):\n\n    __slots__ = ()\n\n    def geturl(self):\n        return urlunparse(self)\n\n\ndef urlparse(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 6 components:\n    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n    Return a 6-tuple: (scheme, netloc, path, params, query, fragment).\n    Note that we don't break the components up in smaller bits\n    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n    tuple = urlsplit(url, scheme, allow_fragments)\n    scheme, netloc, url, query, fragment = tuple\n    if scheme in uses_params and ';' in url:\n        url, params = _splitparams(url)\n    else:\n        params = ''\n    return ParseResult(scheme, netloc, url, params, query, fragment)\n\ndef _splitparams(url):\n    if '/'  in url:\n        i = url.find(';', url.rfind('/'))\n        if i < 0:\n            return url, ''\n    else:\n        i = url.find(';')\n    return url[:i], url[i+1:]\n\ndef _splitnetloc(url, start=0):\n    delim = len(url)   # position of end of domain part of url, default is end\n    for c in '/?#':    # look for delimiters; the order is NOT important\n        wdelim = url.find(c, start)        # find first of this delim\n        if wdelim >= 0:                    # if found\n            delim = min(delim, wdelim)     # use earliest delim position\n    return url[start:delim], url[delim:]   # return (domain, rest)\n\ndef urlsplit(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 5 components:\n    <scheme>://<netloc>/<path>?<query>#<fragment>\n    Return a 5-tuple: (scheme, netloc, path, query, fragment).\n    Note that we don't break the components up in smaller bits\n    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n    allow_fragments = bool(allow_fragments)\n    key = url, scheme, allow_fragments, type(url), type(scheme)\n    cached = _parse_cache.get(key, None)\n    if cached:\n        return cached\n    if len(_parse_cache) >= MAX_CACHE_SIZE: # avoid runaway growth\n        clear_cache()\n    netloc = query = fragment = ''\n    i = url.find(':')\n    if i > 0:\n        if url[:i] == 'http': # optimize the common case\n            scheme = url[:i].lower()\n            url = url[i+1:]\n            if url[:2] == '//':\n                netloc, url = _splitnetloc(url, 2)\n                if (('[' in netloc and ']' not in netloc) or\n                        (']' in netloc and '[' not in netloc)):\n                    raise ValueError(\"Invalid IPv6 URL\")\n            if allow_fragments and '#' in url:\n                url, fragment = url.split('#', 1)\n            if '?' in url:\n                url, query = url.split('?', 1)\n            v = SplitResult(scheme, netloc, url, query, fragment)\n            _parse_cache[key] = v\n            return v\n        for c in url[:i]:\n            if c not in scheme_chars:\n                break\n        else:\n            # make sure \"url\" is not actually a port number (in which case\n            # \"scheme\" is really part of the path)\n            rest = url[i+1:]\n            if not rest or any(c not in '0123456789' for c in rest):\n                # not a port number\n                scheme, url = url[:i].lower(), rest\n\n    if url[:2] == '//':\n        netloc, url = _splitnetloc(url, 2)\n        if (('[' in netloc and ']' not in netloc) or\n                (']' in netloc and '[' not in netloc)):\n            raise ValueError(\"Invalid IPv6 URL\")\n    if allow_fragments and '#' in url:\n        url, fragment = url.split('#', 1)\n    if '?' in url:\n        url, query = url.split('?', 1)\n    v = SplitResult(scheme, netloc, url, query, fragment)\n    _parse_cache[key] = v\n    return v\n\ndef urlunparse(data):\n    \"\"\"Put a parsed URL back together again.  This may result in a\n    slightly different, but equivalent URL, if the URL that was parsed\n    originally had redundant delimiters, e.g. a ? with an empty query\n    (the draft states that these are equivalent).\"\"\"\n    scheme, netloc, url, params, query, fragment = data\n    if params:\n        url = \"%s;%s\" % (url, params)\n    return urlunsplit((scheme, netloc, url, query, fragment))\n\ndef urlunsplit(data):\n    \"\"\"Combine the elements of a tuple as returned by urlsplit() into a\n    complete URL as a string. The data argument can be any five-item iterable.\n    This may result in a slightly different, but equivalent URL, if the URL that\n    was parsed originally had unnecessary delimiters (for example, a ? with an\n    empty query; the RFC states that these are equivalent).\"\"\"\n    scheme, netloc, url, query, fragment = data\n    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):\n        if url and url[:1] != '/': url = '/' + url\n        url = '//' + (netloc or '') + url\n    if scheme:\n        url = scheme + ':' + url\n    if query:\n        url = url + '?' + query\n    if fragment:\n        url = url + '#' + fragment\n    return url\n\ndef urljoin(base, url, allow_fragments=True):\n    \"\"\"Join a base URL and a possibly relative URL to form an absolute\n    interpretation of the latter.\"\"\"\n    if not base:\n        return url\n    if not url:\n        return base\n    bscheme, bnetloc, bpath, bparams, bquery, bfragment = \\\n            urlparse(base, '', allow_fragments)\n    scheme, netloc, path, params, query, fragment = \\\n            urlparse(url, bscheme, allow_fragments)\n    if scheme != bscheme or scheme not in uses_relative:\n        return url\n    if scheme in uses_netloc:\n        if netloc:\n            return urlunparse((scheme, netloc, path,\n                               params, query, fragment))\n        netloc = bnetloc\n    if path[:1] == '/':\n        return urlunparse((scheme, netloc, path,\n                           params, query, fragment))\n    if not path and not params:\n        path = bpath\n        params = bparams\n        if not query:\n            query = bquery\n        return urlunparse((scheme, netloc, path,\n                           params, query, fragment))\n    segments = bpath.split('/')[:-1] + path.split('/')\n    # XXX The stuff below is bogus in various ways...\n    if segments[-1] == '.':\n        segments[-1] = ''\n    while '.' in segments:\n        segments.remove('.')\n    while 1:\n        i = 1\n        n = len(segments) - 1\n        while i < n:\n            if (segments[i] == '..'\n                and segments[i-1] not in ('', '..')):\n                del segments[i-1:i+1]\n                break\n            i = i+1\n        else:\n            break\n    if segments == ['', '..']:\n        segments[-1] = ''\n    elif len(segments) >= 2 and segments[-1] == '..':\n        segments[-2:] = ['']\n    return urlunparse((scheme, netloc, '/'.join(segments),\n                       params, query, fragment))\n\ndef urldefrag(url):\n    \"\"\"Removes any existing fragment from URL.\n\n    Returns a tuple of the defragmented URL and the fragment.  If\n    the URL contained no fragments, the second element is the\n    empty string.\n    \"\"\"\n    if '#' in url:\n        s, n, p, a, q, frag = urlparse(url)\n        defrag = urlunparse((s, n, p, a, q, ''))\n        return defrag, frag\n    else:\n        return url, ''\n\ntry:\n    unicode\nexcept NameError:\n    def _is_unicode(x):\n        return 0\nelse:\n    def _is_unicode(x):\n        return isinstance(x, unicode)\n\n# unquote method for parse_qs and parse_qsl\n# Cannot use directly from urllib as it would create a circular reference\n# because urllib uses urlparse methods (urljoin).  If you update this function,\n# update it also in urllib.  This code duplication does not existin in Python3.\n\n_hexdig = '0123456789ABCDEFabcdef'\n_hextochr = dict((a+b, chr(int(a+b,16)))\n                 for a in _hexdig for b in _hexdig)\n_asciire = re.compile('([\\x00-\\x7f]+)')\n\ndef unquote(s):\n    \"\"\"unquote('abc%20def') -> 'abc def'.\"\"\"\n    if _is_unicode(s):\n        if '%' not in s:\n            return s\n        bits = _asciire.split(s)\n        res = [bits[0]]\n        append = res.append\n        for i in range(1, len(bits), 2):\n            append(unquote(str(bits[i])).decode('latin1'))\n            append(bits[i + 1])\n        return ''.join(res)\n\n    bits = s.split('%')\n    # fastpath\n    if len(bits) == 1:\n        return s\n    res = [bits[0]]\n    append = res.append\n    for j in xrange(1, len(bits)):\n        item = bits[j]\n        try:\n            append(_hextochr[item[:2]])\n            append(item[2:])\n        except KeyError:\n            append('%')\n            append(item)\n    return ''.join(res)\n\ndef parse_qs(qs, keep_blank_values=0, strict_parsing=0):\n    \"\"\"Parse a query given as a string argument.\n\n        Arguments:\n\n        qs: percent-encoded query string to be parsed\n\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.\n            A true value indicates that blanks should be retained as\n            blank strings.  The default false value indicates that\n            blank values are to be ignored and treated as if they were\n            not included.\n\n        strict_parsing: flag indicating what to do with parsing errors.\n            If false (the default), errors are silently ignored.\n            If true, errors raise a ValueError exception.\n    \"\"\"\n    dict = {}\n    for name, value in parse_qsl(qs, keep_blank_values, strict_parsing):\n        if name in dict:\n            dict[name].append(value)\n        else:\n            dict[name] = [value]\n    return dict\n\ndef parse_qsl(qs, keep_blank_values=0, strict_parsing=0):\n    \"\"\"Parse a query given as a string argument.\n\n    Arguments:\n\n    qs: percent-encoded query string to be parsed\n\n    keep_blank_values: flag indicating whether blank values in\n        percent-encoded queries should be treated as blank strings.  A\n        true value indicates that blanks should be retained as blank\n        strings.  The default false value indicates that blank values\n        are to be ignored and treated as if they were  not included.\n\n    strict_parsing: flag indicating what to do with parsing errors. If\n        false (the default), errors are silently ignored. If true,\n        errors raise a ValueError exception.\n\n    Returns a list, as G-d intended.\n    \"\"\"\n    pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n    r = []\n    for name_value in pairs:\n        if not name_value and not strict_parsing:\n            continue\n        nv = name_value.split('=', 1)\n        if len(nv) != 2:\n            if strict_parsing:\n                raise ValueError, \"bad query field: %r\" % (name_value,)\n            # Handle case of a control-name with no equal sign\n            if keep_blank_values:\n                nv.append('')\n            else:\n                continue\n        if len(nv[1]) or keep_blank_values:\n            name = unquote(nv[0].replace('+', ' '))\n            value = unquote(nv[1].replace('+', ' '))\n            r.append((name, value))\n\n    return r\n", 
    "uu": "#! /usr/bin/env python\n\n# Copyright 1994 by Lance Ellinghouse\n# Cathedral City, California Republic, United States of America.\n#                        All Rights Reserved\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Lance Ellinghouse\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# LANCE ELLINGHOUSE DISCLAIMS ALL WARRANTIES WITH REGARD TO\n# THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND\n# FITNESS, IN NO EVENT SHALL LANCE ELLINGHOUSE CENTRUM BE LIABLE\n# FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n#\n# Modified by Jack Jansen, CWI, July 1995:\n# - Use binascii module to do the actual line-by-line conversion\n#   between ascii and binary. This results in a 1000-fold speedup. The C\n#   version is still 5 times faster, though.\n# - Arguments more compliant with python standard\n\n\"\"\"Implementation of the UUencode and UUdecode functions.\n\nencode(in_file, out_file [,name, mode])\ndecode(in_file [, out_file, mode])\n\"\"\"\n\nimport binascii\nimport os\nimport sys\n\n__all__ = [\"Error\", \"encode\", \"decode\"]\n\nclass Error(Exception):\n    pass\n\ndef encode(in_file, out_file, name=None, mode=None):\n    \"\"\"Uuencode file\"\"\"\n    #\n    # If in_file is a pathname open it and change defaults\n    #\n    opened_files = []\n    try:\n        if in_file == '-':\n            in_file = sys.stdin\n        elif isinstance(in_file, basestring):\n            if name is None:\n                name = os.path.basename(in_file)\n            if mode is None:\n                try:\n                    mode = os.stat(in_file).st_mode\n                except AttributeError:\n                    pass\n            in_file = open(in_file, 'rb')\n            opened_files.append(in_file)\n        #\n        # Open out_file if it is a pathname\n        #\n        if out_file == '-':\n            out_file = sys.stdout\n        elif isinstance(out_file, basestring):\n            out_file = open(out_file, 'wb')\n            opened_files.append(out_file)\n        #\n        # Set defaults for name and mode\n        #\n        if name is None:\n            name = '-'\n        if mode is None:\n            mode = 0666\n        #\n        # Write the data\n        #\n        out_file.write('begin %o %s\\n' % ((mode&0777),name))\n        data = in_file.read(45)\n        while len(data) > 0:\n            out_file.write(binascii.b2a_uu(data))\n            data = in_file.read(45)\n        out_file.write(' \\nend\\n')\n    finally:\n        for f in opened_files:\n            f.close()\n\n\ndef decode(in_file, out_file=None, mode=None, quiet=0):\n    \"\"\"Decode uuencoded file\"\"\"\n    #\n    # Open the input file, if needed.\n    #\n    opened_files = []\n    if in_file == '-':\n        in_file = sys.stdin\n    elif isinstance(in_file, basestring):\n        in_file = open(in_file)\n        opened_files.append(in_file)\n    try:\n        #\n        # Read until a begin is encountered or we've exhausted the file\n        #\n        while True:\n            hdr = in_file.readline()\n            if not hdr:\n                raise Error('No valid begin line found in input file')\n            if not hdr.startswith('begin'):\n                continue\n            hdrfields = hdr.split(' ', 2)\n            if len(hdrfields) == 3 and hdrfields[0] == 'begin':\n                try:\n                    int(hdrfields[1], 8)\n                    break\n                except ValueError:\n                    pass\n        if out_file is None:\n            out_file = hdrfields[2].rstrip()\n            if os.path.exists(out_file):\n                raise Error('Cannot overwrite existing file: %s' % out_file)\n        if mode is None:\n            mode = int(hdrfields[1], 8)\n        #\n        # Open the output file\n        #\n        if out_file == '-':\n            out_file = sys.stdout\n        elif isinstance(out_file, basestring):\n            fp = open(out_file, 'wb')\n            try:\n                os.path.chmod(out_file, mode)\n            except AttributeError:\n                pass\n            out_file = fp\n            opened_files.append(out_file)\n        #\n        # Main decoding loop\n        #\n        s = in_file.readline()\n        while s and s.strip() != 'end':\n            try:\n                data = binascii.a2b_uu(s)\n            except binascii.Error, v:\n                # Workaround for broken uuencoders by /Fredrik Lundh\n                nbytes = (((ord(s[0])-32) & 63) * 4 + 5) // 3\n                data = binascii.a2b_uu(s[:nbytes])\n                if not quiet:\n                    sys.stderr.write(\"Warning: %s\\n\" % v)\n            out_file.write(data)\n            s = in_file.readline()\n        if not s:\n            raise Error('Truncated input file')\n    finally:\n        for f in opened_files:\n            f.close()\n\ndef test():\n    \"\"\"uuencode/uudecode main program\"\"\"\n\n    import optparse\n    parser = optparse.OptionParser(usage='usage: %prog [-d] [-t] [input [output]]')\n    parser.add_option('-d', '--decode', dest='decode', help='Decode (instead of encode)?', default=False, action='store_true')\n    parser.add_option('-t', '--text', dest='text', help='data is text, encoded format unix-compatible text?', default=False, action='store_true')\n\n    (options, args) = parser.parse_args()\n    if len(args) > 2:\n        parser.error('incorrect number of arguments')\n        sys.exit(1)\n\n    input = sys.stdin\n    output = sys.stdout\n    if len(args) > 0:\n        input = args[0]\n    if len(args) > 1:\n        output = args[1]\n\n    if options.decode:\n        if options.text:\n            if isinstance(output, basestring):\n                output = open(output, 'w')\n            else:\n                print sys.argv[0], ': cannot do -t to stdout'\n                sys.exit(1)\n        decode(input, output)\n    else:\n        if options.text:\n            if isinstance(input, basestring):\n                input = open(input, 'r')\n            else:\n                print sys.argv[0], ': cannot do -t from stdin'\n                sys.exit(1)\n        encode(input, output)\n\nif __name__ == '__main__':\n    test()\n", 
    "warnings": "\"\"\"Python part of the warnings subsystem.\"\"\"\n\n# Note: function level imports should *not* be used\n# in this module as it may cause import lock deadlock.\n# See bug 683658.\nimport linecache\nimport sys\nimport types\n\n__all__ = [\"warn\", \"showwarning\", \"formatwarning\", \"filterwarnings\",\n           \"resetwarnings\", \"catch_warnings\"]\n\n\ndef warnpy3k(message, category=None, stacklevel=1):\n    \"\"\"Issue a deprecation warning for Python 3.x related changes.\n\n    Warnings are omitted unless Python is started with the -3 option.\n    \"\"\"\n    if sys.py3kwarning:\n        if category is None:\n            category = DeprecationWarning\n        warn(message, category, stacklevel+1)\n\ndef _show_warning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"Hook to write a warning to a file; replace if you like.\"\"\"\n    if file is None:\n        file = sys.stderr\n    try:\n        file.write(formatwarning(message, category, filename, lineno, line))\n    except IOError:\n        pass # the file (probably stderr) is invalid - this warning gets lost.\n# Keep a working version around in case the deprecation of the old API is\n# triggered.\nshowwarning = _show_warning\n\ndef formatwarning(message, category, filename, lineno, line=None):\n    \"\"\"Function to format a warning the standard way.\"\"\"\n    s =  \"%s:%s: %s: %s\\n\" % (filename, lineno, category.__name__, message)\n    line = linecache.getline(filename, lineno) if line is None else line\n    if line:\n        line = line.strip()\n        s += \"  %s\\n\" % line\n    return s\n\ndef filterwarnings(action, message=\"\", category=Warning, module=\"\", lineno=0,\n                   append=0):\n    \"\"\"Insert an entry into the list of warnings filters (at the front).\n\n    'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n                or \"once\"\n    'message' -- a regex that the warning message must match\n    'category' -- a class that the warning must be a subclass of\n    'module' -- a regex that the module name must match\n    'lineno' -- an integer line number, 0 matches all warnings\n    'append' -- if true, append to the list of filters\n    \"\"\"\n    import re\n    assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n                      \"once\"), \"invalid action: %r\" % (action,)\n    assert isinstance(message, basestring), \"message must be a string\"\n    assert isinstance(category, (type, types.ClassType)), \\\n           \"category must be a class\"\n    assert issubclass(category, Warning), \"category must be a Warning subclass\"\n    assert isinstance(module, basestring), \"module must be a string\"\n    assert isinstance(lineno, int) and lineno >= 0, \\\n           \"lineno must be an int >= 0\"\n    item = (action, re.compile(message, re.I), category,\n            re.compile(module), lineno)\n    if append:\n        filters.append(item)\n    else:\n        filters.insert(0, item)\n\ndef simplefilter(action, category=Warning, lineno=0, append=0):\n    \"\"\"Insert a simple entry into the list of warnings filters (at the front).\n\n    A simple filter matches all modules and messages.\n    'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n                or \"once\"\n    'category' -- a class that the warning must be a subclass of\n    'lineno' -- an integer line number, 0 matches all warnings\n    'append' -- if true, append to the list of filters\n    \"\"\"\n    assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n                      \"once\"), \"invalid action: %r\" % (action,)\n    assert isinstance(lineno, int) and lineno >= 0, \\\n           \"lineno must be an int >= 0\"\n    item = (action, None, category, None, lineno)\n    if append:\n        filters.append(item)\n    else:\n        filters.insert(0, item)\n\ndef resetwarnings():\n    \"\"\"Clear the list of warning filters, so that no filters are active.\"\"\"\n    filters[:] = []\n\nclass _OptionError(Exception):\n    \"\"\"Exception used by option processing helpers.\"\"\"\n    pass\n\n# Helper to process -W options passed via sys.warnoptions\ndef _processoptions(args):\n    for arg in args:\n        try:\n            _setoption(arg)\n        except _OptionError, msg:\n            print >>sys.stderr, \"Invalid -W option ignored:\", msg\n\n# Helper for _processoptions()\ndef _setoption(arg):\n    import re\n    parts = arg.split(':')\n    if len(parts) > 5:\n        raise _OptionError(\"too many fields (max 5): %r\" % (arg,))\n    while len(parts) < 5:\n        parts.append('')\n    action, message, category, module, lineno = [s.strip()\n                                                 for s in parts]\n    action = _getaction(action)\n    message = re.escape(message)\n    category = _getcategory(category)\n    module = re.escape(module)\n    if module:\n        module = module + '$'\n    if lineno:\n        try:\n            lineno = int(lineno)\n            if lineno < 0:\n                raise ValueError\n        except (ValueError, OverflowError):\n            raise _OptionError(\"invalid lineno %r\" % (lineno,))\n    else:\n        lineno = 0\n    filterwarnings(action, message, category, module, lineno)\n\n# Helper for _setoption()\ndef _getaction(action):\n    if not action:\n        return \"default\"\n    if action == \"all\": return \"always\" # Alias\n    for a in ('default', 'always', 'ignore', 'module', 'once', 'error'):\n        if a.startswith(action):\n            return a\n    raise _OptionError(\"invalid action: %r\" % (action,))\n\n# Helper for _setoption()\ndef _getcategory(category):\n    import re\n    if not category:\n        return Warning\n    if re.match(\"^[a-zA-Z0-9_]+$\", category):\n        try:\n            cat = eval(category)\n        except NameError:\n            raise _OptionError(\"unknown warning category: %r\" % (category,))\n    else:\n        i = category.rfind(\".\")\n        module = category[:i]\n        klass = category[i+1:]\n        try:\n            m = __import__(module, None, None, [klass])\n        except ImportError:\n            raise _OptionError(\"invalid module name: %r\" % (module,))\n        try:\n            cat = getattr(m, klass)\n        except AttributeError:\n            raise _OptionError(\"unknown warning category: %r\" % (category,))\n    if not issubclass(cat, Warning):\n        raise _OptionError(\"invalid warning category: %r\" % (category,))\n    return cat\n\n\n# Code typically replaced by _warnings\ndef warn(message, category=None, stacklevel=1):\n    \"\"\"Issue a warning, or maybe ignore it or raise an exception.\"\"\"\n    # Check if message is already a Warning object\n    if isinstance(message, Warning):\n        category = message.__class__\n    # Check category argument\n    if category is None:\n        category = UserWarning\n    assert issubclass(category, Warning)\n    # Get context information\n    try:\n        caller = sys._getframe(stacklevel)\n    except ValueError:\n        globals = sys.__dict__\n        lineno = 1\n    else:\n        globals = caller.f_globals\n        lineno = caller.f_lineno\n    if '__name__' in globals:\n        module = globals['__name__']\n    else:\n        module = \"<string>\"\n    filename = globals.get('__file__')\n    if filename:\n        fnl = filename.lower()\n        if fnl.endswith((\".pyc\", \".pyo\")):\n            filename = filename[:-1]\n    else:\n        if module == \"__main__\":\n            try:\n                filename = sys.argv[0]\n            except AttributeError:\n                # embedded interpreters don't have sys.argv, see bug #839151\n                filename = '__main__'\n        if not filename:\n            filename = module\n    registry = globals.setdefault(\"__warningregistry__\", {})\n    warn_explicit(message, category, filename, lineno, module, registry,\n                  globals)\n\ndef warn_explicit(message, category, filename, lineno,\n                  module=None, registry=None, module_globals=None):\n    lineno = int(lineno)\n    if module is None:\n        module = filename or \"<unknown>\"\n        if module[-3:].lower() == \".py\":\n            module = module[:-3] # XXX What about leading pathname?\n    if registry is None:\n        registry = {}\n    if isinstance(message, Warning):\n        text = str(message)\n        category = message.__class__\n    else:\n        text = message\n        message = category(message)\n    key = (text, category, lineno)\n    # Quick test for common case\n    if registry.get(key):\n        return\n    # Search the filters\n    for item in filters:\n        action, msg, cat, mod, ln = item\n        if ((msg is None or msg.match(text)) and\n            issubclass(category, cat) and\n            (mod is None or mod.match(module)) and\n            (ln == 0 or lineno == ln)):\n            break\n    else:\n        action = defaultaction\n    # Early exit actions\n    if action == \"ignore\":\n        registry[key] = 1\n        return\n\n    # Prime the linecache for formatting, in case the\n    # \"file\" is actually in a zipfile or something.\n    linecache.getlines(filename, module_globals)\n\n    if action == \"error\":\n        raise message\n    # Other actions\n    if action == \"once\":\n        registry[key] = 1\n        oncekey = (text, category)\n        if onceregistry.get(oncekey):\n            return\n        onceregistry[oncekey] = 1\n    elif action == \"always\":\n        pass\n    elif action == \"module\":\n        registry[key] = 1\n        altkey = (text, category, 0)\n        if registry.get(altkey):\n            return\n        registry[altkey] = 1\n    elif action == \"default\":\n        registry[key] = 1\n    else:\n        # Unrecognized actions are errors\n        raise RuntimeError(\n              \"Unrecognized action (%r) in warnings.filters:\\n %s\" %\n              (action, item))\n    # Print message and context\n    showwarning(message, category, filename, lineno)\n\n\nclass WarningMessage(object):\n\n    \"\"\"Holds the result of a single showwarning() call.\"\"\"\n\n    _WARNING_DETAILS = (\"message\", \"category\", \"filename\", \"lineno\", \"file\",\n                        \"line\")\n\n    def __init__(self, message, category, filename, lineno, file=None,\n                    line=None):\n        local_values = locals()\n        for attr in self._WARNING_DETAILS:\n            setattr(self, attr, local_values[attr])\n        self._category_name = category.__name__ if category else None\n\n    def __str__(self):\n        return (\"{message : %r, category : %r, filename : %r, lineno : %s, \"\n                    \"line : %r}\" % (self.message, self._category_name,\n                                    self.filename, self.lineno, self.line))\n\n\nclass catch_warnings(object):\n\n    \"\"\"A context manager that copies and restores the warnings filter upon\n    exiting the context.\n\n    The 'record' argument specifies whether warnings should be captured by a\n    custom implementation of warnings.showwarning() and be appended to a list\n    returned by the context manager. Otherwise None is returned by the context\n    manager. The objects appended to the list are arguments whose attributes\n    mirror the arguments to showwarning().\n\n    The 'module' argument is to specify an alternative module to the module\n    named 'warnings' and imported under that name. This argument is only useful\n    when testing the warnings module itself.\n\n    \"\"\"\n\n    def __init__(self, record=False, module=None):\n        \"\"\"Specify whether to record warnings and if an alternative module\n        should be used other than sys.modules['warnings'].\n\n        For compatibility with Python 3.0, please consider all arguments to be\n        keyword-only.\n\n        \"\"\"\n        self._record = record\n        self._module = sys.modules['warnings'] if module is None else module\n        self._entered = False\n\n    def __repr__(self):\n        args = []\n        if self._record:\n            args.append(\"record=True\")\n        if self._module is not sys.modules['warnings']:\n            args.append(\"module=%r\" % self._module)\n        name = type(self).__name__\n        return \"%s(%s)\" % (name, \", \".join(args))\n\n    def __enter__(self):\n        if self._entered:\n            raise RuntimeError(\"Cannot enter %r twice\" % self)\n        self._entered = True\n        self._filters = self._module.filters\n        self._module.filters = self._filters[:]\n        self._showwarning = self._module.showwarning\n        if self._record:\n            log = []\n            def showwarning(*args, **kwargs):\n                log.append(WarningMessage(*args, **kwargs))\n            self._module.showwarning = showwarning\n            return log\n        else:\n            return None\n\n    def __exit__(self, *exc_info):\n        if not self._entered:\n            raise RuntimeError(\"Cannot exit %r without entering first\" % self)\n        self._module.filters = self._filters\n        self._module.showwarning = self._showwarning\n\n\n# filters contains a sequence of filter 5-tuples\n# The components of the 5-tuple are:\n# - an action: error, ignore, always, default, module, or once\n# - a compiled regex that must match the warning message\n# - a class representing the warning category\n# - a compiled regex that must match the module that is being warned\n# - a line number for the line being warning, or 0 to mean any line\n# If either if the compiled regexs are None, match anything.\n_warnings_defaults = False\ntry:\n    from _warnings import (filters, default_action, once_registry,\n                            warn, warn_explicit)\n    defaultaction = default_action\n    onceregistry = once_registry\n    _warnings_defaults = True\nexcept ImportError:\n    filters = []\n    defaultaction = \"default\"\n    onceregistry = {}\n\n\n# Module initialization\n_processoptions(sys.warnoptions)\nif not _warnings_defaults:\n    silence = [ImportWarning, PendingDeprecationWarning]\n    # Don't silence DeprecationWarning if -3 or -Q was used.\n    if not sys.py3kwarning and not sys.flags.division_warning:\n        silence.append(DeprecationWarning)\n    for cls in silence:\n        simplefilter(\"ignore\", category=cls)\n    bytes_warning = sys.flags.bytes_warning\n    if bytes_warning > 1:\n        bytes_action = \"error\"\n    elif bytes_warning:\n        bytes_action = \"default\"\n    else:\n        bytes_action = \"ignore\"\n    simplefilter(bytes_action, category=BytesWarning, append=1)\ndel _warnings_defaults\n", 
    "weakref": "\"\"\"Weak reference support for Python.\n\nThis module is an implementation of PEP 205:\n\nhttp://www.python.org/dev/peps/pep-0205/\n\"\"\"\n\n# Naming convention: Variables named \"wr\" are weak reference objects;\n# they are called this instead of \"ref\" to avoid name collisions with\n# the module-global ref() function imported from _weakref.\n\nimport UserDict\n\nfrom _weakref import (\n     getweakrefcount,\n     getweakrefs,\n     ref,\n     proxy,\n     CallableProxyType,\n     ProxyType,\n     ReferenceType)\n\nfrom _weakrefset import WeakSet, _IterationGuard\n\nfrom exceptions import ReferenceError\n\n\nProxyTypes = (ProxyType, CallableProxyType)\n\n__all__ = [\"ref\", \"proxy\", \"getweakrefcount\", \"getweakrefs\",\n           \"WeakKeyDictionary\", \"ReferenceError\", \"ReferenceType\", \"ProxyType\",\n           \"CallableProxyType\", \"ProxyTypes\", \"WeakValueDictionary\", 'WeakSet']\n\n\nclass WeakValueDictionary(UserDict.UserDict):\n    \"\"\"Mapping class that references values weakly.\n\n    Entries in the dictionary will be discarded when no strong\n    reference to the value exists anymore\n    \"\"\"\n    # We inherit the constructor without worrying about the input\n    # dictionary; since it uses our .update() method, we get the right\n    # checks (if the other dictionary is a WeakValueDictionary,\n    # objects are unwrapped on the way out, and we always wrap on the\n    # way in).\n\n    def __init__(self, *args, **kw):\n        def remove(wr, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(wr.key)\n                else:\n                    # Changed this for PyPy: made more resistent.  The\n                    # issue is that in some corner cases, self.data\n                    # might already be changed or removed by the time\n                    # this weakref's callback is called.  If that is\n                    # the case, we don't want to randomly kill an\n                    # unrelated entry.\n                    if self.data.get(wr.key) is wr:\n                        del self.data[wr.key]\n        self._remove = remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        UserDict.UserDict.__init__(self, *args, **kw)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        d = self.data\n        # We shouldn't encounter any KeyError, because this method should\n        # always be called *before* mutating the dict.\n        while l:\n            del d[l.pop()]\n\n    def __getitem__(self, key):\n        o = self.data[key]()\n        if o is None:\n            raise KeyError, key\n        else:\n            return o\n\n    def __delitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        del self.data[key]\n\n    def __contains__(self, key):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def has_key(self, key):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def __repr__(self):\n        return \"<WeakValueDictionary at %s>\" % id(self)\n\n    def __setitem__(self, key, value):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data[key] = KeyedRef(value, self._remove, key)\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        new = WeakValueDictionary()\n        for key, wr in self.data.items():\n            o = wr()\n            if o is not None:\n                new[key] = o\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        for key, wr in self.data.items():\n            o = wr()\n            if o is not None:\n                new[deepcopy(key, memo)] = o\n        return new\n\n    def get(self, key, default=None):\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            o = wr()\n            if o is None:\n                # This should only happen\n                return default\n            else:\n                return o\n\n    def items(self):\n        L = []\n        for key, wr in self.data.items():\n            o = wr()\n            if o is not None:\n                L.append((key, o))\n        return L\n\n    def iteritems(self):\n        with _IterationGuard(self):\n            for wr in self.data.itervalues():\n                value = wr()\n                if value is not None:\n                    yield wr.key, value\n\n    def iterkeys(self):\n        with _IterationGuard(self):\n            for k in self.data.iterkeys():\n                yield k\n\n    __iter__ = iterkeys\n\n    def itervaluerefs(self):\n        \"\"\"Return an iterator that yields the weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        with _IterationGuard(self):\n            for wr in self.data.itervalues():\n                yield wr\n\n    def itervalues(self):\n        with _IterationGuard(self):\n            for wr in self.data.itervalues():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    def popitem(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while 1:\n            key, wr = self.data.popitem()\n            o = wr()\n            if o is not None:\n                return key, o\n\n    def pop(self, key, *args):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data.pop(key)()\n        except KeyError:\n            o = None\n        if o is None:\n            if args:\n                return args[0]\n            raise KeyError, key\n        else:\n            return o\n        # The logic above was fixed in PyPy\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = KeyedRef(default, self._remove, key)\n            return default\n        else:\n            return o\n        # The logic above was fixed in PyPy\n\n    def update(self, dict=None, **kwargs):\n        if self._pending_removals:\n            self._commit_removals()\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, o in dict.items():\n                d[key] = KeyedRef(o, self._remove, key)\n        if len(kwargs):\n            self.update(kwargs)\n\n    def valuerefs(self):\n        \"\"\"Return a list of weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        return self.data.values()\n\n    def values(self):\n        L = []\n        for wr in self.data.values():\n            o = wr()\n            if o is not None:\n                L.append(o)\n        return L\n\n\nclass KeyedRef(ref):\n    \"\"\"Specialized reference that includes a key corresponding to the value.\n\n    This is used in the WeakValueDictionary to avoid having to create\n    a function object for each key stored in the mapping.  A shared\n    callback object can use the 'key' attribute of a KeyedRef instead\n    of getting a reference to the key from an enclosing scope.\n\n    \"\"\"\n\n    __slots__ = \"key\",\n\n    def __new__(type, ob, callback, key):\n        self = ref.__new__(type, ob, callback)\n        self.key = key\n        return self\n\n    def __init__(self, ob, callback, key):\n        super(KeyedRef,  self).__init__(ob, callback)\n\n\nclass WeakKeyDictionary(UserDict.UserDict):\n    \"\"\" Mapping class that references keys weakly.\n\n    Entries in the dictionary will be discarded when there is no\n    longer a strong reference to the key. This can be used to\n    associate additional data with an object owned by other parts of\n    an application without adding attributes to those objects. This\n    can be especially useful with objects that override attribute\n    accesses.\n    \"\"\"\n\n    def __init__(self, dict=None):\n        self.data = {}\n        def remove(k, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(k)\n                else:\n                    del self.data[k]\n        self._remove = remove\n        # A list of dead weakrefs (keys to be removed)\n        self._pending_removals = []\n        self._iterating = set()\n        if dict is not None:\n            self.update(dict)\n\n    def _commit_removals(self):\n        # NOTE: We don't need to call this method before mutating the dict,\n        # because a dead weakref never compares equal to a live weakref,\n        # even if they happened to refer to equal objects.\n        # However, it means keys may already have been removed.\n        l = self._pending_removals\n        d = self.data\n        while l:\n            try:\n                del d[l.pop()]\n            except KeyError:\n                pass\n\n    def __delitem__(self, key):\n        del self.data[ref(key)]\n\n    def __getitem__(self, key):\n        return self.data[ref(key)]\n\n    def __repr__(self):\n        return \"<WeakKeyDictionary at %s>\" % id(self)\n\n    def __setitem__(self, key, value):\n        self.data[ref(key, self._remove)] = value\n\n    def copy(self):\n        new = WeakKeyDictionary()\n        for key, value in self.data.items():\n            o = key()\n            if o is not None:\n                new[o] = value\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        for key, value in self.data.items():\n            o = key()\n            if o is not None:\n                new[o] = deepcopy(value, memo)\n        return new\n\n    def get(self, key, default=None):\n        return self.data.get(ref(key),default)\n\n    def has_key(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return 0\n        return wr in self.data\n\n    def __contains__(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return 0\n        return wr in self.data\n\n    def items(self):\n        L = []\n        for key, value in self.data.items():\n            o = key()\n            if o is not None:\n                L.append((o, value))\n        return L\n\n    def iteritems(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.iteritems():\n                key = wr()\n                if key is not None:\n                    yield key, value\n\n    def iterkeyrefs(self):\n        \"\"\"Return an iterator that yields the weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        with _IterationGuard(self):\n            for wr in self.data.iterkeys():\n                yield wr\n\n    def iterkeys(self):\n        with _IterationGuard(self):\n            for wr in self.data.iterkeys():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    __iter__ = iterkeys\n\n    def itervalues(self):\n        with _IterationGuard(self):\n            for value in self.data.itervalues():\n                yield value\n\n    def keyrefs(self):\n        \"\"\"Return a list of weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        return self.data.keys()\n\n    def keys(self):\n        L = []\n        for wr in self.data.keys():\n            o = wr()\n            if o is not None:\n                L.append(o)\n        return L\n\n    def popitem(self):\n        while 1:\n            key, value = self.data.popitem()\n            o = key()\n            if o is not None:\n                return o, value\n\n    def pop(self, key, *args):\n        return self.data.pop(ref(key), *args)\n\n    def setdefault(self, key, default=None):\n        return self.data.setdefault(ref(key, self._remove),default)\n\n    def update(self, dict=None, **kwargs):\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, value in dict.items():\n                d[ref(key, self._remove)] = value\n        if len(kwargs):\n            self.update(kwargs)\n", 
    "xml.__init__": "\"\"\"Core XML support for Python.\n\nThis package contains four sub-packages:\n\ndom -- The W3C Document Object Model.  This supports DOM Level 1 +\n       Namespaces.\n\nparsers -- Python wrappers for XML parsers (currently only supports Expat).\n\nsax -- The Simple API for XML, developed by XML-Dev, led by David\n       Megginson and ported to Python by Lars Marius Garshol.  This\n       supports the SAX 2 API.\n\netree -- The ElementTree XML library.  This is a subset of the full\n       ElementTree XML release.\n\n\"\"\"\n\n\n__all__ = [\"dom\", \"parsers\", \"sax\", \"etree\"]\n\n_MINIMUM_XMLPLUS_VERSION = (0, 8, 4)\n\n\ntry:\n    import _xmlplus\nexcept ImportError:\n    pass\nelse:\n    try:\n        v = _xmlplus.version_info\n    except AttributeError:\n        # _xmlplus is too old; ignore it\n        pass\n    else:\n        if v >= _MINIMUM_XMLPLUS_VERSION:\n            import sys\n            _xmlplus.__path__.extend(__path__)\n            sys.modules[__name__] = _xmlplus\n        else:\n            del v\n", 
    "xml.dom.NodeFilter": "# This is the Python mapping for interface NodeFilter from\n# DOM2-Traversal-Range. It contains only constants.\n\nclass NodeFilter:\n    \"\"\"\n    This is the DOM2 NodeFilter interface. It contains only constants.\n    \"\"\"\n    FILTER_ACCEPT = 1\n    FILTER_REJECT = 2\n    FILTER_SKIP   = 3\n\n    SHOW_ALL                    = 0xFFFFFFFFL\n    SHOW_ELEMENT                = 0x00000001\n    SHOW_ATTRIBUTE              = 0x00000002\n    SHOW_TEXT                   = 0x00000004\n    SHOW_CDATA_SECTION          = 0x00000008\n    SHOW_ENTITY_REFERENCE       = 0x00000010\n    SHOW_ENTITY                 = 0x00000020\n    SHOW_PROCESSING_INSTRUCTION = 0x00000040\n    SHOW_COMMENT                = 0x00000080\n    SHOW_DOCUMENT               = 0x00000100\n    SHOW_DOCUMENT_TYPE          = 0x00000200\n    SHOW_DOCUMENT_FRAGMENT      = 0x00000400\n    SHOW_NOTATION               = 0x00000800\n\n    def acceptNode(self, node):\n        raise NotImplementedError\n", 
    "xml.dom.__init__": "\"\"\"W3C Document Object Model implementation for Python.\n\nThe Python mapping of the Document Object Model is documented in the\nPython Library Reference in the section on the xml.dom package.\n\nThis package contains the following modules:\n\nminidom -- A simple implementation of the Level 1 DOM with namespace\n           support added (based on the Level 2 specification) and other\n           minor Level 2 functionality.\n\npulldom -- DOM builder supporting on-demand tree-building for selected\n           subtrees of the document.\n\n\"\"\"\n\n\nclass Node:\n    \"\"\"Class giving the NodeType constants.\"\"\"\n\n    # DOM implementations may use this as a base class for their own\n    # Node implementations.  If they don't, the constants defined here\n    # should still be used as the canonical definitions as they match\n    # the values given in the W3C recommendation.  Client code can\n    # safely refer to these values in all tests of Node.nodeType\n    # values.\n\n    ELEMENT_NODE                = 1\n    ATTRIBUTE_NODE              = 2\n    TEXT_NODE                   = 3\n    CDATA_SECTION_NODE          = 4\n    ENTITY_REFERENCE_NODE       = 5\n    ENTITY_NODE                 = 6\n    PROCESSING_INSTRUCTION_NODE = 7\n    COMMENT_NODE                = 8\n    DOCUMENT_NODE               = 9\n    DOCUMENT_TYPE_NODE          = 10\n    DOCUMENT_FRAGMENT_NODE      = 11\n    NOTATION_NODE               = 12\n\n\n#ExceptionCode\nINDEX_SIZE_ERR                 = 1\nDOMSTRING_SIZE_ERR             = 2\nHIERARCHY_REQUEST_ERR          = 3\nWRONG_DOCUMENT_ERR             = 4\nINVALID_CHARACTER_ERR          = 5\nNO_DATA_ALLOWED_ERR            = 6\nNO_MODIFICATION_ALLOWED_ERR    = 7\nNOT_FOUND_ERR                  = 8\nNOT_SUPPORTED_ERR              = 9\nINUSE_ATTRIBUTE_ERR            = 10\nINVALID_STATE_ERR              = 11\nSYNTAX_ERR                     = 12\nINVALID_MODIFICATION_ERR       = 13\nNAMESPACE_ERR                  = 14\nINVALID_ACCESS_ERR             = 15\nVALIDATION_ERR                 = 16\n\n\nclass DOMException(Exception):\n    \"\"\"Abstract base class for DOM exceptions.\n    Exceptions with specific codes are specializations of this class.\"\"\"\n\n    def __init__(self, *args, **kw):\n        if self.__class__ is DOMException:\n            raise RuntimeError(\n                \"DOMException should not be instantiated directly\")\n        Exception.__init__(self, *args, **kw)\n\n    def _get_code(self):\n        return self.code\n\n\nclass IndexSizeErr(DOMException):\n    code = INDEX_SIZE_ERR\n\nclass DomstringSizeErr(DOMException):\n    code = DOMSTRING_SIZE_ERR\n\nclass HierarchyRequestErr(DOMException):\n    code = HIERARCHY_REQUEST_ERR\n\nclass WrongDocumentErr(DOMException):\n    code = WRONG_DOCUMENT_ERR\n\nclass InvalidCharacterErr(DOMException):\n    code = INVALID_CHARACTER_ERR\n\nclass NoDataAllowedErr(DOMException):\n    code = NO_DATA_ALLOWED_ERR\n\nclass NoModificationAllowedErr(DOMException):\n    code = NO_MODIFICATION_ALLOWED_ERR\n\nclass NotFoundErr(DOMException):\n    code = NOT_FOUND_ERR\n\nclass NotSupportedErr(DOMException):\n    code = NOT_SUPPORTED_ERR\n\nclass InuseAttributeErr(DOMException):\n    code = INUSE_ATTRIBUTE_ERR\n\nclass InvalidStateErr(DOMException):\n    code = INVALID_STATE_ERR\n\nclass SyntaxErr(DOMException):\n    code = SYNTAX_ERR\n\nclass InvalidModificationErr(DOMException):\n    code = INVALID_MODIFICATION_ERR\n\nclass NamespaceErr(DOMException):\n    code = NAMESPACE_ERR\n\nclass InvalidAccessErr(DOMException):\n    code = INVALID_ACCESS_ERR\n\nclass ValidationErr(DOMException):\n    code = VALIDATION_ERR\n\nclass UserDataHandler:\n    \"\"\"Class giving the operation constants for UserDataHandler.handle().\"\"\"\n\n    # Based on DOM Level 3 (WD 9 April 2002)\n\n    NODE_CLONED   = 1\n    NODE_IMPORTED = 2\n    NODE_DELETED  = 3\n    NODE_RENAMED  = 4\n\nXML_NAMESPACE = \"http://www.w3.org/XML/1998/namespace\"\nXMLNS_NAMESPACE = \"http://www.w3.org/2000/xmlns/\"\nXHTML_NAMESPACE = \"http://www.w3.org/1999/xhtml\"\nEMPTY_NAMESPACE = None\nEMPTY_PREFIX = None\n\nfrom domreg import getDOMImplementation,registerDOMImplementation\n", 
    "xml.dom.domreg": "\"\"\"Registration facilities for DOM. This module should not be used\ndirectly. Instead, the functions getDOMImplementation and\nregisterDOMImplementation should be imported from xml.dom.\"\"\"\n\nfrom xml.dom.minicompat import *  # isinstance, StringTypes\n\n# This is a list of well-known implementations.  Well-known names\n# should be published by posting to xml-sig@python.org, and are\n# subsequently recorded in this file.\n\nwell_known_implementations = {\n    'minidom':'xml.dom.minidom',\n    '4DOM': 'xml.dom.DOMImplementation',\n    }\n\n# DOM implementations not officially registered should register\n# themselves with their\n\nregistered = {}\n\ndef registerDOMImplementation(name, factory):\n    \"\"\"registerDOMImplementation(name, factory)\n\n    Register the factory function with the name. The factory function\n    should return an object which implements the DOMImplementation\n    interface. The factory function can either return the same object,\n    or a new one (e.g. if that implementation supports some\n    customization).\"\"\"\n\n    registered[name] = factory\n\ndef _good_enough(dom, features):\n    \"_good_enough(dom, features) -> Return 1 if the dom offers the features\"\n    for f,v in features:\n        if not dom.hasFeature(f,v):\n            return 0\n    return 1\n\ndef getDOMImplementation(name = None, features = ()):\n    \"\"\"getDOMImplementation(name = None, features = ()) -> DOM implementation.\n\n    Return a suitable DOM implementation. The name is either\n    well-known, the module name of a DOM implementation, or None. If\n    it is not None, imports the corresponding module and returns\n    DOMImplementation object if the import succeeds.\n\n    If name is not given, consider the available implementations to\n    find one with the required feature set. If no implementation can\n    be found, raise an ImportError. The features list must be a sequence\n    of (feature, version) pairs which are passed to hasFeature.\"\"\"\n\n    import os\n    creator = None\n    mod = well_known_implementations.get(name)\n    if mod:\n        mod = __import__(mod, {}, {}, ['getDOMImplementation'])\n        return mod.getDOMImplementation()\n    elif name:\n        return registered[name]()\n    elif \"PYTHON_DOM\" in os.environ:\n        return getDOMImplementation(name = os.environ[\"PYTHON_DOM\"])\n\n    # User did not specify a name, try implementations in arbitrary\n    # order, returning the one that has the required features\n    if isinstance(features, StringTypes):\n        features = _parse_feature_string(features)\n    for creator in registered.values():\n        dom = creator()\n        if _good_enough(dom, features):\n            return dom\n\n    for creator in well_known_implementations.keys():\n        try:\n            dom = getDOMImplementation(name = creator)\n        except StandardError: # typically ImportError, or AttributeError\n            continue\n        if _good_enough(dom, features):\n            return dom\n\n    raise ImportError,\"no suitable DOM implementation found\"\n\ndef _parse_feature_string(s):\n    features = []\n    parts = s.split()\n    i = 0\n    length = len(parts)\n    while i < length:\n        feature = parts[i]\n        if feature[0] in \"0123456789\":\n            raise ValueError, \"bad feature name: %r\" % (feature,)\n        i = i + 1\n        version = None\n        if i < length:\n            v = parts[i]\n            if v[0] in \"0123456789\":\n                i = i + 1\n                version = v\n        features.append((feature, version))\n    return tuple(features)\n", 
    "xml.dom.expatbuilder": "\"\"\"Facility to use the Expat parser to load a minidom instance\nfrom a string or file.\n\nThis avoids all the overhead of SAX and pulldom to gain performance.\n\"\"\"\n\n# Warning!\n#\n# This module is tightly bound to the implementation details of the\n# minidom DOM and can't be used with other DOM implementations.  This\n# is due, in part, to a lack of appropriate methods in the DOM (there is\n# no way to create Entity and Notation nodes via the DOM Level 2\n# interface), and for performance.  The later is the cause of some fairly\n# cryptic code.\n#\n# Performance hacks:\n#\n#   -  .character_data_handler() has an extra case in which continuing\n#      data is appended to an existing Text node; this can be a\n#      speedup since pyexpat can break up character data into multiple\n#      callbacks even though we set the buffer_text attribute on the\n#      parser.  This also gives us the advantage that we don't need a\n#      separate normalization pass.\n#\n#   -  Determining that a node exists is done using an identity comparison\n#      with None rather than a truth test; this avoids searching for and\n#      calling any methods on the node object if it exists.  (A rather\n#      nice speedup is achieved this way as well!)\n\nfrom xml.dom import xmlbuilder, minidom, Node\nfrom xml.dom import EMPTY_NAMESPACE, EMPTY_PREFIX, XMLNS_NAMESPACE\nfrom xml.parsers import expat\nfrom xml.dom.minidom import _append_child, _set_attribute_node\nfrom xml.dom.NodeFilter import NodeFilter\n\nfrom xml.dom.minicompat import *\n\nTEXT_NODE = Node.TEXT_NODE\nCDATA_SECTION_NODE = Node.CDATA_SECTION_NODE\nDOCUMENT_NODE = Node.DOCUMENT_NODE\n\nFILTER_ACCEPT = xmlbuilder.DOMBuilderFilter.FILTER_ACCEPT\nFILTER_REJECT = xmlbuilder.DOMBuilderFilter.FILTER_REJECT\nFILTER_SKIP = xmlbuilder.DOMBuilderFilter.FILTER_SKIP\nFILTER_INTERRUPT = xmlbuilder.DOMBuilderFilter.FILTER_INTERRUPT\n\ntheDOMImplementation = minidom.getDOMImplementation()\n\n# Expat typename -> TypeInfo\n_typeinfo_map = {\n    \"CDATA\":    minidom.TypeInfo(None, \"cdata\"),\n    \"ENUM\":     minidom.TypeInfo(None, \"enumeration\"),\n    \"ENTITY\":   minidom.TypeInfo(None, \"entity\"),\n    \"ENTITIES\": minidom.TypeInfo(None, \"entities\"),\n    \"ID\":       minidom.TypeInfo(None, \"id\"),\n    \"IDREF\":    minidom.TypeInfo(None, \"idref\"),\n    \"IDREFS\":   minidom.TypeInfo(None, \"idrefs\"),\n    \"NMTOKEN\":  minidom.TypeInfo(None, \"nmtoken\"),\n    \"NMTOKENS\": minidom.TypeInfo(None, \"nmtokens\"),\n    }\n\nclass ElementInfo(object):\n    __slots__ = '_attr_info', '_model', 'tagName'\n\n    def __init__(self, tagName, model=None):\n        self.tagName = tagName\n        self._attr_info = []\n        self._model = model\n\n    def __getstate__(self):\n        return self._attr_info, self._model, self.tagName\n\n    def __setstate__(self, state):\n        self._attr_info, self._model, self.tagName = state\n\n    def getAttributeType(self, aname):\n        for info in self._attr_info:\n            if info[1] == aname:\n                t = info[-2]\n                if t[0] == \"(\":\n                    return _typeinfo_map[\"ENUM\"]\n                else:\n                    return _typeinfo_map[info[-2]]\n        return minidom._no_type\n\n    def getAttributeTypeNS(self, namespaceURI, localName):\n        return minidom._no_type\n\n    def isElementContent(self):\n        if self._model:\n            type = self._model[0]\n            return type not in (expat.model.XML_CTYPE_ANY,\n                                expat.model.XML_CTYPE_MIXED)\n        else:\n            return False\n\n    def isEmpty(self):\n        if self._model:\n            return self._model[0] == expat.model.XML_CTYPE_EMPTY\n        else:\n            return False\n\n    def isId(self, aname):\n        for info in self._attr_info:\n            if info[1] == aname:\n                return info[-2] == \"ID\"\n        return False\n\n    def isIdNS(self, euri, ename, auri, aname):\n        # not sure this is meaningful\n        return self.isId((auri, aname))\n\ndef _intern(builder, s):\n    return builder._intern_setdefault(s, s)\n\ndef _parse_ns_name(builder, name):\n    assert ' ' in name\n    parts = name.split(' ')\n    intern = builder._intern_setdefault\n    if len(parts) == 3:\n        uri, localname, prefix = parts\n        prefix = intern(prefix, prefix)\n        qname = \"%s:%s\" % (prefix, localname)\n        qname = intern(qname, qname)\n        localname = intern(localname, localname)\n    else:\n        uri, localname = parts\n        prefix = EMPTY_PREFIX\n        qname = localname = intern(localname, localname)\n    return intern(uri, uri), localname, prefix, qname\n\n\nclass ExpatBuilder:\n    \"\"\"Document builder that uses Expat to build a ParsedXML.DOM document\n    instance.\"\"\"\n\n    def __init__(self, options=None):\n        if options is None:\n            options = xmlbuilder.Options()\n        self._options = options\n        if self._options.filter is not None:\n            self._filter = FilterVisibilityController(self._options.filter)\n        else:\n            self._filter = None\n            # This *really* doesn't do anything in this case, so\n            # override it with something fast & minimal.\n            self._finish_start_element = id\n        self._parser = None\n        self.reset()\n\n    def createParser(self):\n        \"\"\"Create a new parser object.\"\"\"\n        return expat.ParserCreate()\n\n    def getParser(self):\n        \"\"\"Return the parser object, creating a new one if needed.\"\"\"\n        if not self._parser:\n            self._parser = self.createParser()\n            self._intern_setdefault = self._parser.intern.setdefault\n            self._parser.buffer_text = True\n            self._parser.ordered_attributes = True\n            self._parser.specified_attributes = True\n            self.install(self._parser)\n        return self._parser\n\n    def reset(self):\n        \"\"\"Free all data structures used during DOM construction.\"\"\"\n        self.document = theDOMImplementation.createDocument(\n            EMPTY_NAMESPACE, None, None)\n        self.curNode = self.document\n        self._elem_info = self.document._elem_info\n        self._cdata = False\n\n    def install(self, parser):\n        \"\"\"Install the callbacks needed to build the DOM into the parser.\"\"\"\n        # This creates circular references!\n        parser.StartDoctypeDeclHandler = self.start_doctype_decl_handler\n        parser.StartElementHandler = self.first_element_handler\n        parser.EndElementHandler = self.end_element_handler\n        parser.ProcessingInstructionHandler = self.pi_handler\n        if self._options.entities:\n            parser.EntityDeclHandler = self.entity_decl_handler\n        parser.NotationDeclHandler = self.notation_decl_handler\n        if self._options.comments:\n            parser.CommentHandler = self.comment_handler\n        if self._options.cdata_sections:\n            parser.StartCdataSectionHandler = self.start_cdata_section_handler\n            parser.EndCdataSectionHandler = self.end_cdata_section_handler\n            parser.CharacterDataHandler = self.character_data_handler_cdata\n        else:\n            parser.CharacterDataHandler = self.character_data_handler\n        parser.ExternalEntityRefHandler = self.external_entity_ref_handler\n        parser.XmlDeclHandler = self.xml_decl_handler\n        parser.ElementDeclHandler = self.element_decl_handler\n        parser.AttlistDeclHandler = self.attlist_decl_handler\n\n    def parseFile(self, file):\n        \"\"\"Parse a document from a file object, returning the document\n        node.\"\"\"\n        parser = self.getParser()\n        first_buffer = True\n        try:\n            while 1:\n                buffer = file.read(16*1024)\n                if not buffer:\n                    break\n                parser.Parse(buffer, 0)\n                if first_buffer and self.document.documentElement:\n                    self._setup_subset(buffer)\n                first_buffer = False\n            parser.Parse(\"\", True)\n        except ParseEscape:\n            pass\n        doc = self.document\n        self.reset()\n        self._parser = None\n        return doc\n\n    def parseString(self, string):\n        \"\"\"Parse a document from a string, returning the document node.\"\"\"\n        parser = self.getParser()\n        try:\n            parser.Parse(string, True)\n            self._setup_subset(string)\n        except ParseEscape:\n            pass\n        doc = self.document\n        self.reset()\n        self._parser = None\n        return doc\n\n    def _setup_subset(self, buffer):\n        \"\"\"Load the internal subset if there might be one.\"\"\"\n        if self.document.doctype:\n            extractor = InternalSubsetExtractor()\n            extractor.parseString(buffer)\n            subset = extractor.getSubset()\n            self.document.doctype.internalSubset = subset\n\n    def start_doctype_decl_handler(self, doctypeName, systemId, publicId,\n                                   has_internal_subset):\n        doctype = self.document.implementation.createDocumentType(\n            doctypeName, publicId, systemId)\n        doctype.ownerDocument = self.document\n        _append_child(self.document, doctype)\n        self.document.doctype = doctype\n        if self._filter and self._filter.acceptNode(doctype) == FILTER_REJECT:\n            self.document.doctype = None\n            del self.document.childNodes[-1]\n            doctype = None\n            self._parser.EntityDeclHandler = None\n            self._parser.NotationDeclHandler = None\n        if has_internal_subset:\n            if doctype is not None:\n                doctype.entities._seq = []\n                doctype.notations._seq = []\n            self._parser.CommentHandler = None\n            self._parser.ProcessingInstructionHandler = None\n            self._parser.EndDoctypeDeclHandler = self.end_doctype_decl_handler\n\n    def end_doctype_decl_handler(self):\n        if self._options.comments:\n            self._parser.CommentHandler = self.comment_handler\n        self._parser.ProcessingInstructionHandler = self.pi_handler\n        if not (self._elem_info or self._filter):\n            self._finish_end_element = id\n\n    def pi_handler(self, target, data):\n        node = self.document.createProcessingInstruction(target, data)\n        _append_child(self.curNode, node)\n        if self._filter and self._filter.acceptNode(node) == FILTER_REJECT:\n            self.curNode.removeChild(node)\n\n    def character_data_handler_cdata(self, data):\n        childNodes = self.curNode.childNodes\n        if self._cdata:\n            if (  self._cdata_continue\n                  and childNodes[-1].nodeType == CDATA_SECTION_NODE):\n                childNodes[-1].appendData(data)\n                return\n            node = self.document.createCDATASection(data)\n            self._cdata_continue = True\n        elif childNodes and childNodes[-1].nodeType == TEXT_NODE:\n            node = childNodes[-1]\n            value = node.data + data\n            d = node.__dict__\n            d['data'] = d['nodeValue'] = value\n            return\n        else:\n            node = minidom.Text()\n            d = node.__dict__\n            d['data'] = d['nodeValue'] = data\n            d['ownerDocument'] = self.document\n        _append_child(self.curNode, node)\n\n    def character_data_handler(self, data):\n        childNodes = self.curNode.childNodes\n        if childNodes and childNodes[-1].nodeType == TEXT_NODE:\n            node = childNodes[-1]\n            d = node.__dict__\n            d['data'] = d['nodeValue'] = node.data + data\n            return\n        node = minidom.Text()\n        d = node.__dict__\n        d['data'] = d['nodeValue'] = node.data + data\n        d['ownerDocument'] = self.document\n        _append_child(self.curNode, node)\n\n    def entity_decl_handler(self, entityName, is_parameter_entity, value,\n                            base, systemId, publicId, notationName):\n        if is_parameter_entity:\n            # we don't care about parameter entities for the DOM\n            return\n        if not self._options.entities:\n            return\n        node = self.document._create_entity(entityName, publicId,\n                                            systemId, notationName)\n        if value is not None:\n            # internal entity\n            # node *should* be readonly, but we'll cheat\n            child = self.document.createTextNode(value)\n            node.childNodes.append(child)\n        self.document.doctype.entities._seq.append(node)\n        if self._filter and self._filter.acceptNode(node) == FILTER_REJECT:\n            del self.document.doctype.entities._seq[-1]\n\n    def notation_decl_handler(self, notationName, base, systemId, publicId):\n        node = self.document._create_notation(notationName, publicId, systemId)\n        self.document.doctype.notations._seq.append(node)\n        if self._filter and self._filter.acceptNode(node) == FILTER_ACCEPT:\n            del self.document.doctype.notations._seq[-1]\n\n    def comment_handler(self, data):\n        node = self.document.createComment(data)\n        _append_child(self.curNode, node)\n        if self._filter and self._filter.acceptNode(node) == FILTER_REJECT:\n            self.curNode.removeChild(node)\n\n    def start_cdata_section_handler(self):\n        self._cdata = True\n        self._cdata_continue = False\n\n    def end_cdata_section_handler(self):\n        self._cdata = False\n        self._cdata_continue = False\n\n    def external_entity_ref_handler(self, context, base, systemId, publicId):\n        return 1\n\n    def first_element_handler(self, name, attributes):\n        if self._filter is None and not self._elem_info:\n            self._finish_end_element = id\n        self.getParser().StartElementHandler = self.start_element_handler\n        self.start_element_handler(name, attributes)\n\n    def start_element_handler(self, name, attributes):\n        node = self.document.createElement(name)\n        _append_child(self.curNode, node)\n        self.curNode = node\n\n        if attributes:\n            for i in range(0, len(attributes), 2):\n                a = minidom.Attr(attributes[i], EMPTY_NAMESPACE,\n                                 None, EMPTY_PREFIX)\n                value = attributes[i+1]\n                d = a.childNodes[0].__dict__\n                d['data'] = d['nodeValue'] = value\n                d = a.__dict__\n                d['value'] = d['nodeValue'] = value\n                d['ownerDocument'] = self.document\n                _set_attribute_node(node, a)\n\n        if node is not self.document.documentElement:\n            self._finish_start_element(node)\n\n    def _finish_start_element(self, node):\n        if self._filter:\n            # To be general, we'd have to call isSameNode(), but this\n            # is sufficient for minidom:\n            if node is self.document.documentElement:\n                return\n            filt = self._filter.startContainer(node)\n            if filt == FILTER_REJECT:\n                # ignore this node & all descendents\n                Rejecter(self)\n            elif filt == FILTER_SKIP:\n                # ignore this node, but make it's children become\n                # children of the parent node\n                Skipper(self)\n            else:\n                return\n            self.curNode = node.parentNode\n            node.parentNode.removeChild(node)\n            node.unlink()\n\n    # If this ever changes, Namespaces.end_element_handler() needs to\n    # be changed to match.\n    #\n    def end_element_handler(self, name):\n        curNode = self.curNode\n        self.curNode = curNode.parentNode\n        self._finish_end_element(curNode)\n\n    def _finish_end_element(self, curNode):\n        info = self._elem_info.get(curNode.tagName)\n        if info:\n            self._handle_white_text_nodes(curNode, info)\n        if self._filter:\n            if curNode is self.document.documentElement:\n                return\n            if self._filter.acceptNode(curNode) == FILTER_REJECT:\n                self.curNode.removeChild(curNode)\n                curNode.unlink()\n\n    def _handle_white_text_nodes(self, node, info):\n        if (self._options.whitespace_in_element_content\n            or not info.isElementContent()):\n            return\n\n        # We have element type information and should remove ignorable\n        # whitespace; identify for text nodes which contain only\n        # whitespace.\n        L = []\n        for child in node.childNodes:\n            if child.nodeType == TEXT_NODE and not child.data.strip():\n                L.append(child)\n\n        # Remove ignorable whitespace from the tree.\n        for child in L:\n            node.removeChild(child)\n\n    def element_decl_handler(self, name, model):\n        info = self._elem_info.get(name)\n        if info is None:\n            self._elem_info[name] = ElementInfo(name, model)\n        else:\n            assert info._model is None\n            info._model = model\n\n    def attlist_decl_handler(self, elem, name, type, default, required):\n        info = self._elem_info.get(elem)\n        if info is None:\n            info = ElementInfo(elem)\n            self._elem_info[elem] = info\n        info._attr_info.append(\n            [None, name, None, None, default, 0, type, required])\n\n    def xml_decl_handler(self, version, encoding, standalone):\n        self.document.version = version\n        self.document.encoding = encoding\n        # This is still a little ugly, thanks to the pyexpat API. ;-(\n        if standalone >= 0:\n            if standalone:\n                self.document.standalone = True\n            else:\n                self.document.standalone = False\n\n\n# Don't include FILTER_INTERRUPT, since that's checked separately\n# where allowed.\n_ALLOWED_FILTER_RETURNS = (FILTER_ACCEPT, FILTER_REJECT, FILTER_SKIP)\n\nclass FilterVisibilityController(object):\n    \"\"\"Wrapper around a DOMBuilderFilter which implements the checks\n    to make the whatToShow filter attribute work.\"\"\"\n\n    __slots__ = 'filter',\n\n    def __init__(self, filter):\n        self.filter = filter\n\n    def startContainer(self, node):\n        mask = self._nodetype_mask[node.nodeType]\n        if self.filter.whatToShow & mask:\n            val = self.filter.startContainer(node)\n            if val == FILTER_INTERRUPT:\n                raise ParseEscape\n            if val not in _ALLOWED_FILTER_RETURNS:\n                raise ValueError, \\\n                      \"startContainer() returned illegal value: \" + repr(val)\n            return val\n        else:\n            return FILTER_ACCEPT\n\n    def acceptNode(self, node):\n        mask = self._nodetype_mask[node.nodeType]\n        if self.filter.whatToShow & mask:\n            val = self.filter.acceptNode(node)\n            if val == FILTER_INTERRUPT:\n                raise ParseEscape\n            if val == FILTER_SKIP:\n                # move all child nodes to the parent, and remove this node\n                parent = node.parentNode\n                for child in node.childNodes[:]:\n                    parent.appendChild(child)\n                # node is handled by the caller\n                return FILTER_REJECT\n            if val not in _ALLOWED_FILTER_RETURNS:\n                raise ValueError, \\\n                      \"acceptNode() returned illegal value: \" + repr(val)\n            return val\n        else:\n            return FILTER_ACCEPT\n\n    _nodetype_mask = {\n        Node.ELEMENT_NODE:                NodeFilter.SHOW_ELEMENT,\n        Node.ATTRIBUTE_NODE:              NodeFilter.SHOW_ATTRIBUTE,\n        Node.TEXT_NODE:                   NodeFilter.SHOW_TEXT,\n        Node.CDATA_SECTION_NODE:          NodeFilter.SHOW_CDATA_SECTION,\n        Node.ENTITY_REFERENCE_NODE:       NodeFilter.SHOW_ENTITY_REFERENCE,\n        Node.ENTITY_NODE:                 NodeFilter.SHOW_ENTITY,\n        Node.PROCESSING_INSTRUCTION_NODE: NodeFilter.SHOW_PROCESSING_INSTRUCTION,\n        Node.COMMENT_NODE:                NodeFilter.SHOW_COMMENT,\n        Node.DOCUMENT_NODE:               NodeFilter.SHOW_DOCUMENT,\n        Node.DOCUMENT_TYPE_NODE:          NodeFilter.SHOW_DOCUMENT_TYPE,\n        Node.DOCUMENT_FRAGMENT_NODE:      NodeFilter.SHOW_DOCUMENT_FRAGMENT,\n        Node.NOTATION_NODE:               NodeFilter.SHOW_NOTATION,\n        }\n\n\nclass FilterCrutch(object):\n    __slots__ = '_builder', '_level', '_old_start', '_old_end'\n\n    def __init__(self, builder):\n        self._level = 0\n        self._builder = builder\n        parser = builder._parser\n        self._old_start = parser.StartElementHandler\n        self._old_end = parser.EndElementHandler\n        parser.StartElementHandler = self.start_element_handler\n        parser.EndElementHandler = self.end_element_handler\n\nclass Rejecter(FilterCrutch):\n    __slots__ = ()\n\n    def __init__(self, builder):\n        FilterCrutch.__init__(self, builder)\n        parser = builder._parser\n        for name in (\"ProcessingInstructionHandler\",\n                     \"CommentHandler\",\n                     \"CharacterDataHandler\",\n                     \"StartCdataSectionHandler\",\n                     \"EndCdataSectionHandler\",\n                     \"ExternalEntityRefHandler\",\n                     ):\n            setattr(parser, name, None)\n\n    def start_element_handler(self, *args):\n        self._level = self._level + 1\n\n    def end_element_handler(self, *args):\n        if self._level == 0:\n            # restore the old handlers\n            parser = self._builder._parser\n            self._builder.install(parser)\n            parser.StartElementHandler = self._old_start\n            parser.EndElementHandler = self._old_end\n        else:\n            self._level = self._level - 1\n\nclass Skipper(FilterCrutch):\n    __slots__ = ()\n\n    def start_element_handler(self, *args):\n        node = self._builder.curNode\n        self._old_start(*args)\n        if self._builder.curNode is not node:\n            self._level = self._level + 1\n\n    def end_element_handler(self, *args):\n        if self._level == 0:\n            # We're popping back out of the node we're skipping, so we\n            # shouldn't need to do anything but reset the handlers.\n            self._builder._parser.StartElementHandler = self._old_start\n            self._builder._parser.EndElementHandler = self._old_end\n            self._builder = None\n        else:\n            self._level = self._level - 1\n            self._old_end(*args)\n\n\n# framework document used by the fragment builder.\n# Takes a string for the doctype, subset string, and namespace attrs string.\n\n_FRAGMENT_BUILDER_INTERNAL_SYSTEM_ID = \\\n    \"http://xml.python.org/entities/fragment-builder/internal\"\n\n_FRAGMENT_BUILDER_TEMPLATE = (\n    '''\\\n<!DOCTYPE wrapper\n  %%s [\n  <!ENTITY fragment-builder-internal\n    SYSTEM \"%s\">\n%%s\n]>\n<wrapper %%s\n>&fragment-builder-internal;</wrapper>'''\n    % _FRAGMENT_BUILDER_INTERNAL_SYSTEM_ID)\n\n\nclass FragmentBuilder(ExpatBuilder):\n    \"\"\"Builder which constructs document fragments given XML source\n    text and a context node.\n\n    The context node is expected to provide information about the\n    namespace declarations which are in scope at the start of the\n    fragment.\n    \"\"\"\n\n    def __init__(self, context, options=None):\n        if context.nodeType == DOCUMENT_NODE:\n            self.originalDocument = context\n            self.context = context\n        else:\n            self.originalDocument = context.ownerDocument\n            self.context = context\n        ExpatBuilder.__init__(self, options)\n\n    def reset(self):\n        ExpatBuilder.reset(self)\n        self.fragment = None\n\n    def parseFile(self, file):\n        \"\"\"Parse a document fragment from a file object, returning the\n        fragment node.\"\"\"\n        return self.parseString(file.read())\n\n    def parseString(self, string):\n        \"\"\"Parse a document fragment from a string, returning the\n        fragment node.\"\"\"\n        self._source = string\n        parser = self.getParser()\n        doctype = self.originalDocument.doctype\n        ident = \"\"\n        if doctype:\n            subset = doctype.internalSubset or self._getDeclarations()\n            if doctype.publicId:\n                ident = ('PUBLIC \"%s\" \"%s\"'\n                         % (doctype.publicId, doctype.systemId))\n            elif doctype.systemId:\n                ident = 'SYSTEM \"%s\"' % doctype.systemId\n        else:\n            subset = \"\"\n        nsattrs = self._getNSattrs() # get ns decls from node's ancestors\n        document = _FRAGMENT_BUILDER_TEMPLATE % (ident, subset, nsattrs)\n        try:\n            parser.Parse(document, 1)\n        except:\n            self.reset()\n            raise\n        fragment = self.fragment\n        self.reset()\n##         self._parser = None\n        return fragment\n\n    def _getDeclarations(self):\n        \"\"\"Re-create the internal subset from the DocumentType node.\n\n        This is only needed if we don't already have the\n        internalSubset as a string.\n        \"\"\"\n        doctype = self.context.ownerDocument.doctype\n        s = \"\"\n        if doctype:\n            for i in range(doctype.notations.length):\n                notation = doctype.notations.item(i)\n                if s:\n                    s = s + \"\\n  \"\n                s = \"%s<!NOTATION %s\" % (s, notation.nodeName)\n                if notation.publicId:\n                    s = '%s PUBLIC \"%s\"\\n             \"%s\">' \\\n                        % (s, notation.publicId, notation.systemId)\n                else:\n                    s = '%s SYSTEM \"%s\">' % (s, notation.systemId)\n            for i in range(doctype.entities.length):\n                entity = doctype.entities.item(i)\n                if s:\n                    s = s + \"\\n  \"\n                s = \"%s<!ENTITY %s\" % (s, entity.nodeName)\n                if entity.publicId:\n                    s = '%s PUBLIC \"%s\"\\n             \"%s\"' \\\n                        % (s, entity.publicId, entity.systemId)\n                elif entity.systemId:\n                    s = '%s SYSTEM \"%s\"' % (s, entity.systemId)\n                else:\n                    s = '%s \"%s\"' % (s, entity.firstChild.data)\n                if entity.notationName:\n                    s = \"%s NOTATION %s\" % (s, entity.notationName)\n                s = s + \">\"\n        return s\n\n    def _getNSattrs(self):\n        return \"\"\n\n    def external_entity_ref_handler(self, context, base, systemId, publicId):\n        if systemId == _FRAGMENT_BUILDER_INTERNAL_SYSTEM_ID:\n            # this entref is the one that we made to put the subtree\n            # in; all of our given input is parsed in here.\n            old_document = self.document\n            old_cur_node = self.curNode\n            parser = self._parser.ExternalEntityParserCreate(context)\n            # put the real document back, parse into the fragment to return\n            self.document = self.originalDocument\n            self.fragment = self.document.createDocumentFragment()\n            self.curNode = self.fragment\n            try:\n                parser.Parse(self._source, 1)\n            finally:\n                self.curNode = old_cur_node\n                self.document = old_document\n                self._source = None\n            return -1\n        else:\n            return ExpatBuilder.external_entity_ref_handler(\n                self, context, base, systemId, publicId)\n\n\nclass Namespaces:\n    \"\"\"Mix-in class for builders; adds support for namespaces.\"\"\"\n\n    def _initNamespaces(self):\n        # list of (prefix, uri) ns declarations.  Namespace attrs are\n        # constructed from this and added to the element's attrs.\n        self._ns_ordered_prefixes = []\n\n    def createParser(self):\n        \"\"\"Create a new namespace-handling parser.\"\"\"\n        parser = expat.ParserCreate(namespace_separator=\" \")\n        parser.namespace_prefixes = True\n        return parser\n\n    def install(self, parser):\n        \"\"\"Insert the namespace-handlers onto the parser.\"\"\"\n        ExpatBuilder.install(self, parser)\n        if self._options.namespace_declarations:\n            parser.StartNamespaceDeclHandler = (\n                self.start_namespace_decl_handler)\n\n    def start_namespace_decl_handler(self, prefix, uri):\n        \"\"\"Push this namespace declaration on our storage.\"\"\"\n        self._ns_ordered_prefixes.append((prefix, uri))\n\n    def start_element_handler(self, name, attributes):\n        if ' ' in name:\n            uri, localname, prefix, qname = _parse_ns_name(self, name)\n        else:\n            uri = EMPTY_NAMESPACE\n            qname = name\n            localname = None\n            prefix = EMPTY_PREFIX\n        node = minidom.Element(qname, uri, prefix, localname)\n        node.ownerDocument = self.document\n        _append_child(self.curNode, node)\n        self.curNode = node\n\n        if self._ns_ordered_prefixes:\n            for prefix, uri in self._ns_ordered_prefixes:\n                if prefix:\n                    a = minidom.Attr(_intern(self, 'xmlns:' + prefix),\n                                     XMLNS_NAMESPACE, prefix, \"xmlns\")\n                else:\n                    a = minidom.Attr(\"xmlns\", XMLNS_NAMESPACE,\n                                     \"xmlns\", EMPTY_PREFIX)\n                d = a.childNodes[0].__dict__\n                d['data'] = d['nodeValue'] = uri\n                d = a.__dict__\n                d['value'] = d['nodeValue'] = uri\n                d['ownerDocument'] = self.document\n                _set_attribute_node(node, a)\n            del self._ns_ordered_prefixes[:]\n\n        if attributes:\n            _attrs = node._attrs\n            _attrsNS = node._attrsNS\n            for i in range(0, len(attributes), 2):\n                aname = attributes[i]\n                value = attributes[i+1]\n                if ' ' in aname:\n                    uri, localname, prefix, qname = _parse_ns_name(self, aname)\n                    a = minidom.Attr(qname, uri, localname, prefix)\n                    _attrs[qname] = a\n                    _attrsNS[(uri, localname)] = a\n                else:\n                    a = minidom.Attr(aname, EMPTY_NAMESPACE,\n                                     aname, EMPTY_PREFIX)\n                    _attrs[aname] = a\n                    _attrsNS[(EMPTY_NAMESPACE, aname)] = a\n                d = a.childNodes[0].__dict__\n                d['data'] = d['nodeValue'] = value\n                d = a.__dict__\n                d['ownerDocument'] = self.document\n                d['value'] = d['nodeValue'] = value\n                d['ownerElement'] = node\n\n    if __debug__:\n        # This only adds some asserts to the original\n        # end_element_handler(), so we only define this when -O is not\n        # used.  If changing one, be sure to check the other to see if\n        # it needs to be changed as well.\n        #\n        def end_element_handler(self, name):\n            curNode = self.curNode\n            if ' ' in name:\n                uri, localname, prefix, qname = _parse_ns_name(self, name)\n                assert (curNode.namespaceURI == uri\n                        and curNode.localName == localname\n                        and curNode.prefix == prefix), \\\n                        \"element stack messed up! (namespace)\"\n            else:\n                assert curNode.nodeName == name, \\\n                       \"element stack messed up - bad nodeName\"\n                assert curNode.namespaceURI == EMPTY_NAMESPACE, \\\n                       \"element stack messed up - bad namespaceURI\"\n            self.curNode = curNode.parentNode\n            self._finish_end_element(curNode)\n\n\nclass ExpatBuilderNS(Namespaces, ExpatBuilder):\n    \"\"\"Document builder that supports namespaces.\"\"\"\n\n    def reset(self):\n        ExpatBuilder.reset(self)\n        self._initNamespaces()\n\n\nclass FragmentBuilderNS(Namespaces, FragmentBuilder):\n    \"\"\"Fragment builder that supports namespaces.\"\"\"\n\n    def reset(self):\n        FragmentBuilder.reset(self)\n        self._initNamespaces()\n\n    def _getNSattrs(self):\n        \"\"\"Return string of namespace attributes from this element and\n        ancestors.\"\"\"\n        # XXX This needs to be re-written to walk the ancestors of the\n        # context to build up the namespace information from\n        # declarations, elements, and attributes found in context.\n        # Otherwise we have to store a bunch more data on the DOM\n        # (though that *might* be more reliable -- not clear).\n        attrs = \"\"\n        context = self.context\n        L = []\n        while context:\n            if hasattr(context, '_ns_prefix_uri'):\n                for prefix, uri in context._ns_prefix_uri.items():\n                    # add every new NS decl from context to L and attrs string\n                    if prefix in L:\n                        continue\n                    L.append(prefix)\n                    if prefix:\n                        declname = \"xmlns:\" + prefix\n                    else:\n                        declname = \"xmlns\"\n                    if attrs:\n                        attrs = \"%s\\n    %s='%s'\" % (attrs, declname, uri)\n                    else:\n                        attrs = \" %s='%s'\" % (declname, uri)\n            context = context.parentNode\n        return attrs\n\n\nclass ParseEscape(Exception):\n    \"\"\"Exception raised to short-circuit parsing in InternalSubsetExtractor.\"\"\"\n    pass\n\nclass InternalSubsetExtractor(ExpatBuilder):\n    \"\"\"XML processor which can rip out the internal document type subset.\"\"\"\n\n    subset = None\n\n    def getSubset(self):\n        \"\"\"Return the internal subset as a string.\"\"\"\n        return self.subset\n\n    def parseFile(self, file):\n        try:\n            ExpatBuilder.parseFile(self, file)\n        except ParseEscape:\n            pass\n\n    def parseString(self, string):\n        try:\n            ExpatBuilder.parseString(self, string)\n        except ParseEscape:\n            pass\n\n    def install(self, parser):\n        parser.StartDoctypeDeclHandler = self.start_doctype_decl_handler\n        parser.StartElementHandler = self.start_element_handler\n\n    def start_doctype_decl_handler(self, name, publicId, systemId,\n                                   has_internal_subset):\n        if has_internal_subset:\n            parser = self.getParser()\n            self.subset = []\n            parser.DefaultHandler = self.subset.append\n            parser.EndDoctypeDeclHandler = self.end_doctype_decl_handler\n        else:\n            raise ParseEscape()\n\n    def end_doctype_decl_handler(self):\n        s = ''.join(self.subset).replace('\\r\\n', '\\n').replace('\\r', '\\n')\n        self.subset = s\n        raise ParseEscape()\n\n    def start_element_handler(self, name, attrs):\n        raise ParseEscape()\n\n\ndef parse(file, namespaces=True):\n    \"\"\"Parse a document, returning the resulting Document node.\n\n    'file' may be either a file name or an open file object.\n    \"\"\"\n    if namespaces:\n        builder = ExpatBuilderNS()\n    else:\n        builder = ExpatBuilder()\n\n    if isinstance(file, StringTypes):\n        fp = open(file, 'rb')\n        try:\n            result = builder.parseFile(fp)\n        finally:\n            fp.close()\n    else:\n        result = builder.parseFile(file)\n    return result\n\n\ndef parseString(string, namespaces=True):\n    \"\"\"Parse a document from a string, returning the resulting\n    Document node.\n    \"\"\"\n    if namespaces:\n        builder = ExpatBuilderNS()\n    else:\n        builder = ExpatBuilder()\n    return builder.parseString(string)\n\n\ndef parseFragment(file, context, namespaces=True):\n    \"\"\"Parse a fragment of a document, given the context from which it\n    was originally extracted.  context should be the parent of the\n    node(s) which are in the fragment.\n\n    'file' may be either a file name or an open file object.\n    \"\"\"\n    if namespaces:\n        builder = FragmentBuilderNS(context)\n    else:\n        builder = FragmentBuilder(context)\n\n    if isinstance(file, StringTypes):\n        fp = open(file, 'rb')\n        try:\n            result = builder.parseFile(fp)\n        finally:\n            fp.close()\n    else:\n        result = builder.parseFile(file)\n    return result\n\n\ndef parseFragmentString(string, context, namespaces=True):\n    \"\"\"Parse a fragment of a document from a string, given the context\n    from which it was originally extracted.  context should be the\n    parent of the node(s) which are in the fragment.\n    \"\"\"\n    if namespaces:\n        builder = FragmentBuilderNS(context)\n    else:\n        builder = FragmentBuilder(context)\n    return builder.parseString(string)\n\n\ndef makeBuilder(options):\n    \"\"\"Create a builder based on an Options object.\"\"\"\n    if options.namespaces:\n        return ExpatBuilderNS(options)\n    else:\n        return ExpatBuilder(options)\n", 
    "xml.dom.minicompat": "\"\"\"Python version compatibility support for minidom.\"\"\"\n\n# This module should only be imported using \"import *\".\n#\n# The following names are defined:\n#\n#   NodeList      -- lightest possible NodeList implementation\n#\n#   EmptyNodeList -- lightest possible NodeList that is guaranteed to\n#                    remain empty (immutable)\n#\n#   StringTypes   -- tuple of defined string types\n#\n#   defproperty   -- function used in conjunction with GetattrMagic;\n#                    using these together is needed to make them work\n#                    as efficiently as possible in both Python 2.2+\n#                    and older versions.  For example:\n#\n#                        class MyClass(GetattrMagic):\n#                            def _get_myattr(self):\n#                                return something\n#\n#                        defproperty(MyClass, \"myattr\",\n#                                    \"return some value\")\n#\n#                    For Python 2.2 and newer, this will construct a\n#                    property object on the class, which avoids\n#                    needing to override __getattr__().  It will only\n#                    work for read-only attributes.\n#\n#                    For older versions of Python, inheriting from\n#                    GetattrMagic will use the traditional\n#                    __getattr__() hackery to achieve the same effect,\n#                    but less efficiently.\n#\n#                    defproperty() should be used for each version of\n#                    the relevant _get_<property>() function.\n\n__all__ = [\"NodeList\", \"EmptyNodeList\", \"StringTypes\", \"defproperty\"]\n\nimport xml.dom\n\ntry:\n    unicode\nexcept NameError:\n    StringTypes = type(''),\nelse:\n    StringTypes = type(''), type(unicode(''))\n\n\nclass NodeList(list):\n    __slots__ = ()\n\n    def item(self, index):\n        if 0 <= index < len(self):\n            return self[index]\n\n    def _get_length(self):\n        return len(self)\n\n    def _set_length(self, value):\n        raise xml.dom.NoModificationAllowedErr(\n            \"attempt to modify read-only attribute 'length'\")\n\n    length = property(_get_length, _set_length,\n                      doc=\"The number of nodes in the NodeList.\")\n\n    def __getstate__(self):\n        return list(self)\n\n    def __setstate__(self, state):\n        self[:] = state\n\n\nclass EmptyNodeList(tuple):\n    __slots__ = ()\n\n    def __add__(self, other):\n        NL = NodeList()\n        NL.extend(other)\n        return NL\n\n    def __radd__(self, other):\n        NL = NodeList()\n        NL.extend(other)\n        return NL\n\n    def item(self, index):\n        return None\n\n    def _get_length(self):\n        return 0\n\n    def _set_length(self, value):\n        raise xml.dom.NoModificationAllowedErr(\n            \"attempt to modify read-only attribute 'length'\")\n\n    length = property(_get_length, _set_length,\n                      doc=\"The number of nodes in the NodeList.\")\n\n\ndef defproperty(klass, name, doc):\n    get = getattr(klass, (\"_get_\" + name)).im_func\n    def set(self, value, name=name):\n        raise xml.dom.NoModificationAllowedErr(\n            \"attempt to modify read-only attribute \" + repr(name))\n    assert not hasattr(klass, \"_set_\" + name), \\\n           \"expected not to find _set_\" + name\n    prop = property(get, set, doc=doc)\n    setattr(klass, name, prop)\n", 
    "xml.dom.minidom": "\"\"\"Simple implementation of the Level 1 DOM.\n\nNamespaces and other minor Level 2 features are also supported.\n\nparse(\"foo.xml\")\n\nparseString(\"<foo><bar/></foo>\")\n\nTodo:\n=====\n * convenience methods for getting elements and text.\n * more testing\n * bring some of the writer and linearizer code into conformance with this\n        interface\n * SAX 2 namespaces\n\"\"\"\n\nimport xml.dom\n\nfrom xml.dom import EMPTY_NAMESPACE, EMPTY_PREFIX, XMLNS_NAMESPACE, domreg\nfrom xml.dom.minicompat import *\nfrom xml.dom.xmlbuilder import DOMImplementationLS, DocumentLS\n\n# This is used by the ID-cache invalidation checks; the list isn't\n# actually complete, since the nodes being checked will never be the\n# DOCUMENT_NODE or DOCUMENT_FRAGMENT_NODE.  (The node being checked is\n# the node being added or removed, not the node being modified.)\n#\n_nodeTypes_with_children = (xml.dom.Node.ELEMENT_NODE,\n                            xml.dom.Node.ENTITY_REFERENCE_NODE)\n\n\nclass Node(xml.dom.Node):\n    namespaceURI = None # this is non-null only for elements and attributes\n    parentNode = None\n    ownerDocument = None\n    nextSibling = None\n    previousSibling = None\n\n    prefix = EMPTY_PREFIX # non-null only for NS elements and attributes\n\n    def __nonzero__(self):\n        return True\n\n    def toxml(self, encoding = None):\n        return self.toprettyxml(\"\", \"\", encoding)\n\n    def toprettyxml(self, indent=\"\\t\", newl=\"\\n\", encoding = None):\n        # indent = the indentation string to prepend, per level\n        # newl = the newline string to append\n        writer = _get_StringIO()\n        if encoding is not None:\n            import codecs\n            # Can't use codecs.getwriter to preserve 2.0 compatibility\n            writer = codecs.lookup(encoding)[3](writer)\n        if self.nodeType == Node.DOCUMENT_NODE:\n            # Can pass encoding only to document, to put it into XML header\n            self.writexml(writer, \"\", indent, newl, encoding)\n        else:\n            self.writexml(writer, \"\", indent, newl)\n        return writer.getvalue()\n\n    def hasChildNodes(self):\n        if self.childNodes:\n            return True\n        else:\n            return False\n\n    def _get_childNodes(self):\n        return self.childNodes\n\n    def _get_firstChild(self):\n        if self.childNodes:\n            return self.childNodes[0]\n\n    def _get_lastChild(self):\n        if self.childNodes:\n            return self.childNodes[-1]\n\n    def insertBefore(self, newChild, refChild):\n        if newChild.nodeType == self.DOCUMENT_FRAGMENT_NODE:\n            for c in tuple(newChild.childNodes):\n                self.insertBefore(c, refChild)\n            ### The DOM does not clearly specify what to return in this case\n            return newChild\n        if newChild.nodeType not in self._child_node_types:\n            raise xml.dom.HierarchyRequestErr(\n                \"%s cannot be child of %s\" % (repr(newChild), repr(self)))\n        if newChild.parentNode is not None:\n            newChild.parentNode.removeChild(newChild)\n        if refChild is None:\n            self.appendChild(newChild)\n        else:\n            try:\n                index = self.childNodes.index(refChild)\n            except ValueError:\n                raise xml.dom.NotFoundErr()\n            if newChild.nodeType in _nodeTypes_with_children:\n                _clear_id_cache(self)\n            self.childNodes.insert(index, newChild)\n            newChild.nextSibling = refChild\n            refChild.previousSibling = newChild\n            if index:\n                node = self.childNodes[index-1]\n                node.nextSibling = newChild\n                newChild.previousSibling = node\n            else:\n                newChild.previousSibling = None\n            newChild.parentNode = self\n        return newChild\n\n    def appendChild(self, node):\n        if node.nodeType == self.DOCUMENT_FRAGMENT_NODE:\n            for c in tuple(node.childNodes):\n                self.appendChild(c)\n            ### The DOM does not clearly specify what to return in this case\n            return node\n        if node.nodeType not in self._child_node_types:\n            raise xml.dom.HierarchyRequestErr(\n                \"%s cannot be child of %s\" % (repr(node), repr(self)))\n        elif node.nodeType in _nodeTypes_with_children:\n            _clear_id_cache(self)\n        if node.parentNode is not None:\n            node.parentNode.removeChild(node)\n        _append_child(self, node)\n        node.nextSibling = None\n        return node\n\n    def replaceChild(self, newChild, oldChild):\n        if newChild.nodeType == self.DOCUMENT_FRAGMENT_NODE:\n            refChild = oldChild.nextSibling\n            self.removeChild(oldChild)\n            return self.insertBefore(newChild, refChild)\n        if newChild.nodeType not in self._child_node_types:\n            raise xml.dom.HierarchyRequestErr(\n                \"%s cannot be child of %s\" % (repr(newChild), repr(self)))\n        if newChild is oldChild:\n            return\n        if newChild.parentNode is not None:\n            newChild.parentNode.removeChild(newChild)\n        try:\n            index = self.childNodes.index(oldChild)\n        except ValueError:\n            raise xml.dom.NotFoundErr()\n        self.childNodes[index] = newChild\n        newChild.parentNode = self\n        oldChild.parentNode = None\n        if (newChild.nodeType in _nodeTypes_with_children\n            or oldChild.nodeType in _nodeTypes_with_children):\n            _clear_id_cache(self)\n        newChild.nextSibling = oldChild.nextSibling\n        newChild.previousSibling = oldChild.previousSibling\n        oldChild.nextSibling = None\n        oldChild.previousSibling = None\n        if newChild.previousSibling:\n            newChild.previousSibling.nextSibling = newChild\n        if newChild.nextSibling:\n            newChild.nextSibling.previousSibling = newChild\n        return oldChild\n\n    def removeChild(self, oldChild):\n        try:\n            self.childNodes.remove(oldChild)\n        except ValueError:\n            raise xml.dom.NotFoundErr()\n        if oldChild.nextSibling is not None:\n            oldChild.nextSibling.previousSibling = oldChild.previousSibling\n        if oldChild.previousSibling is not None:\n            oldChild.previousSibling.nextSibling = oldChild.nextSibling\n        oldChild.nextSibling = oldChild.previousSibling = None\n        if oldChild.nodeType in _nodeTypes_with_children:\n            _clear_id_cache(self)\n\n        oldChild.parentNode = None\n        return oldChild\n\n    def normalize(self):\n        L = []\n        for child in self.childNodes:\n            if child.nodeType == Node.TEXT_NODE:\n                if not child.data:\n                    # empty text node; discard\n                    if L:\n                        L[-1].nextSibling = child.nextSibling\n                    if child.nextSibling:\n                        child.nextSibling.previousSibling = child.previousSibling\n                    child.unlink()\n                elif L and L[-1].nodeType == child.nodeType:\n                    # collapse text node\n                    node = L[-1]\n                    node.data = node.data + child.data\n                    node.nextSibling = child.nextSibling\n                    if child.nextSibling:\n                        child.nextSibling.previousSibling = node\n                    child.unlink()\n                else:\n                    L.append(child)\n            else:\n                L.append(child)\n                if child.nodeType == Node.ELEMENT_NODE:\n                    child.normalize()\n        self.childNodes[:] = L\n\n    def cloneNode(self, deep):\n        return _clone_node(self, deep, self.ownerDocument or self)\n\n    def isSupported(self, feature, version):\n        return self.ownerDocument.implementation.hasFeature(feature, version)\n\n    def _get_localName(self):\n        # Overridden in Element and Attr where localName can be Non-Null\n        return None\n\n    # Node interfaces from Level 3 (WD 9 April 2002)\n\n    def isSameNode(self, other):\n        return self is other\n\n    def getInterface(self, feature):\n        if self.isSupported(feature, None):\n            return self\n        else:\n            return None\n\n    # The \"user data\" functions use a dictionary that is only present\n    # if some user data has been set, so be careful not to assume it\n    # exists.\n\n    def getUserData(self, key):\n        try:\n            return self._user_data[key][0]\n        except (AttributeError, KeyError):\n            return None\n\n    def setUserData(self, key, data, handler):\n        old = None\n        try:\n            d = self._user_data\n        except AttributeError:\n            d = {}\n            self._user_data = d\n        if key in d:\n            old = d[key][0]\n        if data is None:\n            # ignore handlers passed for None\n            handler = None\n            if old is not None:\n                del d[key]\n        else:\n            d[key] = (data, handler)\n        return old\n\n    def _call_user_data_handler(self, operation, src, dst):\n        if hasattr(self, \"_user_data\"):\n            for key, (data, handler) in self._user_data.items():\n                if handler is not None:\n                    handler.handle(operation, key, data, src, dst)\n\n    # minidom-specific API:\n\n    def unlink(self):\n        self.parentNode = self.ownerDocument = None\n        if self.childNodes:\n            for child in self.childNodes:\n                child.unlink()\n            self.childNodes = NodeList()\n        self.previousSibling = None\n        self.nextSibling = None\n\ndefproperty(Node, \"firstChild\", doc=\"First child node, or None.\")\ndefproperty(Node, \"lastChild\",  doc=\"Last child node, or None.\")\ndefproperty(Node, \"localName\",  doc=\"Namespace-local name of this node.\")\n\n\ndef _append_child(self, node):\n    # fast path with less checks; usable by DOM builders if careful\n    childNodes = self.childNodes\n    if childNodes:\n        last = childNodes[-1]\n        node.__dict__[\"previousSibling\"] = last\n        last.__dict__[\"nextSibling\"] = node\n    childNodes.append(node)\n    node.__dict__[\"parentNode\"] = self\n\ndef _in_document(node):\n    # return True iff node is part of a document tree\n    while node is not None:\n        if node.nodeType == Node.DOCUMENT_NODE:\n            return True\n        node = node.parentNode\n    return False\n\ndef _write_data(writer, data):\n    \"Writes datachars to writer.\"\n    if data:\n        data = data.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\"). \\\n                    replace(\"\\\"\", \"&quot;\").replace(\">\", \"&gt;\")\n        writer.write(data)\n\ndef _get_elements_by_tagName_helper(parent, name, rc):\n    for node in parent.childNodes:\n        if node.nodeType == Node.ELEMENT_NODE and \\\n            (name == \"*\" or node.tagName == name):\n            rc.append(node)\n        _get_elements_by_tagName_helper(node, name, rc)\n    return rc\n\ndef _get_elements_by_tagName_ns_helper(parent, nsURI, localName, rc):\n    for node in parent.childNodes:\n        if node.nodeType == Node.ELEMENT_NODE:\n            if ((localName == \"*\" or node.localName == localName) and\n                (nsURI == \"*\" or node.namespaceURI == nsURI)):\n                rc.append(node)\n            _get_elements_by_tagName_ns_helper(node, nsURI, localName, rc)\n    return rc\n\nclass DocumentFragment(Node):\n    nodeType = Node.DOCUMENT_FRAGMENT_NODE\n    nodeName = \"#document-fragment\"\n    nodeValue = None\n    attributes = None\n    parentNode = None\n    _child_node_types = (Node.ELEMENT_NODE,\n                         Node.TEXT_NODE,\n                         Node.CDATA_SECTION_NODE,\n                         Node.ENTITY_REFERENCE_NODE,\n                         Node.PROCESSING_INSTRUCTION_NODE,\n                         Node.COMMENT_NODE,\n                         Node.NOTATION_NODE)\n\n    def __init__(self):\n        self.childNodes = NodeList()\n\n\nclass Attr(Node):\n    nodeType = Node.ATTRIBUTE_NODE\n    attributes = None\n    ownerElement = None\n    specified = False\n    _is_id = False\n\n    _child_node_types = (Node.TEXT_NODE, Node.ENTITY_REFERENCE_NODE)\n\n    def __init__(self, qName, namespaceURI=EMPTY_NAMESPACE, localName=None,\n                 prefix=None):\n        # skip setattr for performance\n        d = self.__dict__\n        d[\"nodeName\"] = d[\"name\"] = qName\n        d[\"namespaceURI\"] = namespaceURI\n        d[\"prefix\"] = prefix\n        d['childNodes'] = NodeList()\n\n        # Add the single child node that represents the value of the attr\n        self.childNodes.append(Text())\n\n        # nodeValue and value are set elsewhere\n\n    def _get_localName(self):\n        return self.nodeName.split(\":\", 1)[-1]\n\n    def _get_specified(self):\n        return self.specified\n\n    def __setattr__(self, name, value):\n        d = self.__dict__\n        if name in (\"value\", \"nodeValue\"):\n            d[\"value\"] = d[\"nodeValue\"] = value\n            d2 = self.childNodes[0].__dict__\n            d2[\"data\"] = d2[\"nodeValue\"] = value\n            if self.ownerElement is not None:\n                _clear_id_cache(self.ownerElement)\n        elif name in (\"name\", \"nodeName\"):\n            d[\"name\"] = d[\"nodeName\"] = value\n            if self.ownerElement is not None:\n                _clear_id_cache(self.ownerElement)\n        else:\n            d[name] = value\n\n    def _set_prefix(self, prefix):\n        nsuri = self.namespaceURI\n        if prefix == \"xmlns\":\n            if nsuri and nsuri != XMLNS_NAMESPACE:\n                raise xml.dom.NamespaceErr(\n                    \"illegal use of 'xmlns' prefix for the wrong namespace\")\n        d = self.__dict__\n        d['prefix'] = prefix\n        if prefix is None:\n            newName = self.localName\n        else:\n            newName = \"%s:%s\" % (prefix, self.localName)\n        if self.ownerElement:\n            _clear_id_cache(self.ownerElement)\n        d['nodeName'] = d['name'] = newName\n\n    def _set_value(self, value):\n        d = self.__dict__\n        d['value'] = d['nodeValue'] = value\n        if self.ownerElement:\n            _clear_id_cache(self.ownerElement)\n        self.childNodes[0].data = value\n\n    def unlink(self):\n        # This implementation does not call the base implementation\n        # since most of that is not needed, and the expense of the\n        # method call is not warranted.  We duplicate the removal of\n        # children, but that's all we needed from the base class.\n        elem = self.ownerElement\n        if elem is not None:\n            del elem._attrs[self.nodeName]\n            del elem._attrsNS[(self.namespaceURI, self.localName)]\n            if self._is_id:\n                self._is_id = False\n                elem._magic_id_nodes -= 1\n                self.ownerDocument._magic_id_count -= 1\n        for child in self.childNodes:\n            child.unlink()\n        del self.childNodes[:]\n\n    def _get_isId(self):\n        if self._is_id:\n            return True\n        doc = self.ownerDocument\n        elem = self.ownerElement\n        if doc is None or elem is None:\n            return False\n\n        info = doc._get_elem_info(elem)\n        if info is None:\n            return False\n        if self.namespaceURI:\n            return info.isIdNS(self.namespaceURI, self.localName)\n        else:\n            return info.isId(self.nodeName)\n\n    def _get_schemaType(self):\n        doc = self.ownerDocument\n        elem = self.ownerElement\n        if doc is None or elem is None:\n            return _no_type\n\n        info = doc._get_elem_info(elem)\n        if info is None:\n            return _no_type\n        if self.namespaceURI:\n            return info.getAttributeTypeNS(self.namespaceURI, self.localName)\n        else:\n            return info.getAttributeType(self.nodeName)\n\ndefproperty(Attr, \"isId\",       doc=\"True if this attribute is an ID.\")\ndefproperty(Attr, \"localName\",  doc=\"Namespace-local name of this attribute.\")\ndefproperty(Attr, \"schemaType\", doc=\"Schema type for this attribute.\")\n\n\nclass NamedNodeMap(object):\n    \"\"\"The attribute list is a transient interface to the underlying\n    dictionaries.  Mutations here will change the underlying element's\n    dictionary.\n\n    Ordering is imposed artificially and does not reflect the order of\n    attributes as found in an input document.\n    \"\"\"\n\n    __slots__ = ('_attrs', '_attrsNS', '_ownerElement')\n\n    def __init__(self, attrs, attrsNS, ownerElement):\n        self._attrs = attrs\n        self._attrsNS = attrsNS\n        self._ownerElement = ownerElement\n\n    def _get_length(self):\n        return len(self._attrs)\n\n    def item(self, index):\n        try:\n            return self[self._attrs.keys()[index]]\n        except IndexError:\n            return None\n\n    def items(self):\n        L = []\n        for node in self._attrs.values():\n            L.append((node.nodeName, node.value))\n        return L\n\n    def itemsNS(self):\n        L = []\n        for node in self._attrs.values():\n            L.append(((node.namespaceURI, node.localName), node.value))\n        return L\n\n    def has_key(self, key):\n        if isinstance(key, StringTypes):\n            return key in self._attrs\n        else:\n            return key in self._attrsNS\n\n    def keys(self):\n        return self._attrs.keys()\n\n    def keysNS(self):\n        return self._attrsNS.keys()\n\n    def values(self):\n        return self._attrs.values()\n\n    def get(self, name, value=None):\n        return self._attrs.get(name, value)\n\n    __len__ = _get_length\n\n    __hash__ = None # Mutable type can't be correctly hashed\n    def __cmp__(self, other):\n        if self._attrs is getattr(other, \"_attrs\", None):\n            return 0\n        else:\n            return cmp(id(self), id(other))\n\n    def __getitem__(self, attname_or_tuple):\n        if isinstance(attname_or_tuple, tuple):\n            return self._attrsNS[attname_or_tuple]\n        else:\n            return self._attrs[attname_or_tuple]\n\n    # same as set\n    def __setitem__(self, attname, value):\n        if isinstance(value, StringTypes):\n            try:\n                node = self._attrs[attname]\n            except KeyError:\n                node = Attr(attname)\n                node.ownerDocument = self._ownerElement.ownerDocument\n                self.setNamedItem(node)\n            node.value = value\n        else:\n            if not isinstance(value, Attr):\n                raise TypeError, \"value must be a string or Attr object\"\n            node = value\n            self.setNamedItem(node)\n\n    def getNamedItem(self, name):\n        try:\n            return self._attrs[name]\n        except KeyError:\n            return None\n\n    def getNamedItemNS(self, namespaceURI, localName):\n        try:\n            return self._attrsNS[(namespaceURI, localName)]\n        except KeyError:\n            return None\n\n    def removeNamedItem(self, name):\n        n = self.getNamedItem(name)\n        if n is not None:\n            _clear_id_cache(self._ownerElement)\n            del self._attrs[n.nodeName]\n            del self._attrsNS[(n.namespaceURI, n.localName)]\n            if 'ownerElement' in n.__dict__:\n                n.__dict__['ownerElement'] = None\n            return n\n        else:\n            raise xml.dom.NotFoundErr()\n\n    def removeNamedItemNS(self, namespaceURI, localName):\n        n = self.getNamedItemNS(namespaceURI, localName)\n        if n is not None:\n            _clear_id_cache(self._ownerElement)\n            del self._attrsNS[(n.namespaceURI, n.localName)]\n            del self._attrs[n.nodeName]\n            if 'ownerElement' in n.__dict__:\n                n.__dict__['ownerElement'] = None\n            return n\n        else:\n            raise xml.dom.NotFoundErr()\n\n    def setNamedItem(self, node):\n        if not isinstance(node, Attr):\n            raise xml.dom.HierarchyRequestErr(\n                \"%s cannot be child of %s\" % (repr(node), repr(self)))\n        old = self._attrs.get(node.name)\n        if old:\n            old.unlink()\n        self._attrs[node.name] = node\n        self._attrsNS[(node.namespaceURI, node.localName)] = node\n        node.ownerElement = self._ownerElement\n        _clear_id_cache(node.ownerElement)\n        return old\n\n    def setNamedItemNS(self, node):\n        return self.setNamedItem(node)\n\n    def __delitem__(self, attname_or_tuple):\n        node = self[attname_or_tuple]\n        _clear_id_cache(node.ownerElement)\n        node.unlink()\n\n    def __getstate__(self):\n        return self._attrs, self._attrsNS, self._ownerElement\n\n    def __setstate__(self, state):\n        self._attrs, self._attrsNS, self._ownerElement = state\n\ndefproperty(NamedNodeMap, \"length\",\n            doc=\"Number of nodes in the NamedNodeMap.\")\n\nAttributeList = NamedNodeMap\n\n\nclass TypeInfo(object):\n    __slots__ = 'namespace', 'name'\n\n    def __init__(self, namespace, name):\n        self.namespace = namespace\n        self.name = name\n\n    def __repr__(self):\n        if self.namespace:\n            return \"<TypeInfo %r (from %r)>\" % (self.name, self.namespace)\n        else:\n            return \"<TypeInfo %r>\" % self.name\n\n    def _get_name(self):\n        return self.name\n\n    def _get_namespace(self):\n        return self.namespace\n\n_no_type = TypeInfo(None, None)\n\nclass Element(Node):\n    nodeType = Node.ELEMENT_NODE\n    nodeValue = None\n    schemaType = _no_type\n\n    _magic_id_nodes = 0\n\n    _child_node_types = (Node.ELEMENT_NODE,\n                         Node.PROCESSING_INSTRUCTION_NODE,\n                         Node.COMMENT_NODE,\n                         Node.TEXT_NODE,\n                         Node.CDATA_SECTION_NODE,\n                         Node.ENTITY_REFERENCE_NODE)\n\n    def __init__(self, tagName, namespaceURI=EMPTY_NAMESPACE, prefix=None,\n                 localName=None):\n        self.tagName = self.nodeName = tagName\n        self.prefix = prefix\n        self.namespaceURI = namespaceURI\n        self.childNodes = NodeList()\n\n        self._attrs = {}   # attributes are double-indexed:\n        self._attrsNS = {} #    tagName -> Attribute\n                           #    URI,localName -> Attribute\n                           # in the future: consider lazy generation\n                           # of attribute objects this is too tricky\n                           # for now because of headaches with\n                           # namespaces.\n\n    def _get_localName(self):\n        return self.tagName.split(\":\", 1)[-1]\n\n    def _get_tagName(self):\n        return self.tagName\n\n    def unlink(self):\n        for attr in self._attrs.values():\n            attr.unlink()\n        self._attrs = None\n        self._attrsNS = None\n        Node.unlink(self)\n\n    def getAttribute(self, attname):\n        try:\n            return self._attrs[attname].value\n        except KeyError:\n            return \"\"\n\n    def getAttributeNS(self, namespaceURI, localName):\n        try:\n            return self._attrsNS[(namespaceURI, localName)].value\n        except KeyError:\n            return \"\"\n\n    def setAttribute(self, attname, value):\n        attr = self.getAttributeNode(attname)\n        if attr is None:\n            attr = Attr(attname)\n            # for performance\n            d = attr.__dict__\n            d[\"value\"] = d[\"nodeValue\"] = value\n            d[\"ownerDocument\"] = self.ownerDocument\n            self.setAttributeNode(attr)\n        elif value != attr.value:\n            d = attr.__dict__\n            d[\"value\"] = d[\"nodeValue\"] = value\n            if attr.isId:\n                _clear_id_cache(self)\n\n    def setAttributeNS(self, namespaceURI, qualifiedName, value):\n        prefix, localname = _nssplit(qualifiedName)\n        attr = self.getAttributeNodeNS(namespaceURI, localname)\n        if attr is None:\n            # for performance\n            attr = Attr(qualifiedName, namespaceURI, localname, prefix)\n            d = attr.__dict__\n            d[\"prefix\"] = prefix\n            d[\"nodeName\"] = qualifiedName\n            d[\"value\"] = d[\"nodeValue\"] = value\n            d[\"ownerDocument\"] = self.ownerDocument\n            self.setAttributeNode(attr)\n        else:\n            d = attr.__dict__\n            if value != attr.value:\n                d[\"value\"] = d[\"nodeValue\"] = value\n                if attr.isId:\n                    _clear_id_cache(self)\n            if attr.prefix != prefix:\n                d[\"prefix\"] = prefix\n                d[\"nodeName\"] = qualifiedName\n\n    def getAttributeNode(self, attrname):\n        return self._attrs.get(attrname)\n\n    def getAttributeNodeNS(self, namespaceURI, localName):\n        return self._attrsNS.get((namespaceURI, localName))\n\n    def setAttributeNode(self, attr):\n        if attr.ownerElement not in (None, self):\n            raise xml.dom.InuseAttributeErr(\"attribute node already owned\")\n        old1 = self._attrs.get(attr.name, None)\n        if old1 is not None:\n            self.removeAttributeNode(old1)\n        old2 = self._attrsNS.get((attr.namespaceURI, attr.localName), None)\n        if old2 is not None and old2 is not old1:\n            self.removeAttributeNode(old2)\n        _set_attribute_node(self, attr)\n\n        if old1 is not attr:\n            # It might have already been part of this node, in which case\n            # it doesn't represent a change, and should not be returned.\n            return old1\n        if old2 is not attr:\n            return old2\n\n    setAttributeNodeNS = setAttributeNode\n\n    def removeAttribute(self, name):\n        try:\n            attr = self._attrs[name]\n        except KeyError:\n            raise xml.dom.NotFoundErr()\n        self.removeAttributeNode(attr)\n\n    def removeAttributeNS(self, namespaceURI, localName):\n        try:\n            attr = self._attrsNS[(namespaceURI, localName)]\n        except KeyError:\n            raise xml.dom.NotFoundErr()\n        self.removeAttributeNode(attr)\n\n    def removeAttributeNode(self, node):\n        if node is None:\n            raise xml.dom.NotFoundErr()\n        try:\n            self._attrs[node.name]\n        except KeyError:\n            raise xml.dom.NotFoundErr()\n        _clear_id_cache(self)\n        node.unlink()\n        # Restore this since the node is still useful and otherwise\n        # unlinked\n        node.ownerDocument = self.ownerDocument\n\n    removeAttributeNodeNS = removeAttributeNode\n\n    def hasAttribute(self, name):\n        return name in self._attrs\n\n    def hasAttributeNS(self, namespaceURI, localName):\n        return (namespaceURI, localName) in self._attrsNS\n\n    def getElementsByTagName(self, name):\n        return _get_elements_by_tagName_helper(self, name, NodeList())\n\n    def getElementsByTagNameNS(self, namespaceURI, localName):\n        return _get_elements_by_tagName_ns_helper(\n            self, namespaceURI, localName, NodeList())\n\n    def __repr__(self):\n        return \"<DOM Element: %s at %#x>\" % (self.tagName, id(self))\n\n    def writexml(self, writer, indent=\"\", addindent=\"\", newl=\"\"):\n        # indent = current indentation\n        # addindent = indentation to add to higher levels\n        # newl = newline string\n        writer.write(indent+\"<\" + self.tagName)\n\n        attrs = self._get_attributes()\n        a_names = attrs.keys()\n        a_names.sort()\n\n        for a_name in a_names:\n            writer.write(\" %s=\\\"\" % a_name)\n            _write_data(writer, attrs[a_name].value)\n            writer.write(\"\\\"\")\n        if self.childNodes:\n            writer.write(\">\")\n            if (len(self.childNodes) == 1 and\n                self.childNodes[0].nodeType == Node.TEXT_NODE):\n                self.childNodes[0].writexml(writer, '', '', '')\n            else:\n                writer.write(newl)\n                for node in self.childNodes:\n                    node.writexml(writer, indent+addindent, addindent, newl)\n                writer.write(indent)\n            writer.write(\"</%s>%s\" % (self.tagName, newl))\n        else:\n            writer.write(\"/>%s\"%(newl))\n\n    def _get_attributes(self):\n        return NamedNodeMap(self._attrs, self._attrsNS, self)\n\n    def hasAttributes(self):\n        if self._attrs:\n            return True\n        else:\n            return False\n\n    # DOM Level 3 attributes, based on the 22 Oct 2002 draft\n\n    def setIdAttribute(self, name):\n        idAttr = self.getAttributeNode(name)\n        self.setIdAttributeNode(idAttr)\n\n    def setIdAttributeNS(self, namespaceURI, localName):\n        idAttr = self.getAttributeNodeNS(namespaceURI, localName)\n        self.setIdAttributeNode(idAttr)\n\n    def setIdAttributeNode(self, idAttr):\n        if idAttr is None or not self.isSameNode(idAttr.ownerElement):\n            raise xml.dom.NotFoundErr()\n        if _get_containing_entref(self) is not None:\n            raise xml.dom.NoModificationAllowedErr()\n        if not idAttr._is_id:\n            idAttr.__dict__['_is_id'] = True\n            self._magic_id_nodes += 1\n            self.ownerDocument._magic_id_count += 1\n            _clear_id_cache(self)\n\ndefproperty(Element, \"attributes\",\n            doc=\"NamedNodeMap of attributes on the element.\")\ndefproperty(Element, \"localName\",\n            doc=\"Namespace-local name of this element.\")\n\n\ndef _set_attribute_node(element, attr):\n    _clear_id_cache(element)\n    element._attrs[attr.name] = attr\n    element._attrsNS[(attr.namespaceURI, attr.localName)] = attr\n\n    # This creates a circular reference, but Element.unlink()\n    # breaks the cycle since the references to the attribute\n    # dictionaries are tossed.\n    attr.__dict__['ownerElement'] = element\n\n\nclass Childless:\n    \"\"\"Mixin that makes childless-ness easy to implement and avoids\n    the complexity of the Node methods that deal with children.\n    \"\"\"\n\n    attributes = None\n    childNodes = EmptyNodeList()\n    firstChild = None\n    lastChild = None\n\n    def _get_firstChild(self):\n        return None\n\n    def _get_lastChild(self):\n        return None\n\n    def appendChild(self, node):\n        raise xml.dom.HierarchyRequestErr(\n            self.nodeName + \" nodes cannot have children\")\n\n    def hasChildNodes(self):\n        return False\n\n    def insertBefore(self, newChild, refChild):\n        raise xml.dom.HierarchyRequestErr(\n            self.nodeName + \" nodes do not have children\")\n\n    def removeChild(self, oldChild):\n        raise xml.dom.NotFoundErr(\n            self.nodeName + \" nodes do not have children\")\n\n    def normalize(self):\n        # For childless nodes, normalize() has nothing to do.\n        pass\n\n    def replaceChild(self, newChild, oldChild):\n        raise xml.dom.HierarchyRequestErr(\n            self.nodeName + \" nodes do not have children\")\n\n\nclass ProcessingInstruction(Childless, Node):\n    nodeType = Node.PROCESSING_INSTRUCTION_NODE\n\n    def __init__(self, target, data):\n        self.target = self.nodeName = target\n        self.data = self.nodeValue = data\n\n    def _get_data(self):\n        return self.data\n    def _set_data(self, value):\n        d = self.__dict__\n        d['data'] = d['nodeValue'] = value\n\n    def _get_target(self):\n        return self.target\n    def _set_target(self, value):\n        d = self.__dict__\n        d['target'] = d['nodeName'] = value\n\n    def __setattr__(self, name, value):\n        if name == \"data\" or name == \"nodeValue\":\n            self.__dict__['data'] = self.__dict__['nodeValue'] = value\n        elif name == \"target\" or name == \"nodeName\":\n            self.__dict__['target'] = self.__dict__['nodeName'] = value\n        else:\n            self.__dict__[name] = value\n\n    def writexml(self, writer, indent=\"\", addindent=\"\", newl=\"\"):\n        writer.write(\"%s<?%s %s?>%s\" % (indent,self.target, self.data, newl))\n\n\nclass CharacterData(Childless, Node):\n    def _get_length(self):\n        return len(self.data)\n    __len__ = _get_length\n\n    def _get_data(self):\n        return self.__dict__['data']\n    def _set_data(self, data):\n        d = self.__dict__\n        d['data'] = d['nodeValue'] = data\n\n    _get_nodeValue = _get_data\n    _set_nodeValue = _set_data\n\n    def __setattr__(self, name, value):\n        if name == \"data\" or name == \"nodeValue\":\n            self.__dict__['data'] = self.__dict__['nodeValue'] = value\n        else:\n            self.__dict__[name] = value\n\n    def __repr__(self):\n        data = self.data\n        if len(data) > 10:\n            dotdotdot = \"...\"\n        else:\n            dotdotdot = \"\"\n        return '<DOM %s node \"%r%s\">' % (\n            self.__class__.__name__, data[0:10], dotdotdot)\n\n    def substringData(self, offset, count):\n        if offset < 0:\n            raise xml.dom.IndexSizeErr(\"offset cannot be negative\")\n        if offset >= len(self.data):\n            raise xml.dom.IndexSizeErr(\"offset cannot be beyond end of data\")\n        if count < 0:\n            raise xml.dom.IndexSizeErr(\"count cannot be negative\")\n        return self.data[offset:offset+count]\n\n    def appendData(self, arg):\n        self.data = self.data + arg\n\n    def insertData(self, offset, arg):\n        if offset < 0:\n            raise xml.dom.IndexSizeErr(\"offset cannot be negative\")\n        if offset >= len(self.data):\n            raise xml.dom.IndexSizeErr(\"offset cannot be beyond end of data\")\n        if arg:\n            self.data = \"%s%s%s\" % (\n                self.data[:offset], arg, self.data[offset:])\n\n    def deleteData(self, offset, count):\n        if offset < 0:\n            raise xml.dom.IndexSizeErr(\"offset cannot be negative\")\n        if offset >= len(self.data):\n            raise xml.dom.IndexSizeErr(\"offset cannot be beyond end of data\")\n        if count < 0:\n            raise xml.dom.IndexSizeErr(\"count cannot be negative\")\n        if count:\n            self.data = self.data[:offset] + self.data[offset+count:]\n\n    def replaceData(self, offset, count, arg):\n        if offset < 0:\n            raise xml.dom.IndexSizeErr(\"offset cannot be negative\")\n        if offset >= len(self.data):\n            raise xml.dom.IndexSizeErr(\"offset cannot be beyond end of data\")\n        if count < 0:\n            raise xml.dom.IndexSizeErr(\"count cannot be negative\")\n        if count:\n            self.data = \"%s%s%s\" % (\n                self.data[:offset], arg, self.data[offset+count:])\n\ndefproperty(CharacterData, \"length\", doc=\"Length of the string data.\")\n\n\nclass Text(CharacterData):\n    # Make sure we don't add an instance __dict__ if we don't already\n    # have one, at least when that's possible:\n    # XXX this does not work, CharacterData is an old-style class\n    # __slots__ = ()\n\n    nodeType = Node.TEXT_NODE\n    nodeName = \"#text\"\n    attributes = None\n\n    def splitText(self, offset):\n        if offset < 0 or offset > len(self.data):\n            raise xml.dom.IndexSizeErr(\"illegal offset value\")\n        newText = self.__class__()\n        newText.data = self.data[offset:]\n        newText.ownerDocument = self.ownerDocument\n        next = self.nextSibling\n        if self.parentNode and self in self.parentNode.childNodes:\n            if next is None:\n                self.parentNode.appendChild(newText)\n            else:\n                self.parentNode.insertBefore(newText, next)\n        self.data = self.data[:offset]\n        return newText\n\n    def writexml(self, writer, indent=\"\", addindent=\"\", newl=\"\"):\n        _write_data(writer, \"%s%s%s\" % (indent, self.data, newl))\n\n    # DOM Level 3 (WD 9 April 2002)\n\n    def _get_wholeText(self):\n        L = [self.data]\n        n = self.previousSibling\n        while n is not None:\n            if n.nodeType in (Node.TEXT_NODE, Node.CDATA_SECTION_NODE):\n                L.insert(0, n.data)\n                n = n.previousSibling\n            else:\n                break\n        n = self.nextSibling\n        while n is not None:\n            if n.nodeType in (Node.TEXT_NODE, Node.CDATA_SECTION_NODE):\n                L.append(n.data)\n                n = n.nextSibling\n            else:\n                break\n        return ''.join(L)\n\n    def replaceWholeText(self, content):\n        # XXX This needs to be seriously changed if minidom ever\n        # supports EntityReference nodes.\n        parent = self.parentNode\n        n = self.previousSibling\n        while n is not None:\n            if n.nodeType in (Node.TEXT_NODE, Node.CDATA_SECTION_NODE):\n                next = n.previousSibling\n                parent.removeChild(n)\n                n = next\n            else:\n                break\n        n = self.nextSibling\n        if not content:\n            parent.removeChild(self)\n        while n is not None:\n            if n.nodeType in (Node.TEXT_NODE, Node.CDATA_SECTION_NODE):\n                next = n.nextSibling\n                parent.removeChild(n)\n                n = next\n            else:\n                break\n        if content:\n            d = self.__dict__\n            d['data'] = content\n            d['nodeValue'] = content\n            return self\n        else:\n            return None\n\n    def _get_isWhitespaceInElementContent(self):\n        if self.data.strip():\n            return False\n        elem = _get_containing_element(self)\n        if elem is None:\n            return False\n        info = self.ownerDocument._get_elem_info(elem)\n        if info is None:\n            return False\n        else:\n            return info.isElementContent()\n\ndefproperty(Text, \"isWhitespaceInElementContent\",\n            doc=\"True iff this text node contains only whitespace\"\n                \" and is in element content.\")\ndefproperty(Text, \"wholeText\",\n            doc=\"The text of all logically-adjacent text nodes.\")\n\n\ndef _get_containing_element(node):\n    c = node.parentNode\n    while c is not None:\n        if c.nodeType == Node.ELEMENT_NODE:\n            return c\n        c = c.parentNode\n    return None\n\ndef _get_containing_entref(node):\n    c = node.parentNode\n    while c is not None:\n        if c.nodeType == Node.ENTITY_REFERENCE_NODE:\n            return c\n        c = c.parentNode\n    return None\n\n\nclass Comment(Childless, CharacterData):\n    nodeType = Node.COMMENT_NODE\n    nodeName = \"#comment\"\n\n    def __init__(self, data):\n        self.data = self.nodeValue = data\n\n    def writexml(self, writer, indent=\"\", addindent=\"\", newl=\"\"):\n        if \"--\" in self.data:\n            raise ValueError(\"'--' is not allowed in a comment node\")\n        writer.write(\"%s<!--%s-->%s\" % (indent, self.data, newl))\n\n\nclass CDATASection(Text):\n    # Make sure we don't add an instance __dict__ if we don't already\n    # have one, at least when that's possible:\n    # XXX this does not work, Text is an old-style class\n    # __slots__ = ()\n\n    nodeType = Node.CDATA_SECTION_NODE\n    nodeName = \"#cdata-section\"\n\n    def writexml(self, writer, indent=\"\", addindent=\"\", newl=\"\"):\n        if self.data.find(\"]]>\") >= 0:\n            raise ValueError(\"']]>' not allowed in a CDATA section\")\n        writer.write(\"<![CDATA[%s]]>\" % self.data)\n\n\nclass ReadOnlySequentialNamedNodeMap(object):\n    __slots__ = '_seq',\n\n    def __init__(self, seq=()):\n        # seq should be a list or tuple\n        self._seq = seq\n\n    def __len__(self):\n        return len(self._seq)\n\n    def _get_length(self):\n        return len(self._seq)\n\n    def getNamedItem(self, name):\n        for n in self._seq:\n            if n.nodeName == name:\n                return n\n\n    def getNamedItemNS(self, namespaceURI, localName):\n        for n in self._seq:\n            if n.namespaceURI == namespaceURI and n.localName == localName:\n                return n\n\n    def __getitem__(self, name_or_tuple):\n        if isinstance(name_or_tuple, tuple):\n            node = self.getNamedItemNS(*name_or_tuple)\n        else:\n            node = self.getNamedItem(name_or_tuple)\n        if node is None:\n            raise KeyError, name_or_tuple\n        return node\n\n    def item(self, index):\n        if index < 0:\n            return None\n        try:\n            return self._seq[index]\n        except IndexError:\n            return None\n\n    def removeNamedItem(self, name):\n        raise xml.dom.NoModificationAllowedErr(\n            \"NamedNodeMap instance is read-only\")\n\n    def removeNamedItemNS(self, namespaceURI, localName):\n        raise xml.dom.NoModificationAllowedErr(\n            \"NamedNodeMap instance is read-only\")\n\n    def setNamedItem(self, node):\n        raise xml.dom.NoModificationAllowedErr(\n            \"NamedNodeMap instance is read-only\")\n\n    def setNamedItemNS(self, node):\n        raise xml.dom.NoModificationAllowedErr(\n            \"NamedNodeMap instance is read-only\")\n\n    def __getstate__(self):\n        return [self._seq]\n\n    def __setstate__(self, state):\n        self._seq = state[0]\n\ndefproperty(ReadOnlySequentialNamedNodeMap, \"length\",\n            doc=\"Number of entries in the NamedNodeMap.\")\n\n\nclass Identified:\n    \"\"\"Mix-in class that supports the publicId and systemId attributes.\"\"\"\n\n    # XXX this does not work, this is an old-style class\n    # __slots__ = 'publicId', 'systemId'\n\n    def _identified_mixin_init(self, publicId, systemId):\n        self.publicId = publicId\n        self.systemId = systemId\n\n    def _get_publicId(self):\n        return self.publicId\n\n    def _get_systemId(self):\n        return self.systemId\n\nclass DocumentType(Identified, Childless, Node):\n    nodeType = Node.DOCUMENT_TYPE_NODE\n    nodeValue = None\n    name = None\n    publicId = None\n    systemId = None\n    internalSubset = None\n\n    def __init__(self, qualifiedName):\n        self.entities = ReadOnlySequentialNamedNodeMap()\n        self.notations = ReadOnlySequentialNamedNodeMap()\n        if qualifiedName:\n            prefix, localname = _nssplit(qualifiedName)\n            self.name = localname\n        self.nodeName = self.name\n\n    def _get_internalSubset(self):\n        return self.internalSubset\n\n    def cloneNode(self, deep):\n        if self.ownerDocument is None:\n            # it's ok\n            clone = DocumentType(None)\n            clone.name = self.name\n            clone.nodeName = self.name\n            operation = xml.dom.UserDataHandler.NODE_CLONED\n            if deep:\n                clone.entities._seq = []\n                clone.notations._seq = []\n                for n in self.notations._seq:\n                    notation = Notation(n.nodeName, n.publicId, n.systemId)\n                    clone.notations._seq.append(notation)\n                    n._call_user_data_handler(operation, n, notation)\n                for e in self.entities._seq:\n                    entity = Entity(e.nodeName, e.publicId, e.systemId,\n                                    e.notationName)\n                    entity.actualEncoding = e.actualEncoding\n                    entity.encoding = e.encoding\n                    entity.version = e.version\n                    clone.entities._seq.append(entity)\n                    e._call_user_data_handler(operation, n, entity)\n            self._call_user_data_handler(operation, self, clone)\n            return clone\n        else:\n            return None\n\n    def writexml(self, writer, indent=\"\", addindent=\"\", newl=\"\"):\n        writer.write(\"<!DOCTYPE \")\n        writer.write(self.name)\n        if self.publicId:\n            writer.write(\"%s  PUBLIC '%s'%s  '%s'\"\n                         % (newl, self.publicId, newl, self.systemId))\n        elif self.systemId:\n            writer.write(\"%s  SYSTEM '%s'\" % (newl, self.systemId))\n        if self.internalSubset is not None:\n            writer.write(\" [\")\n            writer.write(self.internalSubset)\n            writer.write(\"]\")\n        writer.write(\">\"+newl)\n\nclass Entity(Identified, Node):\n    attributes = None\n    nodeType = Node.ENTITY_NODE\n    nodeValue = None\n\n    actualEncoding = None\n    encoding = None\n    version = None\n\n    def __init__(self, name, publicId, systemId, notation):\n        self.nodeName = name\n        self.notationName = notation\n        self.childNodes = NodeList()\n        self._identified_mixin_init(publicId, systemId)\n\n    def _get_actualEncoding(self):\n        return self.actualEncoding\n\n    def _get_encoding(self):\n        return self.encoding\n\n    def _get_version(self):\n        return self.version\n\n    def appendChild(self, newChild):\n        raise xml.dom.HierarchyRequestErr(\n            \"cannot append children to an entity node\")\n\n    def insertBefore(self, newChild, refChild):\n        raise xml.dom.HierarchyRequestErr(\n            \"cannot insert children below an entity node\")\n\n    def removeChild(self, oldChild):\n        raise xml.dom.HierarchyRequestErr(\n            \"cannot remove children from an entity node\")\n\n    def replaceChild(self, newChild, oldChild):\n        raise xml.dom.HierarchyRequestErr(\n            \"cannot replace children of an entity node\")\n\nclass Notation(Identified, Childless, Node):\n    nodeType = Node.NOTATION_NODE\n    nodeValue = None\n\n    def __init__(self, name, publicId, systemId):\n        self.nodeName = name\n        self._identified_mixin_init(publicId, systemId)\n\n\nclass DOMImplementation(DOMImplementationLS):\n    _features = [(\"core\", \"1.0\"),\n                 (\"core\", \"2.0\"),\n                 (\"core\", None),\n                 (\"xml\", \"1.0\"),\n                 (\"xml\", \"2.0\"),\n                 (\"xml\", None),\n                 (\"ls-load\", \"3.0\"),\n                 (\"ls-load\", None),\n                 ]\n\n    def hasFeature(self, feature, version):\n        if version == \"\":\n            version = None\n        return (feature.lower(), version) in self._features\n\n    def createDocument(self, namespaceURI, qualifiedName, doctype):\n        if doctype and doctype.parentNode is not None:\n            raise xml.dom.WrongDocumentErr(\n                \"doctype object owned by another DOM tree\")\n        doc = self._create_document()\n\n        add_root_element = not (namespaceURI is None\n                                and qualifiedName is None\n                                and doctype is None)\n\n        if not qualifiedName and add_root_element:\n            # The spec is unclear what to raise here; SyntaxErr\n            # would be the other obvious candidate. Since Xerces raises\n            # InvalidCharacterErr, and since SyntaxErr is not listed\n            # for createDocument, that seems to be the better choice.\n            # XXX: need to check for illegal characters here and in\n            # createElement.\n\n            # DOM Level III clears this up when talking about the return value\n            # of this function.  If namespaceURI, qName and DocType are\n            # Null the document is returned without a document element\n            # Otherwise if doctype or namespaceURI are not None\n            # Then we go back to the above problem\n            raise xml.dom.InvalidCharacterErr(\"Element with no name\")\n\n        if add_root_element:\n            prefix, localname = _nssplit(qualifiedName)\n            if prefix == \"xml\" \\\n               and namespaceURI != \"http://www.w3.org/XML/1998/namespace\":\n                raise xml.dom.NamespaceErr(\"illegal use of 'xml' prefix\")\n            if prefix and not namespaceURI:\n                raise xml.dom.NamespaceErr(\n                    \"illegal use of prefix without namespaces\")\n            element = doc.createElementNS(namespaceURI, qualifiedName)\n            if doctype:\n                doc.appendChild(doctype)\n            doc.appendChild(element)\n\n        if doctype:\n            doctype.parentNode = doctype.ownerDocument = doc\n\n        doc.doctype = doctype\n        doc.implementation = self\n        return doc\n\n    def createDocumentType(self, qualifiedName, publicId, systemId):\n        doctype = DocumentType(qualifiedName)\n        doctype.publicId = publicId\n        doctype.systemId = systemId\n        return doctype\n\n    # DOM Level 3 (WD 9 April 2002)\n\n    def getInterface(self, feature):\n        if self.hasFeature(feature, None):\n            return self\n        else:\n            return None\n\n    # internal\n    def _create_document(self):\n        return Document()\n\nclass ElementInfo(object):\n    \"\"\"Object that represents content-model information for an element.\n\n    This implementation is not expected to be used in practice; DOM\n    builders should provide implementations which do the right thing\n    using information available to it.\n\n    \"\"\"\n\n    __slots__ = 'tagName',\n\n    def __init__(self, name):\n        self.tagName = name\n\n    def getAttributeType(self, aname):\n        return _no_type\n\n    def getAttributeTypeNS(self, namespaceURI, localName):\n        return _no_type\n\n    def isElementContent(self):\n        return False\n\n    def isEmpty(self):\n        \"\"\"Returns true iff this element is declared to have an EMPTY\n        content model.\"\"\"\n        return False\n\n    def isId(self, aname):\n        \"\"\"Returns true iff the named attribute is a DTD-style ID.\"\"\"\n        return False\n\n    def isIdNS(self, namespaceURI, localName):\n        \"\"\"Returns true iff the identified attribute is a DTD-style ID.\"\"\"\n        return False\n\n    def __getstate__(self):\n        return self.tagName\n\n    def __setstate__(self, state):\n        self.tagName = state\n\ndef _clear_id_cache(node):\n    if node.nodeType == Node.DOCUMENT_NODE:\n        node._id_cache.clear()\n        node._id_search_stack = None\n    elif _in_document(node):\n        node.ownerDocument._id_cache.clear()\n        node.ownerDocument._id_search_stack= None\n\nclass Document(Node, DocumentLS):\n    _child_node_types = (Node.ELEMENT_NODE, Node.PROCESSING_INSTRUCTION_NODE,\n                         Node.COMMENT_NODE, Node.DOCUMENT_TYPE_NODE)\n\n    nodeType = Node.DOCUMENT_NODE\n    nodeName = \"#document\"\n    nodeValue = None\n    attributes = None\n    doctype = None\n    parentNode = None\n    previousSibling = nextSibling = None\n\n    implementation = DOMImplementation()\n\n    # Document attributes from Level 3 (WD 9 April 2002)\n\n    actualEncoding = None\n    encoding = None\n    standalone = None\n    version = None\n    strictErrorChecking = False\n    errorHandler = None\n    documentURI = None\n\n    _magic_id_count = 0\n\n    def __init__(self):\n        self.childNodes = NodeList()\n        # mapping of (namespaceURI, localName) -> ElementInfo\n        #        and tagName -> ElementInfo\n        self._elem_info = {}\n        self._id_cache = {}\n        self._id_search_stack = None\n\n    def _get_elem_info(self, element):\n        if element.namespaceURI:\n            key = element.namespaceURI, element.localName\n        else:\n            key = element.tagName\n        return self._elem_info.get(key)\n\n    def _get_actualEncoding(self):\n        return self.actualEncoding\n\n    def _get_doctype(self):\n        return self.doctype\n\n    def _get_documentURI(self):\n        return self.documentURI\n\n    def _get_encoding(self):\n        return self.encoding\n\n    def _get_errorHandler(self):\n        return self.errorHandler\n\n    def _get_standalone(self):\n        return self.standalone\n\n    def _get_strictErrorChecking(self):\n        return self.strictErrorChecking\n\n    def _get_version(self):\n        return self.version\n\n    def appendChild(self, node):\n        if node.nodeType not in self._child_node_types:\n            raise xml.dom.HierarchyRequestErr(\n                \"%s cannot be child of %s\" % (repr(node), repr(self)))\n        if node.parentNode is not None:\n            # This needs to be done before the next test since this\n            # may *be* the document element, in which case it should\n            # end up re-ordered to the end.\n            node.parentNode.removeChild(node)\n\n        if node.nodeType == Node.ELEMENT_NODE \\\n           and self._get_documentElement():\n            raise xml.dom.HierarchyRequestErr(\n                \"two document elements disallowed\")\n        return Node.appendChild(self, node)\n\n    def removeChild(self, oldChild):\n        try:\n            self.childNodes.remove(oldChild)\n        except ValueError:\n            raise xml.dom.NotFoundErr()\n        oldChild.nextSibling = oldChild.previousSibling = None\n        oldChild.parentNode = None\n        if self.documentElement is oldChild:\n            self.documentElement = None\n\n        return oldChild\n\n    def _get_documentElement(self):\n        for node in self.childNodes:\n            if node.nodeType == Node.ELEMENT_NODE:\n                return node\n\n    def unlink(self):\n        if self.doctype is not None:\n            self.doctype.unlink()\n            self.doctype = None\n        Node.unlink(self)\n\n    def cloneNode(self, deep):\n        if not deep:\n            return None\n        clone = self.implementation.createDocument(None, None, None)\n        clone.encoding = self.encoding\n        clone.standalone = self.standalone\n        clone.version = self.version\n        for n in self.childNodes:\n            childclone = _clone_node(n, deep, clone)\n            assert childclone.ownerDocument.isSameNode(clone)\n            clone.childNodes.append(childclone)\n            if childclone.nodeType == Node.DOCUMENT_NODE:\n                assert clone.documentElement is None\n            elif childclone.nodeType == Node.DOCUMENT_TYPE_NODE:\n                assert clone.doctype is None\n                clone.doctype = childclone\n            childclone.parentNode = clone\n        self._call_user_data_handler(xml.dom.UserDataHandler.NODE_CLONED,\n                                     self, clone)\n        return clone\n\n    def createDocumentFragment(self):\n        d = DocumentFragment()\n        d.ownerDocument = self\n        return d\n\n    def createElement(self, tagName):\n        e = Element(tagName)\n        e.ownerDocument = self\n        return e\n\n    def createTextNode(self, data):\n        if not isinstance(data, StringTypes):\n            raise TypeError, \"node contents must be a string\"\n        t = Text()\n        t.data = data\n        t.ownerDocument = self\n        return t\n\n    def createCDATASection(self, data):\n        if not isinstance(data, StringTypes):\n            raise TypeError, \"node contents must be a string\"\n        c = CDATASection()\n        c.data = data\n        c.ownerDocument = self\n        return c\n\n    def createComment(self, data):\n        c = Comment(data)\n        c.ownerDocument = self\n        return c\n\n    def createProcessingInstruction(self, target, data):\n        p = ProcessingInstruction(target, data)\n        p.ownerDocument = self\n        return p\n\n    def createAttribute(self, qName):\n        a = Attr(qName)\n        a.ownerDocument = self\n        a.value = \"\"\n        return a\n\n    def createElementNS(self, namespaceURI, qualifiedName):\n        prefix, localName = _nssplit(qualifiedName)\n        e = Element(qualifiedName, namespaceURI, prefix)\n        e.ownerDocument = self\n        return e\n\n    def createAttributeNS(self, namespaceURI, qualifiedName):\n        prefix, localName = _nssplit(qualifiedName)\n        a = Attr(qualifiedName, namespaceURI, localName, prefix)\n        a.ownerDocument = self\n        a.value = \"\"\n        return a\n\n    # A couple of implementation-specific helpers to create node types\n    # not supported by the W3C DOM specs:\n\n    def _create_entity(self, name, publicId, systemId, notationName):\n        e = Entity(name, publicId, systemId, notationName)\n        e.ownerDocument = self\n        return e\n\n    def _create_notation(self, name, publicId, systemId):\n        n = Notation(name, publicId, systemId)\n        n.ownerDocument = self\n        return n\n\n    def getElementById(self, id):\n        if id in self._id_cache:\n            return self._id_cache[id]\n        if not (self._elem_info or self._magic_id_count):\n            return None\n\n        stack = self._id_search_stack\n        if stack is None:\n            # we never searched before, or the cache has been cleared\n            stack = [self.documentElement]\n            self._id_search_stack = stack\n        elif not stack:\n            # Previous search was completed and cache is still valid;\n            # no matching node.\n            return None\n\n        result = None\n        while stack:\n            node = stack.pop()\n            # add child elements to stack for continued searching\n            stack.extend([child for child in node.childNodes\n                          if child.nodeType in _nodeTypes_with_children])\n            # check this node\n            info = self._get_elem_info(node)\n            if info:\n                # We have to process all ID attributes before\n                # returning in order to get all the attributes set to\n                # be IDs using Element.setIdAttribute*().\n                for attr in node.attributes.values():\n                    if attr.namespaceURI:\n                        if info.isIdNS(attr.namespaceURI, attr.localName):\n                            self._id_cache[attr.value] = node\n                            if attr.value == id:\n                                result = node\n                            elif not node._magic_id_nodes:\n                                break\n                    elif info.isId(attr.name):\n                        self._id_cache[attr.value] = node\n                        if attr.value == id:\n                            result = node\n                        elif not node._magic_id_nodes:\n                            break\n                    elif attr._is_id:\n                        self._id_cache[attr.value] = node\n                        if attr.value == id:\n                            result = node\n                        elif node._magic_id_nodes == 1:\n                            break\n            elif node._magic_id_nodes:\n                for attr in node.attributes.values():\n                    if attr._is_id:\n                        self._id_cache[attr.value] = node\n                        if attr.value == id:\n                            result = node\n            if result is not None:\n                break\n        return result\n\n    def getElementsByTagName(self, name):\n        return _get_elements_by_tagName_helper(self, name, NodeList())\n\n    def getElementsByTagNameNS(self, namespaceURI, localName):\n        return _get_elements_by_tagName_ns_helper(\n            self, namespaceURI, localName, NodeList())\n\n    def isSupported(self, feature, version):\n        return self.implementation.hasFeature(feature, version)\n\n    def importNode(self, node, deep):\n        if node.nodeType == Node.DOCUMENT_NODE:\n            raise xml.dom.NotSupportedErr(\"cannot import document nodes\")\n        elif node.nodeType == Node.DOCUMENT_TYPE_NODE:\n            raise xml.dom.NotSupportedErr(\"cannot import document type nodes\")\n        return _clone_node(node, deep, self)\n\n    def writexml(self, writer, indent=\"\", addindent=\"\", newl=\"\",\n                 encoding = None):\n        if encoding is None:\n            writer.write('<?xml version=\"1.0\" ?>'+newl)\n        else:\n            writer.write('<?xml version=\"1.0\" encoding=\"%s\"?>%s' % (encoding, newl))\n        for node in self.childNodes:\n            node.writexml(writer, indent, addindent, newl)\n\n    # DOM Level 3 (WD 9 April 2002)\n\n    def renameNode(self, n, namespaceURI, name):\n        if n.ownerDocument is not self:\n            raise xml.dom.WrongDocumentErr(\n                \"cannot rename nodes from other documents;\\n\"\n                \"expected %s,\\nfound %s\" % (self, n.ownerDocument))\n        if n.nodeType not in (Node.ELEMENT_NODE, Node.ATTRIBUTE_NODE):\n            raise xml.dom.NotSupportedErr(\n                \"renameNode() only applies to element and attribute nodes\")\n        if namespaceURI != EMPTY_NAMESPACE:\n            if ':' in name:\n                prefix, localName = name.split(':', 1)\n                if (  prefix == \"xmlns\"\n                      and namespaceURI != xml.dom.XMLNS_NAMESPACE):\n                    raise xml.dom.NamespaceErr(\n                        \"illegal use of 'xmlns' prefix\")\n            else:\n                if (  name == \"xmlns\"\n                      and namespaceURI != xml.dom.XMLNS_NAMESPACE\n                      and n.nodeType == Node.ATTRIBUTE_NODE):\n                    raise xml.dom.NamespaceErr(\n                        \"illegal use of the 'xmlns' attribute\")\n                prefix = None\n                localName = name\n        else:\n            prefix = None\n            localName = None\n        if n.nodeType == Node.ATTRIBUTE_NODE:\n            element = n.ownerElement\n            if element is not None:\n                is_id = n._is_id\n                element.removeAttributeNode(n)\n        else:\n            element = None\n        # avoid __setattr__\n        d = n.__dict__\n        d['prefix'] = prefix\n        d['localName'] = localName\n        d['namespaceURI'] = namespaceURI\n        d['nodeName'] = name\n        if n.nodeType == Node.ELEMENT_NODE:\n            d['tagName'] = name\n        else:\n            # attribute node\n            d['name'] = name\n            if element is not None:\n                element.setAttributeNode(n)\n                if is_id:\n                    element.setIdAttributeNode(n)\n        # It's not clear from a semantic perspective whether we should\n        # call the user data handlers for the NODE_RENAMED event since\n        # we're re-using the existing node.  The draft spec has been\n        # interpreted as meaning \"no, don't call the handler unless a\n        # new node is created.\"\n        return n\n\ndefproperty(Document, \"documentElement\",\n            doc=\"Top-level element of this document.\")\n\n\ndef _clone_node(node, deep, newOwnerDocument):\n    \"\"\"\n    Clone a node and give it the new owner document.\n    Called by Node.cloneNode and Document.importNode\n    \"\"\"\n    if node.ownerDocument.isSameNode(newOwnerDocument):\n        operation = xml.dom.UserDataHandler.NODE_CLONED\n    else:\n        operation = xml.dom.UserDataHandler.NODE_IMPORTED\n    if node.nodeType == Node.ELEMENT_NODE:\n        clone = newOwnerDocument.createElementNS(node.namespaceURI,\n                                                 node.nodeName)\n        for attr in node.attributes.values():\n            clone.setAttributeNS(attr.namespaceURI, attr.nodeName, attr.value)\n            a = clone.getAttributeNodeNS(attr.namespaceURI, attr.localName)\n            a.specified = attr.specified\n\n        if deep:\n            for child in node.childNodes:\n                c = _clone_node(child, deep, newOwnerDocument)\n                clone.appendChild(c)\n\n    elif node.nodeType == Node.DOCUMENT_FRAGMENT_NODE:\n        clone = newOwnerDocument.createDocumentFragment()\n        if deep:\n            for child in node.childNodes:\n                c = _clone_node(child, deep, newOwnerDocument)\n                clone.appendChild(c)\n\n    elif node.nodeType == Node.TEXT_NODE:\n        clone = newOwnerDocument.createTextNode(node.data)\n    elif node.nodeType == Node.CDATA_SECTION_NODE:\n        clone = newOwnerDocument.createCDATASection(node.data)\n    elif node.nodeType == Node.PROCESSING_INSTRUCTION_NODE:\n        clone = newOwnerDocument.createProcessingInstruction(node.target,\n                                                             node.data)\n    elif node.nodeType == Node.COMMENT_NODE:\n        clone = newOwnerDocument.createComment(node.data)\n    elif node.nodeType == Node.ATTRIBUTE_NODE:\n        clone = newOwnerDocument.createAttributeNS(node.namespaceURI,\n                                                   node.nodeName)\n        clone.specified = True\n        clone.value = node.value\n    elif node.nodeType == Node.DOCUMENT_TYPE_NODE:\n        assert node.ownerDocument is not newOwnerDocument\n        operation = xml.dom.UserDataHandler.NODE_IMPORTED\n        clone = newOwnerDocument.implementation.createDocumentType(\n            node.name, node.publicId, node.systemId)\n        clone.ownerDocument = newOwnerDocument\n        if deep:\n            clone.entities._seq = []\n            clone.notations._seq = []\n            for n in node.notations._seq:\n                notation = Notation(n.nodeName, n.publicId, n.systemId)\n                notation.ownerDocument = newOwnerDocument\n                clone.notations._seq.append(notation)\n                if hasattr(n, '_call_user_data_handler'):\n                    n._call_user_data_handler(operation, n, notation)\n            for e in node.entities._seq:\n                entity = Entity(e.nodeName, e.publicId, e.systemId,\n                                e.notationName)\n                entity.actualEncoding = e.actualEncoding\n                entity.encoding = e.encoding\n                entity.version = e.version\n                entity.ownerDocument = newOwnerDocument\n                clone.entities._seq.append(entity)\n                if hasattr(e, '_call_user_data_handler'):\n                    e._call_user_data_handler(operation, n, entity)\n    else:\n        # Note the cloning of Document and DocumentType nodes is\n        # implementation specific.  minidom handles those cases\n        # directly in the cloneNode() methods.\n        raise xml.dom.NotSupportedErr(\"Cannot clone node %s\" % repr(node))\n\n    # Check for _call_user_data_handler() since this could conceivably\n    # used with other DOM implementations (one of the FourThought\n    # DOMs, perhaps?).\n    if hasattr(node, '_call_user_data_handler'):\n        node._call_user_data_handler(operation, node, clone)\n    return clone\n\n\ndef _nssplit(qualifiedName):\n    fields = qualifiedName.split(':', 1)\n    if len(fields) == 2:\n        return fields\n    else:\n        return (None, fields[0])\n\n\ndef _get_StringIO():\n    # we can't use cStringIO since it doesn't support Unicode strings\n    from StringIO import StringIO\n    return StringIO()\n\ndef _do_pulldom_parse(func, args, kwargs):\n    events = func(*args, **kwargs)\n    toktype, rootNode = events.getEvent()\n    events.expandNode(rootNode)\n    events.clear()\n    return rootNode\n\ndef parse(file, parser=None, bufsize=None):\n    \"\"\"Parse a file into a DOM by filename or file object.\"\"\"\n    if parser is None and not bufsize:\n        from xml.dom import expatbuilder\n        return expatbuilder.parse(file)\n    else:\n        from xml.dom import pulldom\n        return _do_pulldom_parse(pulldom.parse, (file,),\n            {'parser': parser, 'bufsize': bufsize})\n\ndef parseString(string, parser=None):\n    \"\"\"Parse a file into a DOM from a string.\"\"\"\n    if parser is None:\n        from xml.dom import expatbuilder\n        return expatbuilder.parseString(string)\n    else:\n        from xml.dom import pulldom\n        return _do_pulldom_parse(pulldom.parseString, (string,),\n                                 {'parser': parser})\n\ndef getDOMImplementation(features=None):\n    if features:\n        if isinstance(features, StringTypes):\n            features = domreg._parse_feature_string(features)\n        for f, v in features:\n            if not Document.implementation.hasFeature(f, v):\n                return None\n    return Document.implementation\n", 
    "xml.dom.pulldom": "import xml.sax\nimport xml.sax.handler\nimport types\n\ntry:\n    _StringTypes = [types.StringType, types.UnicodeType]\nexcept AttributeError:\n    _StringTypes = [types.StringType]\n\nSTART_ELEMENT = \"START_ELEMENT\"\nEND_ELEMENT = \"END_ELEMENT\"\nCOMMENT = \"COMMENT\"\nSTART_DOCUMENT = \"START_DOCUMENT\"\nEND_DOCUMENT = \"END_DOCUMENT\"\nPROCESSING_INSTRUCTION = \"PROCESSING_INSTRUCTION\"\nIGNORABLE_WHITESPACE = \"IGNORABLE_WHITESPACE\"\nCHARACTERS = \"CHARACTERS\"\n\nclass PullDOM(xml.sax.ContentHandler):\n    _locator = None\n    document = None\n\n    def __init__(self, documentFactory=None):\n        from xml.dom import XML_NAMESPACE\n        self.documentFactory = documentFactory\n        self.firstEvent = [None, None]\n        self.lastEvent = self.firstEvent\n        self.elementStack = []\n        self.push = self.elementStack.append\n        try:\n            self.pop = self.elementStack.pop\n        except AttributeError:\n            # use class' pop instead\n            pass\n        self._ns_contexts = [{XML_NAMESPACE:'xml'}] # contains uri -> prefix dicts\n        self._current_context = self._ns_contexts[-1]\n        self.pending_events = []\n\n    def pop(self):\n        result = self.elementStack[-1]\n        del self.elementStack[-1]\n        return result\n\n    def setDocumentLocator(self, locator):\n        self._locator = locator\n\n    def startPrefixMapping(self, prefix, uri):\n        if not hasattr(self, '_xmlns_attrs'):\n            self._xmlns_attrs = []\n        self._xmlns_attrs.append((prefix or 'xmlns', uri))\n        self._ns_contexts.append(self._current_context.copy())\n        self._current_context[uri] = prefix or None\n\n    def endPrefixMapping(self, prefix):\n        self._current_context = self._ns_contexts.pop()\n\n    def startElementNS(self, name, tagName , attrs):\n        # Retrieve xml namespace declaration attributes.\n        xmlns_uri = 'http://www.w3.org/2000/xmlns/'\n        xmlns_attrs = getattr(self, '_xmlns_attrs', None)\n        if xmlns_attrs is not None:\n            for aname, value in xmlns_attrs:\n                attrs._attrs[(xmlns_uri, aname)] = value\n            self._xmlns_attrs = []\n        uri, localname = name\n        if uri:\n            # When using namespaces, the reader may or may not\n            # provide us with the original name. If not, create\n            # *a* valid tagName from the current context.\n            if tagName is None:\n                prefix = self._current_context[uri]\n                if prefix:\n                    tagName = prefix + \":\" + localname\n                else:\n                    tagName = localname\n            if self.document:\n                node = self.document.createElementNS(uri, tagName)\n            else:\n                node = self.buildDocument(uri, tagName)\n        else:\n            # When the tagname is not prefixed, it just appears as\n            # localname\n            if self.document:\n                node = self.document.createElement(localname)\n            else:\n                node = self.buildDocument(None, localname)\n\n        for aname,value in attrs.items():\n            a_uri, a_localname = aname\n            if a_uri == xmlns_uri:\n                if a_localname == 'xmlns':\n                    qname = a_localname\n                else:\n                    qname = 'xmlns:' + a_localname\n                attr = self.document.createAttributeNS(a_uri, qname)\n                node.setAttributeNodeNS(attr)\n            elif a_uri:\n                prefix = self._current_context[a_uri]\n                if prefix:\n                    qname = prefix + \":\" + a_localname\n                else:\n                    qname = a_localname\n                attr = self.document.createAttributeNS(a_uri, qname)\n                node.setAttributeNodeNS(attr)\n            else:\n                attr = self.document.createAttribute(a_localname)\n                node.setAttributeNode(attr)\n            attr.value = value\n\n        self.lastEvent[1] = [(START_ELEMENT, node), None]\n        self.lastEvent = self.lastEvent[1]\n        self.push(node)\n\n    def endElementNS(self, name, tagName):\n        self.lastEvent[1] = [(END_ELEMENT, self.pop()), None]\n        self.lastEvent = self.lastEvent[1]\n\n    def startElement(self, name, attrs):\n        if self.document:\n            node = self.document.createElement(name)\n        else:\n            node = self.buildDocument(None, name)\n\n        for aname,value in attrs.items():\n            attr = self.document.createAttribute(aname)\n            attr.value = value\n            node.setAttributeNode(attr)\n\n        self.lastEvent[1] = [(START_ELEMENT, node), None]\n        self.lastEvent = self.lastEvent[1]\n        self.push(node)\n\n    def endElement(self, name):\n        self.lastEvent[1] = [(END_ELEMENT, self.pop()), None]\n        self.lastEvent = self.lastEvent[1]\n\n    def comment(self, s):\n        if self.document:\n            node = self.document.createComment(s)\n            self.lastEvent[1] = [(COMMENT, node), None]\n            self.lastEvent = self.lastEvent[1]\n        else:\n            event = [(COMMENT, s), None]\n            self.pending_events.append(event)\n\n    def processingInstruction(self, target, data):\n        if self.document:\n            node = self.document.createProcessingInstruction(target, data)\n            self.lastEvent[1] = [(PROCESSING_INSTRUCTION, node), None]\n            self.lastEvent = self.lastEvent[1]\n        else:\n            event = [(PROCESSING_INSTRUCTION, target, data), None]\n            self.pending_events.append(event)\n\n    def ignorableWhitespace(self, chars):\n        node = self.document.createTextNode(chars)\n        self.lastEvent[1] = [(IGNORABLE_WHITESPACE, node), None]\n        self.lastEvent = self.lastEvent[1]\n\n    def characters(self, chars):\n        node = self.document.createTextNode(chars)\n        self.lastEvent[1] = [(CHARACTERS, node), None]\n        self.lastEvent = self.lastEvent[1]\n\n    def startDocument(self):\n        if self.documentFactory is None:\n            import xml.dom.minidom\n            self.documentFactory = xml.dom.minidom.Document.implementation\n\n    def buildDocument(self, uri, tagname):\n        # Can't do that in startDocument, since we need the tagname\n        # XXX: obtain DocumentType\n        node = self.documentFactory.createDocument(uri, tagname, None)\n        self.document = node\n        self.lastEvent[1] = [(START_DOCUMENT, node), None]\n        self.lastEvent = self.lastEvent[1]\n        self.push(node)\n        # Put everything we have seen so far into the document\n        for e in self.pending_events:\n            if e[0][0] == PROCESSING_INSTRUCTION:\n                _,target,data = e[0]\n                n = self.document.createProcessingInstruction(target, data)\n                e[0] = (PROCESSING_INSTRUCTION, n)\n            elif e[0][0] == COMMENT:\n                n = self.document.createComment(e[0][1])\n                e[0] = (COMMENT, n)\n            else:\n                raise AssertionError(\"Unknown pending event \",e[0][0])\n            self.lastEvent[1] = e\n            self.lastEvent = e\n        self.pending_events = None\n        return node.firstChild\n\n    def endDocument(self):\n        self.lastEvent[1] = [(END_DOCUMENT, self.document), None]\n        self.pop()\n\n    def clear(self):\n        \"clear(): Explicitly release parsing structures\"\n        self.document = None\n\nclass ErrorHandler:\n    def warning(self, exception):\n        print exception\n    def error(self, exception):\n        raise exception\n    def fatalError(self, exception):\n        raise exception\n\nclass DOMEventStream:\n    def __init__(self, stream, parser, bufsize):\n        self.stream = stream\n        self.parser = parser\n        self.bufsize = bufsize\n        if not hasattr(self.parser, 'feed'):\n            self.getEvent = self._slurp\n        self.reset()\n\n    def reset(self):\n        self.pulldom = PullDOM()\n        # This content handler relies on namespace support\n        self.parser.setFeature(xml.sax.handler.feature_namespaces, 1)\n        self.parser.setContentHandler(self.pulldom)\n\n    def __getitem__(self, pos):\n        rc = self.getEvent()\n        if rc:\n            return rc\n        raise IndexError\n\n    def next(self):\n        rc = self.getEvent()\n        if rc:\n            return rc\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    def expandNode(self, node):\n        event = self.getEvent()\n        parents = [node]\n        while event:\n            token, cur_node = event\n            if cur_node is node:\n                return\n            if token != END_ELEMENT:\n                parents[-1].appendChild(cur_node)\n            if token == START_ELEMENT:\n                parents.append(cur_node)\n            elif token == END_ELEMENT:\n                del parents[-1]\n            event = self.getEvent()\n\n    def getEvent(self):\n        # use IncrementalParser interface, so we get the desired\n        # pull effect\n        if not self.pulldom.firstEvent[1]:\n            self.pulldom.lastEvent = self.pulldom.firstEvent\n        while not self.pulldom.firstEvent[1]:\n            buf = self.stream.read(self.bufsize)\n            if not buf:\n                self.parser.close()\n                return None\n            self.parser.feed(buf)\n        rc = self.pulldom.firstEvent[1][0]\n        self.pulldom.firstEvent[1] = self.pulldom.firstEvent[1][1]\n        return rc\n\n    def _slurp(self):\n        \"\"\" Fallback replacement for getEvent() using the\n            standard SAX2 interface, which means we slurp the\n            SAX events into memory (no performance gain, but\n            we are compatible to all SAX parsers).\n        \"\"\"\n        self.parser.parse(self.stream)\n        self.getEvent = self._emit\n        return self._emit()\n\n    def _emit(self):\n        \"\"\" Fallback replacement for getEvent() that emits\n            the events that _slurp() read previously.\n        \"\"\"\n        rc = self.pulldom.firstEvent[1][0]\n        self.pulldom.firstEvent[1] = self.pulldom.firstEvent[1][1]\n        return rc\n\n    def clear(self):\n        \"\"\"clear(): Explicitly release parsing objects\"\"\"\n        self.pulldom.clear()\n        del self.pulldom\n        self.parser = None\n        self.stream = None\n\nclass SAX2DOM(PullDOM):\n\n    def startElementNS(self, name, tagName , attrs):\n        PullDOM.startElementNS(self, name, tagName, attrs)\n        curNode = self.elementStack[-1]\n        parentNode = self.elementStack[-2]\n        parentNode.appendChild(curNode)\n\n    def startElement(self, name, attrs):\n        PullDOM.startElement(self, name, attrs)\n        curNode = self.elementStack[-1]\n        parentNode = self.elementStack[-2]\n        parentNode.appendChild(curNode)\n\n    def processingInstruction(self, target, data):\n        PullDOM.processingInstruction(self, target, data)\n        node = self.lastEvent[0][1]\n        parentNode = self.elementStack[-1]\n        parentNode.appendChild(node)\n\n    def ignorableWhitespace(self, chars):\n        PullDOM.ignorableWhitespace(self, chars)\n        node = self.lastEvent[0][1]\n        parentNode = self.elementStack[-1]\n        parentNode.appendChild(node)\n\n    def characters(self, chars):\n        PullDOM.characters(self, chars)\n        node = self.lastEvent[0][1]\n        parentNode = self.elementStack[-1]\n        parentNode.appendChild(node)\n\n\ndefault_bufsize = (2 ** 14) - 20\n\ndef parse(stream_or_string, parser=None, bufsize=None):\n    if bufsize is None:\n        bufsize = default_bufsize\n    if type(stream_or_string) in _StringTypes:\n        stream = open(stream_or_string)\n    else:\n        stream = stream_or_string\n    if not parser:\n        parser = xml.sax.make_parser()\n    return DOMEventStream(stream, parser, bufsize)\n\ndef parseString(string, parser=None):\n    try:\n        from cStringIO import StringIO\n    except ImportError:\n        from StringIO import StringIO\n\n    bufsize = len(string)\n    buf = StringIO(string)\n    if not parser:\n        parser = xml.sax.make_parser()\n    return DOMEventStream(buf, parser, bufsize)\n", 
    "xml.dom.xmlbuilder": "\"\"\"Implementation of the DOM Level 3 'LS-Load' feature.\"\"\"\n\nimport copy\nimport xml.dom\n\nfrom xml.dom.NodeFilter import NodeFilter\n\n\n__all__ = [\"DOMBuilder\", \"DOMEntityResolver\", \"DOMInputSource\"]\n\n\nclass Options:\n    \"\"\"Features object that has variables set for each DOMBuilder feature.\n\n    The DOMBuilder class uses an instance of this class to pass settings to\n    the ExpatBuilder class.\n    \"\"\"\n\n    # Note that the DOMBuilder class in LoadSave constrains which of these\n    # values can be set using the DOM Level 3 LoadSave feature.\n\n    namespaces = 1\n    namespace_declarations = True\n    validation = False\n    external_parameter_entities = True\n    external_general_entities = True\n    external_dtd_subset = True\n    validate_if_schema = False\n    validate = False\n    datatype_normalization = False\n    create_entity_ref_nodes = True\n    entities = True\n    whitespace_in_element_content = True\n    cdata_sections = True\n    comments = True\n    charset_overrides_xml_encoding = True\n    infoset = False\n    supported_mediatypes_only = False\n\n    errorHandler = None\n    filter = None\n\n\nclass DOMBuilder:\n    entityResolver = None\n    errorHandler = None\n    filter = None\n\n    ACTION_REPLACE = 1\n    ACTION_APPEND_AS_CHILDREN = 2\n    ACTION_INSERT_AFTER = 3\n    ACTION_INSERT_BEFORE = 4\n\n    _legal_actions = (ACTION_REPLACE, ACTION_APPEND_AS_CHILDREN,\n                      ACTION_INSERT_AFTER, ACTION_INSERT_BEFORE)\n\n    def __init__(self):\n        self._options = Options()\n\n    def _get_entityResolver(self):\n        return self.entityResolver\n    def _set_entityResolver(self, entityResolver):\n        self.entityResolver = entityResolver\n\n    def _get_errorHandler(self):\n        return self.errorHandler\n    def _set_errorHandler(self, errorHandler):\n        self.errorHandler = errorHandler\n\n    def _get_filter(self):\n        return self.filter\n    def _set_filter(self, filter):\n        self.filter = filter\n\n    def setFeature(self, name, state):\n        if self.supportsFeature(name):\n            state = state and 1 or 0\n            try:\n                settings = self._settings[(_name_xform(name), state)]\n            except KeyError:\n                raise xml.dom.NotSupportedErr(\n                    \"unsupported feature: %r\" % (name,))\n            else:\n                for name, value in settings:\n                    setattr(self._options, name, value)\n        else:\n            raise xml.dom.NotFoundErr(\"unknown feature: \" + repr(name))\n\n    def supportsFeature(self, name):\n        return hasattr(self._options, _name_xform(name))\n\n    def canSetFeature(self, name, state):\n        key = (_name_xform(name), state and 1 or 0)\n        return key in self._settings\n\n    # This dictionary maps from (feature,value) to a list of\n    # (option,value) pairs that should be set on the Options object.\n    # If a (feature,value) setting is not in this dictionary, it is\n    # not supported by the DOMBuilder.\n    #\n    _settings = {\n        (\"namespace_declarations\", 0): [\n            (\"namespace_declarations\", 0)],\n        (\"namespace_declarations\", 1): [\n            (\"namespace_declarations\", 1)],\n        (\"validation\", 0): [\n            (\"validation\", 0)],\n        (\"external_general_entities\", 0): [\n            (\"external_general_entities\", 0)],\n        (\"external_general_entities\", 1): [\n            (\"external_general_entities\", 1)],\n        (\"external_parameter_entities\", 0): [\n            (\"external_parameter_entities\", 0)],\n        (\"external_parameter_entities\", 1): [\n            (\"external_parameter_entities\", 1)],\n        (\"validate_if_schema\", 0): [\n            (\"validate_if_schema\", 0)],\n        (\"create_entity_ref_nodes\", 0): [\n            (\"create_entity_ref_nodes\", 0)],\n        (\"create_entity_ref_nodes\", 1): [\n            (\"create_entity_ref_nodes\", 1)],\n        (\"entities\", 0): [\n            (\"create_entity_ref_nodes\", 0),\n            (\"entities\", 0)],\n        (\"entities\", 1): [\n            (\"entities\", 1)],\n        (\"whitespace_in_element_content\", 0): [\n            (\"whitespace_in_element_content\", 0)],\n        (\"whitespace_in_element_content\", 1): [\n            (\"whitespace_in_element_content\", 1)],\n        (\"cdata_sections\", 0): [\n            (\"cdata_sections\", 0)],\n        (\"cdata_sections\", 1): [\n            (\"cdata_sections\", 1)],\n        (\"comments\", 0): [\n            (\"comments\", 0)],\n        (\"comments\", 1): [\n            (\"comments\", 1)],\n        (\"charset_overrides_xml_encoding\", 0): [\n            (\"charset_overrides_xml_encoding\", 0)],\n        (\"charset_overrides_xml_encoding\", 1): [\n            (\"charset_overrides_xml_encoding\", 1)],\n        (\"infoset\", 0): [],\n        (\"infoset\", 1): [\n            (\"namespace_declarations\", 0),\n            (\"validate_if_schema\", 0),\n            (\"create_entity_ref_nodes\", 0),\n            (\"entities\", 0),\n            (\"cdata_sections\", 0),\n            (\"datatype_normalization\", 1),\n            (\"whitespace_in_element_content\", 1),\n            (\"comments\", 1),\n            (\"charset_overrides_xml_encoding\", 1)],\n        (\"supported_mediatypes_only\", 0): [\n            (\"supported_mediatypes_only\", 0)],\n        (\"namespaces\", 0): [\n            (\"namespaces\", 0)],\n        (\"namespaces\", 1): [\n            (\"namespaces\", 1)],\n    }\n\n    def getFeature(self, name):\n        xname = _name_xform(name)\n        try:\n            return getattr(self._options, xname)\n        except AttributeError:\n            if name == \"infoset\":\n                options = self._options\n                return (options.datatype_normalization\n                        and options.whitespace_in_element_content\n                        and options.comments\n                        and options.charset_overrides_xml_encoding\n                        and not (options.namespace_declarations\n                                 or options.validate_if_schema\n                                 or options.create_entity_ref_nodes\n                                 or options.entities\n                                 or options.cdata_sections))\n            raise xml.dom.NotFoundErr(\"feature %s not known\" % repr(name))\n\n    def parseURI(self, uri):\n        if self.entityResolver:\n            input = self.entityResolver.resolveEntity(None, uri)\n        else:\n            input = DOMEntityResolver().resolveEntity(None, uri)\n        return self.parse(input)\n\n    def parse(self, input):\n        options = copy.copy(self._options)\n        options.filter = self.filter\n        options.errorHandler = self.errorHandler\n        fp = input.byteStream\n        if fp is None and options.systemId:\n            import urllib2\n            fp = urllib2.urlopen(input.systemId)\n        return self._parse_bytestream(fp, options)\n\n    def parseWithContext(self, input, cnode, action):\n        if action not in self._legal_actions:\n            raise ValueError(\"not a legal action\")\n        raise NotImplementedError(\"Haven't written this yet...\")\n\n    def _parse_bytestream(self, stream, options):\n        import xml.dom.expatbuilder\n        builder = xml.dom.expatbuilder.makeBuilder(options)\n        return builder.parseFile(stream)\n\n\ndef _name_xform(name):\n    return name.lower().replace('-', '_')\n\n\nclass DOMEntityResolver(object):\n    __slots__ = '_opener',\n\n    def resolveEntity(self, publicId, systemId):\n        assert systemId is not None\n        source = DOMInputSource()\n        source.publicId = publicId\n        source.systemId = systemId\n        source.byteStream = self._get_opener().open(systemId)\n\n        # determine the encoding if the transport provided it\n        source.encoding = self._guess_media_encoding(source)\n\n        # determine the base URI is we can\n        import posixpath, urlparse\n        parts = urlparse.urlparse(systemId)\n        scheme, netloc, path, params, query, fragment = parts\n        # XXX should we check the scheme here as well?\n        if path and not path.endswith(\"/\"):\n            path = posixpath.dirname(path) + \"/\"\n            parts = scheme, netloc, path, params, query, fragment\n            source.baseURI = urlparse.urlunparse(parts)\n\n        return source\n\n    def _get_opener(self):\n        try:\n            return self._opener\n        except AttributeError:\n            self._opener = self._create_opener()\n            return self._opener\n\n    def _create_opener(self):\n        import urllib2\n        return urllib2.build_opener()\n\n    def _guess_media_encoding(self, source):\n        info = source.byteStream.info()\n        if \"Content-Type\" in info:\n            for param in info.getplist():\n                if param.startswith(\"charset=\"):\n                    return param.split(\"=\", 1)[1].lower()\n\n\nclass DOMInputSource(object):\n    __slots__ = ('byteStream', 'characterStream', 'stringData',\n                 'encoding', 'publicId', 'systemId', 'baseURI')\n\n    def __init__(self):\n        self.byteStream = None\n        self.characterStream = None\n        self.stringData = None\n        self.encoding = None\n        self.publicId = None\n        self.systemId = None\n        self.baseURI = None\n\n    def _get_byteStream(self):\n        return self.byteStream\n    def _set_byteStream(self, byteStream):\n        self.byteStream = byteStream\n\n    def _get_characterStream(self):\n        return self.characterStream\n    def _set_characterStream(self, characterStream):\n        self.characterStream = characterStream\n\n    def _get_stringData(self):\n        return self.stringData\n    def _set_stringData(self, data):\n        self.stringData = data\n\n    def _get_encoding(self):\n        return self.encoding\n    def _set_encoding(self, encoding):\n        self.encoding = encoding\n\n    def _get_publicId(self):\n        return self.publicId\n    def _set_publicId(self, publicId):\n        self.publicId = publicId\n\n    def _get_systemId(self):\n        return self.systemId\n    def _set_systemId(self, systemId):\n        self.systemId = systemId\n\n    def _get_baseURI(self):\n        return self.baseURI\n    def _set_baseURI(self, uri):\n        self.baseURI = uri\n\n\nclass DOMBuilderFilter:\n    \"\"\"Element filter which can be used to tailor construction of\n    a DOM instance.\n    \"\"\"\n\n    # There's really no need for this class; concrete implementations\n    # should just implement the endElement() and startElement()\n    # methods as appropriate.  Using this makes it easy to only\n    # implement one of them.\n\n    FILTER_ACCEPT = 1\n    FILTER_REJECT = 2\n    FILTER_SKIP = 3\n    FILTER_INTERRUPT = 4\n\n    whatToShow = NodeFilter.SHOW_ALL\n\n    def _get_whatToShow(self):\n        return self.whatToShow\n\n    def acceptNode(self, element):\n        return self.FILTER_ACCEPT\n\n    def startContainer(self, element):\n        return self.FILTER_ACCEPT\n\ndel NodeFilter\n\n\nclass DocumentLS:\n    \"\"\"Mixin to create documents that conform to the load/save spec.\"\"\"\n\n    async = False\n\n    def _get_async(self):\n        return False\n    def _set_async(self, async):\n        if async:\n            raise xml.dom.NotSupportedErr(\n                \"asynchronous document loading is not supported\")\n\n    def abort(self):\n        # What does it mean to \"clear\" a document?  Does the\n        # documentElement disappear?\n        raise NotImplementedError(\n            \"haven't figured out what this means yet\")\n\n    def load(self, uri):\n        raise NotImplementedError(\"haven't written this yet\")\n\n    def loadXML(self, source):\n        raise NotImplementedError(\"haven't written this yet\")\n\n    def saveXML(self, snode):\n        if snode is None:\n            snode = self\n        elif snode.ownerDocument is not self:\n            raise xml.dom.WrongDocumentErr()\n        return snode.toxml()\n\n\nclass DOMImplementationLS:\n    MODE_SYNCHRONOUS = 1\n    MODE_ASYNCHRONOUS = 2\n\n    def createDOMBuilder(self, mode, schemaType):\n        if schemaType is not None:\n            raise xml.dom.NotSupportedErr(\n                \"schemaType not yet supported\")\n        if mode == self.MODE_SYNCHRONOUS:\n            return DOMBuilder()\n        if mode == self.MODE_ASYNCHRONOUS:\n            raise xml.dom.NotSupportedErr(\n                \"asynchronous builders are not supported\")\n        raise ValueError(\"unknown value for mode\")\n\n    def createDOMWriter(self):\n        raise NotImplementedError(\n            \"the writer interface hasn't been written yet!\")\n\n    def createDOMInputSource(self):\n        return DOMInputSource()\n", 
    "xml.etree.ElementPath": "#\n# ElementTree\n# $Id: ElementPath.py 3375 2008-02-13 08:05:08Z fredrik $\n#\n# limited xpath support for element trees\n#\n# history:\n# 2003-05-23 fl   created\n# 2003-05-28 fl   added support for // etc\n# 2003-08-27 fl   fixed parsing of periods in element names\n# 2007-09-10 fl   new selection engine\n# 2007-09-12 fl   fixed parent selector\n# 2007-09-13 fl   added iterfind; changed findall to return a list\n# 2007-11-30 fl   added namespaces support\n# 2009-10-30 fl   added child element value filter\n#\n# Copyright (c) 2003-2009 by Fredrik Lundh.  All rights reserved.\n#\n# fredrik@pythonware.com\n# http://www.pythonware.com\n#\n# --------------------------------------------------------------------\n# The ElementTree toolkit is\n#\n# Copyright (c) 1999-2009 by Fredrik Lundh\n#\n# By obtaining, using, and/or copying this software and/or its\n# associated documentation, you agree that you have read, understood,\n# and will comply with the following terms and conditions:\n#\n# Permission to use, copy, modify, and distribute this software and\n# its associated documentation for any purpose and without fee is\n# hereby granted, provided that the above copyright notice appears in\n# all copies, and that both that copyright notice and this permission\n# notice appear in supporting documentation, and that the name of\n# Secret Labs AB or the author not be used in advertising or publicity\n# pertaining to distribution of the software without specific, written\n# prior permission.\n#\n# SECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD\n# TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANT-\n# ABILITY AND FITNESS.  IN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR\n# BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY\n# DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS,\n# WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS\n# ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE\n# OF THIS SOFTWARE.\n# --------------------------------------------------------------------\n\n# Licensed to PSF under a Contributor Agreement.\n# See http://www.python.org/psf/license for licensing details.\n\n##\n# Implementation module for XPath support.  There's usually no reason\n# to import this module directly; the <b>ElementTree</b> does this for\n# you, if needed.\n##\n\nimport re\n\nxpath_tokenizer_re = re.compile(\n    \"(\"\n    \"'[^']*'|\\\"[^\\\"]*\\\"|\"\n    \"::|\"\n    \"//?|\"\n    \"\\.\\.|\"\n    \"\\(\\)|\"\n    \"[/.*:\\[\\]\\(\\)@=])|\"\n    \"((?:\\{[^}]+\\})?[^/\\[\\]\\(\\)@=\\s]+)|\"\n    \"\\s+\"\n    )\n\ndef xpath_tokenizer(pattern, namespaces=None):\n    for token in xpath_tokenizer_re.findall(pattern):\n        tag = token[1]\n        if tag and tag[0] != \"{\" and \":\" in tag:\n            try:\n                prefix, uri = tag.split(\":\", 1)\n                if not namespaces:\n                    raise KeyError\n                yield token[0], \"{%s}%s\" % (namespaces[prefix], uri)\n            except KeyError:\n                raise SyntaxError(\"prefix %r not found in prefix map\" % prefix)\n        else:\n            yield token\n\ndef get_parent_map(context):\n    parent_map = context.parent_map\n    if parent_map is None:\n        context.parent_map = parent_map = {}\n        for p in context.root.iter():\n            for e in p:\n                parent_map[e] = p\n    return parent_map\n\ndef prepare_child(next, token):\n    tag = token[1]\n    def select(context, result):\n        for elem in result:\n            for e in elem:\n                if e.tag == tag:\n                    yield e\n    return select\n\ndef prepare_star(next, token):\n    def select(context, result):\n        for elem in result:\n            for e in elem:\n                yield e\n    return select\n\ndef prepare_self(next, token):\n    def select(context, result):\n        for elem in result:\n            yield elem\n    return select\n\ndef prepare_descendant(next, token):\n    token = next()\n    if token[0] == \"*\":\n        tag = \"*\"\n    elif not token[0]:\n        tag = token[1]\n    else:\n        raise SyntaxError(\"invalid descendant\")\n    def select(context, result):\n        for elem in result:\n            for e in elem.iter(tag):\n                if e is not elem:\n                    yield e\n    return select\n\ndef prepare_parent(next, token):\n    def select(context, result):\n        # FIXME: raise error if .. is applied at toplevel?\n        parent_map = get_parent_map(context)\n        result_map = {}\n        for elem in result:\n            if elem in parent_map:\n                parent = parent_map[elem]\n                if parent not in result_map:\n                    result_map[parent] = None\n                    yield parent\n    return select\n\ndef prepare_predicate(next, token):\n    # FIXME: replace with real parser!!! refs:\n    # http://effbot.org/zone/simple-iterator-parser.htm\n    # http://javascript.crockford.com/tdop/tdop.html\n    signature = []\n    predicate = []\n    while 1:\n        token = next()\n        if token[0] == \"]\":\n            break\n        if token[0] and token[0][:1] in \"'\\\"\":\n            token = \"'\", token[0][1:-1]\n        signature.append(token[0] or \"-\")\n        predicate.append(token[1])\n    signature = \"\".join(signature)\n    # use signature to determine predicate type\n    if signature == \"@-\":\n        # [@attribute] predicate\n        key = predicate[1]\n        def select(context, result):\n            for elem in result:\n                if elem.get(key) is not None:\n                    yield elem\n        return select\n    if signature == \"@-='\":\n        # [@attribute='value']\n        key = predicate[1]\n        value = predicate[-1]\n        def select(context, result):\n            for elem in result:\n                if elem.get(key) == value:\n                    yield elem\n        return select\n    if signature == \"-\" and not re.match(\"\\d+$\", predicate[0]):\n        # [tag]\n        tag = predicate[0]\n        def select(context, result):\n            for elem in result:\n                if elem.find(tag) is not None:\n                    yield elem\n        return select\n    if signature == \"-='\" and not re.match(\"\\d+$\", predicate[0]):\n        # [tag='value']\n        tag = predicate[0]\n        value = predicate[-1]\n        def select(context, result):\n            for elem in result:\n                for e in elem.findall(tag):\n                    if \"\".join(e.itertext()) == value:\n                        yield elem\n                        break\n        return select\n    if signature == \"-\" or signature == \"-()\" or signature == \"-()-\":\n        # [index] or [last()] or [last()-index]\n        if signature == \"-\":\n            index = int(predicate[0]) - 1\n        else:\n            if predicate[0] != \"last\":\n                raise SyntaxError(\"unsupported function\")\n            if signature == \"-()-\":\n                try:\n                    index = int(predicate[2]) - 1\n                except ValueError:\n                    raise SyntaxError(\"unsupported expression\")\n            else:\n                index = -1\n        def select(context, result):\n            parent_map = get_parent_map(context)\n            for elem in result:\n                try:\n                    parent = parent_map[elem]\n                    # FIXME: what if the selector is \"*\" ?\n                    elems = list(parent.findall(elem.tag))\n                    if elems[index] is elem:\n                        yield elem\n                except (IndexError, KeyError):\n                    pass\n        return select\n    raise SyntaxError(\"invalid predicate\")\n\nops = {\n    \"\": prepare_child,\n    \"*\": prepare_star,\n    \".\": prepare_self,\n    \"..\": prepare_parent,\n    \"//\": prepare_descendant,\n    \"[\": prepare_predicate,\n    }\n\n_cache = {}\n\nclass _SelectorContext:\n    parent_map = None\n    def __init__(self, root):\n        self.root = root\n\n# --------------------------------------------------------------------\n\n##\n# Generate all matching objects.\n\ndef iterfind(elem, path, namespaces=None):\n    # compile selector pattern\n    if path[-1:] == \"/\":\n        path = path + \"*\" # implicit all (FIXME: keep this?)\n    try:\n        selector = _cache[path]\n    except KeyError:\n        if len(_cache) > 100:\n            _cache.clear()\n        if path[:1] == \"/\":\n            raise SyntaxError(\"cannot use absolute path on element\")\n        next = iter(xpath_tokenizer(path, namespaces)).next\n        token = next()\n        selector = []\n        while 1:\n            try:\n                selector.append(ops[token[0]](next, token))\n            except StopIteration:\n                raise SyntaxError(\"invalid path\")\n            try:\n                token = next()\n                if token[0] == \"/\":\n                    token = next()\n            except StopIteration:\n                break\n        _cache[path] = selector\n    # execute selector pattern\n    result = [elem]\n    context = _SelectorContext(elem)\n    for select in selector:\n        result = select(context, result)\n    return result\n\n##\n# Find first matching object.\n\ndef find(elem, path, namespaces=None):\n    try:\n        return iterfind(elem, path, namespaces).next()\n    except StopIteration:\n        return None\n\n##\n# Find all matching objects.\n\ndef findall(elem, path, namespaces=None):\n    return list(iterfind(elem, path, namespaces))\n\n##\n# Find text for first matching object.\n\ndef findtext(elem, path, default=None, namespaces=None):\n    try:\n        elem = iterfind(elem, path, namespaces).next()\n        return elem.text or \"\"\n    except StopIteration:\n        return default\n", 
    "xml.etree.ElementTree": "#\n# ElementTree\n# $Id: ElementTree.py 3440 2008-07-18 14:45:01Z fredrik $\n#\n# light-weight XML support for Python 2.3 and later.\n#\n# history (since 1.2.6):\n# 2005-11-12 fl   added tostringlist/fromstringlist helpers\n# 2006-07-05 fl   merged in selected changes from the 1.3 sandbox\n# 2006-07-05 fl   removed support for 2.1 and earlier\n# 2007-06-21 fl   added deprecation/future warnings\n# 2007-08-25 fl   added doctype hook, added parser version attribute etc\n# 2007-08-26 fl   added new serializer code (better namespace handling, etc)\n# 2007-08-27 fl   warn for broken /tag searches on tree level\n# 2007-09-02 fl   added html/text methods to serializer (experimental)\n# 2007-09-05 fl   added method argument to tostring/tostringlist\n# 2007-09-06 fl   improved error handling\n# 2007-09-13 fl   added itertext, iterfind; assorted cleanups\n# 2007-12-15 fl   added C14N hooks, copy method (experimental)\n#\n# Copyright (c) 1999-2008 by Fredrik Lundh.  All rights reserved.\n#\n# fredrik@pythonware.com\n# http://www.pythonware.com\n#\n# --------------------------------------------------------------------\n# The ElementTree toolkit is\n#\n# Copyright (c) 1999-2008 by Fredrik Lundh\n#\n# By obtaining, using, and/or copying this software and/or its\n# associated documentation, you agree that you have read, understood,\n# and will comply with the following terms and conditions:\n#\n# Permission to use, copy, modify, and distribute this software and\n# its associated documentation for any purpose and without fee is\n# hereby granted, provided that the above copyright notice appears in\n# all copies, and that both that copyright notice and this permission\n# notice appear in supporting documentation, and that the name of\n# Secret Labs AB or the author not be used in advertising or publicity\n# pertaining to distribution of the software without specific, written\n# prior permission.\n#\n# SECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD\n# TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANT-\n# ABILITY AND FITNESS.  IN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR\n# BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY\n# DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS,\n# WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS\n# ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE\n# OF THIS SOFTWARE.\n# --------------------------------------------------------------------\n\n# Licensed to PSF under a Contributor Agreement.\n# See http://www.python.org/psf/license for licensing details.\n\n__all__ = [\n    # public symbols\n    \"Comment\",\n    \"dump\",\n    \"Element\", \"ElementTree\",\n    \"fromstring\", \"fromstringlist\",\n    \"iselement\", \"iterparse\",\n    \"parse\", \"ParseError\",\n    \"PI\", \"ProcessingInstruction\",\n    \"QName\",\n    \"SubElement\",\n    \"tostring\", \"tostringlist\",\n    \"TreeBuilder\",\n    \"VERSION\",\n    \"XML\",\n    \"XMLParser\", \"XMLTreeBuilder\",\n    ]\n\nVERSION = \"1.3.0\"\n\n##\n# The <b>Element</b> type is a flexible container object, designed to\n# store hierarchical data structures in memory. The type can be\n# described as a cross between a list and a dictionary.\n# <p>\n# Each element has a number of properties associated with it:\n# <ul>\n# <li>a <i>tag</i>. This is a string identifying what kind of data\n# this element represents (the element type, in other words).</li>\n# <li>a number of <i>attributes</i>, stored in a Python dictionary.</li>\n# <li>a <i>text</i> string.</li>\n# <li>an optional <i>tail</i> string.</li>\n# <li>a number of <i>child elements</i>, stored in a Python sequence</li>\n# </ul>\n#\n# To create an element instance, use the {@link #Element} constructor\n# or the {@link #SubElement} factory function.\n# <p>\n# The {@link #ElementTree} class can be used to wrap an element\n# structure, and convert it from and to XML.\n##\n\nimport sys\nimport re\nimport warnings\n\n\nclass _SimpleElementPath(object):\n    # emulate pre-1.2 find/findtext/findall behaviour\n    def find(self, element, tag, namespaces=None):\n        for elem in element:\n            if elem.tag == tag:\n                return elem\n        return None\n    def findtext(self, element, tag, default=None, namespaces=None):\n        elem = self.find(element, tag)\n        if elem is None:\n            return default\n        return elem.text or \"\"\n    def iterfind(self, element, tag, namespaces=None):\n        if tag[:3] == \".//\":\n            for elem in element.iter(tag[3:]):\n                yield elem\n        for elem in element:\n            if elem.tag == tag:\n                yield elem\n    def findall(self, element, tag, namespaces=None):\n        return list(self.iterfind(element, tag, namespaces))\n\ntry:\n    from . import ElementPath\nexcept ImportError:\n    ElementPath = _SimpleElementPath()\n\n##\n# Parser error.  This is a subclass of <b>SyntaxError</b>.\n# <p>\n# In addition to the exception value, an exception instance contains a\n# specific exception code in the <b>code</b> attribute, and the line and\n# column of the error in the <b>position</b> attribute.\n\nclass ParseError(SyntaxError):\n    pass\n\n# --------------------------------------------------------------------\n\n##\n# Checks if an object appears to be a valid element object.\n#\n# @param An element instance.\n# @return A true value if this is an element object.\n# @defreturn flag\n\ndef iselement(element):\n    # FIXME: not sure about this; might be a better idea to look\n    # for tag/attrib/text attributes\n    return isinstance(element, Element) or hasattr(element, \"tag\")\n\n##\n# Element class.  This class defines the Element interface, and\n# provides a reference implementation of this interface.\n# <p>\n# The element name, attribute names, and attribute values can be\n# either ASCII strings (ordinary Python strings containing only 7-bit\n# ASCII characters) or Unicode strings.\n#\n# @param tag The element name.\n# @param attrib An optional dictionary, containing element attributes.\n# @param **extra Additional attributes, given as keyword arguments.\n# @see Element\n# @see SubElement\n# @see Comment\n# @see ProcessingInstruction\n\nclass Element(object):\n    # <tag attrib>text<child/>...</tag>tail\n\n    ##\n    # (Attribute) Element tag.\n\n    tag = None\n\n    ##\n    # (Attribute) Element attribute dictionary.  Where possible, use\n    # {@link #Element.get},\n    # {@link #Element.set},\n    # {@link #Element.keys}, and\n    # {@link #Element.items} to access\n    # element attributes.\n\n    attrib = None\n\n    ##\n    # (Attribute) Text before first subelement.  This is either a\n    # string or the value None.  Note that if there was no text, this\n    # attribute may be either None or an empty string, depending on\n    # the parser.\n\n    text = None\n\n    ##\n    # (Attribute) Text after this element's end tag, but before the\n    # next sibling element's start tag.  This is either a string or\n    # the value None.  Note that if there was no text, this attribute\n    # may be either None or an empty string, depending on the parser.\n\n    tail = None # text after end tag, if any\n\n    # constructor\n\n    def __init__(self, tag, attrib={}, **extra):\n        attrib = attrib.copy()\n        attrib.update(extra)\n        self.tag = tag\n        self.attrib = attrib\n        self._children = []\n\n    def __repr__(self):\n        return \"<Element %s at 0x%x>\" % (repr(self.tag), id(self))\n\n    ##\n    # Creates a new element object of the same type as this element.\n    #\n    # @param tag Element tag.\n    # @param attrib Element attributes, given as a dictionary.\n    # @return A new element instance.\n\n    def makeelement(self, tag, attrib):\n        return self.__class__(tag, attrib)\n\n    ##\n    # (Experimental) Copies the current element.  This creates a\n    # shallow copy; subelements will be shared with the original tree.\n    #\n    # @return A new element instance.\n\n    def copy(self):\n        elem = self.makeelement(self.tag, self.attrib)\n        elem.text = self.text\n        elem.tail = self.tail\n        elem[:] = self\n        return elem\n\n    ##\n    # Returns the number of subelements.  Note that this only counts\n    # full elements; to check if there's any content in an element, you\n    # have to check both the length and the <b>text</b> attribute.\n    #\n    # @return The number of subelements.\n\n    def __len__(self):\n        return len(self._children)\n\n    def __nonzero__(self):\n        warnings.warn(\n            \"The behavior of this method will change in future versions.  \"\n            \"Use specific 'len(elem)' or 'elem is not None' test instead.\",\n            FutureWarning, stacklevel=2\n            )\n        return len(self._children) != 0 # emulate old behaviour, for now\n\n    ##\n    # Returns the given subelement, by index.\n    #\n    # @param index What subelement to return.\n    # @return The given subelement.\n    # @exception IndexError If the given element does not exist.\n\n    def __getitem__(self, index):\n        return self._children[index]\n\n    ##\n    # Replaces the given subelement, by index.\n    #\n    # @param index What subelement to replace.\n    # @param element The new element value.\n    # @exception IndexError If the given element does not exist.\n\n    def __setitem__(self, index, element):\n        # if isinstance(index, slice):\n        #     for elt in element:\n        #         assert iselement(elt)\n        # else:\n        #     assert iselement(element)\n        self._children[index] = element\n\n    ##\n    # Deletes the given subelement, by index.\n    #\n    # @param index What subelement to delete.\n    # @exception IndexError If the given element does not exist.\n\n    def __delitem__(self, index):\n        del self._children[index]\n\n    ##\n    # Adds a subelement to the end of this element.  In document order,\n    # the new element will appear after the last existing subelement (or\n    # directly after the text, if it's the first subelement), but before\n    # the end tag for this element.\n    #\n    # @param element The element to add.\n\n    def append(self, element):\n        # assert iselement(element)\n        self._children.append(element)\n\n    ##\n    # Appends subelements from a sequence.\n    #\n    # @param elements A sequence object with zero or more elements.\n    # @since 1.3\n\n    def extend(self, elements):\n        # for element in elements:\n        #     assert iselement(element)\n        self._children.extend(elements)\n\n    ##\n    # Inserts a subelement at the given position in this element.\n    #\n    # @param index Where to insert the new subelement.\n\n    def insert(self, index, element):\n        # assert iselement(element)\n        self._children.insert(index, element)\n\n    ##\n    # Removes a matching subelement.  Unlike the <b>find</b> methods,\n    # this method compares elements based on identity, not on tag\n    # value or contents.  To remove subelements by other means, the\n    # easiest way is often to use a list comprehension to select what\n    # elements to keep, and use slice assignment to update the parent\n    # element.\n    #\n    # @param element What element to remove.\n    # @exception ValueError If a matching element could not be found.\n\n    def remove(self, element):\n        # assert iselement(element)\n        self._children.remove(element)\n\n    ##\n    # (Deprecated) Returns all subelements.  The elements are returned\n    # in document order.\n    #\n    # @return A list of subelements.\n    # @defreturn list of Element instances\n\n    def getchildren(self):\n        warnings.warn(\n            \"This method will be removed in future versions.  \"\n            \"Use 'list(elem)' or iteration over elem instead.\",\n            DeprecationWarning, stacklevel=2\n            )\n        return self._children\n\n    ##\n    # Finds the first matching subelement, by tag name or path.\n    #\n    # @param path What element to look for.\n    # @keyparam namespaces Optional namespace prefix map.\n    # @return The first matching element, or None if no element was found.\n    # @defreturn Element or None\n\n    def find(self, path, namespaces=None):\n        return ElementPath.find(self, path, namespaces)\n\n    ##\n    # Finds text for the first matching subelement, by tag name or path.\n    #\n    # @param path What element to look for.\n    # @param default What to return if the element was not found.\n    # @keyparam namespaces Optional namespace prefix map.\n    # @return The text content of the first matching element, or the\n    #     default value no element was found.  Note that if the element\n    #     is found, but has no text content, this method returns an\n    #     empty string.\n    # @defreturn string\n\n    def findtext(self, path, default=None, namespaces=None):\n        return ElementPath.findtext(self, path, default, namespaces)\n\n    ##\n    # Finds all matching subelements, by tag name or path.\n    #\n    # @param path What element to look for.\n    # @keyparam namespaces Optional namespace prefix map.\n    # @return A list or other sequence containing all matching elements,\n    #    in document order.\n    # @defreturn list of Element instances\n\n    def findall(self, path, namespaces=None):\n        return ElementPath.findall(self, path, namespaces)\n\n    ##\n    # Finds all matching subelements, by tag name or path.\n    #\n    # @param path What element to look for.\n    # @keyparam namespaces Optional namespace prefix map.\n    # @return An iterator or sequence containing all matching elements,\n    #    in document order.\n    # @defreturn a generated sequence of Element instances\n\n    def iterfind(self, path, namespaces=None):\n        return ElementPath.iterfind(self, path, namespaces)\n\n    ##\n    # Resets an element.  This function removes all subelements, clears\n    # all attributes, and sets the <b>text</b> and <b>tail</b> attributes\n    # to None.\n\n    def clear(self):\n        self.attrib.clear()\n        self._children = []\n        self.text = self.tail = None\n\n    ##\n    # Gets an element attribute.  Equivalent to <b>attrib.get</b>, but\n    # some implementations may handle this a bit more efficiently.\n    #\n    # @param key What attribute to look for.\n    # @param default What to return if the attribute was not found.\n    # @return The attribute value, or the default value, if the\n    #     attribute was not found.\n    # @defreturn string or None\n\n    def get(self, key, default=None):\n        return self.attrib.get(key, default)\n\n    ##\n    # Sets an element attribute.  Equivalent to <b>attrib[key] = value</b>,\n    # but some implementations may handle this a bit more efficiently.\n    #\n    # @param key What attribute to set.\n    # @param value The attribute value.\n\n    def set(self, key, value):\n        self.attrib[key] = value\n\n    ##\n    # Gets a list of attribute names.  The names are returned in an\n    # arbitrary order (just like for an ordinary Python dictionary).\n    # Equivalent to <b>attrib.keys()</b>.\n    #\n    # @return A list of element attribute names.\n    # @defreturn list of strings\n\n    def keys(self):\n        return self.attrib.keys()\n\n    ##\n    # Gets element attributes, as a sequence.  The attributes are\n    # returned in an arbitrary order.  Equivalent to <b>attrib.items()</b>.\n    #\n    # @return A list of (name, value) tuples for all attributes.\n    # @defreturn list of (string, string) tuples\n\n    def items(self):\n        return self.attrib.items()\n\n    ##\n    # Creates a tree iterator.  The iterator loops over this element\n    # and all subelements, in document order, and returns all elements\n    # with a matching tag.\n    # <p>\n    # If the tree structure is modified during iteration, new or removed\n    # elements may or may not be included.  To get a stable set, use the\n    # list() function on the iterator, and loop over the resulting list.\n    #\n    # @param tag What tags to look for (default is to return all elements).\n    # @return An iterator containing all the matching elements.\n    # @defreturn iterator\n\n    def iter(self, tag=None):\n        if tag == \"*\":\n            tag = None\n        if tag is None or self.tag == tag:\n            yield self\n        for e in self._children:\n            for e in e.iter(tag):\n                yield e\n\n    # compatibility\n    def getiterator(self, tag=None):\n        # Change for a DeprecationWarning in 1.4\n        warnings.warn(\n            \"This method will be removed in future versions.  \"\n            \"Use 'elem.iter()' or 'list(elem.iter())' instead.\",\n            PendingDeprecationWarning, stacklevel=2\n        )\n        return list(self.iter(tag))\n\n    ##\n    # Creates a text iterator.  The iterator loops over this element\n    # and all subelements, in document order, and returns all inner\n    # text.\n    #\n    # @return An iterator containing all inner text.\n    # @defreturn iterator\n\n    def itertext(self):\n        tag = self.tag\n        if not isinstance(tag, basestring) and tag is not None:\n            return\n        if self.text:\n            yield self.text\n        for e in self:\n            for s in e.itertext():\n                yield s\n            if e.tail:\n                yield e.tail\n\n# compatibility\n_Element = _ElementInterface = Element\n\n##\n# Subelement factory.  This function creates an element instance, and\n# appends it to an existing element.\n# <p>\n# The element name, attribute names, and attribute values can be\n# either 8-bit ASCII strings or Unicode strings.\n#\n# @param parent The parent element.\n# @param tag The subelement name.\n# @param attrib An optional dictionary, containing element attributes.\n# @param **extra Additional attributes, given as keyword arguments.\n# @return An element instance.\n# @defreturn Element\n\ndef SubElement(parent, tag, attrib={}, **extra):\n    attrib = attrib.copy()\n    attrib.update(extra)\n    element = parent.makeelement(tag, attrib)\n    parent.append(element)\n    return element\n\n##\n# Comment element factory.  This factory function creates a special\n# element that will be serialized as an XML comment by the standard\n# serializer.\n# <p>\n# The comment string can be either an 8-bit ASCII string or a Unicode\n# string.\n#\n# @param text A string containing the comment string.\n# @return An element instance, representing a comment.\n# @defreturn Element\n\ndef Comment(text=None):\n    element = Element(Comment)\n    element.text = text\n    return element\n\n##\n# PI element factory.  This factory function creates a special element\n# that will be serialized as an XML processing instruction by the standard\n# serializer.\n#\n# @param target A string containing the PI target.\n# @param text A string containing the PI contents, if any.\n# @return An element instance, representing a PI.\n# @defreturn Element\n\ndef ProcessingInstruction(target, text=None):\n    element = Element(ProcessingInstruction)\n    element.text = target\n    if text:\n        element.text = element.text + \" \" + text\n    return element\n\nPI = ProcessingInstruction\n\n##\n# QName wrapper.  This can be used to wrap a QName attribute value, in\n# order to get proper namespace handling on output.\n#\n# @param text A string containing the QName value, in the form {uri}local,\n#     or, if the tag argument is given, the URI part of a QName.\n# @param tag Optional tag.  If given, the first argument is interpreted as\n#     an URI, and this argument is interpreted as a local name.\n# @return An opaque object, representing the QName.\n\nclass QName(object):\n    def __init__(self, text_or_uri, tag=None):\n        if tag:\n            text_or_uri = \"{%s}%s\" % (text_or_uri, tag)\n        self.text = text_or_uri\n    def __str__(self):\n        return self.text\n    def __hash__(self):\n        return hash(self.text)\n    def __cmp__(self, other):\n        if isinstance(other, QName):\n            return cmp(self.text, other.text)\n        return cmp(self.text, other)\n\n# --------------------------------------------------------------------\n\n##\n# ElementTree wrapper class.  This class represents an entire element\n# hierarchy, and adds some extra support for serialization to and from\n# standard XML.\n#\n# @param element Optional root element.\n# @keyparam file Optional file handle or file name.  If given, the\n#     tree is initialized with the contents of this XML file.\n\nclass ElementTree(object):\n\n    def __init__(self, element=None, file=None):\n        # assert element is None or iselement(element)\n        self._root = element # first node\n        if file:\n            self.parse(file)\n\n    ##\n    # Gets the root element for this tree.\n    #\n    # @return An element instance.\n    # @defreturn Element\n\n    def getroot(self):\n        return self._root\n\n    ##\n    # Replaces the root element for this tree.  This discards the\n    # current contents of the tree, and replaces it with the given\n    # element.  Use with care.\n    #\n    # @param element An element instance.\n\n    def _setroot(self, element):\n        # assert iselement(element)\n        self._root = element\n\n    ##\n    # Loads an external XML document into this element tree.\n    #\n    # @param source A file name or file object.  If a file object is\n    #     given, it only has to implement a <b>read(n)</b> method.\n    # @keyparam parser An optional parser instance.  If not given, the\n    #     standard {@link XMLParser} parser is used.\n    # @return The document root element.\n    # @defreturn Element\n    # @exception ParseError If the parser fails to parse the document.\n\n    def parse(self, source, parser=None):\n        close_source = False\n        if not hasattr(source, \"read\"):\n            source = open(source, \"rb\")\n            close_source = True\n        try:\n            if not parser:\n                parser = XMLParser(target=TreeBuilder())\n            while 1:\n                data = source.read(65536)\n                if not data:\n                    break\n                parser.feed(data)\n            self._root = parser.close()\n            return self._root\n        finally:\n            if close_source:\n                source.close()\n\n    ##\n    # Creates a tree iterator for the root element.  The iterator loops\n    # over all elements in this tree, in document order.\n    #\n    # @param tag What tags to look for (default is to return all elements)\n    # @return An iterator.\n    # @defreturn iterator\n\n    def iter(self, tag=None):\n        # assert self._root is not None\n        return self._root.iter(tag)\n\n    # compatibility\n    def getiterator(self, tag=None):\n        # Change for a DeprecationWarning in 1.4\n        warnings.warn(\n            \"This method will be removed in future versions.  \"\n            \"Use 'tree.iter()' or 'list(tree.iter())' instead.\",\n            PendingDeprecationWarning, stacklevel=2\n        )\n        return list(self.iter(tag))\n\n    ##\n    # Same as getroot().find(path), starting at the root of the\n    # tree.\n    #\n    # @param path What element to look for.\n    # @keyparam namespaces Optional namespace prefix map.\n    # @return The first matching element, or None if no element was found.\n    # @defreturn Element or None\n\n    def find(self, path, namespaces=None):\n        # assert self._root is not None\n        if path[:1] == \"/\":\n            path = \".\" + path\n            warnings.warn(\n                \"This search is broken in 1.3 and earlier, and will be \"\n                \"fixed in a future version.  If you rely on the current \"\n                \"behaviour, change it to %r\" % path,\n                FutureWarning, stacklevel=2\n                )\n        return self._root.find(path, namespaces)\n\n    ##\n    # Same as getroot().findtext(path), starting at the root of the tree.\n    #\n    # @param path What element to look for.\n    # @param default What to return if the element was not found.\n    # @keyparam namespaces Optional namespace prefix map.\n    # @return The text content of the first matching element, or the\n    #     default value no element was found.  Note that if the element\n    #     is found, but has no text content, this method returns an\n    #     empty string.\n    # @defreturn string\n\n    def findtext(self, path, default=None, namespaces=None):\n        # assert self._root is not None\n        if path[:1] == \"/\":\n            path = \".\" + path\n            warnings.warn(\n                \"This search is broken in 1.3 and earlier, and will be \"\n                \"fixed in a future version.  If you rely on the current \"\n                \"behaviour, change it to %r\" % path,\n                FutureWarning, stacklevel=2\n                )\n        return self._root.findtext(path, default, namespaces)\n\n    ##\n    # Same as getroot().findall(path), starting at the root of the tree.\n    #\n    # @param path What element to look for.\n    # @keyparam namespaces Optional namespace prefix map.\n    # @return A list or iterator containing all matching elements,\n    #    in document order.\n    # @defreturn list of Element instances\n\n    def findall(self, path, namespaces=None):\n        # assert self._root is not None\n        if path[:1] == \"/\":\n            path = \".\" + path\n            warnings.warn(\n                \"This search is broken in 1.3 and earlier, and will be \"\n                \"fixed in a future version.  If you rely on the current \"\n                \"behaviour, change it to %r\" % path,\n                FutureWarning, stacklevel=2\n                )\n        return self._root.findall(path, namespaces)\n\n    ##\n    # Finds all matching subelements, by tag name or path.\n    # Same as getroot().iterfind(path).\n    #\n    # @param path What element to look for.\n    # @keyparam namespaces Optional namespace prefix map.\n    # @return An iterator or sequence containing all matching elements,\n    #    in document order.\n    # @defreturn a generated sequence of Element instances\n\n    def iterfind(self, path, namespaces=None):\n        # assert self._root is not None\n        if path[:1] == \"/\":\n            path = \".\" + path\n            warnings.warn(\n                \"This search is broken in 1.3 and earlier, and will be \"\n                \"fixed in a future version.  If you rely on the current \"\n                \"behaviour, change it to %r\" % path,\n                FutureWarning, stacklevel=2\n                )\n        return self._root.iterfind(path, namespaces)\n\n    ##\n    # Writes the element tree to a file, as XML.\n    #\n    # @def write(file, **options)\n    # @param file A file name, or a file object opened for writing.\n    # @param **options Options, given as keyword arguments.\n    # @keyparam encoding Optional output encoding (default is US-ASCII).\n    # @keyparam xml_declaration Controls if an XML declaration should\n    #     be added to the file.  Use False for never, True for always,\n    #     None for only if not US-ASCII or UTF-8.  None is default.\n    # @keyparam default_namespace Sets the default XML namespace (for \"xmlns\").\n    # @keyparam method Optional output method (\"xml\", \"html\", \"text\" or\n    #     \"c14n\"; default is \"xml\").\n\n    def write(self, file_or_filename,\n              # keyword arguments\n              encoding=None,\n              xml_declaration=None,\n              default_namespace=None,\n              method=None):\n        # assert self._root is not None\n        if not method:\n            method = \"xml\"\n        elif method not in _serialize:\n            # FIXME: raise an ImportError for c14n if ElementC14N is missing?\n            raise ValueError(\"unknown method %r\" % method)\n        if hasattr(file_or_filename, \"write\"):\n            file = file_or_filename\n        else:\n            file = open(file_or_filename, \"wb\")\n        write = file.write\n        if not encoding:\n            if method == \"c14n\":\n                encoding = \"utf-8\"\n            else:\n                encoding = \"us-ascii\"\n        elif xml_declaration or (xml_declaration is None and\n                                 encoding not in (\"utf-8\", \"us-ascii\")):\n            if method == \"xml\":\n                write(\"<?xml version='1.0' encoding='%s'?>\\n\" % encoding)\n        if method == \"text\":\n            _serialize_text(write, self._root, encoding)\n        else:\n            qnames, namespaces = _namespaces(\n                self._root, encoding, default_namespace\n                )\n            serialize = _serialize[method]\n            serialize(write, self._root, encoding, qnames, namespaces)\n        if file_or_filename is not file:\n            file.close()\n\n    def write_c14n(self, file):\n        # lxml.etree compatibility.  use output method instead\n        return self.write(file, method=\"c14n\")\n\n# --------------------------------------------------------------------\n# serialization support\n\ndef _namespaces(elem, encoding, default_namespace=None):\n    # identify namespaces used in this tree\n\n    # maps qnames to *encoded* prefix:local names\n    qnames = {None: None}\n\n    # maps uri:s to prefixes\n    namespaces = {}\n    if default_namespace:\n        namespaces[default_namespace] = \"\"\n\n    def encode(text):\n        return text.encode(encoding)\n\n    def add_qname(qname):\n        # calculate serialized qname representation\n        try:\n            if qname[:1] == \"{\":\n                uri, tag = qname[1:].rsplit(\"}\", 1)\n                prefix = namespaces.get(uri)\n                if prefix is None:\n                    prefix = _namespace_map.get(uri)\n                    if prefix is None:\n                        prefix = \"ns%d\" % len(namespaces)\n                    if prefix != \"xml\":\n                        namespaces[uri] = prefix\n                if prefix:\n                    qnames[qname] = encode(\"%s:%s\" % (prefix, tag))\n                else:\n                    qnames[qname] = encode(tag) # default element\n            else:\n                if default_namespace:\n                    # FIXME: can this be handled in XML 1.0?\n                    raise ValueError(\n                        \"cannot use non-qualified names with \"\n                        \"default_namespace option\"\n                        )\n                qnames[qname] = encode(qname)\n        except TypeError:\n            _raise_serialization_error(qname)\n\n    # populate qname and namespaces table\n    try:\n        iterate = elem.iter\n    except AttributeError:\n        iterate = elem.getiterator # cET compatibility\n    for elem in iterate():\n        tag = elem.tag\n        if isinstance(tag, QName):\n            if tag.text not in qnames:\n                add_qname(tag.text)\n        elif isinstance(tag, basestring):\n            if tag not in qnames:\n                add_qname(tag)\n        elif tag is not None and tag is not Comment and tag is not PI:\n            _raise_serialization_error(tag)\n        for key, value in elem.items():\n            if isinstance(key, QName):\n                key = key.text\n            if key not in qnames:\n                add_qname(key)\n            if isinstance(value, QName) and value.text not in qnames:\n                add_qname(value.text)\n        text = elem.text\n        if isinstance(text, QName) and text.text not in qnames:\n            add_qname(text.text)\n    return qnames, namespaces\n\ndef _serialize_xml(write, elem, encoding, qnames, namespaces):\n    tag = elem.tag\n    text = elem.text\n    if tag is Comment:\n        write(\"<!--%s-->\" % _encode(text, encoding))\n    elif tag is ProcessingInstruction:\n        write(\"<?%s?>\" % _encode(text, encoding))\n    else:\n        tag = qnames[tag]\n        if tag is None:\n            if text:\n                write(_escape_cdata(text, encoding))\n            for e in elem:\n                _serialize_xml(write, e, encoding, qnames, None)\n        else:\n            write(\"<\" + tag)\n            items = elem.items()\n            if items or namespaces:\n                if namespaces:\n                    for v, k in sorted(namespaces.items(),\n                                       key=lambda x: x[1]):  # sort on prefix\n                        if k:\n                            k = \":\" + k\n                        write(\" xmlns%s=\\\"%s\\\"\" % (\n                            k.encode(encoding),\n                            _escape_attrib(v, encoding)\n                            ))\n                for k, v in sorted(items):  # lexical order\n                    if isinstance(k, QName):\n                        k = k.text\n                    if isinstance(v, QName):\n                        v = qnames[v.text]\n                    else:\n                        v = _escape_attrib(v, encoding)\n                    write(\" %s=\\\"%s\\\"\" % (qnames[k], v))\n            if text or len(elem):\n                write(\">\")\n                if text:\n                    write(_escape_cdata(text, encoding))\n                for e in elem:\n                    _serialize_xml(write, e, encoding, qnames, None)\n                write(\"</\" + tag + \">\")\n            else:\n                write(\" />\")\n    if elem.tail:\n        write(_escape_cdata(elem.tail, encoding))\n\nHTML_EMPTY = (\"area\", \"base\", \"basefont\", \"br\", \"col\", \"frame\", \"hr\",\n              \"img\", \"input\", \"isindex\", \"link\", \"meta\", \"param\")\n\ntry:\n    HTML_EMPTY = set(HTML_EMPTY)\nexcept NameError:\n    pass\n\ndef _serialize_html(write, elem, encoding, qnames, namespaces):\n    tag = elem.tag\n    text = elem.text\n    if tag is Comment:\n        write(\"<!--%s-->\" % _escape_cdata(text, encoding))\n    elif tag is ProcessingInstruction:\n        write(\"<?%s?>\" % _escape_cdata(text, encoding))\n    else:\n        tag = qnames[tag]\n        if tag is None:\n            if text:\n                write(_escape_cdata(text, encoding))\n            for e in elem:\n                _serialize_html(write, e, encoding, qnames, None)\n        else:\n            write(\"<\" + tag)\n            items = elem.items()\n            if items or namespaces:\n                if namespaces:\n                    for v, k in sorted(namespaces.items(),\n                                       key=lambda x: x[1]):  # sort on prefix\n                        if k:\n                            k = \":\" + k\n                        write(\" xmlns%s=\\\"%s\\\"\" % (\n                            k.encode(encoding),\n                            _escape_attrib(v, encoding)\n                            ))\n                for k, v in sorted(items):  # lexical order\n                    if isinstance(k, QName):\n                        k = k.text\n                    if isinstance(v, QName):\n                        v = qnames[v.text]\n                    else:\n                        v = _escape_attrib_html(v, encoding)\n                    # FIXME: handle boolean attributes\n                    write(\" %s=\\\"%s\\\"\" % (qnames[k], v))\n            write(\">\")\n            ltag = tag.lower()\n            if text:\n                if ltag == \"script\" or ltag == \"style\":\n                    write(_encode(text, encoding))\n                else:\n                    write(_escape_cdata(text, encoding))\n            for e in elem:\n                _serialize_html(write, e, encoding, qnames, None)\n            if ltag not in HTML_EMPTY:\n                write(\"</\" + tag + \">\")\n    if elem.tail:\n        write(_escape_cdata(elem.tail, encoding))\n\ndef _serialize_text(write, elem, encoding):\n    for part in elem.itertext():\n        write(part.encode(encoding))\n    if elem.tail:\n        write(elem.tail.encode(encoding))\n\n_serialize = {\n    \"xml\": _serialize_xml,\n    \"html\": _serialize_html,\n    \"text\": _serialize_text,\n# this optional method is imported at the end of the module\n#   \"c14n\": _serialize_c14n,\n}\n\n##\n# Registers a namespace prefix.  The registry is global, and any\n# existing mapping for either the given prefix or the namespace URI\n# will be removed.\n#\n# @param prefix Namespace prefix.\n# @param uri Namespace uri.  Tags and attributes in this namespace\n#     will be serialized with the given prefix, if at all possible.\n# @exception ValueError If the prefix is reserved, or is otherwise\n#     invalid.\n\ndef register_namespace(prefix, uri):\n    if re.match(\"ns\\d+$\", prefix):\n        raise ValueError(\"Prefix format reserved for internal use\")\n    for k, v in _namespace_map.items():\n        if k == uri or v == prefix:\n            del _namespace_map[k]\n    _namespace_map[uri] = prefix\n\n_namespace_map = {\n    # \"well-known\" namespace prefixes\n    \"http://www.w3.org/XML/1998/namespace\": \"xml\",\n    \"http://www.w3.org/1999/xhtml\": \"html\",\n    \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\": \"rdf\",\n    \"http://schemas.xmlsoap.org/wsdl/\": \"wsdl\",\n    # xml schema\n    \"http://www.w3.org/2001/XMLSchema\": \"xs\",\n    \"http://www.w3.org/2001/XMLSchema-instance\": \"xsi\",\n    # dublin core\n    \"http://purl.org/dc/elements/1.1/\": \"dc\",\n}\n\ndef _raise_serialization_error(text):\n    raise TypeError(\n        \"cannot serialize %r (type %s)\" % (text, type(text).__name__)\n        )\n\ndef _encode(text, encoding):\n    try:\n        return text.encode(encoding, \"xmlcharrefreplace\")\n    except (TypeError, AttributeError):\n        _raise_serialization_error(text)\n\ndef _escape_cdata(text, encoding):\n    # escape character data\n    try:\n        # it's worth avoiding do-nothing calls for strings that are\n        # shorter than 500 character, or so.  assume that's, by far,\n        # the most common case in most applications.\n        if \"&\" in text:\n            text = text.replace(\"&\", \"&amp;\")\n        if \"<\" in text:\n            text = text.replace(\"<\", \"&lt;\")\n        if \">\" in text:\n            text = text.replace(\">\", \"&gt;\")\n        return text.encode(encoding, \"xmlcharrefreplace\")\n    except (TypeError, AttributeError):\n        _raise_serialization_error(text)\n\ndef _escape_attrib(text, encoding):\n    # escape attribute value\n    try:\n        if \"&\" in text:\n            text = text.replace(\"&\", \"&amp;\")\n        if \"<\" in text:\n            text = text.replace(\"<\", \"&lt;\")\n        if \">\" in text:\n            text = text.replace(\">\", \"&gt;\")\n        if \"\\\"\" in text:\n            text = text.replace(\"\\\"\", \"&quot;\")\n        if \"\\n\" in text:\n            text = text.replace(\"\\n\", \"&#10;\")\n        return text.encode(encoding, \"xmlcharrefreplace\")\n    except (TypeError, AttributeError):\n        _raise_serialization_error(text)\n\ndef _escape_attrib_html(text, encoding):\n    # escape attribute value\n    try:\n        if \"&\" in text:\n            text = text.replace(\"&\", \"&amp;\")\n        if \">\" in text:\n            text = text.replace(\">\", \"&gt;\")\n        if \"\\\"\" in text:\n            text = text.replace(\"\\\"\", \"&quot;\")\n        return text.encode(encoding, \"xmlcharrefreplace\")\n    except (TypeError, AttributeError):\n        _raise_serialization_error(text)\n\n# --------------------------------------------------------------------\n\n##\n# Generates a string representation of an XML element, including all\n# subelements.\n#\n# @param element An Element instance.\n# @keyparam encoding Optional output encoding (default is US-ASCII).\n# @keyparam method Optional output method (\"xml\", \"html\", \"text\" or\n#     \"c14n\"; default is \"xml\").\n# @return An encoded string containing the XML data.\n# @defreturn string\n\ndef tostring(element, encoding=None, method=None):\n    class dummy:\n        pass\n    data = []\n    file = dummy()\n    file.write = data.append\n    ElementTree(element).write(file, encoding, method=method)\n    return \"\".join(data)\n\n##\n# Generates a string representation of an XML element, including all\n# subelements.  The string is returned as a sequence of string fragments.\n#\n# @param element An Element instance.\n# @keyparam encoding Optional output encoding (default is US-ASCII).\n# @keyparam method Optional output method (\"xml\", \"html\", \"text\" or\n#     \"c14n\"; default is \"xml\").\n# @return A sequence object containing the XML data.\n# @defreturn sequence\n# @since 1.3\n\ndef tostringlist(element, encoding=None, method=None):\n    class dummy:\n        pass\n    data = []\n    file = dummy()\n    file.write = data.append\n    ElementTree(element).write(file, encoding, method=method)\n    # FIXME: merge small fragments into larger parts\n    return data\n\n##\n# Writes an element tree or element structure to sys.stdout.  This\n# function should be used for debugging only.\n# <p>\n# The exact output format is implementation dependent.  In this\n# version, it's written as an ordinary XML file.\n#\n# @param elem An element tree or an individual element.\n\ndef dump(elem):\n    # debugging\n    if not isinstance(elem, ElementTree):\n        elem = ElementTree(elem)\n    elem.write(sys.stdout)\n    tail = elem.getroot().tail\n    if not tail or tail[-1] != \"\\n\":\n        sys.stdout.write(\"\\n\")\n\n# --------------------------------------------------------------------\n# parsing\n\n##\n# Parses an XML document into an element tree.\n#\n# @param source A filename or file object containing XML data.\n# @param parser An optional parser instance.  If not given, the\n#     standard {@link XMLParser} parser is used.\n# @return An ElementTree instance\n\ndef parse(source, parser=None):\n    tree = ElementTree()\n    tree.parse(source, parser)\n    return tree\n\n##\n# Parses an XML document into an element tree incrementally, and reports\n# what's going on to the user.\n#\n# @param source A filename or file object containing XML data.\n# @param events A list of events to report back.  If omitted, only \"end\"\n#     events are reported.\n# @param parser An optional parser instance.  If not given, the\n#     standard {@link XMLParser} parser is used.\n# @return A (event, elem) iterator.\n\ndef iterparse(source, events=None, parser=None):\n    close_source = False\n    if not hasattr(source, \"read\"):\n        source = open(source, \"rb\")\n        close_source = True\n    if not parser:\n        parser = XMLParser(target=TreeBuilder())\n    return _IterParseIterator(source, events, parser, close_source)\n\nclass _IterParseIterator(object):\n\n    def __init__(self, source, events, parser, close_source=False):\n        self._file = source\n        self._close_file = close_source\n        self._events = []\n        self._index = 0\n        self._error = None\n        self.root = self._root = None\n        self._parser = parser\n        # wire up the parser for event reporting\n        parser = self._parser._parser\n        append = self._events.append\n        if events is None:\n            events = [\"end\"]\n        for event in events:\n            if event == \"start\":\n                try:\n                    parser.ordered_attributes = 1\n                    parser.specified_attributes = 1\n                    def handler(tag, attrib_in, event=event, append=append,\n                                start=self._parser._start_list):\n                        append((event, start(tag, attrib_in)))\n                    parser.StartElementHandler = handler\n                except AttributeError:\n                    def handler(tag, attrib_in, event=event, append=append,\n                                start=self._parser._start):\n                        append((event, start(tag, attrib_in)))\n                    parser.StartElementHandler = handler\n            elif event == \"end\":\n                def handler(tag, event=event, append=append,\n                            end=self._parser._end):\n                    append((event, end(tag)))\n                parser.EndElementHandler = handler\n            elif event == \"start-ns\":\n                def handler(prefix, uri, event=event, append=append):\n                    try:\n                        uri = (uri or \"\").encode(\"ascii\")\n                    except UnicodeError:\n                        pass\n                    append((event, (prefix or \"\", uri or \"\")))\n                parser.StartNamespaceDeclHandler = handler\n            elif event == \"end-ns\":\n                def handler(prefix, event=event, append=append):\n                    append((event, None))\n                parser.EndNamespaceDeclHandler = handler\n            else:\n                raise ValueError(\"unknown event %r\" % event)\n\n    def next(self):\n        while 1:\n            try:\n                item = self._events[self._index]\n                self._index += 1\n                return item\n            except IndexError:\n                pass\n            if self._error:\n                e = self._error\n                self._error = None\n                raise e\n            if self._parser is None:\n                self.root = self._root\n                if self._close_file:\n                    self._file.close()\n                raise StopIteration\n            # load event buffer\n            del self._events[:]\n            self._index = 0\n            data = self._file.read(16384)\n            if data:\n                try:\n                    self._parser.feed(data)\n                except SyntaxError as exc:\n                    self._error = exc\n            else:\n                self._root = self._parser.close()\n                self._parser = None\n\n    def __iter__(self):\n        return self\n\n##\n# Parses an XML document from a string constant.  This function can\n# be used to embed \"XML literals\" in Python code.\n#\n# @param source A string containing XML data.\n# @param parser An optional parser instance.  If not given, the\n#     standard {@link XMLParser} parser is used.\n# @return An Element instance.\n# @defreturn Element\n\ndef XML(text, parser=None):\n    if not parser:\n        parser = XMLParser(target=TreeBuilder())\n    parser.feed(text)\n    return parser.close()\n\n##\n# Parses an XML document from a string constant, and also returns\n# a dictionary which maps from element id:s to elements.\n#\n# @param source A string containing XML data.\n# @param parser An optional parser instance.  If not given, the\n#     standard {@link XMLParser} parser is used.\n# @return A tuple containing an Element instance and a dictionary.\n# @defreturn (Element, dictionary)\n\ndef XMLID(text, parser=None):\n    if not parser:\n        parser = XMLParser(target=TreeBuilder())\n    parser.feed(text)\n    tree = parser.close()\n    ids = {}\n    for elem in tree.iter():\n        id = elem.get(\"id\")\n        if id:\n            ids[id] = elem\n    return tree, ids\n\n##\n# Parses an XML document from a string constant.  Same as {@link #XML}.\n#\n# @def fromstring(text)\n# @param source A string containing XML data.\n# @return An Element instance.\n# @defreturn Element\n\nfromstring = XML\n\n##\n# Parses an XML document from a sequence of string fragments.\n#\n# @param sequence A list or other sequence containing XML data fragments.\n# @param parser An optional parser instance.  If not given, the\n#     standard {@link XMLParser} parser is used.\n# @return An Element instance.\n# @defreturn Element\n# @since 1.3\n\ndef fromstringlist(sequence, parser=None):\n    if not parser:\n        parser = XMLParser(target=TreeBuilder())\n    for text in sequence:\n        parser.feed(text)\n    return parser.close()\n\n# --------------------------------------------------------------------\n\n##\n# Generic element structure builder.  This builder converts a sequence\n# of {@link #TreeBuilder.start}, {@link #TreeBuilder.data}, and {@link\n# #TreeBuilder.end} method calls to a well-formed element structure.\n# <p>\n# You can use this class to build an element structure using a custom XML\n# parser, or a parser for some other XML-like format.\n#\n# @param element_factory Optional element factory.  This factory\n#    is called to create new Element instances, as necessary.\n\nclass TreeBuilder(object):\n\n    def __init__(self, element_factory=None):\n        self._data = [] # data collector\n        self._elem = [] # element stack\n        self._last = None # last element\n        self._tail = None # true if we're after an end tag\n        if element_factory is None:\n            element_factory = Element\n        self._factory = element_factory\n\n    ##\n    # Flushes the builder buffers, and returns the toplevel document\n    # element.\n    #\n    # @return An Element instance.\n    # @defreturn Element\n\n    def close(self):\n        assert len(self._elem) == 0, \"missing end tags\"\n        assert self._last is not None, \"missing toplevel element\"\n        return self._last\n\n    def _flush(self):\n        if self._data:\n            if self._last is not None:\n                text = \"\".join(self._data)\n                if self._tail:\n                    assert self._last.tail is None, \"internal error (tail)\"\n                    self._last.tail = text\n                else:\n                    assert self._last.text is None, \"internal error (text)\"\n                    self._last.text = text\n            self._data = []\n\n    ##\n    # Adds text to the current element.\n    #\n    # @param data A string.  This should be either an 8-bit string\n    #    containing ASCII text, or a Unicode string.\n\n    def data(self, data):\n        self._data.append(data)\n\n    ##\n    # Opens a new element.\n    #\n    # @param tag The element name.\n    # @param attrib A dictionary containing element attributes.\n    # @return The opened element.\n    # @defreturn Element\n\n    def start(self, tag, attrs):\n        self._flush()\n        self._last = elem = self._factory(tag, attrs)\n        if self._elem:\n            self._elem[-1].append(elem)\n        self._elem.append(elem)\n        self._tail = 0\n        return elem\n\n    ##\n    # Closes the current element.\n    #\n    # @param tag The element name.\n    # @return The closed element.\n    # @defreturn Element\n\n    def end(self, tag):\n        self._flush()\n        self._last = self._elem.pop()\n        assert self._last.tag == tag,\\\n               \"end tag mismatch (expected %s, got %s)\" % (\n                   self._last.tag, tag)\n        self._tail = 1\n        return self._last\n\n##\n# Element structure builder for XML source data, based on the\n# <b>expat</b> parser.\n#\n# @keyparam target Target object.  If omitted, the builder uses an\n#     instance of the standard {@link #TreeBuilder} class.\n# @keyparam html Predefine HTML entities.  This flag is not supported\n#     by the current implementation.\n# @keyparam encoding Optional encoding.  If given, the value overrides\n#     the encoding specified in the XML file.\n# @see #ElementTree\n# @see #TreeBuilder\n\nclass XMLParser(object):\n\n    def __init__(self, html=0, target=None, encoding=None):\n        try:\n            from xml.parsers import expat\n        except ImportError:\n            try:\n                import pyexpat as expat\n            except ImportError:\n                raise ImportError(\n                    \"No module named expat; use SimpleXMLTreeBuilder instead\"\n                    )\n        parser = expat.ParserCreate(encoding, \"}\")\n        if target is None:\n            target = TreeBuilder()\n        # underscored names are provided for compatibility only\n        self.parser = self._parser = parser\n        self.target = self._target = target\n        self._error = expat.error\n        self._names = {} # name memo cache\n        # callbacks\n        parser.DefaultHandlerExpand = self._default\n        parser.StartElementHandler = self._start\n        parser.EndElementHandler = self._end\n        parser.CharacterDataHandler = self._data\n        # optional callbacks\n        parser.CommentHandler = self._comment\n        parser.ProcessingInstructionHandler = self._pi\n        # let expat do the buffering, if supported\n        try:\n            self._parser.buffer_text = 1\n        except AttributeError:\n            pass\n        # use new-style attribute handling, if supported\n        try:\n            self._parser.ordered_attributes = 1\n            self._parser.specified_attributes = 1\n            parser.StartElementHandler = self._start_list\n        except AttributeError:\n            pass\n        self._doctype = None\n        self.entity = {}\n        try:\n            self.version = \"Expat %d.%d.%d\" % expat.version_info\n        except AttributeError:\n            pass # unknown\n\n    def _raiseerror(self, value):\n        err = ParseError(value)\n        err.code = value.code\n        err.position = value.lineno, value.offset\n        raise err\n\n    def _fixtext(self, text):\n        # convert text string to ascii, if possible\n        try:\n            return text.encode(\"ascii\")\n        except UnicodeError:\n            return text\n\n    def _fixname(self, key):\n        # expand qname, and convert name string to ascii, if possible\n        try:\n            name = self._names[key]\n        except KeyError:\n            name = key\n            if \"}\" in name:\n                name = \"{\" + name\n            self._names[key] = name = self._fixtext(name)\n        return name\n\n    def _start(self, tag, attrib_in):\n        fixname = self._fixname\n        fixtext = self._fixtext\n        tag = fixname(tag)\n        attrib = {}\n        for key, value in attrib_in.items():\n            attrib[fixname(key)] = fixtext(value)\n        return self.target.start(tag, attrib)\n\n    def _start_list(self, tag, attrib_in):\n        fixname = self._fixname\n        fixtext = self._fixtext\n        tag = fixname(tag)\n        attrib = {}\n        if attrib_in:\n            for i in range(0, len(attrib_in), 2):\n                attrib[fixname(attrib_in[i])] = fixtext(attrib_in[i+1])\n        return self.target.start(tag, attrib)\n\n    def _data(self, text):\n        return self.target.data(self._fixtext(text))\n\n    def _end(self, tag):\n        return self.target.end(self._fixname(tag))\n\n    def _comment(self, data):\n        try:\n            comment = self.target.comment\n        except AttributeError:\n            pass\n        else:\n            return comment(self._fixtext(data))\n\n    def _pi(self, target, data):\n        try:\n            pi = self.target.pi\n        except AttributeError:\n            pass\n        else:\n            return pi(self._fixtext(target), self._fixtext(data))\n\n    def _default(self, text):\n        prefix = text[:1]\n        if prefix == \"&\":\n            # deal with undefined entities\n            try:\n                self.target.data(self.entity[text[1:-1]])\n            except KeyError:\n                from xml.parsers import expat\n                err = expat.error(\n                    \"undefined entity %s: line %d, column %d\" %\n                    (text, self._parser.ErrorLineNumber,\n                    self._parser.ErrorColumnNumber)\n                    )\n                err.code = 11 # XML_ERROR_UNDEFINED_ENTITY\n                err.lineno = self._parser.ErrorLineNumber\n                err.offset = self._parser.ErrorColumnNumber\n                raise err\n        elif prefix == \"<\" and text[:9] == \"<!DOCTYPE\":\n            self._doctype = [] # inside a doctype declaration\n        elif self._doctype is not None:\n            # parse doctype contents\n            if prefix == \">\":\n                self._doctype = None\n                return\n            text = text.strip()\n            if not text:\n                return\n            self._doctype.append(text)\n            n = len(self._doctype)\n            if n > 2:\n                type = self._doctype[1]\n                if type == \"PUBLIC\" and n == 4:\n                    name, type, pubid, system = self._doctype\n                elif type == \"SYSTEM\" and n == 3:\n                    name, type, system = self._doctype\n                    pubid = None\n                else:\n                    return\n                if pubid:\n                    pubid = pubid[1:-1]\n                if hasattr(self.target, \"doctype\"):\n                    self.target.doctype(name, pubid, system[1:-1])\n                elif self.doctype is not self._XMLParser__doctype:\n                    # warn about deprecated call\n                    self._XMLParser__doctype(name, pubid, system[1:-1])\n                    self.doctype(name, pubid, system[1:-1])\n                self._doctype = None\n\n    ##\n    # (Deprecated) Handles a doctype declaration.\n    #\n    # @param name Doctype name.\n    # @param pubid Public identifier.\n    # @param system System identifier.\n\n    def doctype(self, name, pubid, system):\n        \"\"\"This method of XMLParser is deprecated.\"\"\"\n        warnings.warn(\n            \"This method of XMLParser is deprecated.  Define doctype() \"\n            \"method on the TreeBuilder target.\",\n            DeprecationWarning,\n            )\n\n    # sentinel, if doctype is redefined in a subclass\n    __doctype = doctype\n\n    ##\n    # Feeds data to the parser.\n    #\n    # @param data Encoded data.\n\n    def feed(self, data):\n        try:\n            self._parser.Parse(data, 0)\n        except self._error, v:\n            self._raiseerror(v)\n\n    ##\n    # Finishes feeding data to the parser.\n    #\n    # @return An element structure.\n    # @defreturn Element\n\n    def close(self):\n        try:\n            self._parser.Parse(\"\", 1) # end of data\n        except self._error, v:\n            self._raiseerror(v)\n        tree = self.target.close()\n        del self.target, self._parser # get rid of circular references\n        return tree\n\n# compatibility\nXMLTreeBuilder = XMLParser\n\n# workaround circular import.\ntry:\n    from ElementC14N import _serialize_c14n\n    _serialize[\"c14n\"] = _serialize_c14n\nexcept ImportError:\n    pass\n", 
    "xml.etree.__init__": "# $Id: __init__.py 3375 2008-02-13 08:05:08Z fredrik $\n# elementtree package\n\n# --------------------------------------------------------------------\n# The ElementTree toolkit is\n#\n# Copyright (c) 1999-2008 by Fredrik Lundh\n#\n# By obtaining, using, and/or copying this software and/or its\n# associated documentation, you agree that you have read, understood,\n# and will comply with the following terms and conditions:\n#\n# Permission to use, copy, modify, and distribute this software and\n# its associated documentation for any purpose and without fee is\n# hereby granted, provided that the above copyright notice appears in\n# all copies, and that both that copyright notice and this permission\n# notice appear in supporting documentation, and that the name of\n# Secret Labs AB or the author not be used in advertising or publicity\n# pertaining to distribution of the software without specific, written\n# prior permission.\n#\n# SECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD\n# TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANT-\n# ABILITY AND FITNESS.  IN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR\n# BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY\n# DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS,\n# WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS\n# ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE\n# OF THIS SOFTWARE.\n# --------------------------------------------------------------------\n\n# Licensed to PSF under a Contributor Agreement.\n# See http://www.python.org/psf/license for licensing details.\n", 
    "xml.parsers.__init__": "\"\"\"Python interfaces to XML parsers.\n\nThis package contains one module:\n\nexpat -- Python wrapper for James Clark's Expat parser, with namespace\n         support.\n\n\"\"\"\n", 
    "xml.parsers.expat": "\"\"\"Interface to the Expat non-validating XML parser.\"\"\"\n__version__ = '$Revision: 17640 $'\n\nfrom pyexpat import *\n", 
    "xml.sax.__init__": "\"\"\"Simple API for XML (SAX) implementation for Python.\n\nThis module provides an implementation of the SAX 2 interface;\ninformation about the Java version of the interface can be found at\nhttp://www.megginson.com/SAX/.  The Python version of the interface is\ndocumented at <...>.\n\nThis package contains the following modules:\n\nhandler -- Base classes and constants which define the SAX 2 API for\n           the 'client-side' of SAX for Python.\n\nsaxutils -- Implementation of the convenience classes commonly used to\n            work with SAX.\n\nxmlreader -- Base classes and constants which define the SAX 2 API for\n             the parsers used with SAX for Python.\n\nexpatreader -- Driver that allows use of the Expat parser with SAX.\n\"\"\"\n\nfrom xmlreader import InputSource\nfrom handler import ContentHandler, ErrorHandler\nfrom _exceptions import SAXException, SAXNotRecognizedException, \\\n                        SAXParseException, SAXNotSupportedException, \\\n                        SAXReaderNotAvailable\n\n\ndef parse(source, handler, errorHandler=ErrorHandler()):\n    parser = make_parser()\n    parser.setContentHandler(handler)\n    parser.setErrorHandler(errorHandler)\n    parser.parse(source)\n\ndef parseString(string, handler, errorHandler=ErrorHandler()):\n    try:\n        from cStringIO import StringIO\n    except ImportError:\n        from StringIO import StringIO\n\n    if errorHandler is None:\n        errorHandler = ErrorHandler()\n    parser = make_parser()\n    parser.setContentHandler(handler)\n    parser.setErrorHandler(errorHandler)\n\n    inpsrc = InputSource()\n    inpsrc.setByteStream(StringIO(string))\n    parser.parse(inpsrc)\n\n# this is the parser list used by the make_parser function if no\n# alternatives are given as parameters to the function\n\ndefault_parser_list = [\"xml.sax.expatreader\"]\n\n# tell modulefinder that importing sax potentially imports expatreader\n_false = 0\nif _false:\n    import xml.sax.expatreader\n\nimport os, sys\nif \"PY_SAX_PARSER\" in os.environ:\n    default_parser_list = os.environ[\"PY_SAX_PARSER\"].split(\",\")\ndel os\n\n_key = \"python.xml.sax.parser\"\nif sys.platform[:4] == \"java\" and sys.registry.containsKey(_key):\n    default_parser_list = sys.registry.getProperty(_key).split(\",\")\n\n\ndef make_parser(parser_list = []):\n    \"\"\"Creates and returns a SAX parser.\n\n    Creates the first parser it is able to instantiate of the ones\n    given in the list created by doing parser_list +\n    default_parser_list.  The lists must contain the names of Python\n    modules containing both a SAX parser and a create_parser function.\"\"\"\n\n    for parser_name in parser_list + default_parser_list:\n        try:\n            return _create_parser(parser_name)\n        except ImportError,e:\n            import sys\n            if parser_name in sys.modules:\n                # The parser module was found, but importing it\n                # failed unexpectedly, pass this exception through\n                raise\n        except SAXReaderNotAvailable:\n            # The parser module detected that it won't work properly,\n            # so try the next one\n            pass\n\n    raise SAXReaderNotAvailable(\"No parsers found\", None)\n\n# --- Internal utility methods used by make_parser\n\nif sys.platform[ : 4] == \"java\":\n    def _create_parser(parser_name):\n        from org.python.core import imp\n        drv_module = imp.importName(parser_name, 0, globals())\n        return drv_module.create_parser()\n\nelse:\n    def _create_parser(parser_name):\n        drv_module = __import__(parser_name,{},{},['create_parser'])\n        return drv_module.create_parser()\n\ndel sys\n", 
    "xml.sax._exceptions": "\"\"\"Different kinds of SAX Exceptions\"\"\"\nimport sys\nif sys.platform[:4] == \"java\":\n    from java.lang import Exception\ndel sys\n\n# ===== SAXEXCEPTION =====\n\nclass SAXException(Exception):\n    \"\"\"Encapsulate an XML error or warning. This class can contain\n    basic error or warning information from either the XML parser or\n    the application: you can subclass it to provide additional\n    functionality, or to add localization. Note that although you will\n    receive a SAXException as the argument to the handlers in the\n    ErrorHandler interface, you are not actually required to raise\n    the exception; instead, you can simply read the information in\n    it.\"\"\"\n\n    def __init__(self, msg, exception=None):\n        \"\"\"Creates an exception. The message is required, but the exception\n        is optional.\"\"\"\n        self._msg = msg\n        self._exception = exception\n        Exception.__init__(self, msg)\n\n    def getMessage(self):\n        \"Return a message for this exception.\"\n        return self._msg\n\n    def getException(self):\n        \"Return the embedded exception, or None if there was none.\"\n        return self._exception\n\n    def __str__(self):\n        \"Create a string representation of the exception.\"\n        return self._msg\n\n    def __getitem__(self, ix):\n        \"\"\"Avoids weird error messages if someone does exception[ix] by\n        mistake, since Exception has __getitem__ defined.\"\"\"\n        raise AttributeError(\"__getitem__\")\n\n\n# ===== SAXPARSEEXCEPTION =====\n\nclass SAXParseException(SAXException):\n    \"\"\"Encapsulate an XML parse error or warning.\n\n    This exception will include information for locating the error in\n    the original XML document. Note that although the application will\n    receive a SAXParseException as the argument to the handlers in the\n    ErrorHandler interface, the application is not actually required\n    to raise the exception; instead, it can simply read the\n    information in it and take a different action.\n\n    Since this exception is a subclass of SAXException, it inherits\n    the ability to wrap another exception.\"\"\"\n\n    def __init__(self, msg, exception, locator):\n        \"Creates the exception. The exception parameter is allowed to be None.\"\n        SAXException.__init__(self, msg, exception)\n        self._locator = locator\n\n        # We need to cache this stuff at construction time.\n        # If this exception is raised, the objects through which we must\n        # traverse to get this information may be deleted by the time\n        # it gets caught.\n        self._systemId = self._locator.getSystemId()\n        self._colnum = self._locator.getColumnNumber()\n        self._linenum = self._locator.getLineNumber()\n\n    def getColumnNumber(self):\n        \"\"\"The column number of the end of the text where the exception\n        occurred.\"\"\"\n        return self._colnum\n\n    def getLineNumber(self):\n        \"The line number of the end of the text where the exception occurred.\"\n        return self._linenum\n\n    def getPublicId(self):\n        \"Get the public identifier of the entity where the exception occurred.\"\n        return self._locator.getPublicId()\n\n    def getSystemId(self):\n        \"Get the system identifier of the entity where the exception occurred.\"\n        return self._systemId\n\n    def __str__(self):\n        \"Create a string representation of the exception.\"\n        sysid = self.getSystemId()\n        if sysid is None:\n            sysid = \"<unknown>\"\n        linenum = self.getLineNumber()\n        if linenum is None:\n            linenum = \"?\"\n        colnum = self.getColumnNumber()\n        if colnum is None:\n            colnum = \"?\"\n        return \"%s:%s:%s: %s\" % (sysid, linenum, colnum, self._msg)\n\n\n# ===== SAXNOTRECOGNIZEDEXCEPTION =====\n\nclass SAXNotRecognizedException(SAXException):\n    \"\"\"Exception class for an unrecognized identifier.\n\n    An XMLReader will raise this exception when it is confronted with an\n    unrecognized feature or property. SAX applications and extensions may\n    use this class for similar purposes.\"\"\"\n\n\n# ===== SAXNOTSUPPORTEDEXCEPTION =====\n\nclass SAXNotSupportedException(SAXException):\n    \"\"\"Exception class for an unsupported operation.\n\n    An XMLReader will raise this exception when a service it cannot\n    perform is requested (specifically setting a state or value). SAX\n    applications and extensions may use this class for similar\n    purposes.\"\"\"\n\n# ===== SAXNOTSUPPORTEDEXCEPTION =====\n\nclass SAXReaderNotAvailable(SAXNotSupportedException):\n    \"\"\"Exception class for a missing driver.\n\n    An XMLReader module (driver) should raise this exception when it\n    is first imported, e.g. when a support module cannot be imported.\n    It also may be raised during parsing, e.g. if executing an external\n    program is not permitted.\"\"\"\n", 
    "xml.sax.expatreader": "\"\"\"\nSAX driver for the pyexpat C module.  This driver works with\npyexpat.__version__ == '2.22'.\n\"\"\"\n\nversion = \"0.20\"\n\nfrom xml.sax._exceptions import *\nfrom xml.sax.handler import feature_validation, feature_namespaces\nfrom xml.sax.handler import feature_namespace_prefixes\nfrom xml.sax.handler import feature_external_ges, feature_external_pes\nfrom xml.sax.handler import feature_string_interning\nfrom xml.sax.handler import property_xml_string, property_interning_dict\n\n# xml.parsers.expat does not raise ImportError in Jython\nimport sys\nif sys.platform[:4] == \"java\":\n    raise SAXReaderNotAvailable(\"expat not available in Java\", None)\ndel sys\n\ntry:\n    from xml.parsers import expat\nexcept ImportError:\n    raise SAXReaderNotAvailable(\"expat not supported\", None)\nelse:\n    if not hasattr(expat, \"ParserCreate\"):\n        raise SAXReaderNotAvailable(\"expat not supported\", None)\nfrom xml.sax import xmlreader, saxutils, handler\n\nAttributesImpl = xmlreader.AttributesImpl\nAttributesNSImpl = xmlreader.AttributesNSImpl\n\n# If we're using a sufficiently recent version of Python, we can use\n# weak references to avoid cycles between the parser and content\n# handler, otherwise we'll just have to pretend.\ntry:\n    import _weakref\nexcept ImportError:\n    def _mkproxy(o):\n        return o\nelse:\n    import weakref\n    _mkproxy = weakref.proxy\n    del weakref, _weakref\n\n# --- ExpatLocator\n\nclass ExpatLocator(xmlreader.Locator):\n    \"\"\"Locator for use with the ExpatParser class.\n\n    This uses a weak reference to the parser object to avoid creating\n    a circular reference between the parser and the content handler.\n    \"\"\"\n    def __init__(self, parser):\n        self._ref = _mkproxy(parser)\n\n    def getColumnNumber(self):\n        parser = self._ref\n        if parser._parser is None:\n            return None\n        return parser._parser.ErrorColumnNumber\n\n    def getLineNumber(self):\n        parser = self._ref\n        if parser._parser is None:\n            return 1\n        return parser._parser.ErrorLineNumber\n\n    def getPublicId(self):\n        parser = self._ref\n        if parser is None:\n            return None\n        return parser._source.getPublicId()\n\n    def getSystemId(self):\n        parser = self._ref\n        if parser is None:\n            return None\n        return parser._source.getSystemId()\n\n\n# --- ExpatParser\n\nclass ExpatParser(xmlreader.IncrementalParser, xmlreader.Locator):\n    \"\"\"SAX driver for the pyexpat C module.\"\"\"\n\n    def __init__(self, namespaceHandling=0, bufsize=2**16-20):\n        xmlreader.IncrementalParser.__init__(self, bufsize)\n        self._source = xmlreader.InputSource()\n        self._parser = None\n        self._namespaces = namespaceHandling\n        self._lex_handler_prop = None\n        self._parsing = 0\n        self._entity_stack = []\n        self._external_ges = 1\n        self._interning = None\n\n    # XMLReader methods\n\n    def parse(self, source):\n        \"Parse an XML document from a URL or an InputSource.\"\n        source = saxutils.prepare_input_source(source)\n\n        self._source = source\n        self.reset()\n        self._cont_handler.setDocumentLocator(ExpatLocator(self))\n        xmlreader.IncrementalParser.parse(self, source)\n\n    def prepareParser(self, source):\n        if source.getSystemId() is not None:\n            base = source.getSystemId()\n            if isinstance(base, unicode):\n                base = base.encode('utf-8')\n            self._parser.SetBase(base)\n\n    # Redefined setContentHandler to allow changing handlers during parsing\n\n    def setContentHandler(self, handler):\n        xmlreader.IncrementalParser.setContentHandler(self, handler)\n        if self._parsing:\n            self._reset_cont_handler()\n\n    def getFeature(self, name):\n        if name == feature_namespaces:\n            return self._namespaces\n        elif name == feature_string_interning:\n            return self._interning is not None\n        elif name in (feature_validation, feature_external_pes,\n                      feature_namespace_prefixes):\n            return 0\n        elif name == feature_external_ges:\n            return self._external_ges\n        raise SAXNotRecognizedException(\"Feature '%s' not recognized\" % name)\n\n    def setFeature(self, name, state):\n        if self._parsing:\n            raise SAXNotSupportedException(\"Cannot set features while parsing\")\n\n        if name == feature_namespaces:\n            self._namespaces = state\n        elif name == feature_external_ges:\n            self._external_ges = state\n        elif name == feature_string_interning:\n            if state:\n                if self._interning is None:\n                    self._interning = {}\n            else:\n                self._interning = None\n        elif name == feature_validation:\n            if state:\n                raise SAXNotSupportedException(\n                    \"expat does not support validation\")\n        elif name == feature_external_pes:\n            if state:\n                raise SAXNotSupportedException(\n                    \"expat does not read external parameter entities\")\n        elif name == feature_namespace_prefixes:\n            if state:\n                raise SAXNotSupportedException(\n                    \"expat does not report namespace prefixes\")\n        else:\n            raise SAXNotRecognizedException(\n                \"Feature '%s' not recognized\" % name)\n\n    def getProperty(self, name):\n        if name == handler.property_lexical_handler:\n            return self._lex_handler_prop\n        elif name == property_interning_dict:\n            return self._interning\n        elif name == property_xml_string:\n            if self._parser:\n                if hasattr(self._parser, \"GetInputContext\"):\n                    return self._parser.GetInputContext()\n                else:\n                    raise SAXNotRecognizedException(\n                        \"This version of expat does not support getting\"\n                        \" the XML string\")\n            else:\n                raise SAXNotSupportedException(\n                    \"XML string cannot be returned when not parsing\")\n        raise SAXNotRecognizedException(\"Property '%s' not recognized\" % name)\n\n    def setProperty(self, name, value):\n        if name == handler.property_lexical_handler:\n            self._lex_handler_prop = value\n            if self._parsing:\n                self._reset_lex_handler_prop()\n        elif name == property_interning_dict:\n            self._interning = value\n        elif name == property_xml_string:\n            raise SAXNotSupportedException(\"Property '%s' cannot be set\" %\n                                           name)\n        else:\n            raise SAXNotRecognizedException(\"Property '%s' not recognized\" %\n                                            name)\n\n    # IncrementalParser methods\n\n    def feed(self, data, isFinal = 0):\n        if not self._parsing:\n            self.reset()\n            self._parsing = 1\n            self._cont_handler.startDocument()\n\n        try:\n            # The isFinal parameter is internal to the expat reader.\n            # If it is set to true, expat will check validity of the entire\n            # document. When feeding chunks, they are not normally final -\n            # except when invoked from close.\n            self._parser.Parse(data, isFinal)\n        except expat.error, e:\n            exc = SAXParseException(expat.ErrorString(e.code), e, self)\n            # FIXME: when to invoke error()?\n            self._err_handler.fatalError(exc)\n\n    def close(self):\n        if self._entity_stack:\n            # If we are completing an external entity, do nothing here\n            return\n        self.feed(\"\", isFinal = 1)\n        self._cont_handler.endDocument()\n        self._parsing = 0\n        # break cycle created by expat handlers pointing to our methods\n        self._parser = None\n\n    def _reset_cont_handler(self):\n        self._parser.ProcessingInstructionHandler = \\\n                                    self._cont_handler.processingInstruction\n        self._parser.CharacterDataHandler = self._cont_handler.characters\n\n    def _reset_lex_handler_prop(self):\n        lex = self._lex_handler_prop\n        parser = self._parser\n        if lex is None:\n            parser.CommentHandler = None\n            parser.StartCdataSectionHandler = None\n            parser.EndCdataSectionHandler = None\n            parser.StartDoctypeDeclHandler = None\n            parser.EndDoctypeDeclHandler = None\n        else:\n            parser.CommentHandler = lex.comment\n            parser.StartCdataSectionHandler = lex.startCDATA\n            parser.EndCdataSectionHandler = lex.endCDATA\n            parser.StartDoctypeDeclHandler = self.start_doctype_decl\n            parser.EndDoctypeDeclHandler = lex.endDTD\n\n    def reset(self):\n        if self._namespaces:\n            self._parser = expat.ParserCreate(self._source.getEncoding(), \" \",\n                                              intern=self._interning)\n            self._parser.namespace_prefixes = 1\n            self._parser.StartElementHandler = self.start_element_ns\n            self._parser.EndElementHandler = self.end_element_ns\n        else:\n            self._parser = expat.ParserCreate(self._source.getEncoding(),\n                                              intern = self._interning)\n            self._parser.StartElementHandler = self.start_element\n            self._parser.EndElementHandler = self.end_element\n\n        self._reset_cont_handler()\n        self._parser.UnparsedEntityDeclHandler = self.unparsed_entity_decl\n        self._parser.NotationDeclHandler = self.notation_decl\n        self._parser.StartNamespaceDeclHandler = self.start_namespace_decl\n        self._parser.EndNamespaceDeclHandler = self.end_namespace_decl\n\n        self._decl_handler_prop = None\n        if self._lex_handler_prop:\n            self._reset_lex_handler_prop()\n#         self._parser.DefaultHandler =\n#         self._parser.DefaultHandlerExpand =\n#         self._parser.NotStandaloneHandler =\n        self._parser.ExternalEntityRefHandler = self.external_entity_ref\n        try:\n            self._parser.SkippedEntityHandler = self.skipped_entity_handler\n        except AttributeError:\n            # This pyexpat does not support SkippedEntity\n            pass\n        self._parser.SetParamEntityParsing(\n            expat.XML_PARAM_ENTITY_PARSING_UNLESS_STANDALONE)\n\n        self._parsing = 0\n        self._entity_stack = []\n\n    # Locator methods\n\n    def getColumnNumber(self):\n        if self._parser is None:\n            return None\n        return self._parser.ErrorColumnNumber\n\n    def getLineNumber(self):\n        if self._parser is None:\n            return 1\n        return self._parser.ErrorLineNumber\n\n    def getPublicId(self):\n        return self._source.getPublicId()\n\n    def getSystemId(self):\n        return self._source.getSystemId()\n\n    # event handlers\n    def start_element(self, name, attrs):\n        self._cont_handler.startElement(name, AttributesImpl(attrs))\n\n    def end_element(self, name):\n        self._cont_handler.endElement(name)\n\n    def start_element_ns(self, name, attrs):\n        pair = name.split()\n        if len(pair) == 1:\n            # no namespace\n            pair = (None, name)\n        elif len(pair) == 3:\n            pair = pair[0], pair[1]\n        else:\n            # default namespace\n            pair = tuple(pair)\n\n        newattrs = {}\n        qnames = {}\n        for (aname, value) in attrs.items():\n            parts = aname.split()\n            length = len(parts)\n            if length == 1:\n                # no namespace\n                qname = aname\n                apair = (None, aname)\n            elif length == 3:\n                qname = \"%s:%s\" % (parts[2], parts[1])\n                apair = parts[0], parts[1]\n            else:\n                # default namespace\n                qname = parts[1]\n                apair = tuple(parts)\n\n            newattrs[apair] = value\n            qnames[apair] = qname\n\n        self._cont_handler.startElementNS(pair, None,\n                                          AttributesNSImpl(newattrs, qnames))\n\n    def end_element_ns(self, name):\n        pair = name.split()\n        if len(pair) == 1:\n            pair = (None, name)\n        elif len(pair) == 3:\n            pair = pair[0], pair[1]\n        else:\n            pair = tuple(pair)\n\n        self._cont_handler.endElementNS(pair, None)\n\n    # this is not used (call directly to ContentHandler)\n    def processing_instruction(self, target, data):\n        self._cont_handler.processingInstruction(target, data)\n\n    # this is not used (call directly to ContentHandler)\n    def character_data(self, data):\n        self._cont_handler.characters(data)\n\n    def start_namespace_decl(self, prefix, uri):\n        self._cont_handler.startPrefixMapping(prefix, uri)\n\n    def end_namespace_decl(self, prefix):\n        self._cont_handler.endPrefixMapping(prefix)\n\n    def start_doctype_decl(self, name, sysid, pubid, has_internal_subset):\n        self._lex_handler_prop.startDTD(name, pubid, sysid)\n\n    def unparsed_entity_decl(self, name, base, sysid, pubid, notation_name):\n        self._dtd_handler.unparsedEntityDecl(name, pubid, sysid, notation_name)\n\n    def notation_decl(self, name, base, sysid, pubid):\n        self._dtd_handler.notationDecl(name, pubid, sysid)\n\n    def external_entity_ref(self, context, base, sysid, pubid):\n        if not self._external_ges:\n            return 1\n\n        source = self._ent_handler.resolveEntity(pubid, sysid)\n        source = saxutils.prepare_input_source(source,\n                                               self._source.getSystemId() or\n                                               \"\")\n\n        self._entity_stack.append((self._parser, self._source))\n        self._parser = self._parser.ExternalEntityParserCreate(context)\n        self._source = source\n\n        try:\n            xmlreader.IncrementalParser.parse(self, source)\n        except:\n            return 0  # FIXME: save error info here?\n\n        (self._parser, self._source) = self._entity_stack[-1]\n        del self._entity_stack[-1]\n        return 1\n\n    def skipped_entity_handler(self, name, is_pe):\n        if is_pe:\n            # The SAX spec requires to report skipped PEs with a '%'\n            name = '%'+name\n        self._cont_handler.skippedEntity(name)\n\n# ---\n\ndef create_parser(*args, **kwargs):\n    return ExpatParser(*args, **kwargs)\n\n# ---\n\nif __name__ == \"__main__\":\n    import xml.sax.saxutils\n    p = create_parser()\n    p.setContentHandler(xml.sax.saxutils.XMLGenerator())\n    p.setErrorHandler(xml.sax.ErrorHandler())\n    p.parse(\"http://www.ibiblio.org/xml/examples/shakespeare/hamlet.xml\")\n", 
    "xml.sax.handler": "\"\"\"\nThis module contains the core classes of version 2.0 of SAX for Python.\nThis file provides only default classes with absolutely minimum\nfunctionality, from which drivers and applications can be subclassed.\n\nMany of these classes are empty and are included only as documentation\nof the interfaces.\n\n$Id$\n\"\"\"\n\nversion = '2.0beta'\n\n#============================================================================\n#\n# HANDLER INTERFACES\n#\n#============================================================================\n\n# ===== ERRORHANDLER =====\n\nclass ErrorHandler:\n    \"\"\"Basic interface for SAX error handlers.\n\n    If you create an object that implements this interface, then\n    register the object with your XMLReader, the parser will call the\n    methods in your object to report all warnings and errors. There\n    are three levels of errors available: warnings, (possibly)\n    recoverable errors, and unrecoverable errors. All methods take a\n    SAXParseException as the only parameter.\"\"\"\n\n    def error(self, exception):\n        \"Handle a recoverable error.\"\n        raise exception\n\n    def fatalError(self, exception):\n        \"Handle a non-recoverable error.\"\n        raise exception\n\n    def warning(self, exception):\n        \"Handle a warning.\"\n        print exception\n\n\n# ===== CONTENTHANDLER =====\n\nclass ContentHandler:\n    \"\"\"Interface for receiving logical document content events.\n\n    This is the main callback interface in SAX, and the one most\n    important to applications. The order of events in this interface\n    mirrors the order of the information in the document.\"\"\"\n\n    def __init__(self):\n        self._locator = None\n\n    def setDocumentLocator(self, locator):\n        \"\"\"Called by the parser to give the application a locator for\n        locating the origin of document events.\n\n        SAX parsers are strongly encouraged (though not absolutely\n        required) to supply a locator: if it does so, it must supply\n        the locator to the application by invoking this method before\n        invoking any of the other methods in the DocumentHandler\n        interface.\n\n        The locator allows the application to determine the end\n        position of any document-related event, even if the parser is\n        not reporting an error. Typically, the application will use\n        this information for reporting its own errors (such as\n        character content that does not match an application's\n        business rules). The information returned by the locator is\n        probably not sufficient for use with a search engine.\n\n        Note that the locator will return correct information only\n        during the invocation of the events in this interface. The\n        application should not attempt to use it at any other time.\"\"\"\n        self._locator = locator\n\n    def startDocument(self):\n        \"\"\"Receive notification of the beginning of a document.\n\n        The SAX parser will invoke this method only once, before any\n        other methods in this interface or in DTDHandler (except for\n        setDocumentLocator).\"\"\"\n\n    def endDocument(self):\n        \"\"\"Receive notification of the end of a document.\n\n        The SAX parser will invoke this method only once, and it will\n        be the last method invoked during the parse. The parser shall\n        not invoke this method until it has either abandoned parsing\n        (because of an unrecoverable error) or reached the end of\n        input.\"\"\"\n\n    def startPrefixMapping(self, prefix, uri):\n        \"\"\"Begin the scope of a prefix-URI Namespace mapping.\n\n        The information from this event is not necessary for normal\n        Namespace processing: the SAX XML reader will automatically\n        replace prefixes for element and attribute names when the\n        http://xml.org/sax/features/namespaces feature is true (the\n        default).\n\n        There are cases, however, when applications need to use\n        prefixes in character data or in attribute values, where they\n        cannot safely be expanded automatically; the\n        start/endPrefixMapping event supplies the information to the\n        application to expand prefixes in those contexts itself, if\n        necessary.\n\n        Note that start/endPrefixMapping events are not guaranteed to\n        be properly nested relative to each-other: all\n        startPrefixMapping events will occur before the corresponding\n        startElement event, and all endPrefixMapping events will occur\n        after the corresponding endElement event, but their order is\n        not guaranteed.\"\"\"\n\n    def endPrefixMapping(self, prefix):\n        \"\"\"End the scope of a prefix-URI mapping.\n\n        See startPrefixMapping for details. This event will always\n        occur after the corresponding endElement event, but the order\n        of endPrefixMapping events is not otherwise guaranteed.\"\"\"\n\n    def startElement(self, name, attrs):\n        \"\"\"Signals the start of an element in non-namespace mode.\n\n        The name parameter contains the raw XML 1.0 name of the\n        element type as a string and the attrs parameter holds an\n        instance of the Attributes class containing the attributes of\n        the element.\"\"\"\n\n    def endElement(self, name):\n        \"\"\"Signals the end of an element in non-namespace mode.\n\n        The name parameter contains the name of the element type, just\n        as with the startElement event.\"\"\"\n\n    def startElementNS(self, name, qname, attrs):\n        \"\"\"Signals the start of an element in namespace mode.\n\n        The name parameter contains the name of the element type as a\n        (uri, localname) tuple, the qname parameter the raw XML 1.0\n        name used in the source document, and the attrs parameter\n        holds an instance of the Attributes class containing the\n        attributes of the element.\n\n        The uri part of the name tuple is None for elements which have\n        no namespace.\"\"\"\n\n    def endElementNS(self, name, qname):\n        \"\"\"Signals the end of an element in namespace mode.\n\n        The name parameter contains the name of the element type, just\n        as with the startElementNS event.\"\"\"\n\n    def characters(self, content):\n        \"\"\"Receive notification of character data.\n\n        The Parser will call this method to report each chunk of\n        character data. SAX parsers may return all contiguous\n        character data in a single chunk, or they may split it into\n        several chunks; however, all of the characters in any single\n        event must come from the same external entity so that the\n        Locator provides useful information.\"\"\"\n\n    def ignorableWhitespace(self, whitespace):\n        \"\"\"Receive notification of ignorable whitespace in element content.\n\n        Validating Parsers must use this method to report each chunk\n        of ignorable whitespace (see the W3C XML 1.0 recommendation,\n        section 2.10): non-validating parsers may also use this method\n        if they are capable of parsing and using content models.\n\n        SAX parsers may return all contiguous whitespace in a single\n        chunk, or they may split it into several chunks; however, all\n        of the characters in any single event must come from the same\n        external entity, so that the Locator provides useful\n        information.\"\"\"\n\n    def processingInstruction(self, target, data):\n        \"\"\"Receive notification of a processing instruction.\n\n        The Parser will invoke this method once for each processing\n        instruction found: note that processing instructions may occur\n        before or after the main document element.\n\n        A SAX parser should never report an XML declaration (XML 1.0,\n        section 2.8) or a text declaration (XML 1.0, section 4.3.1)\n        using this method.\"\"\"\n\n    def skippedEntity(self, name):\n        \"\"\"Receive notification of a skipped entity.\n\n        The Parser will invoke this method once for each entity\n        skipped. Non-validating processors may skip entities if they\n        have not seen the declarations (because, for example, the\n        entity was declared in an external DTD subset). All processors\n        may skip external entities, depending on the values of the\n        http://xml.org/sax/features/external-general-entities and the\n        http://xml.org/sax/features/external-parameter-entities\n        properties.\"\"\"\n\n\n# ===== DTDHandler =====\n\nclass DTDHandler:\n    \"\"\"Handle DTD events.\n\n    This interface specifies only those DTD events required for basic\n    parsing (unparsed entities and attributes).\"\"\"\n\n    def notationDecl(self, name, publicId, systemId):\n        \"Handle a notation declaration event.\"\n\n    def unparsedEntityDecl(self, name, publicId, systemId, ndata):\n        \"Handle an unparsed entity declaration event.\"\n\n\n# ===== ENTITYRESOLVER =====\n\nclass EntityResolver:\n    \"\"\"Basic interface for resolving entities. If you create an object\n    implementing this interface, then register the object with your\n    Parser, the parser will call the method in your object to\n    resolve all external entities. Note that DefaultHandler implements\n    this interface with the default behaviour.\"\"\"\n\n    def resolveEntity(self, publicId, systemId):\n        \"\"\"Resolve the system identifier of an entity and return either\n        the system identifier to read from as a string, or an InputSource\n        to read from.\"\"\"\n        return systemId\n\n\n#============================================================================\n#\n# CORE FEATURES\n#\n#============================================================================\n\nfeature_namespaces = \"http://xml.org/sax/features/namespaces\"\n# true: Perform Namespace processing (default).\n# false: Optionally do not perform Namespace processing\n#        (implies namespace-prefixes).\n# access: (parsing) read-only; (not parsing) read/write\n\nfeature_namespace_prefixes = \"http://xml.org/sax/features/namespace-prefixes\"\n# true: Report the original prefixed names and attributes used for Namespace\n#       declarations.\n# false: Do not report attributes used for Namespace declarations, and\n#        optionally do not report original prefixed names (default).\n# access: (parsing) read-only; (not parsing) read/write\n\nfeature_string_interning = \"http://xml.org/sax/features/string-interning\"\n# true: All element names, prefixes, attribute names, Namespace URIs, and\n#       local names are interned using the built-in intern function.\n# false: Names are not necessarily interned, although they may be (default).\n# access: (parsing) read-only; (not parsing) read/write\n\nfeature_validation = \"http://xml.org/sax/features/validation\"\n# true: Report all validation errors (implies external-general-entities and\n#       external-parameter-entities).\n# false: Do not report validation errors.\n# access: (parsing) read-only; (not parsing) read/write\n\nfeature_external_ges = \"http://xml.org/sax/features/external-general-entities\"\n# true: Include all external general (text) entities.\n# false: Do not include external general entities.\n# access: (parsing) read-only; (not parsing) read/write\n\nfeature_external_pes = \"http://xml.org/sax/features/external-parameter-entities\"\n# true: Include all external parameter entities, including the external\n#       DTD subset.\n# false: Do not include any external parameter entities, even the external\n#        DTD subset.\n# access: (parsing) read-only; (not parsing) read/write\n\nall_features = [feature_namespaces,\n                feature_namespace_prefixes,\n                feature_string_interning,\n                feature_validation,\n                feature_external_ges,\n                feature_external_pes]\n\n\n#============================================================================\n#\n# CORE PROPERTIES\n#\n#============================================================================\n\nproperty_lexical_handler = \"http://xml.org/sax/properties/lexical-handler\"\n# data type: xml.sax.sax2lib.LexicalHandler\n# description: An optional extension handler for lexical events like comments.\n# access: read/write\n\nproperty_declaration_handler = \"http://xml.org/sax/properties/declaration-handler\"\n# data type: xml.sax.sax2lib.DeclHandler\n# description: An optional extension handler for DTD-related events other\n#              than notations and unparsed entities.\n# access: read/write\n\nproperty_dom_node = \"http://xml.org/sax/properties/dom-node\"\n# data type: org.w3c.dom.Node\n# description: When parsing, the current DOM node being visited if this is\n#              a DOM iterator; when not parsing, the root DOM node for\n#              iteration.\n# access: (parsing) read-only; (not parsing) read/write\n\nproperty_xml_string = \"http://xml.org/sax/properties/xml-string\"\n# data type: String\n# description: The literal string of characters that was the source for\n#              the current event.\n# access: read-only\n\nproperty_encoding = \"http://www.python.org/sax/properties/encoding\"\n# data type: String\n# description: The name of the encoding to assume for input data.\n# access: write: set the encoding, e.g. established by a higher-level\n#                protocol. May change during parsing (e.g. after\n#                processing a META tag)\n#         read:  return the current encoding (possibly established through\n#                auto-detection.\n# initial value: UTF-8\n#\n\nproperty_interning_dict = \"http://www.python.org/sax/properties/interning-dict\"\n# data type: Dictionary\n# description: The dictionary used to intern common strings in the document\n# access: write: Request that the parser uses a specific dictionary, to\n#                allow interning across different documents\n#         read:  return the current interning dictionary, or None\n#\n\nall_properties = [property_lexical_handler,\n                  property_dom_node,\n                  property_declaration_handler,\n                  property_xml_string,\n                  property_encoding,\n                  property_interning_dict]\n", 
    "xml.sax.saxutils": "\"\"\"\\\nA library of useful helper classes to the SAX classes, for the\nconvenience of application and driver writers.\n\"\"\"\n\nimport os, urlparse, urllib, types\nimport io\nimport sys\nimport handler\nimport xmlreader\n\ntry:\n    _StringTypes = [types.StringType, types.UnicodeType]\nexcept AttributeError:\n    _StringTypes = [types.StringType]\n\ndef __dict_replace(s, d):\n    \"\"\"Replace substrings of a string using a dictionary.\"\"\"\n    for key, value in d.items():\n        s = s.replace(key, value)\n    return s\n\ndef escape(data, entities={}):\n    \"\"\"Escape &, <, and > in a string of data.\n\n    You can escape other strings of data by passing a dictionary as\n    the optional entities parameter.  The keys and values must all be\n    strings; each key will be replaced with its corresponding value.\n    \"\"\"\n\n    # must do ampersand first\n    data = data.replace(\"&\", \"&amp;\")\n    data = data.replace(\">\", \"&gt;\")\n    data = data.replace(\"<\", \"&lt;\")\n    if entities:\n        data = __dict_replace(data, entities)\n    return data\n\ndef unescape(data, entities={}):\n    \"\"\"Unescape &amp;, &lt;, and &gt; in a string of data.\n\n    You can unescape other strings of data by passing a dictionary as\n    the optional entities parameter.  The keys and values must all be\n    strings; each key will be replaced with its corresponding value.\n    \"\"\"\n    data = data.replace(\"&lt;\", \"<\")\n    data = data.replace(\"&gt;\", \">\")\n    if entities:\n        data = __dict_replace(data, entities)\n    # must do ampersand last\n    return data.replace(\"&amp;\", \"&\")\n\ndef quoteattr(data, entities={}):\n    \"\"\"Escape and quote an attribute value.\n\n    Escape &, <, and > in a string of data, then quote it for use as\n    an attribute value.  The \\\" character will be escaped as well, if\n    necessary.\n\n    You can escape other strings of data by passing a dictionary as\n    the optional entities parameter.  The keys and values must all be\n    strings; each key will be replaced with its corresponding value.\n    \"\"\"\n    entities = entities.copy()\n    entities.update({'\\n': '&#10;', '\\r': '&#13;', '\\t':'&#9;'})\n    data = escape(data, entities)\n    if '\"' in data:\n        if \"'\" in data:\n            data = '\"%s\"' % data.replace('\"', \"&quot;\")\n        else:\n            data = \"'%s'\" % data\n    else:\n        data = '\"%s\"' % data\n    return data\n\n\ndef _gettextwriter(out, encoding):\n    if out is None:\n        import sys\n        out = sys.stdout\n\n    if isinstance(out, io.RawIOBase):\n        buffer = io.BufferedIOBase(out)\n        # Keep the original file open when the TextIOWrapper is\n        # destroyed\n        buffer.close = lambda: None\n    else:\n        # This is to handle passed objects that aren't in the\n        # IOBase hierarchy, but just have a write method\n        buffer = io.BufferedIOBase()\n        buffer.writable = lambda: True\n        buffer.write = out.write\n        try:\n            # TextIOWrapper uses this methods to determine\n            # if BOM (for UTF-16, etc) should be added\n            buffer.seekable = out.seekable\n            buffer.tell = out.tell\n        except AttributeError:\n            pass\n    # wrap a binary writer with TextIOWrapper\n    return _UnbufferedTextIOWrapper(buffer, encoding=encoding,\n                                   errors='xmlcharrefreplace',\n                                   newline='\\n')\n# PyPy: moved this class outside the function above\nclass _UnbufferedTextIOWrapper(io.TextIOWrapper):\n    def write(self, s):\n        super(_UnbufferedTextIOWrapper, self).write(s)\n        self.flush()\n\nclass XMLGenerator(handler.ContentHandler):\n\n    def __init__(self, out=None, encoding=\"iso-8859-1\"):\n        handler.ContentHandler.__init__(self)\n        out = _gettextwriter(out, encoding)\n        self._write = out.write\n        self._flush = out.flush\n        self._ns_contexts = [{}] # contains uri -> prefix dicts\n        self._current_context = self._ns_contexts[-1]\n        self._undeclared_ns_maps = []\n        self._encoding = encoding\n\n    def _qname(self, name):\n        \"\"\"Builds a qualified name from a (ns_url, localname) pair\"\"\"\n        if name[0]:\n            # Per http://www.w3.org/XML/1998/namespace, The 'xml' prefix is\n            # bound by definition to http://www.w3.org/XML/1998/namespace.  It\n            # does not need to be declared and will not usually be found in\n            # self._current_context.\n            if 'http://www.w3.org/XML/1998/namespace' == name[0]:\n                return 'xml:' + name[1]\n            # The name is in a non-empty namespace\n            prefix = self._current_context[name[0]]\n            if prefix:\n                # If it is not the default namespace, prepend the prefix\n                return prefix + \":\" + name[1]\n        # Return the unqualified name\n        return name[1]\n\n    # ContentHandler methods\n\n    def startDocument(self):\n        self._write(u'<?xml version=\"1.0\" encoding=\"%s\"?>\\n' %\n                        self._encoding)\n\n    def endDocument(self):\n        self._flush()\n\n    def startPrefixMapping(self, prefix, uri):\n        self._ns_contexts.append(self._current_context.copy())\n        self._current_context[uri] = prefix\n        self._undeclared_ns_maps.append((prefix, uri))\n\n    def endPrefixMapping(self, prefix):\n        self._current_context = self._ns_contexts[-1]\n        del self._ns_contexts[-1]\n\n    def startElement(self, name, attrs):\n        self._write(u'<' + name)\n        for (name, value) in attrs.items():\n            self._write(u' %s=%s' % (name, quoteattr(value)))\n        self._write(u'>')\n\n    def endElement(self, name):\n        self._write(u'</%s>' % name)\n\n    def startElementNS(self, name, qname, attrs):\n        self._write(u'<' + self._qname(name))\n\n        for prefix, uri in self._undeclared_ns_maps:\n            if prefix:\n                self._write(u' xmlns:%s=\"%s\"' % (prefix, uri))\n            else:\n                self._write(u' xmlns=\"%s\"' % uri)\n        self._undeclared_ns_maps = []\n\n        for (name, value) in attrs.items():\n            self._write(u' %s=%s' % (self._qname(name), quoteattr(value)))\n        self._write(u'>')\n\n    def endElementNS(self, name, qname):\n        self._write(u'</%s>' % self._qname(name))\n\n    def characters(self, content):\n        if not isinstance(content, unicode):\n            content = unicode(content, self._encoding)\n        self._write(escape(content))\n\n    def ignorableWhitespace(self, content):\n        if not isinstance(content, unicode):\n            content = unicode(content, self._encoding)\n        self._write(content)\n\n    def processingInstruction(self, target, data):\n        self._write(u'<?%s %s?>' % (target, data))\n\n\nclass XMLFilterBase(xmlreader.XMLReader):\n    \"\"\"This class is designed to sit between an XMLReader and the\n    client application's event handlers.  By default, it does nothing\n    but pass requests up to the reader and events on to the handlers\n    unmodified, but subclasses can override specific methods to modify\n    the event stream or the configuration requests as they pass\n    through.\"\"\"\n\n    def __init__(self, parent = None):\n        xmlreader.XMLReader.__init__(self)\n        self._parent = parent\n\n    # ErrorHandler methods\n\n    def error(self, exception):\n        self._err_handler.error(exception)\n\n    def fatalError(self, exception):\n        self._err_handler.fatalError(exception)\n\n    def warning(self, exception):\n        self._err_handler.warning(exception)\n\n    # ContentHandler methods\n\n    def setDocumentLocator(self, locator):\n        self._cont_handler.setDocumentLocator(locator)\n\n    def startDocument(self):\n        self._cont_handler.startDocument()\n\n    def endDocument(self):\n        self._cont_handler.endDocument()\n\n    def startPrefixMapping(self, prefix, uri):\n        self._cont_handler.startPrefixMapping(prefix, uri)\n\n    def endPrefixMapping(self, prefix):\n        self._cont_handler.endPrefixMapping(prefix)\n\n    def startElement(self, name, attrs):\n        self._cont_handler.startElement(name, attrs)\n\n    def endElement(self, name):\n        self._cont_handler.endElement(name)\n\n    def startElementNS(self, name, qname, attrs):\n        self._cont_handler.startElementNS(name, qname, attrs)\n\n    def endElementNS(self, name, qname):\n        self._cont_handler.endElementNS(name, qname)\n\n    def characters(self, content):\n        self._cont_handler.characters(content)\n\n    def ignorableWhitespace(self, chars):\n        self._cont_handler.ignorableWhitespace(chars)\n\n    def processingInstruction(self, target, data):\n        self._cont_handler.processingInstruction(target, data)\n\n    def skippedEntity(self, name):\n        self._cont_handler.skippedEntity(name)\n\n    # DTDHandler methods\n\n    def notationDecl(self, name, publicId, systemId):\n        self._dtd_handler.notationDecl(name, publicId, systemId)\n\n    def unparsedEntityDecl(self, name, publicId, systemId, ndata):\n        self._dtd_handler.unparsedEntityDecl(name, publicId, systemId, ndata)\n\n    # EntityResolver methods\n\n    def resolveEntity(self, publicId, systemId):\n        return self._ent_handler.resolveEntity(publicId, systemId)\n\n    # XMLReader methods\n\n    def parse(self, source):\n        self._parent.setContentHandler(self)\n        self._parent.setErrorHandler(self)\n        self._parent.setEntityResolver(self)\n        self._parent.setDTDHandler(self)\n        self._parent.parse(source)\n\n    def setLocale(self, locale):\n        self._parent.setLocale(locale)\n\n    def getFeature(self, name):\n        return self._parent.getFeature(name)\n\n    def setFeature(self, name, state):\n        self._parent.setFeature(name, state)\n\n    def getProperty(self, name):\n        return self._parent.getProperty(name)\n\n    def setProperty(self, name, value):\n        self._parent.setProperty(name, value)\n\n    # XMLFilter methods\n\n    def getParent(self):\n        return self._parent\n\n    def setParent(self, parent):\n        self._parent = parent\n\n# --- Utility functions\n\ndef prepare_input_source(source, base = \"\"):\n    \"\"\"This function takes an InputSource and an optional base URL and\n    returns a fully resolved InputSource object ready for reading.\"\"\"\n\n    if type(source) in _StringTypes:\n        source = xmlreader.InputSource(source)\n    elif hasattr(source, \"read\"):\n        f = source\n        source = xmlreader.InputSource()\n        source.setByteStream(f)\n        if hasattr(f, \"name\"):\n            source.setSystemId(f.name)\n\n    if source.getByteStream() is None:\n        try:\n            sysid = source.getSystemId()\n            basehead = os.path.dirname(os.path.normpath(base))\n            encoding = sys.getfilesystemencoding()\n            if isinstance(sysid, unicode):\n                if not isinstance(basehead, unicode):\n                    try:\n                        basehead = basehead.decode(encoding)\n                    except UnicodeDecodeError:\n                        sysid = sysid.encode(encoding)\n            else:\n                if isinstance(basehead, unicode):\n                    try:\n                        sysid = sysid.decode(encoding)\n                    except UnicodeDecodeError:\n                        basehead = basehead.encode(encoding)\n            sysidfilename = os.path.join(basehead, sysid)\n            isfile = os.path.isfile(sysidfilename)\n        except UnicodeError:\n            isfile = False\n        if isfile:\n            source.setSystemId(sysidfilename)\n            f = open(sysidfilename, \"rb\")\n        else:\n            source.setSystemId(urlparse.urljoin(base, source.getSystemId()))\n            f = urllib.urlopen(source.getSystemId())\n\n        source.setByteStream(f)\n\n    return source\n", 
    "xml.sax.xmlreader": "\"\"\"An XML Reader is the SAX 2 name for an XML parser. XML Parsers\nshould be based on this code. \"\"\"\n\nimport handler\n\nfrom _exceptions import SAXNotSupportedException, SAXNotRecognizedException\n\n\n# ===== XMLREADER =====\n\nclass XMLReader:\n    \"\"\"Interface for reading an XML document using callbacks.\n\n    XMLReader is the interface that an XML parser's SAX2 driver must\n    implement. This interface allows an application to set and query\n    features and properties in the parser, to register event handlers\n    for document processing, and to initiate a document parse.\n\n    All SAX interfaces are assumed to be synchronous: the parse\n    methods must not return until parsing is complete, and readers\n    must wait for an event-handler callback to return before reporting\n    the next event.\"\"\"\n\n    def __init__(self):\n        self._cont_handler = handler.ContentHandler()\n        self._dtd_handler = handler.DTDHandler()\n        self._ent_handler = handler.EntityResolver()\n        self._err_handler = handler.ErrorHandler()\n\n    def parse(self, source):\n        \"Parse an XML document from a system identifier or an InputSource.\"\n        raise NotImplementedError(\"This method must be implemented!\")\n\n    def getContentHandler(self):\n        \"Returns the current ContentHandler.\"\n        return self._cont_handler\n\n    def setContentHandler(self, handler):\n        \"Registers a new object to receive document content events.\"\n        self._cont_handler = handler\n\n    def getDTDHandler(self):\n        \"Returns the current DTD handler.\"\n        return self._dtd_handler\n\n    def setDTDHandler(self, handler):\n        \"Register an object to receive basic DTD-related events.\"\n        self._dtd_handler = handler\n\n    def getEntityResolver(self):\n        \"Returns the current EntityResolver.\"\n        return self._ent_handler\n\n    def setEntityResolver(self, resolver):\n        \"Register an object to resolve external entities.\"\n        self._ent_handler = resolver\n\n    def getErrorHandler(self):\n        \"Returns the current ErrorHandler.\"\n        return self._err_handler\n\n    def setErrorHandler(self, handler):\n        \"Register an object to receive error-message events.\"\n        self._err_handler = handler\n\n    def setLocale(self, locale):\n        \"\"\"Allow an application to set the locale for errors and warnings.\n\n        SAX parsers are not required to provide localization for errors\n        and warnings; if they cannot support the requested locale,\n        however, they must raise a SAX exception. Applications may\n        request a locale change in the middle of a parse.\"\"\"\n        raise SAXNotSupportedException(\"Locale support not implemented\")\n\n    def getFeature(self, name):\n        \"Looks up and returns the state of a SAX2 feature.\"\n        raise SAXNotRecognizedException(\"Feature '%s' not recognized\" % name)\n\n    def setFeature(self, name, state):\n        \"Sets the state of a SAX2 feature.\"\n        raise SAXNotRecognizedException(\"Feature '%s' not recognized\" % name)\n\n    def getProperty(self, name):\n        \"Looks up and returns the value of a SAX2 property.\"\n        raise SAXNotRecognizedException(\"Property '%s' not recognized\" % name)\n\n    def setProperty(self, name, value):\n        \"Sets the value of a SAX2 property.\"\n        raise SAXNotRecognizedException(\"Property '%s' not recognized\" % name)\n\nclass IncrementalParser(XMLReader):\n    \"\"\"This interface adds three extra methods to the XMLReader\n    interface that allow XML parsers to support incremental\n    parsing. Support for this interface is optional, since not all\n    underlying XML parsers support this functionality.\n\n    When the parser is instantiated it is ready to begin accepting\n    data from the feed method immediately. After parsing has been\n    finished with a call to close the reset method must be called to\n    make the parser ready to accept new data, either from feed or\n    using the parse method.\n\n    Note that these methods must _not_ be called during parsing, that\n    is, after parse has been called and before it returns.\n\n    By default, the class also implements the parse method of the XMLReader\n    interface using the feed, close and reset methods of the\n    IncrementalParser interface as a convenience to SAX 2.0 driver\n    writers.\"\"\"\n\n    def __init__(self, bufsize=2**16):\n        self._bufsize = bufsize\n        XMLReader.__init__(self)\n\n    def parse(self, source):\n        import saxutils\n        source = saxutils.prepare_input_source(source)\n\n        self.prepareParser(source)\n        file = source.getByteStream()\n        buffer = file.read(self._bufsize)\n        while buffer != \"\":\n            self.feed(buffer)\n            buffer = file.read(self._bufsize)\n        self.close()\n\n    def feed(self, data):\n        \"\"\"This method gives the raw XML data in the data parameter to\n        the parser and makes it parse the data, emitting the\n        corresponding events. It is allowed for XML constructs to be\n        split across several calls to feed.\n\n        feed may raise SAXException.\"\"\"\n        raise NotImplementedError(\"This method must be implemented!\")\n\n    def prepareParser(self, source):\n        \"\"\"This method is called by the parse implementation to allow\n        the SAX 2.0 driver to prepare itself for parsing.\"\"\"\n        raise NotImplementedError(\"prepareParser must be overridden!\")\n\n    def close(self):\n        \"\"\"This method is called when the entire XML document has been\n        passed to the parser through the feed method, to notify the\n        parser that there are no more data. This allows the parser to\n        do the final checks on the document and empty the internal\n        data buffer.\n\n        The parser will not be ready to parse another document until\n        the reset method has been called.\n\n        close may raise SAXException.\"\"\"\n        raise NotImplementedError(\"This method must be implemented!\")\n\n    def reset(self):\n        \"\"\"This method is called after close has been called to reset\n        the parser so that it is ready to parse new documents. The\n        results of calling parse or feed after close without calling\n        reset are undefined.\"\"\"\n        raise NotImplementedError(\"This method must be implemented!\")\n\n# ===== LOCATOR =====\n\nclass Locator:\n    \"\"\"Interface for associating a SAX event with a document\n    location. A locator object will return valid results only during\n    calls to DocumentHandler methods; at any other time, the\n    results are unpredictable.\"\"\"\n\n    def getColumnNumber(self):\n        \"Return the column number where the current event ends.\"\n        return -1\n\n    def getLineNumber(self):\n        \"Return the line number where the current event ends.\"\n        return -1\n\n    def getPublicId(self):\n        \"Return the public identifier for the current event.\"\n        return None\n\n    def getSystemId(self):\n        \"Return the system identifier for the current event.\"\n        return None\n\n# ===== INPUTSOURCE =====\n\nclass InputSource:\n    \"\"\"Encapsulation of the information needed by the XMLReader to\n    read entities.\n\n    This class may include information about the public identifier,\n    system identifier, byte stream (possibly with character encoding\n    information) and/or the character stream of an entity.\n\n    Applications will create objects of this class for use in the\n    XMLReader.parse method and for returning from\n    EntityResolver.resolveEntity.\n\n    An InputSource belongs to the application, the XMLReader is not\n    allowed to modify InputSource objects passed to it from the\n    application, although it may make copies and modify those.\"\"\"\n\n    def __init__(self, system_id = None):\n        self.__system_id = system_id\n        self.__public_id = None\n        self.__encoding  = None\n        self.__bytefile  = None\n        self.__charfile  = None\n\n    def setPublicId(self, public_id):\n        \"Sets the public identifier of this InputSource.\"\n        self.__public_id = public_id\n\n    def getPublicId(self):\n        \"Returns the public identifier of this InputSource.\"\n        return self.__public_id\n\n    def setSystemId(self, system_id):\n        \"Sets the system identifier of this InputSource.\"\n        self.__system_id = system_id\n\n    def getSystemId(self):\n        \"Returns the system identifier of this InputSource.\"\n        return self.__system_id\n\n    def setEncoding(self, encoding):\n        \"\"\"Sets the character encoding of this InputSource.\n\n        The encoding must be a string acceptable for an XML encoding\n        declaration (see section 4.3.3 of the XML recommendation).\n\n        The encoding attribute of the InputSource is ignored if the\n        InputSource also contains a character stream.\"\"\"\n        self.__encoding = encoding\n\n    def getEncoding(self):\n        \"Get the character encoding of this InputSource.\"\n        return self.__encoding\n\n    def setByteStream(self, bytefile):\n        \"\"\"Set the byte stream (a Python file-like object which does\n        not perform byte-to-character conversion) for this input\n        source.\n\n        The SAX parser will ignore this if there is also a character\n        stream specified, but it will use a byte stream in preference\n        to opening a URI connection itself.\n\n        If the application knows the character encoding of the byte\n        stream, it should set it with the setEncoding method.\"\"\"\n        self.__bytefile = bytefile\n\n    def getByteStream(self):\n        \"\"\"Get the byte stream for this input source.\n\n        The getEncoding method will return the character encoding for\n        this byte stream, or None if unknown.\"\"\"\n        return self.__bytefile\n\n    def setCharacterStream(self, charfile):\n        \"\"\"Set the character stream for this input source. (The stream\n        must be a Python 2.0 Unicode-wrapped file-like that performs\n        conversion to Unicode strings.)\n\n        If there is a character stream specified, the SAX parser will\n        ignore any byte stream and will not attempt to open a URI\n        connection to the system identifier.\"\"\"\n        self.__charfile = charfile\n\n    def getCharacterStream(self):\n        \"Get the character stream for this input source.\"\n        return self.__charfile\n\n# ===== ATTRIBUTESIMPL =====\n\nclass AttributesImpl:\n\n    def __init__(self, attrs):\n        \"\"\"Non-NS-aware implementation.\n\n        attrs should be of the form {name : value}.\"\"\"\n        self._attrs = attrs\n\n    def getLength(self):\n        return len(self._attrs)\n\n    def getType(self, name):\n        return \"CDATA\"\n\n    def getValue(self, name):\n        return self._attrs[name]\n\n    def getValueByQName(self, name):\n        return self._attrs[name]\n\n    def getNameByQName(self, name):\n        if not name in self._attrs:\n            raise KeyError, name\n        return name\n\n    def getQNameByName(self, name):\n        if not name in self._attrs:\n            raise KeyError, name\n        return name\n\n    def getNames(self):\n        return self._attrs.keys()\n\n    def getQNames(self):\n        return self._attrs.keys()\n\n    def __len__(self):\n        return len(self._attrs)\n\n    def __getitem__(self, name):\n        return self._attrs[name]\n\n    def keys(self):\n        return self._attrs.keys()\n\n    def has_key(self, name):\n        return name in self._attrs\n\n    def __contains__(self, name):\n        return name in self._attrs\n\n    def get(self, name, alternative=None):\n        return self._attrs.get(name, alternative)\n\n    def copy(self):\n        return self.__class__(self._attrs)\n\n    def items(self):\n        return self._attrs.items()\n\n    def values(self):\n        return self._attrs.values()\n\n# ===== ATTRIBUTESNSIMPL =====\n\nclass AttributesNSImpl(AttributesImpl):\n\n    def __init__(self, attrs, qnames):\n        \"\"\"NS-aware implementation.\n\n        attrs should be of the form {(ns_uri, lname): value, ...}.\n        qnames of the form {(ns_uri, lname): qname, ...}.\"\"\"\n        self._attrs = attrs\n        self._qnames = qnames\n\n    def getValueByQName(self, name):\n        for (nsname, qname) in self._qnames.items():\n            if qname == name:\n                return self._attrs[nsname]\n\n        raise KeyError, name\n\n    def getNameByQName(self, name):\n        for (nsname, qname) in self._qnames.items():\n            if qname == name:\n                return nsname\n\n        raise KeyError, name\n\n    def getQNameByName(self, name):\n        return self._qnames[name]\n\n    def getQNames(self):\n        return self._qnames.values()\n\n    def copy(self):\n        return self.__class__(self._attrs, self._qnames)\n\n\ndef _test():\n    XMLReader()\n    IncrementalParser()\n    Locator()\n\nif __name__ == \"__main__\":\n    _test()\n"
  }
}
